---
title: "BSTA 551: Statistical Inference"
subtitle: "Lesson 1: Introduction to Point Estimation"
author: "Jessica Minnier"
title-slide-attributes:
    data-background-color: "#006a4e"
date: "`r library(here); source(here('class_dates.R')); w1d1`"
format: 
  revealjs:
    theme: "../simple_NW.scss"
    chalkboard: true
    scrollable: true
    slide-number: true
    show-slide-number: all
    width: 1955
    height: 1100
    footer: Lesson 1 Slides
    html-math-method: mathjax
    highlight-style: atom-one
execute:
  echo: true
  warning: false
  message: false
---

```{r}
#| label: setup
#| include: false
library(tidyverse)
set.seed(42)
theme_set(theme_minimal(base_size = 18))
```

# Lesson 1: Mathematical Foundations & Introduction to Estimation

## Welcome to Statistical Inference! {.center}

**Course Focus:** How do we learn about populations from samples?

::: incremental
-   **Point Estimation** (Weeks 1-4): What's our best guess for a parameter?
-   **Confidence Intervals** (Weeks 5-6): What's a plausible range?
-   **Hypothesis Testing** (Weeks 7-9): Can we make decisions from data?
-   **Two-Sample Methods** (Week 10): Comparing groups
:::

## Today's Goals

::: incremental
1.  Explain what an estimator is and how it differs from an estimate
2.  Use R to simulate sampling distributions
3.  Understand how optimization finds "best" values numerically
4.  Define bias and calculate it for simple estimators
:::

## Motivating Example: Clinical Trial

A pharmaceutical company is testing a new blood pressure medication.

**The Question:** What is the true average reduction in systolic blood pressure?

. . .

**What we have:** Data from 25 patients in a trial

```{r}
# Actual blood pressure reductions (mmHg) from 25 patients
bp_reductions <- c(12, 8, 15, 10, 7, 14, 11, 9, 13, 16,
                   8, 12, 10, 14, 11, 9, 15, 13, 7, 12,
                   10, 8, 14, 11, 13)

# What's our estimate of the true mean reduction?
mean(bp_reductions)
```

. . .

But how **reliable** is this estimate? Would we get the same answer with different patients?

## The Big Picture: Population vs Sample

```{r}
#| echo: false
#| fig-height: 5
tibble(
  x = c(1, 2),
  y = c(1, 1),
  size = c(4, 2)
) |> 
  ggplot(aes(x = x, y = y)) +
  geom_point(aes(size = size), color = "steelblue", alpha = 0.5) +
  # Regular text labels
  annotate("text", x = 1, y = 1, 
           label = "Population\n(Unknown parameter μ)", size = 6) +
  annotate("text", x = 2, y = 1.05, 
           label = "Sample", size = 6) +
  # Parsed label for x-bar
  annotate("text", x = 2, y = 0.95, 
           label = "(Compute~estimate~bar(X))", size = 6, parse = TRUE) +
  annotate("segment", x = 1.3, xend = 1.7, y = 1, yend = 1,
           arrow = arrow(length = unit(0.3, "cm")), linewidth = 1.5) +
  annotate("text", x = 1.5, y = 1.15, label = "Random\nSampling", size = 5) +
  annotate("segment", x = 1.7, xend = 1.3, y = 0.85, yend = 0.85,
           arrow = arrow(length = unit(0.3, "cm")), linewidth = 1.5, color = "darkred") +
  annotate("text", x = 1.5, y = 0.7, label = "Inference", size = 5, color = "darkred") +
  scale_size_identity() +
  xlim(0.5, 2.5) + ylim(0.5, 1.5) +
  theme_void()
```

::: lob
**Key insight:** We use sample data to make inferences about population parameters.
:::


## Review: Expected Value

The **expected value** $E(X)$ is the long-run average of a random variable.

::: callout-note
## Key Properties We'll Use Today

1.  $E(c) = c$ for any constant $c$
2.  $E(cX) = c \cdot E(X)$
3.  $E(X + Y) = E(X) + E(Y)$
4.  $E\left(\sum_{i=1}^n X_i\right) = \sum_{i=1}^n E(X_i)$
:::

## Worked Example: Expected Value of Sample Mean

**Problem:** If $X_1, X_2, \ldots, X_n$ are *iid* observations from a population with mean $\mu$, what is $E(\bar{X})$?

. . .

**Solution:** Let's work through this step by step.

$$E(\bar{X}) = E\left(\frac{1}{n}\sum_{i=1}^n X_i\right)$$

. . .

$$= \frac{1}{n} E\left(\sum_{i=1}^n X_i\right) \quad \text{(Property 2: constants come out)}$$

. . .

$$= \frac{1}{n} \sum_{i=1}^n E(X_i) \quad \text{(Property 4: sum of expectations)}$$

. . .

$$= \frac{1}{n} \cdot n\mu = \mu \quad \text{(Each } E(X_i) = \mu \text{)}$$

## Your Turn: Calculate Expected Value

**Exercise:** A hospital measures the recovery time (in days) for patients after surgery. Let $X_1, X_2, X_3$ be recovery times for 3 patients. The population mean recovery time is $\mu = 5$ days.

**Questions:**

1.  What is $E(X_1)$?
2.  What is $E(X_1 + X_2 + X_3)$?
3.  What is $E(\bar{X})$ where $\bar{X} = \frac{X_1 + X_2 + X_3}{3}$?

. . .

**Answers:**

1.  $E(X_1) = \mu = 5$ days
2.  $E(X_1 + X_2 + X_3) = 5 + 5 + 5 = 15$ days
3.  $E(\bar{X}) = \frac{15}{3} = 5$ days

## Review: Variance

**Variance** measures the spread of a distribution: $\text{Var}(X) = E[(X - \mu)^2]$

::: callout-note
## Key Properties We'll Use Today

1.  $\text{Var}(c) = 0$ for any constant
2.  $\text{Var}(cX) = c^2 \cdot \text{Var}(X)$
3.  If $X$ and $Y$ are **independent**: $\text{Var}(X + Y) = \text{Var}(X) + \text{Var}(Y)$
:::

## Worked Example: Variance of Sample Mean

**Problem:** If $X_1, \ldots, X_n$ are independent observations with variance $\sigma^2$, what is $\text{Var}(\bar{X})$?

. . .

$$\text{Var}(\bar{X}) = \text{Var}\left(\frac{1}{n}\sum_{i=1}^n X_i\right)$$

. . .

$$= \frac{1}{n^2} \text{Var}\left(\sum_{i=1}^n X_i\right) \quad \text{(Property 2)}$$

. . .

$$= \frac{1}{n^2} \sum_{i=1}^n \text{Var}(X_i) \quad \text{(Property 3: independence)}$$

. . .

$$= \frac{1}{n^2} \cdot n\sigma^2 = \frac{\sigma^2}{n}$$

::: callout-important
## Key Result

The variance of $\bar{X}$ **decreases** as sample size $n$ increases!
:::

## Simulation: Seeing Variance Decrease

```{r}
#| code-line-numbers: "|1-3|5-12|14-15"
# Population parameters
true_mean <- 120  # True mean systolic BP
true_sd <- 15     # Population standard deviation

# Simulate sample means for different sample sizes
simulation_data <- tibble(n = c(5, 25, 100)) |> 
  cross_join(tibble(sim = 1:2000)) |> 
  mutate(
    sample_mean = map2_dbl(n, sim, \(size, s) {
      mean(rnorm(size, true_mean, true_sd))
    })
  )

# Calculate observed standard deviation for each sample size
simulation_data |> 
  group_by(n) |> 
  summarize(
    observed_sd = sd(sample_mean),
    theoretical_sd = true_sd / sqrt(first(n))
  )
```

## Visualizing the Effect of Sample Size

```{r}
#| echo: false
#| fig-height: 5
simulation_data |> 
  mutate(n_label = paste("n =", n) |> fct_reorder(n)) |> 
  ggplot(aes(x = sample_mean)) +
  geom_histogram(bins = 40, fill = "steelblue", alpha = 0.7, color = "white") +
  geom_vline(xintercept = true_mean, color = "red", linetype = "dashed", linewidth = 1) +
  facet_wrap(~n_label, scales = "free_y") +
  labs(title = "Sampling Distribution of X̄ for Different Sample Sizes",
       subtitle = "Larger n → Less spread → More precise estimates",
       x = "Sample Mean", y = "Frequency") +
  annotate("text", x = 128, y = Inf, label = "True μ = 120", 
           vjust = 2, color = "red", size = 4)
```

# Understanding Optimization {background-color="#2c3e50"}

## Why Optimization Matters in Statistics

Many statistical methods require finding the "best" value of a parameter.

**Examples:**

-   **Maximum Likelihood:** Find the parameter value that makes the observed data most probable
-   **Least Squares:** Find the parameter value that minimizes prediction errors
-   **Minimum Variance:** Find the estimator with the smallest spread

. . .

**The Problem:** How do we find maximums and minimums without calculus?

## Optimization: The Graphical Intuition

```{r}
#| echo: false
#| fig-height: 5
# Create a likelihood function to visualize
tibble(p = seq(0, 1, by = 0.01)) |> 
  mutate(likelihood = dbinom(7, size = 10, prob = p)) |> 
  ggplot(aes(x = p, y = likelihood)) +
  geom_line(color = "steelblue", linewidth = 1.5) +
  geom_vline(xintercept = 0.7, color = "red", linetype = "dashed") +
  geom_point(aes(x = 0.7, y = dbinom(7, 10, 0.7)), color = "red", size = 4) +
  labs(title = "Finding the Maximum of a Function",
       subtitle = "Where is this function highest?",
       x = "Parameter value (p)", y = "Function value") +
  annotate("text", x = 0.75, y = 0.28, label = "Maximum\nat p = 0.7", 
           color = "red", size = 5)
```

**The maximum occurs where the function reaches its peak.**

## Concrete Example: Finding the Best Estimate

**Scenario:** In a clinical trial, 7 out of 10 patients respond to treatment. What's the best estimate of the true response rate $p$?

. . .

**Approach:** Find the value of $p$ that makes observing "7 out of 10" most likely.

The probability of observing exactly 7 successes in 10 trials is: $$P(X = 7) = \binom{10}{7} p^7 (1-p)^3$$

. . .

**Question:** For what value of $p$ is this probability largest?

## Grid Search: A Simple Numerical Approach

**Idea:** Try many values and see which gives the largest result.

```{r}
# Try different values of p
grid_search <- tibble(p = seq(0.01, 0.99, by = 0.01)) |> 
  mutate(
    likelihood = dbinom(7, size = 10, prob = p)
  )

# Find the maximum
grid_search |> 
  slice_max(likelihood, n = 1)
```

. . .

The maximum likelihood estimate is $\hat{p} = 0.70 = \frac{7}{10}$. This makes intuitive sense!

## Using R's Optimizer

R has built-in functions to find maximums and minimums more precisely:

```{r}
# Define the likelihood function
likelihood_function <- function(p) {
  dbinom(7, size = 10, prob = p)
}

# Use optimize() to find the maximum
# Note: optimize finds MINIMUM by default, so we negate for maximum
result <- optimize(
  f = function(p) -likelihood_function(p),  # Negative to find max
  interval = c(0, 1)                         # Search between 0 and 1
)

# The maximum occurs at:
cat("Maximum likelihood estimate: p =", result$minimum)
```

## How Numerical Optimization Works

```{r}
#| echo: false
#| fig-height: 5
# Visualize the optimization process
search_points <- tibble(
  iteration = 1:6,
  p = c(0.5, 0.75, 0.625, 0.6875, 0.71875, 0.703125),
  likelihood = dbinom(7, 10, p)
)

tibble(p = seq(0, 1, by = 0.01)) |> 
  mutate(likelihood = dbinom(7, size = 10, prob = p)) |> 
  ggplot(aes(x = p, y = likelihood)) +
  geom_line(color = "steelblue", linewidth = 1.5) +
  geom_point(data = search_points, aes(color = iteration), size = 4) +
  geom_path(data = search_points, aes(color = iteration), 
            arrow = arrow(length = unit(0.2, "cm")), linewidth = 0.8) +
  scale_color_viridis_c(option = "plasma") +
  labs(title = "Numerical Optimization: Searching for the Maximum",
       subtitle = "The algorithm tries different values, homing in on the peak",
       x = "Parameter value (p)", y = "Likelihood",
       color = "Iteration")
```

**Key idea:** The algorithm evaluates the function at different points and iteratively narrows in on the maximum.

## Your Turn: Numerical Optimization

**Exercise:** A diagnostic test correctly identifies a disease in 18 out of 25 patients who have it. Find the maximum likelihood estimate for the test's sensitivity $p$.

```{r}
#| eval: false
# Fill in the blanks:
likelihood_fn <- function(p) {
  dbinom(___, size = ___, prob = p)  # What goes here?
}

result <- optimize(
  f = function(p) -likelihood_fn(p),
  interval = c(0, 1)
)

result$minimum  # This is the MLE
```

. . .

```{r}
# Solution:
likelihood_fn <- function(p) {
  dbinom(18, size = 25, prob = p)
}

result <- optimize(f = function(p) -likelihood_fn(p), interval = c(0, 1))
cat("MLE of sensitivity:", result$minimum)  # Should be 18/25 = 0.72
```

## When Optimization Gets Harder

Sometimes we need to optimize over multiple parameters or complex functions:

```{r}
# Example: Finding mean and SD that best fit data
patient_data <- c(120, 135, 128, 142, 131, 125, 138, 129, 133, 127)

# Negative log-likelihood for normal distribution
neg_log_lik <- function(params) {
  mu <- params[1]
  sigma <- params[2]
  if (sigma <= 0) return(Inf)  # sigma must be positive
  -sum(dnorm(patient_data, mean = mu, sd = sigma, log = TRUE))
}

# Use optim() for multiple parameters
result <- optim(par = c(130, 10), fn = neg_log_lik)
cat("MLE for mean:", round(result$par[1], 2), "\n")
cat("MLE for SD:", round(result$par[2], 2), "\n")
cat("Compare to sample mean:", round(mean(patient_data), 2))
```

## --- Break (15 minutes) --- {.center background-color="#95a5a6"}

# Point Estimation: Core Concepts {background-color="#27ae60"}

## What is a Point Estimator?

::: callout-important
## Definitions

-   A **parameter** is a fixed (but unknown) characteristic of a population (e.g., $\mu$, $\sigma$, $p$)
-   An **estimator** is a rule/formula for calculating an estimate from sample data
-   An **estimate** is the actual number you calculate from a specific sample
:::

. . .

**Example:**

| Concept   | Symbol                          | Example                   |
|-----------|---------------------------------|---------------------------|
| Parameter | $\mu$                           | True mean BP reduction    |
| Estimator | $\bar{X} = \frac{1}{n}\sum X_i$ | The formula "sample mean" |
| Estimate  | $\bar{x} = 11.2$                | The number from our data  |

## The Sampling Distribution

Different samples give different estimates. The **sampling distribution** describes this variability.

```{r}
#| echo: false
#| fig-height: 4.5
# Show multiple samples from same population
set.seed(123)
true_mean <- 10
n <- 25

# Take 6 different samples
sample_results <- tibble(sample_num = 1:6) |> 
  mutate(
    data = map(sample_num, \(s) rnorm(n, true_mean, 3)),
    sample_mean = map_dbl(data, mean)
  )

sample_results |> 
  select(sample_num, sample_mean) |> 
  mutate(sample_mean = round(sample_mean, 2)) |> 
  knitr::kable(col.names = c("Sample", "Sample Mean (x̄)"))
```

. . .

Each sample gives a **different estimate**, but they cluster around the true value $\mu = 10$.

## Simulation: Building a Sampling Distribution

```{r}
#| output-location: slide
# Parameters
true_effect <- 10  # True mean BP reduction (mmHg)
true_sd <- 4       # Standard deviation
n_patients <- 25   # Patients per trial
n_trials <- 5000   # Number of simulated trials

# Simulate many clinical trials
sampling_distribution <- tibble(trial = 1:n_trials) |> 
  mutate(
    sample_mean = map_dbl(trial, \(t) {
      patients <- rnorm(n_patients, true_effect, true_sd)
      mean(patients)
    })
  )

# Visualize
sampling_distribution |> 
  ggplot(aes(x = sample_mean)) +
  geom_histogram(bins = 50, fill = "steelblue", alpha = 0.7, color = "white") +
  geom_vline(xintercept = true_effect, color = "red", linewidth = 1.5) +
  labs(title = "Sampling Distribution of the Sample Mean",
       subtitle = str_glue("True μ = {true_effect}, n = {n_patients}, {n_trials} simulated trials"),
       x = "Sample Mean (estimated BP reduction)", y = "Frequency")
```

## What Makes a Good Estimator?

We want estimators that are:

1.  **Accurate** (unbiased): On average, hits the true value
2.  **Precise** (low variance): Estimates are clustered together
3.  **Efficient**: Best combination of accuracy and precision

```{r}
#| echo: false
#| fig-height: 4
set.seed(42)
estimator_comparison <- bind_rows(
  tibble(type = "Good: Accurate & Precise", 
         estimate = rnorm(500, 10, 0.5)),
  tibble(type = "Bad: Accurate but Imprecise", 
         estimate = rnorm(500, 10, 2)),
  tibble(type = "Bad: Precise but Inaccurate", 
         estimate = rnorm(500, 12, 0.5))
)

estimator_comparison |> 
  ggplot(aes(x = estimate, fill = type)) +
  geom_histogram(bins = 30, alpha = 0.7) +
  geom_vline(xintercept = 10, color = "red", linetype = "dashed", linewidth = 1) +
  facet_wrap(~type) +
  labs(x = "Estimate", y = "Frequency", 
       title = "Comparing Estimator Quality",
       subtitle = "Red line = true parameter value") +
  theme(legend.position = "none")
```

## Bias: Formal Definition

::: callout-important
## Definition

The **bias** of an estimator $\hat{\theta}$ is: $$\text{Bias}(\hat{\theta}) = E(\hat{\theta}) - \theta$$

An estimator is **unbiased** if $\text{Bias}(\hat{\theta}) = 0$, i.e., $E(\hat{\theta}) = \theta$.
:::

. . .

**Interpretation:**

-   Bias measures systematic error
-   Positive bias = tends to overestimate
-   Negative bias = tends to underestimate
-   Zero bias = correct on average

## Worked Example: Proving Sample Mean is Unbiased

**Claim:** The sample mean $\bar{X}$ is an unbiased estimator of $\mu$.

**Proof:** We need to show $E(\bar{X}) = \mu$.

. . .

$$\text{Bias}(\bar{X}) = E(\bar{X}) - \mu$$

. . .

We already showed that $E(\bar{X}) = \mu$, so:

$$\text{Bias}(\bar{X}) = \mu - \mu = 0 \checkmark$$

. . .

**Conclusion:** The sample mean is unbiased for estimating the population mean.

## Worked Example: Sample Proportion

**Setup:** In a vaccine trial, $X$ patients out of $n$ develop immunity. The estimator is $\hat{p} = X/n$.

**Claim:** $\hat{p}$ is unbiased for the true immunity rate $p$.

. . .

**Proof:** Since $X \sim \text{Binomial}(n, p)$, we know $E(X) = np$.

$$E(\hat{p}) = E\left(\frac{X}{n}\right) = \frac{1}{n} E(X) = \frac{1}{n} \cdot np = p$$

. . .

$$\text{Bias}(\hat{p}) = E(\hat{p}) - p = p - p = 0 \checkmark$$

## Concrete Calculation: Bias of Sample Proportion

**Data:** In a study of 80 patients, 52 showed improvement.

```{r}
n <- 80
x <- 52
p_hat <- x / n

cat("Sample proportion:", p_hat, "\n")
cat("If true p = 0.65, what is the bias of this single estimate?\n")
cat("Observed - True =", p_hat - 0.65)
```

. . .

**Important distinction:**

-   A single estimate can be above or below the true value
-   **Bias** refers to the average behavior across many samples
-   An unbiased estimator still gives wrong answers for individual samples!

## Simulation: Verifying Unbiasedness

```{r}
# Verify that sample proportion is unbiased
true_p <- 0.65
n_patients <- 80
n_simulations <- 10000

proportion_simulation <- tibble(sim = 1:n_simulations) |> 
  mutate(
    successes = rbinom(n_simulations, size = n_patients, prob = true_p),
    p_hat = successes / n_patients
  )

proportion_simulation |> 
  summarize(
    true_p = true_p,
    mean_of_estimates = mean(p_hat),
    empirical_bias = mean(p_hat) - true_p
  )
```

The bias is essentially zero (just simulation noise)!

## A Biased Estimator: The Maximum

**Problem:** Estimate the upper bound $\theta$ of a Uniform\[0, $\theta$\] distribution.

**Natural idea:** Use the largest observation: $\hat{\theta} = \max(X_1, \ldots, X_n)$

. . .

**Think about it:** Can this estimator ever *overestimate* $\theta$?

. . .

**No!** The sample maximum is always ≤ the population maximum.

This means the estimator is **biased low**.

## Calculating the Bias Mathematically

For $X_1, \ldots, X_n \sim \text{Uniform}[0, \theta]$, it can be shown that:

$$E(\max(X_1, \ldots, X_n)) = \frac{n}{n+1} \theta$$

. . .

**Bias calculation:**

$$\text{Bias} = E(\hat{\theta}) - \theta = \frac{n}{n+1}\theta - \theta = -\frac{\theta}{n+1}$$

. . .

**Example:** If $\theta = 10$ and $n = 5$:

$$\text{Bias} = -\frac{10}{6} = -1.67$$

The estimator underestimates by about 1.67 on average.

## Your Turn: Calculate Bias

**Exercise:** A lab instrument has a maximum detection limit $\theta$. We take $n = 9$ measurements from Uniform\[0, $\theta$\] and use the maximum as our estimate.

1.  If $\theta = 100$, what is $E(\hat{\theta})$?
2.  What is the bias?
3.  By what percentage does this estimator underestimate on average?

. . .

**Solution:**

1.  $E(\hat{\theta}) = \frac{9}{10} \times 100 = 90$
2.  $\text{Bias} = 90 - 100 = -10$
3.  Underestimates by $\frac{10}{100} = 10\%$

## Simulation: Visualizing the Biased Estimator

```{r}
#| output-location: slide
true_theta <- 100
n <- 9
n_sims <- 5000

max_simulation <- tibble(sim = 1:n_sims) |> 
  mutate(
    max_estimate = map_dbl(sim, \(s) max(runif(n, 0, true_theta)))
  )

# Calculate empirical bias
max_simulation |> 
  summarize(
    theoretical_E = n / (n + 1) * true_theta,
    empirical_mean = mean(max_estimate),
    theoretical_bias = -true_theta / (n + 1),
    empirical_bias = mean(max_estimate) - true_theta
  )
```

## Visualizing the Biased Estimator

```{r}
#| echo: false
max_simulation |> 
  ggplot(aes(x = max_estimate)) +
  geom_histogram(bins = 50, fill = "steelblue", alpha = 0.7, color = "white") +
  geom_vline(xintercept = true_theta, color = "red", linewidth = 1.2, linetype = "dashed") +
  geom_vline(xintercept = mean(max_simulation$max_estimate), color = "darkgreen", linewidth = 1.2) +
  labs(title = "Distribution of Maximum Estimator (Biased)",
       subtitle = str_glue("True θ = {true_theta}, n = {n}"),
       x = "Estimate", y = "Frequency") +
  annotate("text", x = 83, y = 350, label = str_glue("Mean = {round(mean(max_simulation$max_estimate), 1)}"),
           color = "darkgreen", size = 5) +
  annotate("text", x = 103, y = 350, label = str_glue("True θ = {true_theta}"),
           color = "red", size = 5)
```

## Correcting the Bias

**Idea:** Multiply by a correction factor to "un-bias" the estimator.

Since $E(\max) = \frac{n}{n+1}\theta$, we can define:

$$\hat{\theta}_{\text{unbiased}} = \frac{n+1}{n} \cdot \max(X_1, \ldots, X_n)$$

. . .

**Check:**

$$E(\hat{\theta}_{\text{unbiased}}) = \frac{n+1}{n} \cdot E(\max) = \frac{n+1}{n} \cdot \frac{n}{n+1}\theta = \theta \checkmark$$

## Your Turn: Apply the Correction

**Exercise:** Using the lab instrument example with $n = 9$ and $\theta = 100$:

1.  If you observe $\max = 92$, what is the biased estimate?
2.  What is the unbiased estimate?

. . .

**Solution:**

1.  Biased estimate: $\hat{\theta}_b = 92$
2.  Unbiased estimate: $\hat{\theta}_u = \frac{10}{9} \times 92 = 102.2$

```{r}
observed_max <- 92
n <- 9

biased_est <- observed_max
unbiased_est <- (n + 1) / n * observed_max

cat("Biased estimate:", biased_est, "\n")
cat("Unbiased estimate:", round(unbiased_est, 1))
```

## Lesson 1 Summary

**Key Concepts:**

1.  **Expected Value:** $E(\bar{X}) = \mu$ (sample mean is centered at population mean)

2.  **Variance:** $\text{Var}(\bar{X}) = \sigma^2/n$ (precision improves with larger $n$)

3.  **Optimization:** Finding maximum/minimum values numerically

    -   Grid search: try many values
    -   `optimize()`: efficient numerical search

4.  **Bias:** $\text{Bias}(\hat{\theta}) = E(\hat{\theta}) - \theta$

    -   Unbiased if $E(\hat{\theta}) = \theta$
    -   Can sometimes correct biased estimators

## Lesson 1 Practice Problems

1.  Calculate $E(\bar{X})$ and $\text{Var}(\bar{X})$ for a sample of size $n = 16$ from a population with $\mu = 50$ and $\sigma = 12$.

2.  Use `optimize()` to find the MLE for $p$ when you observe 23 successes in 40 trials.

3.  For a Uniform\[0, $\theta$\] distribution with $n = 20$ observations and $\theta = 50$, calculate:

    -   The expected value of the maximum
    -   The bias of using the maximum as an estimator
    -   The corrected unbiased estimator

------------------------------------------------------------------------

# Day 2: Evaluating Estimators {background-color="#8e44ad"}

## Review: Where We Left Off

**Key concepts from Lesson 1:**

-   Estimator vs. estimate
-   Sampling distribution
-   Bias: $\text{Bias}(\hat{\theta}) = E(\hat{\theta}) - \theta$
-   Unbiased estimators: $E(\hat{\theta}) = \theta$

. . .

**Today's Goals:**

-   Standard error and precision
-   Mean squared error (MSE)
-   The bias-variance tradeoff
-   Comparing estimators with simulations

## Standard Error: Measuring Precision

::: callout-important
## Definition

The **standard error** of an estimator is its standard deviation: $$SE(\hat{\theta}) = \sqrt{\text{Var}(\hat{\theta})}$$
:::

. . .

**Key Standard Errors:**

| Estimator                   | Standard Error            |
|-----------------------------|---------------------------|
| Sample mean $\bar{X}$       | $\frac{\sigma}{\sqrt{n}}$ |
| Sample proportion $\hat{p}$ | $\sqrt{\frac{p(1-p)}{n}}$ |

## Worked Example: Standard Error of Sample Mean

**Problem:** In a blood pressure study, the population SD is $\sigma = 15$ mmHg. Calculate the standard error of $\bar{X}$ for sample sizes $n = 25$ and $n = 100$.

. . .

**Solution:**

For $n = 25$: $$SE(\bar{X}) = \frac{15}{\sqrt{25}} = \frac{15}{5} = 3 \text{ mmHg}$$

For $n = 100$: $$SE(\bar{X}) = \frac{15}{\sqrt{100}} = \frac{15}{10} = 1.5 \text{ mmHg}$$

. . .

**Interpretation:** With 100 patients, our estimate is twice as precise as with 25 patients.

## Your Turn: Calculate Standard Error

**Exercise:** A survey measures patient satisfaction on a 0-100 scale. The population standard deviation is $\sigma = 20$.

1.  What is the SE of $\bar{X}$ for $n = 16$ patients?
2.  What sample size is needed to achieve $SE = 2$?

. . .

**Solutions:**

1.  $SE = \frac{20}{\sqrt{16}} = \frac{20}{4} = 5$

2.  We need $\frac{20}{\sqrt{n}} = 2$, so $\sqrt{n} = 10$, thus $n = 100$

```{r}
sigma <- 20
# Problem 1
se_n16 <- sigma / sqrt(16)
# Problem 2
n_needed <- (sigma / 2)^2

cat("SE for n=16:", se_n16, "\nSample size for SE=2:", n_needed)
```

## The Problem with Unknown Parameters

**Issue:** Standard errors often involve unknown parameters!

-   SE of $\bar{X}$ requires knowing $\sigma$
-   SE of $\hat{p}$ requires knowing $p$

. . .

**Solution:** **Estimated standard error** — substitute estimates for unknown parameters

$$\widehat{SE}(\bar{X}) = \frac{s}{\sqrt{n}}$$

$$\widehat{SE}(\hat{p}) = \sqrt{\frac{\hat{p}(1-\hat{p})}{n}}$$

## Example: Estimated Standard Error

```{r}
# Blood pressure data from 25 patients
bp_reductions <- c(12, 8, 15, 10, 7, 14, 11, 9, 13, 16,
                   8, 12, 10, 14, 11, 9, 15, 13, 7, 12,
                   10, 8, 14, 11, 13)

n <- length(bp_reductions)
x_bar <- mean(bp_reductions)
s <- sd(bp_reductions)

# Estimated standard error
se_estimated <- s / sqrt(n)

tibble(
  Statistic = c("Sample Mean", "Sample SD", "Sample Size", "Estimated SE"),
  Value = c(x_bar, round(s, 2), n, round(se_estimated, 2))
)
```

## Mean Squared Error: Combining Bias and Variance

What if we have to choose between a biased estimator with low variance and an unbiased estimator with high variance?

. . .

::: callout-important
## Definition: Mean Squared Error

$$\text{MSE}(\hat{\theta}) = E[(\hat{\theta} - \theta)^2]$$

**Key Formula:** $$\text{MSE} = \text{Variance} + \text{Bias}^2$$
:::

. . .

**MSE captures total error** — both systematic (bias) and random (variance).

## Proving the MSE Formula

$$\text{MSE}(\hat{\theta}) = E[(\hat{\theta} - \theta)^2]$$

. . .

Add and subtract $E(\hat{\theta})$:

$$= E[(\hat{\theta} - E(\hat{\theta}) + E(\hat{\theta}) - \theta)^2]$$

. . .

Expand the square:

$$= E[(\hat{\theta} - E(\hat{\theta}))^2] + 2E[(\hat{\theta} - E(\hat{\theta}))](E(\hat{\theta}) - \theta) + (E(\hat{\theta}) - \theta)^2$$

. . .

The middle term equals zero because $E[\hat{\theta} - E(\hat{\theta})] = 0$:

$$= \text{Var}(\hat{\theta}) + [\text{Bias}(\hat{\theta})]^2$$

## Worked Example: Computing MSE

**Setup:** Estimating $\theta$ from Uniform\[0, $\theta$\] with $n = 9$ and $\theta = 100$.

**Biased estimator:** $\hat{\theta}_b = \max(X_i)$

From theory:

-   $E(\hat{\theta}_b) = \frac{9}{10}(100) = 90$
-   $\text{Var}(\hat{\theta}_b) = \frac{n \theta^2}{(n+1)^2(n+2)} = \frac{9 \times 100^2}{100 \times 11} = 81.82$

. . .

$$\text{Bias} = 90 - 100 = -10$$ $$\text{MSE} = 81.82 + (-10)^2 = 81.82 + 100 = 181.82$$

## Your Turn: Calculate MSE

**Exercise:** For the **unbiased** estimator $\hat{\theta}_u = \frac{n+1}{n}\max(X_i)$ with $n = 9$ and $\theta = 100$:

1.  What is the bias?
2.  If $\text{Var}(\hat{\theta}_u) = 101.01$, what is the MSE?
3.  Which estimator has lower MSE: biased or unbiased?

. . .

**Solutions:**

1.  Bias = 0 (it's unbiased!)
2.  $\text{MSE} = 101.01 + 0^2 = 101.01$
3.  Unbiased has lower MSE (101.01 \< 181.82)

But this isn't always the case!

## --- Break (15 minutes) --- {.center background-color="#95a5a6"}

## The Bias-Variance Tradeoff

Sometimes a **biased** estimator has **lower MSE** than an unbiased one!

. . .

**Example:** Estimating a proportion $p$ with $n = 20$ observations

**Estimator 1:** Standard: $\hat{p}_1 = \frac{X}{n}$ (unbiased)

**Estimator 2:** "Add-two": $\hat{p}_2 = \frac{X + 2}{n + 4}$ (biased toward 0.5)

## Comparing Proportion Estimators: Theory

For $\hat{p}_1 = X/n$ (standard):

-   Bias = 0
-   Variance = $\frac{p(1-p)}{n}$
-   MSE = $\frac{p(1-p)}{n}$

. . .

For $\hat{p}_2 = \frac{X+2}{n+4}$ (add-two):

-   Bias = $\frac{2 - 4p}{n+4}$
-   Variance = $\frac{np(1-p)}{(n+4)^2}$
-   MSE = Variance + Bias²

## Your Turn: Calculate Bias of Add-Two Estimator

**Exercise:** For $n = 20$ and $p = 0.3$:

1.  Calculate the bias of $\hat{p}_2 = \frac{X+2}{n+4}$

**Hint:** $E(X) = np$ for binomial, so $E(\hat{p}_2) = \frac{np + 2}{n + 4}$

. . .

**Solution:**

$$E(\hat{p}_2) = \frac{20(0.3) + 2}{24} = \frac{8}{24} = 0.333$$

$$\text{Bias} = 0.333 - 0.3 = 0.033$$

The add-two estimator is biased **toward 0.5** (and 0.333 is closer to 0.5 than 0.3 is).

## Simulation: Comparing the Estimators

```{r}
#| code-line-numbers: "|1-3|5-13|15-22"
true_p <- 0.3
n <- 20
n_sims <- 10000

# Simulate both estimators
comparison_sim <- tibble(sim = 1:n_sims) |> 
  mutate(
    x = rbinom(n_sims, size = n, prob = true_p),
    p_hat_standard = x / n,
    p_hat_addtwo = (x + 2) / (n + 4)
  )

# Compare MSE
comparison_sim |> 
  summarize(
    `Standard Bias` = mean(p_hat_standard) - true_p,
    `Add-Two Bias` = mean(p_hat_addtwo) - true_p,
    `Standard Variance` = var(p_hat_standard),
    `Add-Two Variance` = var(p_hat_addtwo),
    `Standard MSE` = mean((p_hat_standard - true_p)^2),
    `Add-Two MSE` = mean((p_hat_addtwo - true_p)^2)
  ) |> 
  pivot_longer(everything(), names_to = "Metric", values_to = "Value") |> 
  mutate(Value = round(Value, 5))
```

## Visualizing the Tradeoff

```{r}
#| echo: false
#| fig-height: 5
comparison_sim |> 
  pivot_longer(cols = c(p_hat_standard, p_hat_addtwo),
               names_to = "estimator", values_to = "estimate") |> 
  mutate(
    estimator = case_when(
      estimator == "p_hat_standard" ~ "Standard (unbiased)",
      estimator == "p_hat_addtwo" ~ "Add-Two (biased)"
    )
  ) |> 
  ggplot(aes(x = estimate, fill = estimator)) +
  geom_histogram(bins = 30, alpha = 0.6, position = "identity") +
  geom_vline(xintercept = true_p, color = "red", linewidth = 1.2, linetype = "dashed") +
  labs(title = "Comparing Two Proportion Estimators",
       subtitle = str_glue("True p = {true_p}, n = {n}"),
       x = "Estimate", y = "Frequency", fill = "Estimator") +
  scale_fill_manual(values = c("steelblue", "darkgreen")) +
  theme(legend.position = "top")
```

## MSE Comparison Across Different True Values

```{r}
#| echo: false
#| fig-height: 5
n <- 20

mse_comparison <- tibble(p = seq(0.01, 0.99, by = 0.01)) |> 
  mutate(
    mse_standard = p * (1 - p) / n,
    bias_addtwo = (2 - 4*p) / (n + 4),
    var_addtwo = n * p * (1-p) / (n + 4)^2,
    mse_addtwo = var_addtwo + bias_addtwo^2
  )

mse_comparison |> 
  select(p, mse_standard, mse_addtwo) |> 
  pivot_longer(-p, names_to = "Estimator", values_to = "MSE") |> 
  mutate(Estimator = ifelse(Estimator == "mse_standard", "Standard", "Add-Two")) |> 
  ggplot(aes(x = p, y = MSE, color = Estimator)) +
  geom_line(linewidth = 1.2) +
  labs(title = "MSE Comparison: Which Estimator is Better?",
       subtitle = "Neither dominates everywhere — the winner depends on true p",
       x = "True Proportion (p)", y = "Mean Squared Error") +
  scale_color_manual(values = c("darkgreen", "steelblue")) +
  theme(legend.position = "top")
```

**Key Insight:** The "best" estimator depends on the true parameter value!

## Medical Application: Disease Prevalence

**Scenario:** Estimating prevalence of a rare disease ($p \approx 0.05$) vs. a common condition ($p \approx 0.5$).

```{r}
# Compare MSE at different prevalence levels
n <- 50

mse_at_p <- function(p, n) {
  mse_std <- p * (1-p) / n
  bias_add2 <- (2 - 4*p) / (n + 4)
  var_add2 <- n * p * (1-p) / (n + 4)^2
  mse_add2 <- var_add2 + bias_add2^2
  
  tibble(p = p, MSE_Standard = mse_std, MSE_AddTwo = mse_add2,
         Better = ifelse(mse_std < mse_add2, "Standard", "Add-Two"))
}

bind_rows(
  mse_at_p(0.05, n),
  mse_at_p(0.50, n)
) |> 
  mutate(across(where(is.numeric), \(x) round(x, 5)))
```

## Sample Variance: Why n-1?

Two formulas for sample variance:

$$S^2 = \frac{\sum(X_i - \bar{X})^2}{n-1} \quad \text{vs.} \quad \tilde{S}^2 = \frac{\sum(X_i - \bar{X})^2}{n}$$

. . .

**Question:** Why do we divide by $n-1$ instead of $n$?

**Answer:** Dividing by $n$ gives a biased estimator!

## Simulation: Comparing Variance Estimators

```{r}
true_variance <- 100  # σ² = 100
n <- 10
n_sims <- 10000

variance_sim <- tibble(sim = 1:n_sims) |> 
  mutate(
    sample_data = map(sim, \(s) rnorm(n, 0, sqrt(true_variance))),
    s2_n_minus_1 = map_dbl(sample_data, var),
    s2_n = map_dbl(sample_data, \(x) sum((x - mean(x))^2) / n)
  )

variance_sim |> 
  summarize(
    `True σ²` = true_variance,
    `E[S² with n-1]` = mean(s2_n_minus_1),
    `E[S² with n]` = mean(s2_n),
    `Bias (n-1)` = mean(s2_n_minus_1) - true_variance,
    `Bias (n)` = mean(s2_n) - true_variance
  ) |> 
  mutate(across(where(is.numeric), \(x) round(x, 2)))
```

## Your Turn: Comprehensive Example

**Exercise:** A clinical trial measures cholesterol reduction. Based on $n = 36$ patients:

-   Sample mean: $\bar{x} = 25$ mg/dL
-   Sample SD: $s = 12$ mg/dL

Calculate:

1.  The estimated standard error of $\bar{X}$
2.  If the true mean reduction is $\mu = 24$, and we repeated this trial many times, what would be the expected MSE of $\bar{X}$?

. . .

**Solutions:**

1.  $\widehat{SE} = \frac{12}{\sqrt{36}} = 2$ mg/dL

2.  $\bar{X}$ is unbiased, so $\text{MSE} = \text{Var}(\bar{X}) = \frac{\sigma^2}{n} \approx \frac{144}{36} = 4$

## Putting It All Together: Estimator Summary

| Property  | Formula                                 | Interpretation     |
|-----------|-----------------------------------------|--------------------|
| Bias      | $E(\hat{\theta}) - \theta$              | Systematic error   |
| Variance  | $E[(\hat{\theta} - E(\hat{\theta}))^2]$ | Random variability |
| Std Error | $\sqrt{\text{Var}(\hat{\theta})}$       | Typical deviation  |
| MSE       | $\text{Var} + \text{Bias}^2$            | Total error        |

. . .

::: callout-tip
## Guidelines for Choosing Estimators

1.  Unbiasedness is desirable but not always essential
2.  Lower variance/SE means more precision
3.  MSE provides a single criterion combining both
4.  Sometimes biased estimators have lower MSE!
:::

## Week 1 Summary

**Day 1:**

-   Population parameters vs. sample estimates
-   Expected value and variance of $\bar{X}$
-   Numerical optimization (grid search, `optimize()`)
-   Bias: definition, calculation, and correction

**Day 2:**

-   Standard error and estimated standard error
-   Mean Squared Error = Variance + Bias²
-   Bias-variance tradeoff
-   No single "best" estimator for all situations

## Homework Problems

1.  **Bias Calculation:** For a sample of size $n$ from Exponential($\lambda$), the MLE is $\hat{\lambda} = 1/\bar{X}$. It can be shown that $E(\hat{\lambda}) = \frac{n}{n-1}\lambda$. Calculate the bias and propose an unbiased estimator.

2.  **MSE Comparison:** Using simulation, compare the MSE of the standard proportion estimator vs. the add-two estimator for $n = 10$ and $p = 0.1, 0.3, 0.5$.

3.  **Numerical Optimization:** Use `optimize()` to find the MLE when you observe $x_1 = 2.1, x_2 = 3.5, x_3 = 1.8, x_4 = 2.9$ from an Exponential($\lambda$) distribution. The likelihood is $L(\lambda) = \lambda^4 e^{-\lambda \sum x_i}$.

## Next Week Preview

**Week 2: Minimum Variance Unbiased Estimators**

-   Among all unbiased estimators, which has smallest variance?
-   The Cramér-Rao lower bound
-   Efficiency of estimators
-   Introduction to Maximum Likelihood Estimation

## References

::: nonincremental
-   Devore, Berk, and Carlton. *Modern Mathematical Statistics with Applications* (Springer). Chapter 7.1
-   Chihara and Hesterberg. *Mathematical Statistics with Resampling and R* (Wiley). Chapter 6.
:::

## Questions? {.center background-color="#3498db"}

Thank you!

Office hours: \[Time/Location\]

Course website: \[URL\]

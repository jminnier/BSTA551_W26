[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BSTA 551: Theory of Statistical Inference",
    "section": "",
    "text": "BSTA 551: Theory of Statistical Inference\n\nWinter 2026\n \nThis is course introduces a theoretical foundation of Biostatistics for masters level students focusing on statistical inference. We will study methods of estimation (point and interval), as well as hypothesis testing. Some simulation and boostrap methods for estimation and testing will be included along with more theoretical asympototic and finite sample inference methods.\nPrerequisites: BSTA 550 Probability, familiarity with R.\n \n\n\n\n\n\n\n\nInstructor\n Dr. Jessica Minnier\n (see Sakai)\n minnier@ohsu.edu\n\n\nOffice Hours\nCharles  TBD\nCharles  TBD\nJessica  M/W 12pm-12:30pm after class\n\n\nCourse details\n Mondays, Wednesdays\n Jan 5 - March 18\n 10 AM - 12 PM\n In-person (see Sakai for room)\n\n\nContact\nE-mail or OHSU Teams is the best way to get in contact with me.\n\n\n\n\n\n\n\n\n View the source on GitHub"
  },
  {
    "objectID": "syllabus.html#key-course-info",
    "href": "syllabus.html#key-course-info",
    "title": "BSTA 551 Syllabus",
    "section": "Key Course Info",
    "text": "Key Course Info\n\nIf an assignment on Sakai is closed or you are submitting late work, please email me AND the TA your work\nThe class instruction will end on March 11, 2026. All coursework is expected to be completed by Wednesday of finals week March 18, 2026 at 11:59pm."
  },
  {
    "objectID": "syllabus.html#description",
    "href": "syllabus.html#description",
    "title": "BSTA 551 Syllabus",
    "section": "Description",
    "text": "Description\nWelcome to BSTA 551! In this course we will study the theoretical foundaton of statistical inference which includes estimation and hypothesis testing. This is the official course description:\nThis course introduces theoretical foundation in Biostatistics. Topics will include distributions of random variables (location-scale families and exponential families), data reduction (sufficiency and completeness of statistics), methods of estimation (method of moment estimators and maximum likelihood estimators), convergence, finite and large sample properties of estimators, interval estimation, hypothesis testing, asymptotic tests (likelihood-ratio tests, score tests, and Wald tests), and statistical simulations to evaluate statistical methods.\n\nCourse Learning Objectives\nAt the end of this course, students should be able to…\n\nExplain the major concepts and theorems in statistical inference.\nConnect theoretical concepts to statistical analyses.\nConduct simulations to study and evaluate statistical methods."
  },
  {
    "objectID": "syllabus.html#instructors",
    "href": "syllabus.html#instructors",
    "title": "BSTA 551 Syllabus",
    "section": "Instructors",
    "text": "Instructors\nHere is the instructor page."
  },
  {
    "objectID": "syllabus.html#meeting-times",
    "href": "syllabus.html#meeting-times",
    "title": "BSTA 551 Syllabus",
    "section": "Meeting Times",
    "text": "Meeting Times\nMondays          10:00 AM – 12:00 PM PST (see Sakai for room)\nWednesdays    10:00 AM – 12:00 PM PST\n\nKnown Exceptions\n\nMonday, January 19: No class (holiday)\nWednesday, February 10: No class\nMonday, February 16: No class (holiday)\nMarch 16 and 18: Finals week, no in person teaching, possibly virtual office hours as needed"
  },
  {
    "objectID": "syllabus.html#materials",
    "href": "syllabus.html#materials",
    "title": "BSTA 551 Syllabus",
    "section": "Materials",
    "text": "Materials\n\nTextbook\n\nModern Mathematical Statistics with Applications, 3rd ed.\n\nAuthor: JL Devore, KN Berk, MA Carlton\nTextbook available online through library\nCitation: Devore JL, Berk KN, Carlton MA. Modern Mathematical Statistics with Applications. Third edition. Springer; 2021. doi:10.1007/978-3-030-55156-8\nFocus on chapters 6-10\n\n\n\nSupplemental Readings (Optional)\n\nStatistical Inference, Casella and Berger, 2nd ed. (This was the previous textbook BSTA 551-552 Math Stat., very theoretical) ebook OHSU library link\nAn Introduction to R (free pdf available)\nStatistical Inference via Data Science, A ModernDive into R and the Tidyverse, Ismay, Kim, Valdivia, 2nd ed. free ebook online\n\n\n\n\nOnline Resources\n\nSakai\nWhile most course materials will be delivered online through this website, assignments will be turned in through Sakai, OHSU’s course management system. I will include a link on this website to the Sakai assignment page. \n\n\nPennState STAT 415 Website\nDr. Wakim linked to Penn State’s probability course materials for 550. They also have the subsequent inference course on the website as well. They have all their course notes posted on this page if you wish to have an alternative reference.\n\n\nUniversity of South Carolina STAT 713 Website\nThis is a more thoeretical version (similar to previous math/stats courses taught at OHSU) of statistical inference taught by Professor Tebbs at U of SC. Their notes which are linked on the course website are very well written. If you want more theoretical context for what we are learning in this course this is a good source. Note, it uses Casella & Berger’s textbook Statistical Inference as a reference.\n\n\nR: Statistical Computing Software\nR/Rstudio will be used to complete some homework assignments. Please see Dr. Wakim’s BSTA 550 syllabus section on R if you need any references or refreshers on how to use."
  },
  {
    "objectID": "syllabus.html#assessment",
    "href": "syllabus.html#assessment",
    "title": "BSTA 551 Syllabus",
    "section": "Assessment",
    "text": "Assessment\n\nBreakdown\n\nGrading & Requirements\n\n\nHomework grading\n\n\nViewing Grades in Sakai\nPoints you receive for graded activities will be posted to the Sakai Gradebook. Click on the Gradebook link on the left navigation to view your points."
  },
  {
    "objectID": "syllabus.html#course-instructor-evaluations",
    "href": "syllabus.html#course-instructor-evaluations",
    "title": "BSTA 551 Syllabus",
    "section": "Course & Instructor Evaluations",
    "text": "Course & Instructor Evaluations\n\nOngoing Course Feedback\n\n\nFinal Course Feedback\nAt the conclusion of the course, you will be asked to complete a formal online review of the course and the instructor. Your feedback on this University evaluation is critical to improving future student learning in this course as well as providing metrics relevant to the instructor’s career advancement (or lack of). Since our class is on the smaller side, everyone’s participation is needed for feedback to be released."
  },
  {
    "objectID": "syllabus.html#schedule",
    "href": "syllabus.html#schedule",
    "title": "BSTA 551 Syllabus",
    "section": "Schedule",
    "text": "Schedule\nPlease refer to the Schedule page. I will make changes to this schedule if we need more or less time on a concept. You do not need to read the corresponding chapters in the textbook for each class.\n\nAssignments\nWritten problem sets, approximately weekly, usually assigned Wednesday and due the following Wednesday.\nHomework problems requiring a written solution are assigned approximately one week in advance of their due date. Homework assignments will be posted in the Sakai submissions box."
  },
  {
    "objectID": "syllabus.html#course-policies-and-resources",
    "href": "syllabus.html#course-policies-and-resources",
    "title": "BSTA 551 Syllabus",
    "section": "Course Policies and Resources",
    "text": "Course Policies and Resources\n\nLate Work Policy\nI encourage you to make your best effort to submit all assignments on time, but I understand circumstances arise that are beyond our control. Please see this Swansea University’s page on extenuating circumstances for some examples. Not all circumstances are covered here, so please reach out if you have questions. \n\nThe class will end on March 18, 2026. All coursework is expected to be completed by March 20, 2026 at 11pm. If you have extenuating circumstances, and need additional time to complete class assignments, please contact me. Together, we will come up with a plan for completion and to sort out registrar logistics. If you need an Incomplete, generally this requires request two weeks before the end of class.\nIf you have extenuating circumstances that may jeopardize your ability to do work for several weeks, please contact me. We will come up with a plan to keep you on track in the course and prevent any delay in your education.\nFor homework, you will have TWO no-questions-asked, 3-day extensions: one for the first assignment part and one for either the solutions or presentation. Please use this wisely! You just need to send me a quick email saying “I am using my no-questions-asked extension for Homework __ assignment/solutions/presentation.”\nFor homework, I ask you to email me directly about any late submissions. You can explain your circumstances and may ask us for an extension. I am very likely to grant an extension, but I want to emphasize how important it will be to stay on track with your homework! Your group is depending on you, and delaying homework may only add stress on the next homework!\nIf you have a emergency involving your self, family, pet, friend, classmate, or anything/one deemed important to you, please do not worry about immediately contacting me. We can work something out after your emergency. If I contact you during an emergency, it is only because I am worried, and you do NOT need to respond until you are able. \n\n\n\nRegrade Policy\nIf you think a question was incorrectly graded, first compare your answer to the answer key. If you believe a re-grade would be appropriate, write an email to me containing the question and a short explanation as to why the question(s) was/were incorrectly graded. Deadline: One week after assignments were returned to class (late requests will not be considered).\n\n\nAttendance Policy\nYou are expected to attend class, participate in-class polls, and complete the exit ticket. For students who miss class or need a review, I will make video and audio recordings of lectures available. There are no guarantees against technical or other challenges for the recording availability or quality.\nYou will need to attend all classes. There are 19 classes total, so you are welcome to watch the recordings or come in-person. While I want attendance to be a flexible thing, I need to set certain requirements around in-person attendance to align with the school’s policy. Attendance is measured through exit tickets that will be due 7 days after each class.\nThis is meant to keep you on track within the course and prevent a pile up of material. Make sure to complete the exit ticket at the end of class to demonstrate attendance.\n\n\nPlagiarism and Attribution\nPlease note that this section is directly sourced from Dr. Nicky Wakim’s Course Policies for BSTA 550 which has itself has been motivated by Dr. Steven Bedrick’s Course Policies and Grading site for BMI 525. (Note that this is a good example of informal attribution of someone else’s work.)\nIn this class, it is easy to use ChatGPT or other AI tools to solve your homework for you. Many problems follow a basic structure that is especially easy for ChatGPT to solve. In this class, you may use ChatGPT to help with your homework. You may even ask for direct answers. However, there are a few things I do not want you to do:\n\nDo not copy ChatGPT’s answer directly into your homework. Your homework is graded for full credit if you turn it in, in any state, so turning in ChatGPT’s answers is unacceptable. I rather see half-written answers that show what you’re thinking than see a correct answer from ChatGPT.\nDo not stop once ChatGPT answered a question. If it gives an explanation, interact with it! Make sure you understand the thought process of ChatGPT. Try writing out the process to help cement it in your head. Check the answer with what we learn in class.\nDo not use ChatGPT on our exams! Hence, you need to really understand how to solve these problems even if you use ChatGPT on the homework.\n\nAt the end of the day, ChatGPT is a resource that will be available to you in a job and outside of school. Thus, we should use it as a tool in school as well! Let me know if ChatGPT helped you understand something! I would love to incorporate it into future classes!\n\n\n\n\n\n\nImportant\n\n\n\nYou can think of this class as assembling a toolbox. When a handyperson starts working for the first time, they need to buy their tools. For their first few jobs, they might need help finding their tools or remembering which tool is best used for what action. Eventually, they get to know their tools well, and using them appropriately becomes second nature.\nFor now, ChatGPT can help us find and use our tools, but we need to work towards using them as second nature!"
  },
  {
    "objectID": "syllabus.html#course-expectations",
    "href": "syllabus.html#course-expectations",
    "title": "BSTA 551 Syllabus",
    "section": "Course Expectations",
    "text": "Course Expectations\n\nInstructor Expectations\nCommitment to your learning and your success\nI believe that everyone has the ability to be successful in this course and I have put a lot of effort into designing the course in a way that maximizes your learning to ensure your success. Please talk to me before or after class or stop by my office if there is anything you want to discuss or about which you are unclear. I want to be supportive of your learning and growth.\nInclusive & supportive learning community\nI believe that learning happens best when we all learn together, as a community. This means creating a space characterized by generous listening, civility, humility, patience, and hospitality. I will attempt to promote a safe climate where we examine content from multiple perspectives. I will strive to create and maintain a classroom atmosphere in which you feel free to both listen to others and express your views and ask questions to increase your learning.\nOpenness to feedback\nI appreciate straightforward feedback from you regarding how well the class is meeting your needs. Let me know if material is not clear or when its relevance to the student learning outcomes for the course is not apparent. In particular, let me know if you identify bias or stereotyping in my teaching materials as I will seek to continuously improve. Please also let me know if there’s an aspect of the class you find particularly interesting, helpful, or enjoyable!\nResponsiveness\nI will monitor email as well as the discussion board daily and try respond to all messages within 24 hours Monday-Friday.\nClear guidelines and prompt feedback on assignments\nI will provide clear instructions for all assignments, and a grading rubric when applicable. The TAs and I will provide detailed feedback on your submissions and will update grades promptly in Sakai.\n\n\nStudent Expectations and Resources\nAttend class\nYou are expected to attend at least 12 scheduled class meetings in-person. Attendance is taken through exit tickets. If you have issues accessing the poll on a specific day, please let me know. \nParticipate\nI encourage you to participate actively in class and in Slack discussions. I will expect all students, and all instructors, to be respectful of each other’s contributions, whether I agree with them or not. Professional interactions are expected.\nBuild rapport\nIf you find that you have any trouble keeping up with assignments or other aspects of the course, make sure you let me know as early as possible. As you will find, building rapport and effective relationships are key to becoming an effective professional. Make sure that you are proactive in informing me when difficulties arise during the quarter so that I can help you find a solution in regards to coursework.\nComplete assignments\nAll assignments for this course will be submitted electronically through Sakai unless otherwise instructed. I encourage you to make your best effort to submit all assignments on time, but I understand that sometimes circumstances arise that are beyond our control. If you need an extension, please contact me in congruence with the Late Policy.\nSeek help if you need it\nI believe it is important to support the physical and emotional well‐being of my students. If you are experiencing physical and/or mental health issues, I encourage you to use the resources on campus such as those listed below. If you have a health issue that is affecting your performance or participation in the course, and/or if you need help connecting with these resources, please contact me.\n\nStudent Health and Wellness Center (SHW), Website, 503-494-8665 (OHSU Students only)\nStudent Health and Counseling (SHAC), Website, 503-725-2800\n\nInform your instructor of any accommodations needed\nYou should speak with or email me before or during the first week of classes regarding any special needs. Students seeking academic accommodations should register with the appropriate service under the School policies below.\nSome religious holidays may occur on regularly scheduled class days. Because available class hours are so limited in number, we will have to hold class on all such days. Class video recordings will be available and you are encouraged to engage with the material outside of the regular class time. Please email me about your absence. I will excuse the absece from your grade. You are also encouraged to come to office hours with questions from the session.\nCommit to integrity\nAs a student in this course (and at PSU or OHSU) you are expected to maintain high degrees of professionalism, commitment to active learning and participation in this class and also integrity in your behavior in and out of the classroom.\nCheating and other forms of academic misconduct will not be tolerated in this course and will be dealt with firmly. Student academic misconduct refers to behavior that includes plagiarism, cheating on assignments, fabrication of data, falsification of records or official documents, intentional misuse of equipment or materials (including library materials), or aiding and abetting the perpetration of such acts. Preparation of exams, assigned on an individual basis, must represent each student’s own individual effort. When used, resource materials should be cited in conventional reference format."
  },
  {
    "objectID": "syllabus.html#course-communications",
    "href": "syllabus.html#course-communications",
    "title": "BSTA 551 Syllabus",
    "section": "Course Communications",
    "text": "Course Communications\nSakai/Slack announcements\nFor important/urgent matters, I will communicate with you using announcements via Sakai that will be delivered to your OHSU Email account as well as displayed in the Sakai course site Announcements section. I will copy these announcements in Slack if they do not involve changes to the schedule. Unfortunately, there are certain announcements that OHSU requires I initiate behind the firewall.\nGeneral course questions\nIt is normal to have many questions about things that relate to the course, such as clarification about assignments, course materials, or assessments. Please post these on our Slack Workspace. Please use the channels that I created for questions. You are encouraged to give answers and help each other. I will monitor these threads, so I will endorse or correct responses as needed. Please give me 24 hours to respond to questions within Monday-Friday. Work-life balance is important for me as well, so I will try to respond as quickly as I can within my healthy limits. \nE-mail\nE-mail should be used only for messages that are private in nature. Please send private messages to my OHSU email address (wakim@ohsu.edu). Messages sent through Sakai Inbox will not be answered. Do not send messages asking general information about the class; please post those on Slack instead."
  },
  {
    "objectID": "syllabus.html#further-student-resources",
    "href": "syllabus.html#further-student-resources",
    "title": "BSTA 551 Syllabus",
    "section": "Further Student Resources",
    "text": "Further Student Resources\n\nAcademic Success Center\nOHSU houses an Academic Success Center for all students. Their mission is to create a center for learning support where ALL learners can discover the resources and community that they need for finding academic success at OHSU. They provide many services to students, including: learning skills support, writing support, English for speakers of other languages (ESOL) support, and individual and group content tutoring. Check out the SharePoint site for the Academic Success Center.\n\n\nStudent Wellness\nI am committed to supporting the physical and emotional well-being of my students. Both PSU and OHSU have designated centers for student health. For OHSU, students can visit the Behavioral Health site, where you can find more information including the number to make an appointment. All student visits are free. OHSU students also have access to PSU’s Counseling Services through the school’s Student Health & Counseling. Information on additional student resources for OHSU students are available on the OHSU Health and Wellness Resource page. \n\n\nSupport for Food Insecurity\nStudents across the country experience food insecurity at high rates. OHSU and PSU both provide a list of resources to help combat food insecurity. Of note, the Committee to Improve Student Food Security (CISFS) at PSU provides a Free Food Market on the second Monday of each month. OHSU also provides SNAP Enrollment Assistance. The Supplemental Nutrition Assistance Program (SNAP) allocates money towards food for individuals below a certain income level. If you make less than $2,430 monthly, you may wish to enroll.\n\n\nSupport for Students with Children\nStudents who have children can use the PSU resource: Resource Center for Students with Children. Resources are mostly focused on students with younger children. There are several great resources available, including: family-friendly study spaces, new baby starter packs, free kids clothing, and further information on financial resources for childcare."
  },
  {
    "objectID": "lessons/05_Equally_likely_outcomes/05_Equally_likely_outcomes_key_info.html",
    "href": "lessons/05_Equally_likely_outcomes/05_Equally_likely_outcomes_key_info.html",
    "title": "Key Info and Announcements",
    "section": "",
    "text": "Thanks for being flexible on Monday!\n\nI’m sorry if this messed up your morning flow\n\nToday: need to finish lesson 4, then start lesson 5\nFire drills this week!\nNotes on homework\n\nLook at Charles’ feedback on Sakai\nSet your seed for reproducibility\nself-contained: true\nMake sure to include the full sample space\n\nIf drawing 2, then sample space is each combo of two\n\nDon’t forget to do all the steps in the simulations!"
  },
  {
    "objectID": "lessons/05_Equally_likely_outcomes/05_Equally_likely_outcomes_key_info.html#key-info",
    "href": "lessons/05_Equally_likely_outcomes/05_Equally_likely_outcomes_key_info.html#key-info",
    "title": "Key Info and Announcements",
    "section": "",
    "text": "Thanks for being flexible on Monday!\n\nI’m sorry if this messed up your morning flow\n\nToday: need to finish lesson 4, then start lesson 5\nFire drills this week!\nNotes on homework\n\nLook at Charles’ feedback on Sakai\nSet your seed for reproducibility\nself-contained: true\nMake sure to include the full sample space\n\nIf drawing 2, then sample space is each combo of two\n\nDon’t forget to do all the steps in the simulations!"
  },
  {
    "objectID": "lessons/05_Equally_likely_outcomes/05_Equally_likely_outcomes_key_info.html#announcements",
    "href": "lessons/05_Equally_likely_outcomes/05_Equally_likely_outcomes_key_info.html#announcements",
    "title": "Key Info and Announcements",
    "section": "Announcements",
    "text": "Announcements\n\nHomework 2 due this Sunday at 11pm\nQuiz 1 opens on 10/22 at 3pm"
  },
  {
    "objectID": "lessons/06_Calculus_review/06_Calculus_review_key_info.html",
    "href": "lessons/06_Calculus_review/06_Calculus_review_key_info.html",
    "title": "Key Info",
    "section": "",
    "text": "This class is optional!\nFire drills this week!"
  },
  {
    "objectID": "lessons/06_Calculus_review/06_Calculus_review_key_info.html#announcements",
    "href": "lessons/06_Calculus_review/06_Calculus_review_key_info.html#announcements",
    "title": "Key Info",
    "section": "",
    "text": "This class is optional!\nFire drills this week!"
  },
  {
    "objectID": "lessons/06_Calculus_review/06_Calculus_review_key_info.html#key-dates",
    "href": "lessons/06_Calculus_review/06_Calculus_review_key_info.html#key-dates",
    "title": "Key Info",
    "section": "Key Dates",
    "text": "Key Dates\n\nHW 2 due Sunday\nQuiz 1 will open next Wednesday after class"
  },
  {
    "objectID": "lessons/06_Calculus_review/06_Calculus_review_muddy_points.html",
    "href": "lessons/06_Calculus_review/06_Calculus_review_muddy_points.html",
    "title": "Muddy Points",
    "section": "",
    "text": "Here is a pretty helpful StackExchange post talking about this!\n\n\n\nI have two approaches to identifying \\(u\\) and \\(dv\\):\n\nTry to find a \\(u\\) that will eventually differentiate into a constant. This usually works unless you’re left with a \\(dv\\) that is hard to integrate.\n\nFor example, \\(u=x^6\\). the first derivative, \\(u'=6x^5\\), which may lead us to do another integration by parts, but eventually, at the 6th derivative, we get 720. This means, in our integration by parts, we eventually get an integral that only has \\(x\\) in the function once.\n\nIn this example, \\(x^6\\) would result in many integration by parts. I feel like we don’t typically see that many in our work.\n\nIn Example 3.5 in Calculus Review, setting \\(u=x^2\\) is a bad idea because we don’t really know how to integrate \\(dv=ln(x)\\) into \\(v\\).\n\nTry to find a \\(u\\) where \\(du\\) is the reciprocal of \\(v\\) or \\(du\\) cancels with \\(v\\).\n\nLook again at Example 3.5 in the notes! Hint: What is the derivative of \\(ln(x)\\)? And what does \\(\\dfrac{1}{x}x^3\\) equal?\n\n\n\n\n\nI just stumbled upon this website! Just a bunch of calculus practice problems! Might be some help."
  },
  {
    "objectID": "lessons/06_Calculus_review/06_Calculus_review_muddy_points.html#fall-2023",
    "href": "lessons/06_Calculus_review/06_Calculus_review_muddy_points.html#fall-2023",
    "title": "Muddy Points",
    "section": "",
    "text": "Here is a pretty helpful StackExchange post talking about this!\n\n\n\nI have two approaches to identifying \\(u\\) and \\(dv\\):\n\nTry to find a \\(u\\) that will eventually differentiate into a constant. This usually works unless you’re left with a \\(dv\\) that is hard to integrate.\n\nFor example, \\(u=x^6\\). the first derivative, \\(u'=6x^5\\), which may lead us to do another integration by parts, but eventually, at the 6th derivative, we get 720. This means, in our integration by parts, we eventually get an integral that only has \\(x\\) in the function once.\n\nIn this example, \\(x^6\\) would result in many integration by parts. I feel like we don’t typically see that many in our work.\n\nIn Example 3.5 in Calculus Review, setting \\(u=x^2\\) is a bad idea because we don’t really know how to integrate \\(dv=ln(x)\\) into \\(v\\).\n\nTry to find a \\(u\\) where \\(du\\) is the reciprocal of \\(v\\) or \\(du\\) cancels with \\(v\\).\n\nLook again at Example 3.5 in the notes! Hint: What is the derivative of \\(ln(x)\\)? And what does \\(\\dfrac{1}{x}x^3\\) equal?\n\n\n\n\n\nI just stumbled upon this website! Just a bunch of calculus practice problems! Might be some help."
  },
  {
    "objectID": "lessons/16_Cont_RVs/16_Cont_RVs_muddy_points.html",
    "href": "lessons/16_Cont_RVs/16_Cont_RVs_muddy_points.html",
    "title": "Muddy Points",
    "section": "",
    "text": "In this example, I was demonstrating how we could make a joint pdf from \\(n\\) exponential distributions. The joint pdf was for the random variable \\(M\\), which was the minimum of randomly distributed arrival times (\\(X_i\\))."
  },
  {
    "objectID": "lessons/16_Cont_RVs/16_Cont_RVs_muddy_points.html#fall-2023",
    "href": "lessons/16_Cont_RVs/16_Cont_RVs_muddy_points.html#fall-2023",
    "title": "Muddy Points",
    "section": "",
    "text": "In this example, I was demonstrating how we could make a joint pdf from \\(n\\) exponential distributions. The joint pdf was for the random variable \\(M\\), which was the minimum of randomly distributed arrival times (\\(X_i\\))."
  },
  {
    "objectID": "lessons/18_Moment_Generating_Functions/18_Moment_Generating_Functions_key_info.html",
    "href": "lessons/18_Moment_Generating_Functions/18_Moment_Generating_Functions_key_info.html",
    "title": "Key Info",
    "section": "",
    "text": "LAST CLASS!!!!\n\nOffice hours will continue through 12/12\n\nFinish CLT lesson\nHW 7: Charles said it looked good!\nHW 9 is optional!\n\nCan use HW 9 to replace a 0 for another homework\nBut you can also turn in any homework up until 12/12 at 11pm\n\nPlease, please, please do your course eval!\n\nIt helps me improve the course\nIt helps me get recognition for my work\nIt is anonymous\nIt takes like 5 minutes: you really only need to do the scorings, and maybe a sentence or two of comments"
  },
  {
    "objectID": "lessons/18_Moment_Generating_Functions/18_Moment_Generating_Functions_key_info.html#announcements",
    "href": "lessons/18_Moment_Generating_Functions/18_Moment_Generating_Functions_key_info.html#announcements",
    "title": "Key Info",
    "section": "",
    "text": "LAST CLASS!!!!\n\nOffice hours will continue through 12/12\n\nFinish CLT lesson\nHW 7: Charles said it looked good!\nHW 9 is optional!\n\nCan use HW 9 to replace a 0 for another homework\nBut you can also turn in any homework up until 12/12 at 11pm\n\nPlease, please, please do your course eval!\n\nIt helps me improve the course\nIt helps me get recognition for my work\nIt is anonymous\nIt takes like 5 minutes: you really only need to do the scorings, and maybe a sentence or two of comments"
  },
  {
    "objectID": "lessons/18_Moment_Generating_Functions/18_Moment_Generating_Functions_key_info.html#key-dates",
    "href": "lessons/18_Moment_Generating_Functions/18_Moment_Generating_Functions_key_info.html#key-dates",
    "title": "Key Info",
    "section": "Key Dates",
    "text": "Key Dates\n\nQuiz 3 opens TODAY at 3pm\nI posted some info on it\nHW 8 due 12/7\nIMPORTANT: ALL coursework due by 12/12 at 11pm"
  },
  {
    "objectID": "lessons/08_pdfs/24_01_Continuous_rv_muddy_points.html",
    "href": "lessons/08_pdfs/24_01_Continuous_rv_muddy_points.html",
    "title": "Muddy Points",
    "section": "",
    "text": "Muddy points from Fall 2023:\n\n1. Why we are allowed to “split” the operator d/dx into two pieces as if it were a fraction when it’s an operator\nHere is a pretty helpful StackExchange post talking about this!\n\n\n2. How to know what to use as \\(u\\) and \\(dv\\) for integration by parts\nI have two approaches to identifying \\(u\\) and \\(dv\\):\n\nTry to find a \\(u\\) that will eventually differentiate into a constant. This usually works unless you’re left with a \\(dv\\) that is hard to integrate.\n\nFor example, \\(u=x^6\\). the first derivative, \\(u'=6x^5\\), which may lead us to do another integration by parts, but eventually, at the 6th derivative, we get 720. This means, in our integration by parts, we eventually get an integral that only has \\(x\\) in the function once.\n\nIn this example, \\(x^6\\) would result in many integration by parts. I feel like we don’t typically see that many in our work.\n\nIn Example 3.5 in Calculus Review, setting \\(u=x^2\\) is a bad idea because we don’t really know how to integrate \\(dv=ln(x)\\) into \\(v\\).\n\nTry to find a \\(u\\) where \\(du\\) is the reciprocal of \\(v\\) or \\(du\\) cancels with \\(v\\).\n\nLook again at Example 3.5 in the notes! Hint: What is the derivative of \\(ln(x)\\)? And what does \\(\\dfrac{1}{x}x^3\\) equal?\n\n\n\n\n3. Looking for more practice in calculus?\nI just stumbled upon this website! Just a bunch of calculus practice problems! Might be some help."
  },
  {
    "objectID": "lessons/08_pdfs/08_pdfs_muddy_points.html",
    "href": "lessons/08_pdfs/08_pdfs_muddy_points.html",
    "title": "Muddy Points",
    "section": "",
    "text": "I think you’re on the right track… \\(P(X)\\), \\(P(X=x)\\), and \\(p_X(x)\\) represent probabilities.\nFor discrete random variables, we can use \\(P(X=x)\\) or \\(p_X(x)\\) to represent the probability that the random variable \\(X\\) takes on the value \\(x\\). And then we can discuss the probability mass function (pmf) as the probability distribution as well!\nFor continuous random variables, we use \\(f_X(x)\\) to represent the probability density function (pdf) of \\(X\\) at the value \\(x\\), BUT it is not equal to the actual probability! Aka \\(f_X(x)\\) does not represent a probability, but the area under the curve of \\(f_X(x)\\) over an interval gives us the probability."
  },
  {
    "objectID": "lessons/08_pdfs/08_pdfs_muddy_points.html#fall-2025",
    "href": "lessons/08_pdfs/08_pdfs_muddy_points.html#fall-2025",
    "title": "Muddy Points",
    "section": "",
    "text": "I think you’re on the right track… \\(P(X)\\), \\(P(X=x)\\), and \\(p_X(x)\\) represent probabilities.\nFor discrete random variables, we can use \\(P(X=x)\\) or \\(p_X(x)\\) to represent the probability that the random variable \\(X\\) takes on the value \\(x\\). And then we can discuss the probability mass function (pmf) as the probability distribution as well!\nFor continuous random variables, we use \\(f_X(x)\\) to represent the probability density function (pdf) of \\(X\\) at the value \\(x\\), BUT it is not equal to the actual probability! Aka \\(f_X(x)\\) does not represent a probability, but the area under the curve of \\(f_X(x)\\) over an interval gives us the probability."
  },
  {
    "objectID": "lessons/08_pdfs/08_pdfs_muddy_points.html#fall-2023",
    "href": "lessons/08_pdfs/08_pdfs_muddy_points.html#fall-2023",
    "title": "Muddy Points",
    "section": "Fall 2023",
    "text": "Fall 2023\n\n1. How do pdf, CDF, and probability interact with each other?\nLet’s say we have a pdf, \\(f_X(x) = \\dfrac{1}{9}x^2\\) for \\(0 \\leq x \\leq 3\\). This is just a function. The pdf is not used on its own to report any probability. We must integrate over the pdf to find a probability.\n\nlibrary(\"ggplot2\")\neq = function(x){(1/9)*x^2}\nggplot(data.frame(x=c(1, 50)), aes(x=x)) + \n  stat_function(fun=eq) +\n  xlab(\"x\") + ylab(\"pdf\") +\n  xlim(0,3)\n\n\n\n\n\n\n\n\nThe total area under the pdf is 1. This makes our pdf valid.\n\neq = function(x){(1/9)*x^2}\nggplot(data.frame(x=c(1, 50)), aes(x=x)) + \n  stat_function(fun=eq) +\n  xlab(\"x\") + ylab(\"pdf\") +\n  xlim(0,3) +\n  stat_function(fun=eq, \n                xlim = c(0, 3),\n                geom = \"area\", \n                aes(fill = \"red\")) +\n  theme(legend.position = \"none\") +\n  annotate(\"text\", x = 0.5, y = 0.7, label = \"AUC = 1\", color = \"black\")\n\n\n\n\n\n\n\n\nIf we only look at a proportion of the area under the pdf, then we start constructing our probabilities. For example, we can look at probability that we have a value between 0 and 1.5.\n\nggplot(data.frame(x=c(1, 50)), aes(x=x)) + \n  stat_function(fun=eq) +\n  xlab(\"x\") + ylab(\"pdf\") +\n  xlim(0,3) +\n  stat_function(fun=eq, \n                xlim = c(0, 1.5),\n                geom = \"area\", \n                aes(fill = \"blue\")) +\n  theme(legend.position = \"none\") +\n  annotate(\"text\", x = 0.5, y = 0.7, label = \"AUC = 0.125\", color = \"black\")\n\n\n\n\n\n\n\n\nInstead of calculating the EXACT probability for each value between 0 and 3, we can find the CDF of the pdf.\nThe CDF is: \\[\nF_X(x) = \\left\\{\n        \\begin{array}{ll}\n            0 & \\quad x&lt;3 \\quad \\\\\n            \\dfrac{1}{27}x^3 & \\quad 0 \\leq x \\leq 3\\quad \\\\\n            1 & \\quad x&gt;3 \\quad\n        \\end{array}\n    \\right.\n\\]\n\ncdf = function(x){(1/27)*x^3}\nggplot(data.frame(x=c(1, 50)), aes(x=x)) + \n  stat_function(fun=cdf) +\n  xlab(\"x\") + ylab(\"CDF\") +\n  xlim(0,3) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nWhen \\(x=1.5\\), we can calculate the probability using the CDF. Remember that \\(F_X(x) = P(X \\leq x)\\). So we can say \\(P(X \\leq 1.5) = F_X(1.5) = \\dfrac{1}{27}(1.5)^3\\), which equals 0.125.\n\ncdf = function(x){(1/27)*x^3}\nggplot(data.frame(x=c(1, 50)), aes(x=x)) + \n  stat_function(fun=cdf) +\n  xlab(\"x\") + ylab(\"CDF\") +\n  xlim(0,3) +\n  theme(legend.position = \"none\") +\n  geom_point(aes(x=1.5, y=.125), colour=\"blue\", size=3) +\n  annotate(\"text\", x = 0.5, y = 0.7, label = \"CDF = 0.125\", color = \"black\")\n\nWarning in geom_point(aes(x = 1.5, y = 0.125), colour = \"blue\", size = 3): All aesthetics have length 1, but the data has 2 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\nWe can also calculate the probability with an integral: \\(P(X \\leq 1.5) = \\displaystyle\\int_0^{1.5} \\dfrac{1}{9}x^2 dx\\).\nWe can also find the probability that X is between two numbers. \\(P(1\\leq X \\leq 1.5) = F_X(1.5) - F_X(1)\\) or \\(P(1\\leq X \\leq 1.5) = \\displaystyle\\int_1^{1.5} \\dfrac{1}{9}x^2 dx\\)."
  },
  {
    "objectID": "lessons/03_Lang_prob/03_Lang_prob_key_info.html",
    "href": "lessons/03_Lang_prob/03_Lang_prob_key_info.html",
    "title": "Key Info",
    "section": "",
    "text": "Q: Can we still turn in exit tickets even if its not the same day? For example I was here the first day but forgot to do the exit ticket, if I did it today would it still count towards my grade?\n\nYes! I check them for “attendance” after 7 (sometimes more) days after the class\n\nFor muddy points, it really helps if you can asked a specific question about what you found confusing\n\nI can then address it in a more targeted way"
  },
  {
    "objectID": "lessons/03_Lang_prob/03_Lang_prob_key_info.html#announcements",
    "href": "lessons/03_Lang_prob/03_Lang_prob_key_info.html#announcements",
    "title": "Key Info",
    "section": "",
    "text": "Q: Can we still turn in exit tickets even if its not the same day? For example I was here the first day but forgot to do the exit ticket, if I did it today would it still count towards my grade?\n\nYes! I check them for “attendance” after 7 (sometimes more) days after the class\n\nFor muddy points, it really helps if you can asked a specific question about what you found confusing\n\nI can then address it in a more targeted way"
  },
  {
    "objectID": "lessons/03_Lang_prob/03_Lang_prob_key_info.html#key-dates",
    "href": "lessons/03_Lang_prob/03_Lang_prob_key_info.html#key-dates",
    "title": "Key Info",
    "section": "Key Dates",
    "text": "Key Dates\n\nHW 1 Assignment due this Thursday at 11pm"
  },
  {
    "objectID": "lessons/03_Lang_prob/03_Lang_prob.html",
    "href": "lessons/03_Lang_prob/03_Lang_prob.html",
    "title": "Lesson 3: Language of Probability",
    "section": "",
    "text": "Use set notation, Venn diagrams, and the concepts of unions, intersections, complements, and mutually exclusive events to represent and describe events.\nApply the axioms of probability and related properties to calculate probabilities and prove simple results.\nExplain and use De Morgan’s Laws to simplify and solve probability problems.\nConnect partitions and all rules of probability to calculate probabilities."
  },
  {
    "objectID": "lessons/03_Lang_prob/03_Lang_prob.html#set-theory-12",
    "href": "lessons/03_Lang_prob/03_Lang_prob.html#set-theory-12",
    "title": "Lesson 3: Language of Probability",
    "section": "Set Theory (1/2)",
    "text": "Set Theory (1/2)\n\n\n \n\n\nDefinition: Union\n\n\nThe union of events \\(A\\) and \\(B\\), denoted by \\(A \\cup B\\), contains all outcomes that are in \\(A\\) or \\(B\\) or both\n\n\n\n\nDefinition: Intersection\n\n\nThe intersection of events \\(A\\) and \\(B\\), denoted by \\(A \\cap B\\), contains all outcomes that are both in \\(A\\) and \\(B\\).\n\n\n\nVenn diagrams"
  },
  {
    "objectID": "lessons/03_Lang_prob/03_Lang_prob.html#set-theory-22",
    "href": "lessons/03_Lang_prob/03_Lang_prob.html#set-theory-22",
    "title": "Lesson 3: Language of Probability",
    "section": "Set Theory (2/2)",
    "text": "Set Theory (2/2)\n\n\n \n\n\nDefinition: Complement\n\n\nThe complement of event \\(A\\), denoted by \\(A^C\\) or \\(A'\\), contains all outcomes in the sample space \\(S\\) that are not in \\(A\\) .\n\n\n\n\nDefinition: Mutually Exclusive\n\n\nEvents \\(A\\) and \\(B\\) are mutually exclusive, or disjoint, if they have no outcomes in common. In this case \\(A \\cap B = \\emptyset\\), where \\(\\emptyset\\) is the empty set.\n\n\n\nVenn diagrams"
  },
  {
    "objectID": "lessons/03_Lang_prob/03_Lang_prob.html#how-can-we-code-some-of-these-12",
    "href": "lessons/03_Lang_prob/03_Lang_prob.html#how-can-we-code-some-of-these-12",
    "title": "Lesson 3: Language of Probability",
    "section": "How can we code some of these? (1/2)",
    "text": "How can we code some of these? (1/2)\n\n\nExample: Simulating Two Rolls of a Fair Four-Sided Die\n\n\nWe’re going to roll two four-sided dice. This time, let’s say event A is rolling matching numbers and event B is rolling at least one 2.\n\n\n\nFirst, we simulate rolling two four-sided dice 10,000 times\n\n\nset.seed(1002)\nrolls = replicate(10000, sample(x = 1:4, size = 2, replace = TRUE))\n\n\nNow, we can create logical vectors for events A and B\n\n\nevent_A = ( rolls[1, ] == rolls[2, ] )\nhead(event_A, 10)\n\n [1] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE\n\nevent_B = ( rolls[1, ] == 2 | rolls[2, ] == 2 )\nhead(event_B, 10)\n\n [1] FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE FALSE"
  },
  {
    "objectID": "lessons/03_Lang_prob/03_Lang_prob.html#how-can-we-code-some-of-these-22",
    "href": "lessons/03_Lang_prob/03_Lang_prob.html#how-can-we-code-some-of-these-22",
    "title": "Lesson 3: Language of Probability",
    "section": "How can we code some of these? (2/2)",
    "text": "How can we code some of these? (2/2)\n\n\n\n\nUnion\n\n\n\\(A \\cup B\\)\nA | B\n\nevent_A_or_B = event_A | event_B\nhead(event_A_or_B, 10)\n\n [1] FALSE FALSE FALSE  TRUE FALSE  TRUE  TRUE  TRUE FALSE FALSE\n\n\n\n\n\n\n\nIntersection\n\n\n\\(A \\cap B\\) A & B\n\nevent_A_and_B = event_A & event_B\nhead(event_A_and_B, 10)\n\n [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\n\n\n\n\n\n\nComplement\n\n\n\\(A^c\\) or \\(A'\\) != A\n\nevent_not_A = !event_A\nevent_not_B = event_B != TRUE\nhead(event_not_A, 10)\n\n [1]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE\n\n\n\n\n\n\n\nMutually Exclusive\n\n\n\\(A \\cap B = \\emptyset\\) A & B == NA\n\nsum(event_A_and_B == TRUE)\n\n[1] 621"
  },
  {
    "objectID": "lessons/03_Lang_prob/03_Lang_prob.html#probability-axioms",
    "href": "lessons/03_Lang_prob/03_Lang_prob.html#probability-axioms",
    "title": "Lesson 3: Language of Probability",
    "section": "Probability Axioms",
    "text": "Probability Axioms\n\n\n\n\nAxiom 1\n\n\nFor every event \\(A\\), \\(0\\leq\\mathbb{P}(A)\\leq 1\\). Probability is between 0 and 1.\n\n\n\n\nAxiom 2\n\n\nFor the sample space \\(S\\), \\(\\mathbb{P}(S)=1\\).\n\n\n\n\nAxiom 3\n\n\nIf \\(A_1, A_2, A_3, \\ldots\\), is a collection of disjoint events, then \\[\\mathbb{P}\\Big( \\bigcup \\limits_{i=1}^{\\infty}A_i\\Big) =  \\sum_{i=1}^{\\infty}\\mathbb{P}(A_i).\\] The probability of at least one \\(A_i\\) is the sum of the individual probabilities of each."
  },
  {
    "objectID": "lessons/03_Lang_prob/03_Lang_prob.html#some-probability-properties",
    "href": "lessons/03_Lang_prob/03_Lang_prob.html#some-probability-properties",
    "title": "Lesson 3: Language of Probability",
    "section": "Some probability properties",
    "text": "Some probability properties\nUsing the Axioms, we can prove all other probability properties! Events A, B, and C are not necessarily disjoint!\n\n\n\n\nProposition 1\n\n\nFor any event \\(A\\), \\(\\mathbb{P}(A)= 1 - \\mathbb{P}(A^C)\\)\n\n\n\n\nProposition 2\n\n\n\\(\\mathbb{P}(\\emptyset)=0\\)\n\n\n\n\nProposition 3\n\n\nIf \\(A \\subseteq B\\), then \\(\\mathbb{P}(A) \\leq \\mathbb{P}(B)\\)\n\n\n\n\n\nProposition 4\n\n\n\\[\\mathbb{P}(A \\cup B) = \\mathbb{P}(A) + \\mathbb{P}(B) - \\mathbb{P}(A \\cap B)\\] where \\(A\\) and \\(B\\) are not necessarily disjoint\n\n\n\n\nProposition 5\n\n\n\\(\\begin{aligned} \\mathbb{P}(A \\cup B & \\cup C) = \\mathbb{P}(A) + \\mathbb{P}(B) + \\\\ & \\mathbb{P}(C) - \\mathbb{P}(A \\cap B) - \\mathbb{P}(A \\cap C) - \\\\ & \\mathbb{P}(B \\cap C) + \\mathbb{P}(A \\cap B \\cap C) \\end{aligned}\\)"
  },
  {
    "objectID": "lessons/03_Lang_prob/03_Lang_prob.html#proposition-1-proof",
    "href": "lessons/03_Lang_prob/03_Lang_prob.html#proposition-1-proof",
    "title": "Lesson 3: Language of Probability",
    "section": "Proposition 1 Proof",
    "text": "Proposition 1 Proof\n\n\n\n\nProposition 1\n\n\nFor any event \\(A\\), \\(\\mathbb{P}(A)= 1 - \\mathbb{P}(A^C)\\)\n\n\n\n\n\nUse Axioms!\n\n\nA1: \\(0\\leq\\mathbb{P}(A)\\leq 1\\)\nA2: \\(\\mathbb{P}(S)=1\\)\nA3: For disjoint \\(A_i\\),\n\\(\\mathbb{P}\\Big( \\bigcup \\limits_{i=1}^{\\infty}A_i\\Big) =  \\sum_{i=1}^{\\infty}\\mathbb{P}(A_i)\\)"
  },
  {
    "objectID": "lessons/03_Lang_prob/03_Lang_prob.html#proposition-2-proof",
    "href": "lessons/03_Lang_prob/03_Lang_prob.html#proposition-2-proof",
    "title": "Lesson 3: Language of Probability",
    "section": "Proposition 2 Proof",
    "text": "Proposition 2 Proof\n\n\n\n\nProposition 2\n\n\n\\(\\mathbb{P}(\\emptyset)=0\\)\n\n\n\n\n\nUse Axioms!\n\n\nA1: \\(0\\leq\\mathbb{P}(A)\\leq 1\\)\nA2: \\(\\mathbb{P}(S)=1\\)\nA3: For disjoint \\(A_i\\),\n\\(\\mathbb{P}\\Big( \\bigcup \\limits_{i=1}^{\\infty}A_i\\Big) =  \\sum_{i=1}^{\\infty}\\mathbb{P}(A_i)\\)"
  },
  {
    "objectID": "lessons/03_Lang_prob/03_Lang_prob.html#proposition-3-proof",
    "href": "lessons/03_Lang_prob/03_Lang_prob.html#proposition-3-proof",
    "title": "Lesson 3: Language of Probability",
    "section": "Proposition 3 Proof",
    "text": "Proposition 3 Proof\n\n\n\n\nProposition 3\n\n\nIf \\(A \\subseteq B\\), then \\(\\mathbb{P}(A) \\leq \\mathbb{P}(B)\\)\n\n\n\n\n\nUse Axioms!\n\n\nA1: \\(0\\leq\\mathbb{P}(A)\\leq 1\\)\nA2: \\(\\mathbb{P}(S)=1\\)\nA3: For disjoint \\(A_i\\),\n\\(\\mathbb{P}\\Big( \\bigcup \\limits_{i=1}^{\\infty}A_i\\Big) =  \\sum_{i=1}^{\\infty}\\mathbb{P}(A_i)\\)"
  },
  {
    "objectID": "lessons/03_Lang_prob/03_Lang_prob.html#proposition-4-visual-proof",
    "href": "lessons/03_Lang_prob/03_Lang_prob.html#proposition-4-visual-proof",
    "title": "Lesson 3: Language of Probability",
    "section": "Proposition 4 Visual Proof",
    "text": "Proposition 4 Visual Proof\n\n\nProposition 4\n\n\n\\(\\mathbb{P}(A \\cup B) = \\mathbb{P}(A) + \\mathbb{P}(B) - \\mathbb{P}(A \\cap B)\\)"
  },
  {
    "objectID": "lessons/03_Lang_prob/03_Lang_prob.html#proposition-5-visual-proof",
    "href": "lessons/03_Lang_prob/03_Lang_prob.html#proposition-5-visual-proof",
    "title": "Lesson 3: Language of Probability",
    "section": "Proposition 5 Visual Proof",
    "text": "Proposition 5 Visual Proof\n\n\nProposition 5\n\n\n\\(\\mathbb{P}(A \\cup B \\cup C) = \\mathbb{P}(A) + \\mathbb{P}(B) + \\mathbb{P}(C) - \\mathbb{P}(A \\cap B) - \\mathbb{P}(A \\cap C) - \\mathbb{P}(B \\cap C) + \\mathbb{P}(A \\cap B \\cap C)\\)"
  },
  {
    "objectID": "lessons/03_Lang_prob/03_Lang_prob.html#some-final-remarks-on-these-proposition",
    "href": "lessons/03_Lang_prob/03_Lang_prob.html#some-final-remarks-on-these-proposition",
    "title": "Lesson 3: Language of Probability",
    "section": "Some final remarks on these proposition",
    "text": "Some final remarks on these proposition\n\nNotice how we spliced events into multiple disjoint events\n\nIt is often easier to work with disjoint events\n\n\n \n\nIf we want to calculate the probability for one event, we may need to get creative with how we manipulate other events and the sample space\n\nHelps us use any incomplete information we have"
  },
  {
    "objectID": "lessons/03_Lang_prob/03_Lang_prob.html#de-morgans-laws",
    "href": "lessons/03_Lang_prob/03_Lang_prob.html#de-morgans-laws",
    "title": "Lesson 3: Language of Probability",
    "section": "De Morgan’s Laws",
    "text": "De Morgan’s Laws\n\n\nTheorem: De Morgan’s 1st Law\n\n\nFor a collection of events (sets) \\(A_1, A_2, A_3, \\ldots\\)\n\\[\\bigcap\\limits_{i=1}^{n}A_i^C = \\Big(\\bigcup\\limits_{i=1}^{n}A_i\\Big)^C\\]\n\n\n“all not A = \\((\\)at least one event A\\()^C\\)” or “intersection of the complements is the complement of the union”\n\n\nTheorem: De Morgan’s 2nd Law\n\n\nFor a collection of events (sets) \\(A_1, A_2, A_3, \\ldots\\)\n\\[\\bigcup\\limits_{i=1}^{n}A_i^C = \\Big(\\bigcap\\limits_{i=1}^{n}A_i\\Big)^C\\]\n\n\n“at least one event not A = \\((\\)all A\\()^C\\)” or “union of complements is complement of the intersection”"
  },
  {
    "objectID": "lessons/03_Lang_prob/03_Lang_prob.html#bp-example-variation-13",
    "href": "lessons/03_Lang_prob/03_Lang_prob.html#bp-example-variation-13",
    "title": "Lesson 3: Language of Probability",
    "section": "BP example variation (1/3)",
    "text": "BP example variation (1/3)\n\nSuppose you have \\(n\\) subjects in a study.\nLet \\(H_i\\) be the event that person \\(i\\) has high BP, for \\(i=1\\ldots n\\).\n\n \nUse set theory notation to denote the following events:\n\nEvent subject \\(i\\) does not have high BP\nEvent all \\(n\\) subjects have high BP\nEvent at least one subject has high BP\nEvent all of them do not have high BP\nEvent at least one subject does not have high BP"
  },
  {
    "objectID": "lessons/03_Lang_prob/03_Lang_prob.html#bp-example-variation-23",
    "href": "lessons/03_Lang_prob/03_Lang_prob.html#bp-example-variation-23",
    "title": "Lesson 3: Language of Probability",
    "section": "BP example variation (2/3)",
    "text": "BP example variation (2/3)\n\nSuppose you have \\(n\\) subjects in a study.\nLet \\(H_i\\) be the event that person \\(i\\) has high BP, for \\(i=1\\ldots n\\).\n\nUse set theory notation to denote the following events:\n\nEvent subject \\(i\\) does not have high BP\n \nEvent all \\(n\\) subjects have high BP\n \n \nEvent at least one subject has high BP"
  },
  {
    "objectID": "lessons/03_Lang_prob/03_Lang_prob.html#bp-example-variation-33",
    "href": "lessons/03_Lang_prob/03_Lang_prob.html#bp-example-variation-33",
    "title": "Lesson 3: Language of Probability",
    "section": "BP example variation (3/3)",
    "text": "BP example variation (3/3)\n\nEvent all of them do not have high BP\n \n \n \nEvent at least one subject does not have high BP"
  },
  {
    "objectID": "lessons/03_Lang_prob/03_Lang_prob.html#remarks-on-de-morgans-laws",
    "href": "lessons/03_Lang_prob/03_Lang_prob.html#remarks-on-de-morgans-laws",
    "title": "Lesson 3: Language of Probability",
    "section": "Remarks on De Morgan’s Laws",
    "text": "Remarks on De Morgan’s Laws\n\nThese laws also hold for infinite collections of events.\n \nDraw Venn diagrams to convince yourself that these are true!\n \nThese laws are very useful when calculating probabilities.\n\nThis is because calculating the probability of the intersection of events is often much easier than the union of events.\nThis is not obvious right now, but we will see in the coming chapters why."
  },
  {
    "objectID": "lessons/03_Lang_prob/03_Lang_prob.html#partitions",
    "href": "lessons/03_Lang_prob/03_Lang_prob.html#partitions",
    "title": "Lesson 3: Language of Probability",
    "section": "Partitions",
    "text": "Partitions\n\n\n\n\nDefinition: Partition\n\n\nA set of events \\(\\{A_i\\}_{i=1}^{n}\\) create a partition of \\(A\\), if\n\nthe \\(A_i\\)’s are disjoint (mutually exclusive) and\n\\(\\bigcup \\limits_{i=1}^n A_i = A\\)\n\n\n\n\n\nExample 2\n\n\n\nIf \\(A \\subset B\\), then \\(\\{A, B \\cap A^C\\}\\) is a partition of \\(B\\).\nIf \\(S = \\bigcup \\limits_{i=1}^n A_i\\), and the \\(A_i\\)’s are disjoint, then the \\(A_i\\)’s are a partition of the sample space.\n\n\n\n\n\n\n\nCreating partitions is sometimes used to help calculate probabilities, since by Axiom 3 we can add the probabilities of disjoint events."
  },
  {
    "objectID": "lessons/03_Lang_prob/03_Lang_prob.html#weekly-medications",
    "href": "lessons/03_Lang_prob/03_Lang_prob.html#weekly-medications",
    "title": "Lesson 3: Language of Probability",
    "section": "Weekly medications",
    "text": "Weekly medications\n\n\n\n\nExample 3\n\n\nIf a subject has an\n\n80% chance of taking their medication this week,\n70% chance of taking their medication next week, and\n10% chance of not taking their medication either week,\n\nthen find the probability of them taking their medication exactly one of the two weeks.\n\n\n\nHint: Draw a Venn diagram labelling each of the parts to find the probability."
  },
  {
    "objectID": "lessons/02_Simulations/02_Simulations_key_info.html",
    "href": "lessons/02_Simulations/02_Simulations_key_info.html",
    "title": "Key Info",
    "section": "",
    "text": "Attendance policy: a little confusing"
  },
  {
    "objectID": "lessons/02_Simulations/02_Simulations_key_info.html#announcements",
    "href": "lessons/02_Simulations/02_Simulations_key_info.html#announcements",
    "title": "Key Info",
    "section": "",
    "text": "Attendance policy: a little confusing"
  },
  {
    "objectID": "lessons/02_Simulations/02_Simulations_key_info.html#key-dates",
    "href": "lessons/02_Simulations/02_Simulations_key_info.html#key-dates",
    "title": "Key Info",
    "section": "Key dates",
    "text": "Key dates\n\nHomework 0 is due TOMORROW at 11pm!"
  },
  {
    "objectID": "lessons/02_Simulations/02_Simulations_key_info.html#last-class",
    "href": "lessons/02_Simulations/02_Simulations_key_info.html#last-class",
    "title": "Key Info",
    "section": "Last class",
    "text": "Last class\n\nDid not finish Lesson 1, so we will finish!"
  },
  {
    "objectID": "lessons/19_Moment_Generating_Functions/21_Moment_Generating_Functions.html",
    "href": "lessons/19_Moment_Generating_Functions/21_Moment_Generating_Functions.html",
    "title": "Chapter 43: Moment Generating Functions Part 2",
    "section": "",
    "text": "Chapter 43: Moment Generating Functions Part 2\nRecap: What is an mgf?\n\nExample 1.   Let \\(X\\) be a random variable with mgf \\[M_X(t)= \\frac{1}{5}e^t + \\frac{3}{10}e^{2t} + \\frac{1}{2}e^{3t}.\\] Find the pmf or pdf of \\(X\\).\n\n\nExample 2.   Let \\(X\\) be a normal random variable with mean \\(\\mu\\) and variance \\(\\sigma^2\\), i.e. \\(X \\sim N(\\mu,\\sigma^2)\\).\n\nFind the mgf of \\(X\\).\nFind \\(\\mathbb{E}[X]\\).\nFind \\(Var(X)\\).\n\n\n\n\nTheorem 3.   Let \\(X\\) have mgf \\(M_X(t)\\), and let \\(Y=aX+b\\), where \\(a\\) and \\(b\\) are constants. Then \\[M_Y(t)=\\]\n\n\nProof. Proof. ◻\n\nQuestion: Do linear transformations always preserve the distribution type?\nI.e., if \\(X\\) has a certain probability distribution, does \\(aX+b\\) always have the same distribution type?\n\n\nExample 4.   Let \\(X \\sim U[0,1]\\), and \\(Y = 2X+3\\). Is \\(Y\\) also a uniform rv? If so, what are its parameters?\n\n\nExample 5.   Let \\(X \\sim Exp(\\lambda=5)\\), and \\(Y = 2X+3\\). Is \\(Y\\) also an exponential rv? If so, what is its parameter?\n\nMgf’s of Sums of Independent rv’s\n\n\nTheorem 6.   Let \\(X_1, X_2, \\ldots, X_n\\) be independent rv’s with respective mgf’s \\(M_{X_i}(t)\\), for \\(i=1,2,\\ldots,n\\). Let \\(Y=\\sum_{i=1}^n a_iX_i\\), where \\(a_i\\) are constants. Then \\[M_Y(t)= %\\Pi_{i=1}^n M_{X_i}(a_it).\\]\n\n\nProof. Proof. ◻\n\n\n\nExample 7.   Let \\(X_i \\sim N(\\mu_i, \\sigma_i^2)\\) be independent normal rv’s. What is the distribution of  \\(Y=\\sum_{i=1}^n X_i\\)?\n\n\n\nExample 8.   Let \\(X_i \\sim N(\\mu, \\sigma^2)\\) be iid normal rv’s, for \\(i=1,2,\\ldots,n\\). What is the distribution of  \\(\\bar{X}=\\frac{\\sum_{i=1}^n X_i}{n}\\)?\n\n\nExample 9.   Let \\(Z\\) be a standard normal random variable, i.e. \\(Z \\sim N(0,1)\\). Show that \\(Z^2 \\sim \\chi_1^2\\), i.e. is a chi-squared rv with 1 degree of freedom."
  },
  {
    "objectID": "lessons/17_Central_Limit_Theorem/17_Central_Limit_Theorem_key_info.html",
    "href": "lessons/17_Central_Limit_Theorem/17_Central_Limit_Theorem_key_info.html",
    "title": "Key Info",
    "section": "",
    "text": "We will finish up the example from last class\nNo class on 11/26 and 12/1\nBiostatistics tutoring starts today\n\nSchedule a meeting here\nOr email Charles directly\n\n\n\n\nThe Division of Biostatistics in the Department of Preventive Medicine at the University of Tennessee Health Science Center in Memphis, Tennessee invites applications for paid internships in Biomedical Data Science.\nWe expect to have at least two positions for the summer 2026. Interns will work with one or more of our faculty members on a statistical project of mutual interest. The work is expected to lead to a tangible research product, such as a software package or methodological paper. Interested individuals must have strong programming skills in at least one language (eg. R, Python, Julia, C/C++, SAS), possess basic data analysis skills, and have a willingness to learn biomedical science.\nApplicants should possess a relevant (e.g. statistics, mathematics, computer science) bachelor’s or master’s degree; undergraduate students must be rising juniors or seniors. Most internships are 20-40 hours per week for 10 weeks, but total hours are negotiable. Internships may be paid or for credit at the student’s home institution, depending on institutional policies, student and faculty mentor preference.\nMore info: www.uthsc.edu/preventive-medicine/internships.php\nAlumni info: www.uthsc.edu/preventive-medicine/internshipalumni.php"
  },
  {
    "objectID": "lessons/17_Central_Limit_Theorem/17_Central_Limit_Theorem_key_info.html#announcements",
    "href": "lessons/17_Central_Limit_Theorem/17_Central_Limit_Theorem_key_info.html#announcements",
    "title": "Key Info",
    "section": "",
    "text": "We will finish up the example from last class\nNo class on 11/26 and 12/1\nBiostatistics tutoring starts today\n\nSchedule a meeting here\nOr email Charles directly\n\n\n\n\nThe Division of Biostatistics in the Department of Preventive Medicine at the University of Tennessee Health Science Center in Memphis, Tennessee invites applications for paid internships in Biomedical Data Science.\nWe expect to have at least two positions for the summer 2026. Interns will work with one or more of our faculty members on a statistical project of mutual interest. The work is expected to lead to a tangible research product, such as a software package or methodological paper. Interested individuals must have strong programming skills in at least one language (eg. R, Python, Julia, C/C++, SAS), possess basic data analysis skills, and have a willingness to learn biomedical science.\nApplicants should possess a relevant (e.g. statistics, mathematics, computer science) bachelor’s or master’s degree; undergraduate students must be rising juniors or seniors. Most internships are 20-40 hours per week for 10 weeks, but total hours are negotiable. Internships may be paid or for credit at the student’s home institution, depending on institutional policies, student and faculty mentor preference.\nMore info: www.uthsc.edu/preventive-medicine/internships.php\nAlumni info: www.uthsc.edu/preventive-medicine/internshipalumni.php"
  },
  {
    "objectID": "lessons/17_Central_Limit_Theorem/17_Central_Limit_Theorem_key_info.html#key-dates",
    "href": "lessons/17_Central_Limit_Theorem/17_Central_Limit_Theorem_key_info.html#key-dates",
    "title": "Key Info",
    "section": "Key Dates",
    "text": "Key Dates\n\nQuiz 3 opens 12/3\nI posted some info on it\nHW 8 due 12/7"
  },
  {
    "objectID": "lessons/09_CDFs/09_CDFs_key_info.html",
    "href": "lessons/09_CDFs/09_CDFs_key_info.html",
    "title": "Key Info and Announcements",
    "section": "",
    "text": "Quiz 1 questions?\nCharles is still grading HW 2\n\nPosted solutions so you have as a reference during quiz\n\nIf links do not work on this site, there are two places where the files live:\n\nGithub page: https://github.com/nwakim/BSTA_550_25F\n\nLink also on this homepage\n\nOHSU OneDrive\n\nYou should have access to this folder\n\n\nIf you want to attend class virtually when we’re in-person, then you can use: https://pdx.zoom.us/j/82027616951\n\nHowever, I will not support virtual attendance when we’re in-person\nI will not be monitoring the chat or helping with technical issues"
  },
  {
    "objectID": "lessons/09_CDFs/09_CDFs_key_info.html#key-info",
    "href": "lessons/09_CDFs/09_CDFs_key_info.html#key-info",
    "title": "Key Info and Announcements",
    "section": "",
    "text": "Quiz 1 questions?\nCharles is still grading HW 2\n\nPosted solutions so you have as a reference during quiz\n\nIf links do not work on this site, there are two places where the files live:\n\nGithub page: https://github.com/nwakim/BSTA_550_25F\n\nLink also on this homepage\n\nOHSU OneDrive\n\nYou should have access to this folder\n\n\nIf you want to attend class virtually when we’re in-person, then you can use: https://pdx.zoom.us/j/82027616951\n\nHowever, I will not support virtual attendance when we’re in-person\nI will not be monitoring the chat or helping with technical issues"
  },
  {
    "objectID": "lessons/09_CDFs/09_CDFs_key_info.html#announcements",
    "href": "lessons/09_CDFs/09_CDFs_key_info.html#announcements",
    "title": "Key Info and Announcements",
    "section": "Announcements",
    "text": "Announcements\n\nHomework 3 due 10/26 at 11pm\nQuiz 1 opens on 10/22 at 3pm\n\nCloses 10/26 at 11pm"
  },
  {
    "objectID": "lessons/11_Joint_distributions/11_Transformations.html",
    "href": "lessons/11_Joint_distributions/11_Transformations.html",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "",
    "text": "Calculate probabilities for a pair of discrete random variables\nCalculate a joint, marginal, and conditional probability mass function (pmf)\nCalculate a joint, marginal, and conditional cumulative distribution function (CDF)"
  },
  {
    "objectID": "lessons/11_Joint_distributions/11_Transformations.html#learning-objectives",
    "href": "lessons/11_Joint_distributions/11_Transformations.html#learning-objectives",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "",
    "text": "Calculate probabilities for a pair of discrete random variables\nCalculate a joint, marginal, and conditional probability mass function (pmf)\nCalculate a joint, marginal, and conditional cumulative distribution function (CDF)"
  },
  {
    "objectID": "lessons/11_Joint_distributions/11_Transformations.html#where-are-we",
    "href": "lessons/11_Joint_distributions/11_Transformations.html#where-are-we",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "Where are we?",
    "text": "Where are we?"
  },
  {
    "objectID": "lessons/11_Joint_distributions/11_Transformations.html#what-is-a-joint-pmf",
    "href": "lessons/11_Joint_distributions/11_Transformations.html#what-is-a-joint-pmf",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "What is a joint pmf?",
    "text": "What is a joint pmf?\n\n\nDefinition: joint pmf\n\n\nThe joint pmf of a pair of discrete r.v.’s \\(X\\) and \\(Y\\) is \\[p_{X,Y}(x,y) = \\mathbb{P}(X=x\\ and\\ Y=y) = \\mathbb{P}(X=x, Y=y)\\]"
  },
  {
    "objectID": "lessons/11_Joint_distributions/11_Transformations.html#this-chapters-main-example",
    "href": "lessons/11_Joint_distributions/11_Transformations.html#this-chapters-main-example",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "This chapter’s main example",
    "text": "This chapter’s main example\n\n\n\n\nExample 1\n\n\nLet \\(X\\) and \\(Y\\) be two random draws from a box containing balls labelled 1, 2, and 3 without replacement.\n\nFind \\(p_{X,Y}(x,y)\\).\nFind \\(\\mathbb{P}(X+Y=3).\\)\nFind \\(\\mathbb{P}(Y = 1).\\)\nFind \\(\\mathbb{P}(Y \\leq 2).\\)\nFind the joint CDF \\(F_{X,Y}(x,y)\\) for the joint pmf \\(p_{X,Y}(x,y)\\)\nFind the marginal CDFs \\(F_{X}(x)\\) and \\(F_{Y}(y)\\)\nFind \\(p_{X|Y}(x|y)\\).\nAre \\(X\\) and \\(Y\\) independent? Why or why not?"
  },
  {
    "objectID": "lessons/11_Joint_distributions/11_Transformations.html#joint-pmf",
    "href": "lessons/11_Joint_distributions/11_Transformations.html#joint-pmf",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "Joint pmf",
    "text": "Joint pmf\n\n\n\n\nExample 1\n\n\nLet \\(X\\) and \\(Y\\) be two random draws from a box containing balls labelled 1, 2, and 3 without replacement.\n\nFind \\(p_{X,Y}(x,y)\\).\nFind \\(\\mathbb{P}(X+Y=3).\\)"
  },
  {
    "objectID": "lessons/11_Joint_distributions/11_Transformations.html#marginal-pmfs",
    "href": "lessons/11_Joint_distributions/11_Transformations.html#marginal-pmfs",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "Marginal pmf’s",
    "text": "Marginal pmf’s\n\n\n\n\nExample 1\n\n\nLet \\(X\\) and \\(Y\\) be two random draws from a box containing balls labelled 1, 2, and 3 without replacement.\n\nFind \\(\\mathbb{P}(Y = 1).\\)\nFind \\(\\mathbb{P}(Y \\leq 2).\\)"
  },
  {
    "objectID": "lessons/11_Joint_distributions/11_Transformations.html#remarks-on-the-joint-pmf",
    "href": "lessons/11_Joint_distributions/11_Transformations.html#remarks-on-the-joint-pmf",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "Remarks on the joint pmf",
    "text": "Remarks on the joint pmf\nSome properties of joint pmf’s:\n\nA joint pmf \\(p_{X,Y}(x,y)\\) must satisfy the following properties:\n\n\\(p_{X,Y}(x,y)\\geq 0\\) for all \\(x, y\\).\n\\(\\sum \\limits_{\\{all\\ x\\}} \\sum \\limits_{\\{all\\ y\\}} p_{X,Y}(x,y)=1\\).\n\nMarginal pmf’s:\n\n\\(p_X(x) = \\sum \\limits_{\\{all\\ y\\}} p_{X,Y}(x,y)\\)\n\\(p_Y(y) = \\sum \\limits_{\\{all\\ x\\}} p_{X,Y}(x,y)\\)"
  },
  {
    "objectID": "lessons/11_Joint_distributions/11_Transformations.html#what-is-a-joint-cdf",
    "href": "lessons/11_Joint_distributions/11_Transformations.html#what-is-a-joint-cdf",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "What is a joint CDF?",
    "text": "What is a joint CDF?\n\n\nDefinition: joint CDF\n\n\nThe joint CDF of a pair of discrete r.v.’s \\(X\\) and \\(Y\\) is \\[F_{X,Y}(x,y) = \\mathbb{P}(X \\leq x\\ and\\ Y \\leq y) = \\mathbb{P}(X \\leq x, Y \\leq y)\\]"
  },
  {
    "objectID": "lessons/11_Joint_distributions/11_Transformations.html#joint-cdfs",
    "href": "lessons/11_Joint_distributions/11_Transformations.html#joint-cdfs",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "Joint CDFs",
    "text": "Joint CDFs\n\n\n\n\nExample 1\n\n\nLet \\(X\\) and \\(Y\\) be two random draws from a box containing balls labelled 1, 2, and 3 without replacement.\n\nFind the joint CDF \\(F_{X,Y}(x,y)\\) for the joint pmf \\(p_{X,Y}(x,y)\\)"
  },
  {
    "objectID": "lessons/11_Joint_distributions/11_Transformations.html#marginal-cdfs",
    "href": "lessons/11_Joint_distributions/11_Transformations.html#marginal-cdfs",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "Marginal CDFs",
    "text": "Marginal CDFs\n\n\n\n\nExample 1\n\n\nLet \\(X\\) and \\(Y\\) be two random draws from a box containing balls labelled 1, 2, and 3 without replacement.\n\nFind the marginal CDFs \\(F_{X}(x)\\) and \\(F_{Y}(y)\\)"
  },
  {
    "objectID": "lessons/11_Joint_distributions/11_Transformations.html#remarks-on-the-joint-and-marginal-cdf",
    "href": "lessons/11_Joint_distributions/11_Transformations.html#remarks-on-the-joint-and-marginal-cdf",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "Remarks on the joint and marginal CDF",
    "text": "Remarks on the joint and marginal CDF\n\n\\(F_X(x)\\): right most columns of the CDf table (where the \\(Y\\) values are largest)\n\\(F_Y(y)\\): bottom row of the table (where X values are largest)\n\\(F_X(x)=\\lim\\limits_{y\\rightarrow\\infty}F_{X, Y}(x,y)\\)\n\\(F_Y(y)=\\lim\\limits_{x\\rightarrow\\infty}F_{X, Y}(x,y)\\)"
  },
  {
    "objectID": "lessons/11_Joint_distributions/11_Transformations.html#independence-and-conditioning",
    "href": "lessons/11_Joint_distributions/11_Transformations.html#independence-and-conditioning",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "Independence and Conditioning",
    "text": "Independence and Conditioning\nRecall that for events \\(A\\) and \\(B\\),\n\n\\(\\mathbb{P}(A|B) = \\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(B)}\\)\n\\(A\\) and \\(B\\) are independent if and only if\n\n\\(\\mathbb{P}(A|B) = \\mathbb{P}(A)\\)\n\\(\\mathbb{P}(A \\cap B) = \\mathbb{P}(A)\\cdot\\mathbb{P}(B)\\)\n\n\nIndependence and conditioning are defined similarly for r.v.’s, since \\[p_X(x) = \\mathbb{P}(X=x)\\ \\mathrm{and}\\ \\ p_{X,Y}(x,y) = \\mathbb{P}(X = x ,Y = y).\\]"
  },
  {
    "objectID": "lessons/11_Joint_distributions/11_Transformations.html#what-is-the-conditional-pmf",
    "href": "lessons/11_Joint_distributions/11_Transformations.html#what-is-the-conditional-pmf",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "What is the conditional pmf?",
    "text": "What is the conditional pmf?\n\n\nDefinition: conditional pmf\n\n\nThe conditional pmf of a pair of discrete r.v.’s \\(X\\) and \\(Y\\) is defined as \\[p_{X|Y}(x|y) = \\mathbb{P}(X = x |Y = y) = \\frac{\\mathbb{P}(X = x\\ and\\ Y = y)}{\\mathbb{P}(Y = y)}\n=\\frac{p_{X,Y}(x,y) }{p_{Y}(y) }\\] if \\(p_{Y}(y) &gt; 0\\)."
  },
  {
    "objectID": "lessons/11_Joint_distributions/11_Transformations.html#remarks-on-the-conditional-pmf",
    "href": "lessons/11_Joint_distributions/11_Transformations.html#remarks-on-the-conditional-pmf",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "Remarks on the conditional pmf",
    "text": "Remarks on the conditional pmf\nThe following properties follow from the conditional pmf definition:\n\nIf \\(X \\perp Y\\) (independent)\n\n\\(p_{X|Y}(x|y) = p_X(x)\\) for all \\(x\\) and \\(y\\)\n\\(p_{X,Y}(x,y) = p_X(x)p_Y(y)\\) for all \\(x\\) and \\(y\\)\nWhich also implies (\\(\\Rightarrow\\)): \\(F_{X,Y}(x,y) = F_X(x)F_Y(y)\\) for all \\(x\\) and \\(y\\)\n\nIf \\(X_1, X_2, …, X_n\\) are independent\n\n\\[p_{X_1, X_2, …, X_n}(x_1, x_2, …, x_n) = P(X_1=x_1, X_2=x_2, …, X_n=x_n)=\\prod\\limits_{i=1}^np_{X_i}(x_i)\\]\n\\[F_{X_1, X_2, …, X_n}(x_1, x_2, …, x_n) = P(X_1\\leq x_1, X_2\\leq x_2, …, X_n\\leq x_n)=\\prod\\limits_{i=1}^nP(X_i \\leq x_i) = \\prod\\limits_{i=1}^nF_{X_i}(x_i)\\]"
  },
  {
    "objectID": "lessons/11_Joint_distributions/11_Transformations.html#conditional-pmfs",
    "href": "lessons/11_Joint_distributions/11_Transformations.html#conditional-pmfs",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "Conditional pmf’s",
    "text": "Conditional pmf’s\n\n\n\n\nExample 1\n\n\nLet \\(X\\) and \\(Y\\) be two random draws from a box containing balls labelled 1, 2, and 3 without replacement.\n\nFind \\(p_{X|Y}(x|y)\\).\nAre \\(X\\) and \\(Y\\) independent? Why or why not?\n\n\n\n\n\nRemark:\n\nTo show that \\(X\\) and \\(Y\\) are not independent, we just need to find one counter example\nHowever, to show that they are independent, we need to verify this for all possible pairs of \\(x\\) and \\(y\\)"
  },
  {
    "objectID": "lessons/11_Joint_distributions/11_Transformations.html#hypothetical-4-sided-die",
    "href": "lessons/11_Joint_distributions/11_Transformations.html#hypothetical-4-sided-die",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "Hypothetical 4-sided die",
    "text": "Hypothetical 4-sided die\n\n\nExample 3\n\n\n\nSuppose you have a 4-sided die, and you roll the 4-sided die until the first 4 appears.\nLet \\(X\\) be the number of rolls required until (and including) the first 4.\nAfter the first 4, you keep rolling it again until you roll a 3.\nLet \\(Y\\) be the number of rolls, after the first 4, required until (and including) the 3.\n\n\nFind \\(p_{X,Y}(x,y)\\).\nUsing \\(p_{X,Y}(x,y)\\), find \\(p_{Y}(y)\\).\nFind \\(p_{X}(x)\\).\nAre \\(X\\) and \\(Y\\) are independent? Why or why not?\nFind \\(F_{X,Y}(x,y)\\)."
  },
  {
    "objectID": "lessons/11_Joint_distributions/11_Joint_distributions_muddy_points.html",
    "href": "lessons/11_Joint_distributions/11_Joint_distributions_muddy_points.html",
    "title": "Muddy Points",
    "section": "",
    "text": "Yes! \\(M\\) is just a transformation of \\(X\\) and \\(Y\\)! Think of creating a new random variable that is dependent on \\(X\\) and \\(Y\\). We have created a single random variable from two. Then we want to see for the random variable, M, what does its pdf look like?\n\n\n\nYes! If we are going from a joint pdf to a marginal, we can to integrate over one of the random variables. If we want a marginal of X, then we integrate over Y, and that will be a single integral.\nIf we are working with a joint pdf, then we need to have two integrals if we have two random variables.\n\n\n\nI feel that. In general, a transformation (say \\(M\\)) of \\(X\\) and \\(Y\\) will be in the 3rd dimension (3D plot). The joint pdf of \\(X\\) and \\(Y\\) will be in the 3rd dimension (3D plot). But once we have \\(M\\) and its CDF/pdf, those are in 2D plot of \\(M\\) vs. \\(f_M(m)\\)."
  },
  {
    "objectID": "lessons/11_Joint_distributions/11_Joint_distributions_muddy_points.html#fall-2025",
    "href": "lessons/11_Joint_distributions/11_Joint_distributions_muddy_points.html#fall-2025",
    "title": "Muddy Points",
    "section": "",
    "text": "Yes! \\(M\\) is just a transformation of \\(X\\) and \\(Y\\)! Think of creating a new random variable that is dependent on \\(X\\) and \\(Y\\). We have created a single random variable from two. Then we want to see for the random variable, M, what does its pdf look like?\n\n\n\nYes! If we are going from a joint pdf to a marginal, we can to integrate over one of the random variables. If we want a marginal of X, then we integrate over Y, and that will be a single integral.\nIf we are working with a joint pdf, then we need to have two integrals if we have two random variables.\n\n\n\nI feel that. In general, a transformation (say \\(M\\)) of \\(X\\) and \\(Y\\) will be in the 3rd dimension (3D plot). The joint pdf of \\(X\\) and \\(Y\\) will be in the 3rd dimension (3D plot). But once we have \\(M\\) and its CDF/pdf, those are in 2D plot of \\(M\\) vs. \\(f_M(m)\\)."
  },
  {
    "objectID": "lessons/11_Joint_distributions/11_Joint_distributions_muddy_points.html#fall-2024",
    "href": "lessons/11_Joint_distributions/11_Joint_distributions_muddy_points.html#fall-2024",
    "title": "Muddy Points",
    "section": "Fall 2024",
    "text": "Fall 2024\n\n1. What are we actually finding by solving the double integral. In the first example, we found the probability was 1/16 after integrating but what does 1/16 mean in relation to the random variables X and Y?\nIt means that the volume contained by \\(0\\leq X \\leq 1\\), \\(0\\leq Y \\leq 1/2\\), and their joint pdf is 1/16 of the total volume contained by \\(0\\leq X \\leq 2\\), \\(0\\leq Y \\leq 1\\), and their joint pdf. The probability for a joint pdf is now a measure of the proportion of the volume.\nThis is not be confused with a probability from marginal pdf or pdf from one RV. The probability for marginal/single RV pdfs is the proportion of the area under the pdf for a specific range of values.\n\n\n2. Here’s a 3D plot of one of our joint pdf’s\n\\[\nf_{X,Y}(x,y) = 5e^{-x-3y} \\text{ for } 0 \\leq y \\leq x/2\n\\]\n\nlibrary(plotly)\n\nx = seq(0, 5, 0.1)\ny = seq(0, max(x)/2, 0.1/2)\nfn = expand.grid(x=x,y=y)\nfn$z = ifelse(fn$y&lt;fn$x/2, 5*exp( (-1)*fn$x - 3*fn$y), NA)\n\nz = matrix(fn$z, ncol = 51, nrow = 51, byrow = T)\n\nfig &lt;- plot_ly(x = x, y=y, z=z) %&gt;% add_surface()\n\nfig"
  },
  {
    "objectID": "lessons/11_Joint_distributions/11_Joint_distributions_muddy_points.html#fall-2023",
    "href": "lessons/11_Joint_distributions/11_Joint_distributions_muddy_points.html#fall-2023",
    "title": "Muddy Points",
    "section": "Fall 2023",
    "text": "Fall 2023\n\n1. How do we set the bounds on a double integral?\nWhen the domain of the RVs are not dependent on each other, then we use the bounds as is. In example 2 of Chapter 26 notes, we have the joint pdf \\(f_{X,Y}(x,y)= 18 x^2 y^5\\), for\\(0 \\leq x \\leq 1, \\ 0 \\leq y \\leq 1\\). If we wanted to calculate something like \\(E(X)\\), then we could use the bounds as they are. Below is the domain for \\(x\\) and \\(y\\):\n\n\n\n\n\nHere is the integral for the expected value where we integrate over the whole domain of \\(x\\) and \\(y\\):\n\\[ E(X) = \\displaystyle\\int_0^1 \\displaystyle\\int_0^1 x (18 x^2 y^5 )dy dx \\]If we want to find the probability \\(P(0.25 \\leq X \\leq 0.5, 0.5 \\leq Y \\leq 0.75)\\), then we can look at the specific values of the probability:\n\n\n\n\n\nNote the blue lines above indicate how we integrate over \\(y\\) first from 0.5 to 0.75 and the green lines indicate how integrate over \\(x\\) first from 0.25 to 0.5. It seems like we’ve integrated over an area that isn’t within our specified probability. However, the integrated area is ONLY the overlap of the \\(x\\) and \\(y\\) bounds for the probability.\n\\[P(0.25 \\leq X \\leq 0.5, 0.5 \\leq Y \\leq 0.75) = \\displaystyle\\int_{0.25}^{0.5} \\displaystyle\\int_{0.5}^{0.75} 18 x^2 y^5 dy dx\\]\nLet’s use the same pdf, but now the domain of the two RVs is dependent on one another. We have the joint pdf \\(f_{X,Y}(x,y)= 18 x^2 y^5\\), for\\(0 \\leq x \\leq y\\leq1\\)\nIf we wanted to calculate something like \\(E(X)\\), then we need to account for fact that \\(x\\) must be less than of equal to \\(y\\). We can look back at the domain for this:\n\n\n\n\n\nNote the blue lines above still indicate how we integrate over \\(y\\) first from \\(x\\) to 1, and the green lines indicate how integrate over \\(x\\) first from 0 to 1. Once again, it seems like we’ve integrated over an area that isn’t within the domain. However, the integrated area is ONLY the overlap of the \\(x\\) and \\(y\\) bounds. Thus, once we’ve restricted \\(y\\) to the area between \\(x\\) and 1, we no longer need to restrict \\(x\\) to the are of 0 to \\(y\\).\n\\[ E(X) = \\displaystyle\\int_0^1 \\displaystyle\\int_x^1 x (18 x^2 y^5 )dy dx \\] If we want to find the probability \\(P(0.25 \\leq X \\leq 0.5, 0.5 \\leq Y \\leq 0.75)\\), then we should look back at our domain. For now, we are focusing on the orange area:\n\n\n\n\n\nBecause the orange area is totally within our domain, we can leave our integral our bounds as the exact values we specified:\n\\[P(0.25 \\leq X \\leq 0.5, 0.5 \\leq Y \\leq 0.75) = \\displaystyle\\int_{0.25}^{0.5} \\displaystyle\\int_{0.5}^{0.75} 18 x^2 y^5 dy dx\\]\nHowever, if we want the probability \\(P(0.5 \\leq X \\leq 0.75, 0.5 \\leq Y \\leq 0.75)\\), we would focus on the pink area above. We would limit one of our integrals to the \\(y=x\\) equation:\n\\[P(0.5 \\leq X \\leq 0.75, 0.5 \\leq Y \\leq 0.75) = \\displaystyle\\int_{0.5}^{0.75} \\displaystyle\\int_{x}^{0.75} 18 x^2 y^5 dy dx\\]\nOR\n\\[P(0.5 \\leq X \\leq 0.75, 0.5 \\leq Y \\leq 0.75) = \\displaystyle\\int_{0.5}^{0.75} \\displaystyle\\int_{0.5}^{y} 18 x^2 y^5 dx dy\\]\nThe key to these probabilities is that the bounds with the other variable is on the inside integral! Otherwise we end up with a answer that includes a RV."
  },
  {
    "objectID": "lessons/11_Joint_distributions/11_Joint_distributions_review_key_info.html",
    "href": "lessons/11_Joint_distributions/11_Joint_distributions_review_key_info.html",
    "title": "Key Info",
    "section": "",
    "text": "Midterm feedback!\n\nDid you fill out the final survey to record your name?\nDislikes/suggestions\n\nTextbook\nWant more R code\npdf lectures posted earlier\nSome in-class work\nSlower on slides + more examples:\n\nI can try to post more completed examples, but ultimately both of these are limited by the content we need to cover and the total lecture time in quarter\n\n\nLikes/helping learn\n\nPace of class\nStress-free environment\nAnnotated notes + rewatching lectures\nVisualizations\nHomework based on completion\nActively working on problems together\nUsing AI for help\n\n\nWe will be taking a pause before expected values\n\nFinish conditional pdf problem\nReview joint distributions\n\nHW 4: looked pretty good!\nCome to office hours!\n\nDon’t spend hours on a problem! Charles and I can help you!"
  },
  {
    "objectID": "lessons/11_Joint_distributions/11_Joint_distributions_review_key_info.html#announcements",
    "href": "lessons/11_Joint_distributions/11_Joint_distributions_review_key_info.html#announcements",
    "title": "Key Info",
    "section": "",
    "text": "Midterm feedback!\n\nDid you fill out the final survey to record your name?\nDislikes/suggestions\n\nTextbook\nWant more R code\npdf lectures posted earlier\nSome in-class work\nSlower on slides + more examples:\n\nI can try to post more completed examples, but ultimately both of these are limited by the content we need to cover and the total lecture time in quarter\n\n\nLikes/helping learn\n\nPace of class\nStress-free environment\nAnnotated notes + rewatching lectures\nVisualizations\nHomework based on completion\nActively working on problems together\nUsing AI for help\n\n\nWe will be taking a pause before expected values\n\nFinish conditional pdf problem\nReview joint distributions\n\nHW 4: looked pretty good!\nCome to office hours!\n\nDon’t spend hours on a problem! Charles and I can help you!"
  },
  {
    "objectID": "lessons/11_Joint_distributions/11_Joint_distributions_review_key_info.html#key-dates",
    "href": "lessons/11_Joint_distributions/11_Joint_distributions_review_key_info.html#key-dates",
    "title": "Key Info",
    "section": "Key Dates",
    "text": "Key Dates\n\nHW 05 due this Sunday!"
  },
  {
    "objectID": "lessons/11_Joint_distributions/Old_notes/25_Joint_densities_key_info.html",
    "href": "lessons/11_Joint_distributions/Old_notes/25_Joint_densities_key_info.html",
    "title": "Key Info",
    "section": "",
    "text": "Grading HW 6 assignment today!\n\nFeel free to turn in solutions later than 11/24 since I took so long to grade\n\nI made quite a few changes to the homeworks\n\nHW 10 is gone\nHW 9 is optional\nInstead of a HW assignment due over late November break\n\nHW 8 will include material from Week 8 and 9 and be due 12/5\n\n\nI have quite a bit of grading to catch up on!\nRealizing I never gave you mid-quarter feedback forms…\n\nLet’s chat about this and our assessment breakdown in the coming weeks\n\nAnything else?"
  },
  {
    "objectID": "lessons/11_Joint_distributions/Old_notes/25_Joint_densities_key_info.html#announcements",
    "href": "lessons/11_Joint_distributions/Old_notes/25_Joint_densities_key_info.html#announcements",
    "title": "Key Info",
    "section": "",
    "text": "Grading HW 6 assignment today!\n\nFeel free to turn in solutions later than 11/24 since I took so long to grade\n\nI made quite a few changes to the homeworks\n\nHW 10 is gone\nHW 9 is optional\nInstead of a HW assignment due over late November break\n\nHW 8 will include material from Week 8 and 9 and be due 12/5\n\n\nI have quite a bit of grading to catch up on!\nRealizing I never gave you mid-quarter feedback forms…\n\nLet’s chat about this and our assessment breakdown in the coming weeks\n\nAnything else?"
  },
  {
    "objectID": "lessons/11_Joint_distributions/Old_notes/25_Joint_densities_key_info.html#key-dates",
    "href": "lessons/11_Joint_distributions/Old_notes/25_Joint_densities_key_info.html#key-dates",
    "title": "Key Info",
    "section": "Key Dates",
    "text": "Key Dates\n\nThursday: HW 7 due\nSunday: HW 6 solutions due"
  },
  {
    "objectID": "lessons/00_Intro/00_Intro_key_info.html",
    "href": "lessons/00_Intro/00_Intro_key_info.html",
    "title": "Key Info and Announcements",
    "section": "",
    "text": "Welcome! I am heavily borrowing from Dr. Nicky Wakim’s 550 website and format, because it’s awesome."
  },
  {
    "objectID": "lessons/00_Intro/00_Intro_key_info.html#announcements",
    "href": "lessons/00_Intro/00_Intro_key_info.html#announcements",
    "title": "Key Info and Announcements",
    "section": "",
    "text": "Welcome! I am heavily borrowing from Dr. Nicky Wakim’s 550 website and format, because it’s awesome."
  },
  {
    "objectID": "lessons/00_Intro/00_Intro_key_info.html#key-dates",
    "href": "lessons/00_Intro/00_Intro_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key dates",
    "text": "Key dates\n\nHomework 0 is due THIS THURSDAY at 11pm!"
  },
  {
    "objectID": "lessons/00_Intro/00_Intro_key_info.html#last-class",
    "href": "lessons/00_Intro/00_Intro_key_info.html#last-class",
    "title": "Key Info and Announcements",
    "section": "Last class",
    "text": "Last class\n\nHolidays"
  },
  {
    "objectID": "lessons/14_Variance/14_Variance.html",
    "href": "lessons/14_Variance/14_Variance.html",
    "title": "Lesson 14: Variance",
    "section": "",
    "text": "Define and calculate the expected value for a function of discrete and continuous RVs\nDefine and calculate variance for a single random variable\nDefine and calculate variance for multiple random variables"
  },
  {
    "objectID": "lessons/14_Variance/14_Variance.html#lets-start-building-the-variance-through-expected-values-of-functions",
    "href": "lessons/14_Variance/14_Variance.html#lets-start-building-the-variance-through-expected-values-of-functions",
    "title": "Lesson 14: Variance",
    "section": "Let’s start building the variance through expected values of functions",
    "text": "Let’s start building the variance through expected values of functions\n\n\n\n\nExample 1\n\n\nLet \\(g\\) be a function and let \\(g(x) = ax+b\\), for real-valued constants \\(a\\) and \\(b\\). What is \\(\\mathbb{E}[g(X)]\\)?"
  },
  {
    "objectID": "lessons/14_Variance/14_Variance.html#what-is-the-expected-value-of-a-function",
    "href": "lessons/14_Variance/14_Variance.html#what-is-the-expected-value-of-a-function",
    "title": "Lesson 14: Variance",
    "section": "What is the expected value of a function?",
    "text": "What is the expected value of a function?\n\n\n\n\nExpected value of function of discrete RV\n\n\nFor any function \\(g\\) and discrete RV \\(X\\), the expected value of \\(g(X)\\) is \\[\\mathbb{E}[g(X)] = \\sum_{\\{all\\ x\\}}\\ g(x) p_X(x)\\]\n\n\n\n\n\nExpected value of function of continuous RV\n\n\nFor any function \\(g\\) and continuous RV \\(X\\), the expected value of \\(g(X)\\) is \\[\\mathbb{E}[g(X)] = \\displaystyle\\int_{-\\infty}^\\infty g(x)f_X(x) dx\\]\n\n\n\n\n\nFor example, if we have \\(g(x)=x^2\\), then \\[\\mathbb{E}[X^2)] = \\sum_{\\{all\\ x\\}}\\ x^2 p_X(x) \\neq \\left(\\sum_{\\{all\\ x\\}}\\ x p_X(x)) \\right)^2 = \\left( \\mathbb{E}[X] \\right)^2\\]"
  },
  {
    "objectID": "lessons/14_Variance/14_Variance.html#lets-revisit-the-card-example-12",
    "href": "lessons/14_Variance/14_Variance.html#lets-revisit-the-card-example-12",
    "title": "Lesson 14: Variance",
    "section": "Let’s revisit the card example (1/2)",
    "text": "Let’s revisit the card example (1/2)\n\n\n\n\nExample 2\n\n\nSuppose you draw 2 cards from a standard deck of cards with replacement. Let \\(X\\) be the number of hearts you draw.\n\nFind \\(\\mathbb{E}[X^2]\\).\n\n\n\n\n\nRecall Binomial RV with \\(n=2\\):\n\\[p_X(x) = {2 \\choose x}p^x(1-p)^{2-x} \\text{  for } x = 0, 1, 2\\]"
  },
  {
    "objectID": "lessons/14_Variance/14_Variance.html#lets-revisit-the-card-example-22",
    "href": "lessons/14_Variance/14_Variance.html#lets-revisit-the-card-example-22",
    "title": "Lesson 14: Variance",
    "section": "Let’s revisit the card example (2/2)",
    "text": "Let’s revisit the card example (2/2)\n\n\n\n\nExample 2\n\n\nSuppose you draw 2 cards from a standard deck of cards with replacement. Let \\(X\\) be the number of hearts you draw.\n\nFind \\(\\mathbb{E}\\big[\\big(X-\\frac{1}{2}\\big)^2\\big]\\).\n\n\n\n\n\nRecall Binomial RV with \\(n=2\\):\n\\[p_X(x) = {2 \\choose x}p^x(1-p)^{2-x} \\text{  for } x = 0, 1, 2\\]"
  },
  {
    "objectID": "lessons/14_Variance/14_Variance.html#variance-of-a-rv",
    "href": "lessons/14_Variance/14_Variance.html#variance-of-a-rv",
    "title": "Lesson 14: Variance",
    "section": "Variance of a RV",
    "text": "Variance of a RV\n\n\nDefinition: Variance of RV\n\n\nThe variance of a RV \\(X\\), with (finite) expected value \\(\\mu_X=\\mathbb{E}[X]\\) is \\[\\sigma_X^2=Var(X)=\\mathbb{E}[(X-\\mu_X)^2] = \\mathbb{E}[(X-\\mathbb{E}[X])^2]\\]\n\n\n\n\nDefinition: Standard deviation of RV\n\n\nThe standard deviation of a RV \\(X\\) is \\[\\sigma_X = SD(X) = \\sqrt{\\sigma_X^2}=\\sqrt{Var(X)}.\\]"
  },
  {
    "objectID": "lessons/14_Variance/14_Variance.html#variance-of-discrete-and-continuous-rvs",
    "href": "lessons/14_Variance/14_Variance.html#variance-of-discrete-and-continuous-rvs",
    "title": "Lesson 14: Variance",
    "section": "Variance of discrete and continuous RVs",
    "text": "Variance of discrete and continuous RVs\n\n\nHow do we calculate the variance of a discrete RV?\n   \nFor discrete RVs:\n\\[\n\\begin{align}\nVar(X) & = \\\\ & = \\sum_{\\{all\\ x\\}}(x-\\mu_x)^2 p_{X}(x) \\\\ & =\n\\mathbb{E}[(X-\\mu_X)^2] \\\\ & = \\mathbb{E}[(X-\\mathbb{E}[X])^2] \\\\ &= \\mathbb{E}[X^2]-(\\mathbb{E}[X])^2\n\\end{align}\n\\]\n\n\n\nHow do we calculate the variance of a continuous RV?\n    \nFor continuous RVs:\n\\[\n\\begin{align}\nVar(X) & =\n\\displaystyle\\int_{-\\infty}^\\infty (x-\\mu_X)^2f_X(x) dx \\\\ & = \\mathbb{E}[(X-\\mu_X)^2] \\\\ & = \\mathbb{E}[(X-\\mathbb{E}[X])^2] \\\\ &= \\mathbb{E}[X^2]-(\\mathbb{E}[X])^2 \\end{align}\n\\]"
  },
  {
    "objectID": "lessons/14_Variance/14_Variance.html#questions",
    "href": "lessons/14_Variance/14_Variance.html#questions",
    "title": "Lesson 14: Variance",
    "section": "Questions",
    "text": "Questions\nQuestions: Why do we square the difference in the variance definition? \\((X-\\mu_X)^2\\)\n\nWhy not define the measure of spread as \\(\\mathbb{E}[X-\\mu_X] = \\mathbb{E}[X-\\mathbb{E}[X]]\\)?\nWhy not use \\(\\mathbb{E}[|X-\\mu_X|]\\)?"
  },
  {
    "objectID": "lessons/14_Variance/14_Variance.html#lets-calculate-the-variance-and-prove-it-12",
    "href": "lessons/14_Variance/14_Variance.html#lets-calculate-the-variance-and-prove-it-12",
    "title": "Lesson 14: Variance",
    "section": "Let’s calculate the variance and prove it! (1/2)",
    "text": "Let’s calculate the variance and prove it! (1/2)\n\n\n\n\nLemma 6: “Computation formula” for Variance\n\n\nThe variance of a RV \\(X\\), can be computed as \\[\\begin{align}\n\\sigma_X^2 & =Var(X) \\\\ & = \\mathbb{E}[X^2]-\\mu_X^2 \\\\ & = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2\n\\end{align}\\]"
  },
  {
    "objectID": "lessons/14_Variance/14_Variance.html#lets-calculate-the-variance-and-prove-it-22",
    "href": "lessons/14_Variance/14_Variance.html#lets-calculate-the-variance-and-prove-it-22",
    "title": "Lesson 14: Variance",
    "section": "Let’s calculate the variance and prove it! (2/2)",
    "text": "Let’s calculate the variance and prove it! (2/2)"
  },
  {
    "objectID": "lessons/14_Variance/14_Variance.html#variance-of-an-uniform-distribution",
    "href": "lessons/14_Variance/14_Variance.html#variance-of-an-uniform-distribution",
    "title": "Lesson 14: Variance",
    "section": "Variance of an Uniform distribution",
    "text": "Variance of an Uniform distribution\n\n\n\n\nExample 2\n\n\nLet \\(f_X(x)= \\frac{1}{b-a}\\), for \\(a \\leq x \\leq b\\). Find \\(Var[X]\\)."
  },
  {
    "objectID": "lessons/14_Variance/14_Variance.html#variance-of-exponential-distribution",
    "href": "lessons/14_Variance/14_Variance.html#variance-of-exponential-distribution",
    "title": "Lesson 14: Variance",
    "section": "Variance of exponential distribution",
    "text": "Variance of exponential distribution\n\n\nIn the homework:\n\n\nExample 3\n\n\nLet \\(f_X(x)= \\lambda e^{-\\lambda x}\\), for \\(x &gt; 0\\) and \\(\\lambda&gt; 0\\). Find \\(Var[X]\\)."
  },
  {
    "objectID": "lessons/14_Variance/14_Variance.html#variance-of-a-function-with-a-single-rv",
    "href": "lessons/14_Variance/14_Variance.html#variance-of-a-function-with-a-single-rv",
    "title": "Lesson 14: Variance",
    "section": "Variance of a function with a single RV",
    "text": "Variance of a function with a single RV\n\n\nLemma 7\n\n\nFor a RV \\(X\\) and constants \\(a\\) and \\(b\\), \\[Var(aX+b) = a^2Var(X).\\]\n\n\nProof will be exercise in homework. It’s fun! In a mathy kinda way."
  },
  {
    "objectID": "lessons/14_Variance/14_Variance.html#important-results-for-independent-rvs",
    "href": "lessons/14_Variance/14_Variance.html#important-results-for-independent-rvs",
    "title": "Lesson 14: Variance",
    "section": "Important results for independent RVs",
    "text": "Important results for independent RVs\n\n\nTheorem 8\n\n\nFor independent RV’s \\(X\\) and \\(Y\\), and functions \\(g\\) and \\(h\\), \\[\\mathbb{E}[g(X)h(Y)] = \\mathbb{E}[g(X)]\\mathbb{E}[h(Y)].\\]\n\n\n\n\nCorollary 1\n\n\nFor independent RV’s \\(X\\) and \\(Y\\), \\[\\mathbb{E}[XY] = \\mathbb{E}[X]\\mathbb{E}[Y].\\]"
  },
  {
    "objectID": "lessons/14_Variance/14_Variance.html#variance-of-sum-of-independent-discrete-rvs",
    "href": "lessons/14_Variance/14_Variance.html#variance-of-sum-of-independent-discrete-rvs",
    "title": "Lesson 14: Variance",
    "section": "Variance of sum of independent discrete RVs",
    "text": "Variance of sum of independent discrete RVs\n\n\nTheorem 9: Variance of sum of independent discrete RV’s\n\n\nFor independent discrete RV’s \\(X_i\\) and constants \\(a_i\\), \\(i=1,2,\\dots, n\\), \\[Var\\Bigg(\\sum_{i=1}^n a_iX_i\\Bigg) = \\sum_{i=1}^n a_i^2Var(X_i).\\]\n\n\nSimpler version:\n\\[Var(a_1 X + a_2 Y) = Var(a_1X) + Var(a_2 Y) = a_1^2 Var(X) + a_2^2 Var(Y)\\]"
  },
  {
    "objectID": "lessons/14_Variance/14_Variance.html#corollaries",
    "href": "lessons/14_Variance/14_Variance.html#corollaries",
    "title": "Lesson 14: Variance",
    "section": "Corollaries",
    "text": "Corollaries\n\n\nCorollary 2\n\n\nFor independent discrete RV’s \\(X_i\\), \\(i=1,2,\\dots, n\\), \\[Var\\Bigg(\\sum_{i=1}^n X_i\\Bigg) = \\sum_{i=1}^n Var(X_i).\\]\n\n\n\n\nCorollary 3\n\n\nFor independent identically distributed (i.i.d.) discrete RV’s \\(X_i\\), \\(i=1,2,\\dots, n\\), \\[Var\\Bigg(\\sum_{i=1}^n X_i\\Bigg) = n Var(X_1).\\]"
  },
  {
    "objectID": "lessons/14_Variance/14_Variance.html#ghost-problem-with-replacement",
    "href": "lessons/14_Variance/14_Variance.html#ghost-problem-with-replacement",
    "title": "Lesson 14: Variance",
    "section": "Ghost problem: with replacement",
    "text": "Ghost problem: with replacement\n\n\n\n\nExample 3.2\n\n\nThe ghost is trick-or-treating at a different house now. In this case it is known that the bag of candy has 10 chocolates, 20 lollipops, and 30 laffy taffies. The ghost grabs a handful of five pieces of candy. What is the variance for the number of chocolates the ghost takes? Let’s solve this for the cases with replacement.\n\n\n\n\nRecall probability with replacement:\n\\[\np_X(x) = {n \\choose k}p^k(1-p)^{n-k}\n\\]"
  },
  {
    "objectID": "lessons/14_Variance/14_Variance.html#back-to-our-hotel-example-from-lesson-13",
    "href": "lessons/14_Variance/14_Variance.html#back-to-our-hotel-example-from-lesson-13",
    "title": "Lesson 14: Variance",
    "section": "Back to our hotel example from Lesson 13",
    "text": "Back to our hotel example from Lesson 13\n\n\n\n\nExample 4\n\n\nA tour group is planning a visit to the city of Minneapolis and needs to book 30 hotel rooms. The average price of a room is $200 with standard deviation $10. In addition, there is a 10% tourism tax for each room. What is the standard deviation of the cost for the 30 hotel rooms? Assume rooms are independent.\n\n\nProblem to do at home if we don’t have enough time."
  },
  {
    "objectID": "lessons/14_Variance/14_Variance.html#find-the-mean-and-sd-from-word-problem-12",
    "href": "lessons/14_Variance/14_Variance.html#find-the-mean-and-sd-from-word-problem-12",
    "title": "Lesson 14: Variance",
    "section": "Find the mean and sd from word problem (1/2)",
    "text": "Find the mean and sd from word problem (1/2)\n\n\n\n\nExample 4\n\n\nA machine manufactures cubes with a side length that varies uniformly from 1 to 2 inches. Assume the sides of the base and height are equal. The cost to make a cube is 10 ¢ per cubic inch, and 5 ¢ cents for the general cost per cube. Find the mean and standard deviation of the cost to make 10 cubes."
  },
  {
    "objectID": "lessons/14_Variance/14_Variance.html#find-the-mean-and-sd-from-word-problem-12-1",
    "href": "lessons/14_Variance/14_Variance.html#find-the-mean-and-sd-from-word-problem-12-1",
    "title": "Lesson 14: Variance",
    "section": "Find the mean and sd from word problem (1/2)",
    "text": "Find the mean and sd from word problem (1/2)"
  },
  {
    "objectID": "lessons/14_Variance/14_Variance_muddy_points.html",
    "href": "lessons/14_Variance/14_Variance_muddy_points.html",
    "title": "Muddy Points",
    "section": "",
    "text": "You are right, the hotel room problem did not need the identical distributions in either class. I definitely mispoke by saying they were iid. Even though they have the same expected value and variance, they are not necessarily from the same distribution. They are independent, but not necessarily identically distributed. They all have the same variance, so I can still use \\[\\sum_{i=1}^{30} Var(X_i) = 30Var(X)\\]\nIt was more that, last class, we did not specify the standard deviation. We could find the expected value of the total cost without knowing the standard deviation, and thus without necessarily assuming the cost of the hotel rooms were identical random variables. In this class, we needed the standard deviation for the cost of each hotel room. We could have made them different, but we made all of them $10.\n\n\n\nI go through that breakdown in the first muddy point from Fall 2023 (below). Check out the term 2 part that is highlighted in green-ish.\n\n\n\nHere are the corollaries:\n\n\nCorollary 2\n\n\nFor independent RV’s \\(X_i\\), \\(i=1,2,\\dots, n\\), \\[Var\\Bigg(\\sum_{i=1}^n X_i\\Bigg) = \\sum_{i=1}^n Var(X_i).\\]\n\n\n\n\nCorollary 3\n\n\nFor independent identically distributed (i.i.d.) RV’s \\(X_i\\), \\(i=1,2,\\dots, n\\), \\[Var\\Bigg(\\sum_{i=1}^n X_i\\Bigg) = n Var(X_1).\\]\n\n\nIn Corollary 2, the variance can be different for each \\(X_i\\) so we need to sum each together. In Corollary 3, the variance is the same for each \\(X_i\\) so we can recognize that we will have \\(n\\) identical terms to sum together."
  },
  {
    "objectID": "lessons/14_Variance/14_Variance_muddy_points.html#fall-2025",
    "href": "lessons/14_Variance/14_Variance_muddy_points.html#fall-2025",
    "title": "Muddy Points",
    "section": "",
    "text": "You are right, the hotel room problem did not need the identical distributions in either class. I definitely mispoke by saying they were iid. Even though they have the same expected value and variance, they are not necessarily from the same distribution. They are independent, but not necessarily identically distributed. They all have the same variance, so I can still use \\[\\sum_{i=1}^{30} Var(X_i) = 30Var(X)\\]\nIt was more that, last class, we did not specify the standard deviation. We could find the expected value of the total cost without knowing the standard deviation, and thus without necessarily assuming the cost of the hotel rooms were identical random variables. In this class, we needed the standard deviation for the cost of each hotel room. We could have made them different, but we made all of them $10.\n\n\n\nI go through that breakdown in the first muddy point from Fall 2023 (below). Check out the term 2 part that is highlighted in green-ish.\n\n\n\nHere are the corollaries:\n\n\nCorollary 2\n\n\nFor independent RV’s \\(X_i\\), \\(i=1,2,\\dots, n\\), \\[Var\\Bigg(\\sum_{i=1}^n X_i\\Bigg) = \\sum_{i=1}^n Var(X_i).\\]\n\n\n\n\nCorollary 3\n\n\nFor independent identically distributed (i.i.d.) RV’s \\(X_i\\), \\(i=1,2,\\dots, n\\), \\[Var\\Bigg(\\sum_{i=1}^n X_i\\Bigg) = n Var(X_1).\\]\n\n\nIn Corollary 2, the variance can be different for each \\(X_i\\) so we need to sum each together. In Corollary 3, the variance is the same for each \\(X_i\\) so we can recognize that we will have \\(n\\) identical terms to sum together."
  },
  {
    "objectID": "lessons/14_Variance/14_Variance_muddy_points.html#fall-2023",
    "href": "lessons/14_Variance/14_Variance_muddy_points.html#fall-2023",
    "title": "Muddy Points",
    "section": "Fall 2023",
    "text": "Fall 2023\n\n1. Proof of variance formula\nHere is the variance formula that we worked through in class:\n\n\n\n\n\n\nLemma 6: “Computation formula” for variance\n\n\n\nThe variance of a r.v. \\(X\\), can be computed as \\[\\begin{align}\\sigma_X^2 & =Var(X) \\\\ & = \\mathbb{E}[X^2]-\\mu_X^2 \\\\ & = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2\\end{align}\\]\n\n\nI stepped through this quite quickly and made some implicit steps. So let’s revisit it with explicit steps!\n       \n\n\n2. What progression are we following in the course??\nSomeone asked if this is our progression: RV is function \\(\\to\\) Expected value is function to describe mean of RV \\(\\to\\) Use functions within expected value to set up variance\nBasically, yes! The random variable is a function of a random process. The RV inherits that randomness.\nFrom there, we’ve been working towards calculating the probability of a realized value (\\(x\\)) of the RV. The probability can be different for different realized values (as it links back to the random process).\nWe also want to construct ways to describe our random variables. We may want to figure out what to expect from our random variable (which translates to the mean value of the RV). Since our RV is rooted in a random process, we may want to get an idea of how spread out our realized values are. We use our expected value (mean) as an anchor in our spread. Variance is one way to measure this spread."
  },
  {
    "objectID": "lessons/07_pmfs/07_pmfs_muddy_points.html",
    "href": "lessons/07_pmfs/07_pmfs_muddy_points.html",
    "title": "Muddy Points",
    "section": "",
    "text": "I think you’re on the right track… \\(P(X)\\), \\(P(X=x)\\), and \\(p_X(x)\\) represent probabilities.\nFor discrete random variables, we can use \\(P(X=x)\\) or \\(p_X(x)\\) to represent the probability that the random variable \\(X\\) takes on the value \\(x\\). And then we can discuss the probability mass function (pmf) as the probability distribution as well!\nFor continuous random variables, we use \\(f_X(x)\\) to represent the probability density function (pdf) of \\(X\\) at the value \\(x\\), BUT it is not equal to the actual probability! Aka \\(f_X(x)\\) does not represent a probability, but the area under the curve of \\(f_X(x)\\) over an interval gives us the probability."
  },
  {
    "objectID": "lessons/07_pmfs/07_pmfs_muddy_points.html#fall-2025",
    "href": "lessons/07_pmfs/07_pmfs_muddy_points.html#fall-2025",
    "title": "Muddy Points",
    "section": "",
    "text": "I think you’re on the right track… \\(P(X)\\), \\(P(X=x)\\), and \\(p_X(x)\\) represent probabilities.\nFor discrete random variables, we can use \\(P(X=x)\\) or \\(p_X(x)\\) to represent the probability that the random variable \\(X\\) takes on the value \\(x\\). And then we can discuss the probability mass function (pmf) as the probability distribution as well!\nFor continuous random variables, we use \\(f_X(x)\\) to represent the probability density function (pdf) of \\(X\\) at the value \\(x\\), BUT it is not equal to the actual probability! Aka \\(f_X(x)\\) does not represent a probability, but the area under the curve of \\(f_X(x)\\) over an interval gives us the probability."
  },
  {
    "objectID": "lessons/07_pmfs/07_pmfs_key_info.html",
    "href": "lessons/07_pmfs/07_pmfs_key_info.html",
    "title": "Key Info and Announcements",
    "section": "",
    "text": "Quiz 1 info"
  },
  {
    "objectID": "lessons/07_pmfs/07_pmfs_key_info.html#key-info",
    "href": "lessons/07_pmfs/07_pmfs_key_info.html#key-info",
    "title": "Key Info and Announcements",
    "section": "",
    "text": "Quiz 1 info"
  },
  {
    "objectID": "lessons/07_pmfs/07_pmfs_key_info.html#announcements",
    "href": "lessons/07_pmfs/07_pmfs_key_info.html#announcements",
    "title": "Key Info and Announcements",
    "section": "Announcements",
    "text": "Announcements\n\nHomework 3 due this Sunday at 11pm\nQuiz 1 opens on 10/22 at 3pm"
  },
  {
    "objectID": "lessons/01_Probability/01_Probability_muddy_points_F24.html",
    "href": "lessons/01_Probability/01_Probability_muddy_points_F24.html",
    "title": "Muddy Points",
    "section": "",
    "text": "The muddy points from this year were a subset of the ones from last year, so I just decided to copy those below!"
  },
  {
    "objectID": "lessons/01_Probability/01_Probability_muddy_points_F24.html#proofs-of-propositions",
    "href": "lessons/01_Probability/01_Probability_muddy_points_F24.html#proofs-of-propositions",
    "title": "Muddy Points",
    "section": "1. Proofs of propositions",
    "text": "1. Proofs of propositions\nFurther explanations of the propositions can be found in the textbook from pages 24-27. For many of the explanations in class, I was working to produce a union of disjoint events, so that the probability could easily be calculated. Proposition 3 and 4 were specifically mentioned, so I will include some writing notes on them here:\n\nProposition 3\nIf \\(A \\subseteq B\\), then \\(\\mathbb{P}(A) \\leq \\mathbb{P}(B)\\)\nIn this proposition, I want to define event \\(B\\) as a union of disjoint events so that I can show \\(P(B)\\) is the sum of \\(P(A)\\) and some greater-than-or-equal-to 0 probability event. If the following is my venn diagram of A and B:\n\n\n\n\n\nThen I can define B as the union of disjoint events: \nIf we then take the probability of each side of the equation \\(B = A \\cup (A^c \\cap B)\\), we get \\[P(B) = P\\big(A \\cup (A^c \\cap B)\\big)\\]\nSince events \\(A\\) and \\(A^c \\cap B\\) are disjoint, the probability of their union is just: \\[P(A) + P(A^c \\cap B)\\]\nThus, our equation is now \\[P(B) = P(A) + P(A^c \\cap B)\\]\nFrom Axiom 1, we know for event \\(A^c \\cap B\\), \\(P(A^c \\cap B) \\geq 0\\).\nSo the probability of event B is the sum of the probability of event A and an event that is \\(\\geq\\) 0. This means \\(P(B) \\geq P(A)\\).\n\n\nProposition 4\n\\(\\mathbb{P}(A \\cup B) = \\mathbb{P}(A) + \\mathbb{P}(B) - \\mathbb{P}(A \\cap B)\\)\n\n\nFrom the pictures above, we can see some similar disjoint events.\nIf we look back at \\(A \\cup B\\), we can start manipulating the right side of the equation:"
  },
  {
    "objectID": "lessons/01_Probability/01_Probability_muddy_points_F24.html#example-at-end-of-chapter-2-slides-venn-diagram",
    "href": "lessons/01_Probability/01_Probability_muddy_points_F24.html#example-at-end-of-chapter-2-slides-venn-diagram",
    "title": "Muddy Points",
    "section": "2. Example at end of Chapter 2 slides (Venn Diagram)",
    "text": "2. Example at end of Chapter 2 slides (Venn Diagram)\nI will post this in the previous week’s Muddy Points as well. Please follow this link for my work through of the example. And here is the PDF with my work.\nSub-question: why don’t we just multiply the probability of A and B to get the intersection? This is a specific property of probability when A and B are independent. Only when A and B are independent can we conclude that \\(P(A \\cap B) = P(A)P(B)\\)."
  },
  {
    "objectID": "lessons/01_Probability/01_Probability_muddy_points_F24.html#partition-of-events",
    "href": "lessons/01_Probability/01_Probability_muddy_points_F24.html#partition-of-events",
    "title": "Muddy Points",
    "section": "3. Partition of events",
    "text": "3. Partition of events\nWe’ve been working with event partitions throughout Chapter 2, but we have not formally identified them. Partitions are advantageous to define for two reasons:\n\nThe partitions may be easier to calculate. We can then use the partitions to reconstruct other probabilities that may be more difficult to calculate\nPartitions have nice properties as a consequence of being disjoint. For example, the probability of the union of partitions is the sum of the probabilities across each partition: \\[P\\bigg(\\bigcup_{i=1}^n A_i\\bigg) = P(A_1)P(A_2)P(A_3) \\cdot \\cdot \\cdot P(A_n)\\]"
  },
  {
    "objectID": "lessons/01_Probability/01_Probability.html",
    "href": "lessons/01_Probability/01_Probability.html",
    "title": "Lesson 1: Introduction to Probability",
    "section": "",
    "text": "Use simulations (e.g., coin flips in R) to illustrate randomness in equally likely outcomes.\nIdentify sample spaces and events for basic experiments, and define probability calculations.\n\n\n\n\nBig part of this lesson is motivating why we care about probability and simulations, then we’ll dive into some basics\n\n\n\n\n\n\n\n\n\n\nWe hear the word “probability” pretty often\n\nCommon in news reports, advertisements, sports, medicine, etc.\n\n\n\nResearchers say the probability of living past 110 is on the rise\nCNBC, 2021\n\n\nWe may hear “probability” or similar words like “chance,” “likelihood,” or “odds”\n\n\nScientists fine-tune odds of asteroid Bennu hitting Earth through 2300 with NASA probe’s help\nSpace.com, 2021\n\n\n\n\n\nThe following few slides use some undefined words to define new words that in turn define the previously undefined words\nIt’s confusing!\nWe’re going off assumption that we all have some daily understanding of probability, so stop me if you are confused!"
  },
  {
    "objectID": "lessons/01_Probability/01_Probability.html#where-are-we",
    "href": "lessons/01_Probability/01_Probability.html#where-are-we",
    "title": "Lesson 1: Introduction to Probability",
    "section": "",
    "text": "Big part of this lesson is motivating why we care about probability and simulations, then we’ll dive into some basics"
  },
  {
    "objectID": "lessons/01_Probability/01_Probability.html#what-is-probability-12",
    "href": "lessons/01_Probability/01_Probability.html#what-is-probability-12",
    "title": "Lesson 1: Introduction to Probability",
    "section": "",
    "text": "We hear the word “probability” pretty often\n\nCommon in news reports, advertisements, sports, medicine, etc.\n\n\n\nResearchers say the probability of living past 110 is on the rise\nCNBC, 2021\n\n\nWe may hear “probability” or similar words like “chance,” “likelihood,” or “odds”\n\n\nScientists fine-tune odds of asteroid Bennu hitting Earth through 2300 with NASA probe’s help\nSpace.com, 2021"
  },
  {
    "objectID": "lessons/01_Probability/01_Probability.html#before-we-take-one-more-step",
    "href": "lessons/01_Probability/01_Probability.html#before-we-take-one-more-step",
    "title": "Lesson 1: Introduction to Probability",
    "section": "",
    "text": "The following few slides use some undefined words to define new words that in turn define the previously undefined words\nIt’s confusing!\nWe’re going off assumption that we all have some daily understanding of probability, so stop me if you are confused!"
  },
  {
    "objectID": "lessons/01_Probability/01_Probability.html#probability-and-randomness",
    "href": "lessons/01_Probability/01_Probability.html#probability-and-randomness",
    "title": "Lesson 1: Introduction to Probability",
    "section": "Probability and randomness",
    "text": "Probability and randomness\n\nProbability requires randomness\nSomething is random if there are many potential outcomes, but there is uncertainty which outcome will occur\n\n \n\nOutcomes can be “equally likely,” meaning each outcome has the same probability of happening\n\nBut random does NOT necessarily mean equally likely\n\nWe often use physical randomness to demonstrate equally likely outcomes\n\nThink: flipping coins, rolling a dice, drawing cards\n\n\n \n\nThe occurrence of outcomes can be uncertain, but there is an underlying distribution of the probability of outcomes\n\nThere is a distribution of outcomes over large number of (hypothetical repetitions)"
  },
  {
    "objectID": "lessons/01_Probability/01_Probability.html#a-single-coin-flip-then-100-coin-flips",
    "href": "lessons/01_Probability/01_Probability.html#a-single-coin-flip-then-100-coin-flips",
    "title": "Lesson 1: Introduction to Probability",
    "section": "A single coin flip then 100 coin flips",
    "text": "A single coin flip then 100 coin flips\nSeeing Theory, Chapter 1: Basic Probability, Chance Events"
  },
  {
    "objectID": "lessons/01_Probability/01_Probability.html#how-do-we-simulate-this-in-r",
    "href": "lessons/01_Probability/01_Probability.html#how-do-we-simulate-this-in-r",
    "title": "Lesson 1: Introduction to Probability",
    "section": "How do we simulate this in R?",
    "text": "How do we simulate this in R?\n\nWe know that heads and tails are equally likely for a single flip\n\n\nset.seed(13)\ncoin = c(\"heads\", \"tails\")\nsample(coin, 1)\n\n[1] \"tails\"\n\n\n\nWhen we only flip the coin once, we only only get one outcome (heads or tails)\n\nWe cannot see any distribution of the probability of outcomes"
  },
  {
    "objectID": "lessons/01_Probability/01_Probability.html#what-if-i-sample-10-coin-flips",
    "href": "lessons/01_Probability/01_Probability.html#what-if-i-sample-10-coin-flips",
    "title": "Lesson 1: Introduction to Probability",
    "section": "What if I sample 10 coin flips?",
    "text": "What if I sample 10 coin flips?\n\nHow many tails do we get? Are we getting closer to a distribution of heads and tails that we expect?\n\n\n\n\nResults and running proportion of H for 10 flips of a fair coin.\n\n\nFlip\nResult\nRunning count of H\nRunning proportion of H\n\n\n\n\n1\nH\n1\n1.000\n\n\n2\nT\n1\n0.500\n\n\n3\nH\n2\n0.667\n\n\n4\nT\n2\n0.500\n\n\n5\nH\n3\n0.600\n\n\n6\nT\n3\n0.500\n\n\n7\nT\n3\n0.429\n\n\n8\nT\n3\n0.375\n\n\n9\nT\n3\n0.333\n\n\n10\nT\n3\n0.300"
  },
  {
    "objectID": "lessons/01_Probability/01_Probability.html#what-if-i-sample-100-coin-flips",
    "href": "lessons/01_Probability/01_Probability.html#what-if-i-sample-100-coin-flips",
    "title": "Lesson 1: Introduction to Probability",
    "section": "What if I sample 100 coin flips?",
    "text": "What if I sample 100 coin flips?"
  },
  {
    "objectID": "lessons/01_Probability/01_Probability.html#can-we-simulate-more",
    "href": "lessons/01_Probability/01_Probability.html#can-we-simulate-more",
    "title": "Lesson 1: Introduction to Probability",
    "section": "Can we simulate more??",
    "text": "Can we simulate more??\n\nI can start to count the number of tails in the flips\n\n\nsum( sample(coin, 100, replace = T) == \"heads\" )\n\n[1] 42\n\n\n\nAnd I can see the proportion of tails in the flips\n\n\nsum( sample(coin, 100, replace = T) == \"heads\" ) / 100\n\n[1] 0.42\n\n\n\nI can do this with more flips\n\n\nsum( sample(coin, 1000, replace = T) == \"heads\" ) / 1000\n\n[1] 0.531\n\nsum( sample(coin, 10000, replace = T) == \"heads\" ) / 10000\n\n[1] 0.4956\n\nsum( sample(coin, 100000, replace = T) == \"heads\" ) / 100000\n\n[1] 0.50033"
  },
  {
    "objectID": "lessons/01_Probability/01_Probability.html#whats-the-point",
    "href": "lessons/01_Probability/01_Probability.html#whats-the-point",
    "title": "Lesson 1: Introduction to Probability",
    "section": "What’s the point?",
    "text": "What’s the point?\n\nWe know the probability of a heads is 0.5!\n\n \n\nWhy do we need to simulate 100s or 1000s of coin flips?\n\nWith enough repetitions, we can use simulations to approximate the probability of an event\n\n\n \n\nOkay, but why is that helpful? (next slide!)"
  },
  {
    "objectID": "lessons/01_Probability/01_Probability.html#why-are-simulations-important-bigger-picture",
    "href": "lessons/01_Probability/01_Probability.html#why-are-simulations-important-bigger-picture",
    "title": "Lesson 1: Introduction to Probability",
    "section": "Why are simulations important? (bigger picture)",
    "text": "Why are simulations important? (bigger picture)\nIn the previous example, coding a simulation seemed more educational than necessary\n \n\nSimulations can help us (or be necessary) to solve a problem when calculations are complex\n\nWe can often calculate probabilities mathematically, but we will eventually get to complex calculations\n\nSimulations are a great way to check your work!\nSimulation based reasoning is helpful in statistics\n\nYou’ll see this in confidence intervals in Biostatistics courses\n\nSimulations allow you to change assumptions easily and see how they affect your results\nIt is often how statisticians “run” experiments on their methods of hypotheses"
  },
  {
    "objectID": "lessons/01_Probability/01_Probability.html#outcomes-events-sample-spaces",
    "href": "lessons/01_Probability/01_Probability.html#outcomes-events-sample-spaces",
    "title": "Lesson 1: Introduction to Probability",
    "section": "Outcomes, events, sample spaces",
    "text": "Outcomes, events, sample spaces\n\n\nDefinition: Outcome\n\n\nThe possible results in a random phenomenon.\n\n\n\n\nDefinition: Sample Space\n\n\nThe sample space \\(S\\) is the set of all outcomes\n\n\n\n\nDefinition: Event\n\n\nAn event is a collection of some outcomes. An event can include multiple outcomes or no outcomes (a subset of the sample space).\n\n\nWhen thinking about events, think about outcomes that you might be asking the probability of. For example, what is the probability that you get a heads or a tails in one flip? (Answer: 1)"
  },
  {
    "objectID": "lessons/01_Probability/01_Probability.html#coin-toss-example-1-coin-13",
    "href": "lessons/01_Probability/01_Probability.html#coin-toss-example-1-coin-13",
    "title": "Lesson 1: Introduction to Probability",
    "section": "Coin Toss Example: 1 coin (1/3)",
    "text": "Coin Toss Example: 1 coin (1/3)\n\n\n\n\nSingle coin toss\n\n\nSuppose you toss one coin.\n\nWhat are the possible outcomes?\n \nWhat is the sample space?\n \nWhat are the possible events?"
  },
  {
    "objectID": "lessons/01_Probability/01_Probability.html#coin-toss-example-1-coin-23",
    "href": "lessons/01_Probability/01_Probability.html#coin-toss-example-1-coin-23",
    "title": "Lesson 1: Introduction to Probability",
    "section": "Coin Toss Example: 1 coin (2/3)",
    "text": "Coin Toss Example: 1 coin (2/3)\nSuppose you toss one coin.\n\nWhat are the possible outcomes?\n\nHeads (\\(H\\))\nTails (\\(T\\))\n\n\n \n\n\nNote\n\n\nWhen something happens at random, such as a coin toss, there are several possible outcomes, and exactly one of the outcomes will occur."
  },
  {
    "objectID": "lessons/01_Probability/01_Probability.html#coin-toss-example-1-coin-33",
    "href": "lessons/01_Probability/01_Probability.html#coin-toss-example-1-coin-33",
    "title": "Lesson 1: Introduction to Probability",
    "section": "Coin Toss Example: 1 coin (3/3)",
    "text": "Coin Toss Example: 1 coin (3/3)\n\n\n\nWhat is the sample space?\n\n\\(S =\\)\n\n\n \n \n\nWhat are the possible events?\n\n\n\n\n\n\n \n\n\n\n\nNote #1\n\n\nWe use curly brackets (\\(\\{\\}\\)) to denote a set (collecting a list of outcomes or values)\n\n\n\n\nNote #2\n\n\nThe total number of possible events is \\[2^{|S|}\\] where \\(|S|\\) is the total number of outcomes in the sample space. Also, possible events are not necessarily something that can actually occur (i.e. getting a heads and a tails on a single coin flip)"
  },
  {
    "objectID": "lessons/01_Probability/01_Probability.html#coin-toss-example-2-coins",
    "href": "lessons/01_Probability/01_Probability.html#coin-toss-example-2-coins",
    "title": "Lesson 1: Introduction to Probability",
    "section": "Coin Toss Example: 2 coins",
    "text": "Coin Toss Example: 2 coins\nSuppose you toss two coins.\n\nWhat is the sample space? Assume the coins are distinguishable\n\n\\(S =\\)\n\n\n \n\nWhat are some possible events?\n\n\\(A =\\) exactly one \\(H =\\)\n\\(B =\\) at least one \\(H =\\)"
  },
  {
    "objectID": "lessons/01_Probability/01_Probability.html#more-info-on-events-and-sample-spaces",
    "href": "lessons/01_Probability/01_Probability.html#more-info-on-events-and-sample-spaces",
    "title": "Lesson 1: Introduction to Probability",
    "section": "More info on events and sample spaces",
    "text": "More info on events and sample spaces\n\nWe usually use capital letters from the beginning of the alphabet to denote events. However, other letters might be chosen to be more descriptive.\n\nExamples: \\(A, B, C, A_1, A_2\\)\n\nWe can also define a new event as a combination of other events (next lesson)\n\nExamples: \\(A \\cup B\\) (union), \\(A \\cap B\\) (intersection), \\(A^C\\) (complement)\n\n\n \n\nWe use the notation \\(|S|\\) to denote the size of the sample space.\n\n \n\nThe total number of possible events is \\(2^{|S|}\\), which is the total number of possible subsets of \\(S\\).\n\n \n\nThe empty set, denoted by \\(\\emptyset\\), is the set containing no outcomes."
  },
  {
    "objectID": "lessons/01_Probability/01_Probability.html#example-keep-sampling-until",
    "href": "lessons/01_Probability/01_Probability.html#example-keep-sampling-until",
    "title": "Lesson 1: Introduction to Probability",
    "section": "Example: Keep sampling until…",
    "text": "Example: Keep sampling until…\nSuppose you keep sampling people until you have someone with high blood pressure (BP)\n \nWhat is the sample space?\n\nLet \\(H =\\) denote someone with high BP.\nLet \\(H^C =\\) denote someone with not high blood pressure, such as low or regular BP.\n\n \n\nThen, \\(S =\\)"
  },
  {
    "objectID": "lessons/01_Probability/01_Probability.html#lets-define-probability-with-events-and-spaces",
    "href": "lessons/01_Probability/01_Probability.html#lets-define-probability-with-events-and-spaces",
    "title": "Lesson 1: Introduction to Probability",
    "section": "Let’s define probability with events and spaces",
    "text": "Let’s define probability with events and spaces\nIf \\(S\\) is a finite sample space, with equally likely outcomes, then\n\\[\\mathbb{P}(A) = \\frac{|A|}{|S|}\\]\nIn human speak:\n\nFor equally likely outcomes, the probability that a certain event occurs is: the number of outcomes within the event of interest (\\(|A|\\)) divided by the total number of possible outcomes (\\(|S|\\))\n\n\\[\\mathbb{P}(A) = \\frac{\\text{total number of outcomes in event A}}{\\text{total number of outcomes in sample space}}\\]\n\nThus, it is important to be able to count the outcomes within an event"
  },
  {
    "objectID": "lessons/01_Probability/01_Probability.html#a-probability-is-a-function",
    "href": "lessons/01_Probability/01_Probability.html#a-probability-is-a-function",
    "title": "Lesson 1: Introduction to Probability",
    "section": "A probability is a function…",
    "text": "A probability is a function…\n\n\\(\\mathbb{P}(A)\\) is a function with\n\nInput: event \\(A\\) from the sample space \\(S\\), (\\(A \\subseteq S\\))\n\n\\(A \\subseteq S\\) means “A contained within S” or “A is a subset of S”\n\nOutput: a number between 0 and 1 (inclusive)\n\n\n \n\nThe probability function maps an event (input) to value between 0 and 1 (output)\n\nWhen we speak of the probability function, we often call the values between 0 and 1 “probabilities”\n\nExample: “The probability of drawing a heart is 0.25” for \\(P(\\text{heart}) = 0.25\\)\n\n\n\n \n\nThe probability function needs to follow some specific rules (called axioms)!"
  },
  {
    "objectID": "lessons/01_Probability/01_Probability.html#coin-toss-example-revisited-2-coins",
    "href": "lessons/01_Probability/01_Probability.html#coin-toss-example-revisited-2-coins",
    "title": "Lesson 1: Introduction to Probability",
    "section": "Coin Toss Example Revisited: 2 coins",
    "text": "Coin Toss Example Revisited: 2 coins\n\nFrom our sample space and events:\n\nSample space: \\(S = \\{HH, HT, TH, TT\\}\\)\nEvent A: \\(A = \\text{exactly one H} = \\{HT, TH\\}\\)\nEvent B: \\(B = \\text{at least one H} = \\{HT, TH, HH\\}\\)\n\n\n \n\nCalculate \\(P(A)\\) and \\(P(B)\\)"
  },
  {
    "objectID": "lessons/12_Independence_Conditioning/09_Joint_distributions.html",
    "href": "lessons/12_Independence_Conditioning/09_Joint_distributions.html",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "",
    "text": "Calculate probabilities for a pair of discrete random variables\nCalculate a joint, marginal, and conditional probability mass function (pmf)\nCalculate a joint, marginal, and conditional cumulative distribution function (CDF)"
  },
  {
    "objectID": "lessons/12_Independence_Conditioning/09_Joint_distributions.html#learning-objectives",
    "href": "lessons/12_Independence_Conditioning/09_Joint_distributions.html#learning-objectives",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "",
    "text": "Calculate probabilities for a pair of discrete random variables\nCalculate a joint, marginal, and conditional probability mass function (pmf)\nCalculate a joint, marginal, and conditional cumulative distribution function (CDF)"
  },
  {
    "objectID": "lessons/12_Independence_Conditioning/09_Joint_distributions.html#where-are-we",
    "href": "lessons/12_Independence_Conditioning/09_Joint_distributions.html#where-are-we",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "Where are we?",
    "text": "Where are we?"
  },
  {
    "objectID": "lessons/12_Independence_Conditioning/09_Joint_distributions.html#what-is-a-joint-pmf",
    "href": "lessons/12_Independence_Conditioning/09_Joint_distributions.html#what-is-a-joint-pmf",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "What is a joint pmf?",
    "text": "What is a joint pmf?\n\n\nDefinition: joint pmf\n\n\nThe joint pmf of a pair of discrete r.v.’s \\(X\\) and \\(Y\\) is \\[p_{X,Y}(x,y) = \\mathbb{P}(X=x\\ and\\ Y=y) = \\mathbb{P}(X=x, Y=y)\\]"
  },
  {
    "objectID": "lessons/12_Independence_Conditioning/09_Joint_distributions.html#this-chapters-main-example",
    "href": "lessons/12_Independence_Conditioning/09_Joint_distributions.html#this-chapters-main-example",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "This chapter’s main example",
    "text": "This chapter’s main example\n\n\n\n\nExample 1\n\n\nLet \\(X\\) and \\(Y\\) be two random draws from a box containing balls labelled 1, 2, and 3 without replacement.\n\nFind \\(p_{X,Y}(x,y)\\).\nFind \\(\\mathbb{P}(X+Y=3).\\)\nFind \\(\\mathbb{P}(Y = 1).\\)\nFind \\(\\mathbb{P}(Y \\leq 2).\\)\nFind the joint CDF \\(F_{X,Y}(x,y)\\) for the joint pmf \\(p_{X,Y}(x,y)\\)\nFind the marginal CDFs \\(F_{X}(x)\\) and \\(F_{Y}(y)\\)\nFind \\(p_{X|Y}(x|y)\\).\nAre \\(X\\) and \\(Y\\) independent? Why or why not?"
  },
  {
    "objectID": "lessons/12_Independence_Conditioning/09_Joint_distributions.html#joint-pmf",
    "href": "lessons/12_Independence_Conditioning/09_Joint_distributions.html#joint-pmf",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "Joint pmf",
    "text": "Joint pmf\n\n\n\n\nExample 1\n\n\nLet \\(X\\) and \\(Y\\) be two random draws from a box containing balls labelled 1, 2, and 3 without replacement.\n\nFind \\(p_{X,Y}(x,y)\\).\nFind \\(\\mathbb{P}(X+Y=3).\\)"
  },
  {
    "objectID": "lessons/12_Independence_Conditioning/09_Joint_distributions.html#marginal-pmfs",
    "href": "lessons/12_Independence_Conditioning/09_Joint_distributions.html#marginal-pmfs",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "Marginal pmf’s",
    "text": "Marginal pmf’s\n\n\n\n\nExample 1\n\n\nLet \\(X\\) and \\(Y\\) be two random draws from a box containing balls labelled 1, 2, and 3 without replacement.\n\nFind \\(\\mathbb{P}(Y = 1).\\)\nFind \\(\\mathbb{P}(Y \\leq 2).\\)"
  },
  {
    "objectID": "lessons/12_Independence_Conditioning/09_Joint_distributions.html#remarks-on-the-joint-pmf",
    "href": "lessons/12_Independence_Conditioning/09_Joint_distributions.html#remarks-on-the-joint-pmf",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "Remarks on the joint pmf",
    "text": "Remarks on the joint pmf\nSome properties of joint pmf’s:\n\nA joint pmf \\(p_{X,Y}(x,y)\\) must satisfy the following properties:\n\n\\(p_{X,Y}(x,y)\\geq 0\\) for all \\(x, y\\).\n\\(\\sum \\limits_{\\{all\\ x\\}} \\sum \\limits_{\\{all\\ y\\}} p_{X,Y}(x,y)=1\\).\n\nMarginal pmf’s:\n\n\\(p_X(x) = \\sum \\limits_{\\{all\\ y\\}} p_{X,Y}(x,y)\\)\n\\(p_Y(y) = \\sum \\limits_{\\{all\\ x\\}} p_{X,Y}(x,y)\\)"
  },
  {
    "objectID": "lessons/12_Independence_Conditioning/09_Joint_distributions.html#what-is-a-joint-cdf",
    "href": "lessons/12_Independence_Conditioning/09_Joint_distributions.html#what-is-a-joint-cdf",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "What is a joint CDF?",
    "text": "What is a joint CDF?\n\n\nDefinition: joint CDF\n\n\nThe joint CDF of a pair of discrete r.v.’s \\(X\\) and \\(Y\\) is \\[F_{X,Y}(x,y) = \\mathbb{P}(X \\leq x\\ and\\ Y \\leq y) = \\mathbb{P}(X \\leq x, Y \\leq y)\\]"
  },
  {
    "objectID": "lessons/12_Independence_Conditioning/09_Joint_distributions.html#joint-cdfs",
    "href": "lessons/12_Independence_Conditioning/09_Joint_distributions.html#joint-cdfs",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "Joint CDFs",
    "text": "Joint CDFs\n\n\n\n\nExample 1\n\n\nLet \\(X\\) and \\(Y\\) be two random draws from a box containing balls labelled 1, 2, and 3 without replacement.\n\nFind the joint CDF \\(F_{X,Y}(x,y)\\) for the joint pmf \\(p_{X,Y}(x,y)\\)"
  },
  {
    "objectID": "lessons/12_Independence_Conditioning/09_Joint_distributions.html#marginal-cdfs",
    "href": "lessons/12_Independence_Conditioning/09_Joint_distributions.html#marginal-cdfs",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "Marginal CDFs",
    "text": "Marginal CDFs\n\n\n\n\nExample 1\n\n\nLet \\(X\\) and \\(Y\\) be two random draws from a box containing balls labelled 1, 2, and 3 without replacement.\n\nFind the marginal CDFs \\(F_{X}(x)\\) and \\(F_{Y}(y)\\)"
  },
  {
    "objectID": "lessons/12_Independence_Conditioning/09_Joint_distributions.html#remarks-on-the-joint-and-marginal-cdf",
    "href": "lessons/12_Independence_Conditioning/09_Joint_distributions.html#remarks-on-the-joint-and-marginal-cdf",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "Remarks on the joint and marginal CDF",
    "text": "Remarks on the joint and marginal CDF\n\n\\(F_X(x)\\): right most columns of the CDf table (where the \\(Y\\) values are largest)\n\\(F_Y(y)\\): bottom row of the table (where X values are largest)\n\\(F_X(x)=\\lim\\limits_{y\\rightarrow\\infty}F_{X, Y}(x,y)\\)\n\\(F_Y(y)=\\lim\\limits_{x\\rightarrow\\infty}F_{X, Y}(x,y)\\)"
  },
  {
    "objectID": "lessons/12_Independence_Conditioning/09_Joint_distributions.html#independence-and-conditioning",
    "href": "lessons/12_Independence_Conditioning/09_Joint_distributions.html#independence-and-conditioning",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "Independence and Conditioning",
    "text": "Independence and Conditioning\nRecall that for events \\(A\\) and \\(B\\),\n\n\\(\\mathbb{P}(A|B) = \\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(B)}\\)\n\\(A\\) and \\(B\\) are independent if and only if\n\n\\(\\mathbb{P}(A|B) = \\mathbb{P}(A)\\)\n\\(\\mathbb{P}(A \\cap B) = \\mathbb{P}(A)\\cdot\\mathbb{P}(B)\\)\n\n\nIndependence and conditioning are defined similarly for r.v.’s, since \\[p_X(x) = \\mathbb{P}(X=x)\\ \\mathrm{and}\\ \\ p_{X,Y}(x,y) = \\mathbb{P}(X = x ,Y = y).\\]"
  },
  {
    "objectID": "lessons/12_Independence_Conditioning/09_Joint_distributions.html#what-is-the-conditional-pmf",
    "href": "lessons/12_Independence_Conditioning/09_Joint_distributions.html#what-is-the-conditional-pmf",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "What is the conditional pmf?",
    "text": "What is the conditional pmf?\n\n\nDefinition: conditional pmf\n\n\nThe conditional pmf of a pair of discrete r.v.’s \\(X\\) and \\(Y\\) is defined as \\[p_{X|Y}(x|y) = \\mathbb{P}(X = x |Y = y) = \\frac{\\mathbb{P}(X = x\\ and\\ Y = y)}{\\mathbb{P}(Y = y)}\n=\\frac{p_{X,Y}(x,y) }{p_{Y}(y) }\\] if \\(p_{Y}(y) &gt; 0\\)."
  },
  {
    "objectID": "lessons/12_Independence_Conditioning/09_Joint_distributions.html#remarks-on-the-conditional-pmf",
    "href": "lessons/12_Independence_Conditioning/09_Joint_distributions.html#remarks-on-the-conditional-pmf",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "Remarks on the conditional pmf",
    "text": "Remarks on the conditional pmf\nThe following properties follow from the conditional pmf definition:\n\nIf \\(X \\perp Y\\) (independent)\n\n\\(p_{X|Y}(x|y) = p_X(x)\\) for all \\(x\\) and \\(y\\)\n\\(p_{X,Y}(x,y) = p_X(x)p_Y(y)\\) for all \\(x\\) and \\(y\\)\nWhich also implies (\\(\\Rightarrow\\)): \\(F_{X,Y}(x,y) = F_X(x)F_Y(y)\\) for all \\(x\\) and \\(y\\)\n\nIf \\(X_1, X_2, …, X_n\\) are independent\n\n\\[p_{X_1, X_2, …, X_n}(x_1, x_2, …, x_n) = P(X_1=x_1, X_2=x_2, …, X_n=x_n)=\\prod\\limits_{i=1}^np_{X_i}(x_i)\\]\n\\[F_{X_1, X_2, …, X_n}(x_1, x_2, …, x_n) = P(X_1\\leq x_1, X_2\\leq x_2, …, X_n\\leq x_n)=\\prod\\limits_{i=1}^nP(X_i \\leq x_i) = \\prod\\limits_{i=1}^nF_{X_i}(x_i)\\]"
  },
  {
    "objectID": "lessons/12_Independence_Conditioning/09_Joint_distributions.html#conditional-pmfs",
    "href": "lessons/12_Independence_Conditioning/09_Joint_distributions.html#conditional-pmfs",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "Conditional pmf’s",
    "text": "Conditional pmf’s\n\n\n\n\nExample 1\n\n\nLet \\(X\\) and \\(Y\\) be two random draws from a box containing balls labelled 1, 2, and 3 without replacement.\n\nFind \\(p_{X|Y}(x|y)\\).\nAre \\(X\\) and \\(Y\\) independent? Why or why not?\n\n\n\n\n\nRemark:\n\nTo show that \\(X\\) and \\(Y\\) are not independent, we just need to find one counter example\nHowever, to show that they are independent, we need to verify this for all possible pairs of \\(x\\) and \\(y\\)"
  },
  {
    "objectID": "lessons/12_Independence_Conditioning/09_Joint_distributions.html#hypothetical-4-sided-die",
    "href": "lessons/12_Independence_Conditioning/09_Joint_distributions.html#hypothetical-4-sided-die",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "Hypothetical 4-sided die",
    "text": "Hypothetical 4-sided die\n\n\nExample 3\n\n\n\nSuppose you have a 4-sided die, and you roll the 4-sided die until the first 4 appears.\nLet \\(X\\) be the number of rolls required until (and including) the first 4.\nAfter the first 4, you keep rolling it again until you roll a 3.\nLet \\(Y\\) be the number of rolls, after the first 4, required until (and including) the 3.\n\n\nFind \\(p_{X,Y}(x,y)\\).\nUsing \\(p_{X,Y}(x,y)\\), find \\(p_{Y}(y)\\).\nFind \\(p_{X}(x)\\).\nAre \\(X\\) and \\(Y\\) are independent? Why or why not?\nFind \\(F_{X,Y}(x,y)\\)."
  },
  {
    "objectID": "lessons/12_Independence_Conditioning/12_Independence_conditioning_muddy_points.html",
    "href": "lessons/12_Independence_Conditioning/12_Independence_conditioning_muddy_points.html",
    "title": "Muddy Points",
    "section": "",
    "text": "If we start at a joint probability \\(f_{X,Y}(x,y)\\)…. we can look at a few probabilities:\n\nJoint probability: \\(P(a \\leq X \\leq b, c \\leq Y \\leq d)\\)\n\\[P(a \\leq X \\leq b, c \\leq Y \\leq d) = \\displaystyle\\int_{x=a}^{x=b}\\displaystyle\\int_{y=c}^{y=d} f_{X,Y}(x,y) dydx\\]\nMarginal probability: \\(P(a \\leq X \\leq b)\\)\n\\[P(a \\leq X \\leq b) = \\displaystyle\\int_{x=a}^{x=b} f_{X}(x) dx\\]\nOR\n\\[P(a \\leq X \\leq b) = \\displaystyle\\int_{x=a}^{x=b}\\displaystyle\\int_{y=-\\inf}^{y=\\inf} f_{X,Y}(x,y) dydx\\]\nConditional probability: \\(P(a \\leq X \\leq b | Y = c)\\)\n\\[P(a \\leq X \\leq b | Y=c) = \\displaystyle\\int_{x=a}^{x=b} f_{X|Y}(x|y=c) dx\\]\nYou cannot calculate \\(P(a \\leq X \\leq b | Y = c)\\) by \\(\\dfrac{P(a \\leq X \\leq b, Y=c)}{P(Y = c)}\\) because \\(P(Y = c)\\) is 0. Instead, we need to find \\(f_{X|Y}(x|y=c)\\) by \\(\\dfrac{f_{X,Y}(x,y=c)}{f_{Y}(y=c)}\\) and THEN integrate over X."
  },
  {
    "objectID": "lessons/12_Independence_Conditioning/12_Independence_conditioning_muddy_points.html#fall-2023",
    "href": "lessons/12_Independence_Conditioning/12_Independence_conditioning_muddy_points.html#fall-2023",
    "title": "Muddy Points",
    "section": "",
    "text": "If we start at a joint probability \\(f_{X,Y}(x,y)\\)…. we can look at a few probabilities:\n\nJoint probability: \\(P(a \\leq X \\leq b, c \\leq Y \\leq d)\\)\n\\[P(a \\leq X \\leq b, c \\leq Y \\leq d) = \\displaystyle\\int_{x=a}^{x=b}\\displaystyle\\int_{y=c}^{y=d} f_{X,Y}(x,y) dydx\\]\nMarginal probability: \\(P(a \\leq X \\leq b)\\)\n\\[P(a \\leq X \\leq b) = \\displaystyle\\int_{x=a}^{x=b} f_{X}(x) dx\\]\nOR\n\\[P(a \\leq X \\leq b) = \\displaystyle\\int_{x=a}^{x=b}\\displaystyle\\int_{y=-\\inf}^{y=\\inf} f_{X,Y}(x,y) dydx\\]\nConditional probability: \\(P(a \\leq X \\leq b | Y = c)\\)\n\\[P(a \\leq X \\leq b | Y=c) = \\displaystyle\\int_{x=a}^{x=b} f_{X|Y}(x|y=c) dx\\]\nYou cannot calculate \\(P(a \\leq X \\leq b | Y = c)\\) by \\(\\dfrac{P(a \\leq X \\leq b, Y=c)}{P(Y = c)}\\) because \\(P(Y = c)\\) is 0. Instead, we need to find \\(f_{X|Y}(x|y=c)\\) by \\(\\dfrac{f_{X,Y}(x,y=c)}{f_{Y}(y=c)}\\) and THEN integrate over X."
  },
  {
    "objectID": "lessons/10_Transformations/10_Transformations.html",
    "href": "lessons/10_Transformations/10_Transformations.html",
    "title": "Lesson 10: Transformations",
    "section": "",
    "text": "Find the pdf of a linear rescaling of a random variable\nFind the pdf of a nonlinear transformation of a random variable using the CDF method\n\n\n\n\nOften make transformations of RVs\nA function of a random variable is a random variable\n\nIf \\(X\\) is a random variable and \\(g\\) is a function then \\(Y=g(X)\\) is a random variable\nSince \\(g(X)\\) is a random variable it has a distribution\n\nDistribution of \\(g(X)\\) will have a different shape than the distribution of \\(X\\)\nTwo types:\n\nLinear rescalings: \\(g(u) = a + bu\\)\nNonlinear transformations: e.g. \\(g(u) = u^2\\), \\(g(u) = \\log(u)\\)"
  },
  {
    "objectID": "lessons/10_Transformations/10_Transformations.html#cdf-method",
    "href": "lessons/10_Transformations/10_Transformations.html#cdf-method",
    "title": "Lesson 10: Transformations",
    "section": "",
    "text": "Often make transformations of RVs\nA function of a random variable is a random variable\n\nIf \\(X\\) is a random variable and \\(g\\) is a function then \\(Y=g(X)\\) is a random variable\nSince \\(g(X)\\) is a random variable it has a distribution\n\nDistribution of \\(g(X)\\) will have a different shape than the distribution of \\(X\\)\nTwo types:\n\nLinear rescalings: \\(g(u) = a + bu\\)\nNonlinear transformations: e.g. \\(g(u) = u^2\\), \\(g(u) = \\log(u)\\)"
  },
  {
    "objectID": "lessons/10_Transformations/10_Transformations.html#linear-rescaling",
    "href": "lessons/10_Transformations/10_Transformations.html#linear-rescaling",
    "title": "Lesson 10: Transformations",
    "section": "Linear rescaling",
    "text": "Linear rescaling\n\n\nDefinition: Linear Rescaling\n\n\nA linear rescaling is a transformation of the form \\(g(u) = a + bu\\), where \\(a\\) and \\(b\\) are constants\n\n\n\nThus, if we have a random variable, \\(X\\), then a linear rescaling of \\(X\\) could be \\(M = g(X) = a + bX\\)\nFor example, converting temperature from Celsius to Fahrenheit using \\(g(u) = 32 + 1.8u\\) is a linear rescaling."
  },
  {
    "objectID": "lessons/10_Transformations/10_Transformations.html#sim-linear-rescaling",
    "href": "lessons/10_Transformations/10_Transformations.html#sim-linear-rescaling",
    "title": "Lesson 10: Transformations",
    "section": "Example of linear rescaling (4/4)",
    "text": "Example of linear rescaling (4/4)\n\n\nExample 1: Linear rescaling of \\(U\\)\n\n\nLet \\(U\\) be a random variable with \\(f_U(u)= \\dfrac{4}{15}u^3\\) for \\(1\\leq u \\leq 2\\). Define \\(V=1-U\\)\n\nFind the pdf of \\(V\\).\nDoes \\(V\\) have the same distribution as \\(U\\)?"
  },
  {
    "objectID": "lessons/10_Transformations/10_Transformations.html#summary-of-linear-rescaling",
    "href": "lessons/10_Transformations/10_Transformations.html#summary-of-linear-rescaling",
    "title": "Lesson 10: Transformations",
    "section": "Summary of linear rescaling",
    "text": "Summary of linear rescaling\n\nA linear rescaling of a random variable does not change the basic shape of its distribution, just the range of possible values.\n\nIt can flip it, widen it, condense it, and/or shift it\n\nRemember, do NOT confuse a random variable with its distribution\n\nThe random variable is the numerical quantity being measured\nThe distribution is the long run pattern of variation of many observed values of the random variable"
  },
  {
    "objectID": "lessons/10_Transformations/10_Transformations.html#nonlinear-transformations",
    "href": "lessons/10_Transformations/10_Transformations.html#nonlinear-transformations",
    "title": "Lesson 10: Transformations",
    "section": "Nonlinear transformations",
    "text": "Nonlinear transformations\n\nWhat happens when we make a nonlinear transformation, like a logarithmic or square root transformation?\nNonlinear transformations do not necessarily preserve the distribution shape\nExamples of nonlinear transformations:\n\n\\(g(u) = u^2\\)\n\\(g(u) = \\sqrt{u}\\)\n\\(g(u) = \\log(u)\\)\n\\(g(u) = e^u\\)\n\\(g(u) = \\dfrac{1}{u}\\)"
  },
  {
    "objectID": "lessons/10_Transformations/10_Transformations.html#finding-the-pdf-of-a-transformation",
    "href": "lessons/10_Transformations/10_Transformations.html#finding-the-pdf-of-a-transformation",
    "title": "Lesson 10: Transformations",
    "section": "Finding the pdf of a transformation",
    "text": "Finding the pdf of a transformation\n\nLet \\(M\\) be a transformation of \\(X\\): \\(M = g(X)\\)\nWhen we have a transformation of \\(X\\), \\(M\\), we need to follow the CDF method to find the pdf of \\(M\\)\n\nWe follow CDF method:\n\nStart with the pdf for \\(X\\)\n\naka \\(f_{X}(x)\\)\n\nTranslate the domain of \\(X\\) to \\(M\\): find the possible values of \\(M\\)\nFind the CDF of \\(M\\)\n\naka \\(F_M(m) = P(M \\leq m) = P(g(X) \\leq m)\\)\nWill require manipulating \\(g(X) \\leq m\\) in terms of \\(X\\) (aka \\(X\\) alone on the left side)\n\nTake the derivative of the CDF of \\(M\\) with respect to \\(m\\) to find the pdf of \\(M\\)\n\naka \\(f_M(m) = \\dfrac{d}{dm}F_M(m)\\)"
  },
  {
    "objectID": "lessons/10_Transformations/10_Transformations.html#example-of-nonlinear-transformation-14",
    "href": "lessons/10_Transformations/10_Transformations.html#example-of-nonlinear-transformation-14",
    "title": "Lesson 10: Transformations",
    "section": "Example of nonlinear transformation (1/4)",
    "text": "Example of nonlinear transformation (1/4)\n\n\nExample 2: Nonlinear transformation of \\(U\\)\n\n\nLet \\(U\\) be a random variable with \\(f_U(u)= \\dfrac{4}{15}u^3\\) for \\(1\\leq u \\leq 2\\). Define \\(V=\\log(U)\\)\n\nWhat are the possible values of \\(V\\)?\nFind the CDF of \\(V\\)\nFind the pdf of \\(V\\)"
  },
  {
    "objectID": "lessons/10_Transformations/10_Transformations.html#example-of-nonlinear-transformation-24",
    "href": "lessons/10_Transformations/10_Transformations.html#example-of-nonlinear-transformation-24",
    "title": "Lesson 10: Transformations",
    "section": "Example of nonlinear transformation (2/4)",
    "text": "Example of nonlinear transformation (2/4)\n\n\nExample 2: Nonlinear transformation of \\(U\\)\n\n\nLet \\(U\\) be a random variable with \\(f_U(u)= \\dfrac{4}{15}u^3\\) for \\(1\\leq u \\leq 2\\). Define \\(V=\\log(U)\\)\n\nWhat are the possible values of \\(V\\)?"
  },
  {
    "objectID": "lessons/10_Transformations/10_Transformations.html#example-of-nonlinear-transformation-34",
    "href": "lessons/10_Transformations/10_Transformations.html#example-of-nonlinear-transformation-34",
    "title": "Lesson 10: Transformations",
    "section": "Example of nonlinear transformation (3/4)",
    "text": "Example of nonlinear transformation (3/4)\n\n\nExample 2: Nonlinear transformation of \\(U\\)\n\n\nLet \\(U\\) be a random variable with \\(f_U(u)= \\dfrac{4}{15}u^3\\) for \\(1\\leq u \\leq 2\\). Define \\(V=\\log(U)\\)\n\nFind the CDF of \\(V\\)"
  },
  {
    "objectID": "lessons/10_Transformations/10_Transformations.html#example-of-nonlinear-transformation-44",
    "href": "lessons/10_Transformations/10_Transformations.html#example-of-nonlinear-transformation-44",
    "title": "Lesson 10: Transformations",
    "section": "Example of nonlinear transformation (4/4)",
    "text": "Example of nonlinear transformation (4/4)\n\n\nExample 2: Nonlinear transformation of \\(U\\)\n\n\nLet \\(U\\) be a random variable with \\(f_U(u)= \\dfrac{4}{15}u^3\\) for \\(1\\leq u \\leq 2\\). Define \\(V=\\log(U)\\)\n\nFind the pdf of \\(V\\)"
  },
  {
    "objectID": "lessons/10_Transformations/10_Transformations.html#example-of-nonlinear-transformation-domain-14",
    "href": "lessons/10_Transformations/10_Transformations.html#example-of-nonlinear-transformation-domain-14",
    "title": "Lesson 10: Transformations",
    "section": "Example of nonlinear transformation: domain (1/4)",
    "text": "Example of nonlinear transformation: domain (1/4)\n\n\nExample 3: Nonlinear transformation of \\(X\\)\n\n\nLet \\(X\\) be a random variable with \\(f_X(x)= \\dfrac{1}{2}\\) for \\(-1\\leq x \\leq 1\\). Define \\(Y=X^2\\)\n\nWhat are the possible values of \\(Y\\)?\nFind the CDF of \\(Y\\)\nFind the pdf of \\(Y\\)"
  },
  {
    "objectID": "lessons/10_Transformations/10_Transformations.html#example-of-nonlinear-transformation-domain-24",
    "href": "lessons/10_Transformations/10_Transformations.html#example-of-nonlinear-transformation-domain-24",
    "title": "Lesson 10: Transformations",
    "section": "Example of nonlinear transformation: domain (2/4)",
    "text": "Example of nonlinear transformation: domain (2/4)\n\n\nExample 3: Nonlinear transformation of \\(X\\)\n\n\nLet \\(X\\) be a random variable with \\(f_X(x)= \\dfrac{1}{2}\\) for \\(-1\\leq x \\leq 1\\). Define \\(Y=X^2\\)\n\nWhat are the possible values of \\(Y\\)?"
  },
  {
    "objectID": "lessons/10_Transformations/10_Transformations.html#example-of-nonlinear-transformation-domain-34",
    "href": "lessons/10_Transformations/10_Transformations.html#example-of-nonlinear-transformation-domain-34",
    "title": "Lesson 10: Transformations",
    "section": "Example of nonlinear transformation: domain (3/4)",
    "text": "Example of nonlinear transformation: domain (3/4)\n\n\nExample 3: Nonlinear transformation of \\(X\\)\n\n\nLet \\(X\\) be a random variable with \\(f_X(x)= \\dfrac{1}{2}\\) for \\(-1\\leq x \\leq 1\\). Define \\(Y=X^2\\)\n\nFind the CDF of \\(Y\\)"
  },
  {
    "objectID": "lessons/10_Transformations/10_Transformations.html#example-of-nonlinear-transformation-domain-44",
    "href": "lessons/10_Transformations/10_Transformations.html#example-of-nonlinear-transformation-domain-44",
    "title": "Lesson 10: Transformations",
    "section": "Example of nonlinear transformation: domain (4/4)",
    "text": "Example of nonlinear transformation: domain (4/4)\n\n\nExample 3: Nonlinear transformation of \\(X\\)\n\n\nLet \\(X\\) be a random variable with \\(f_X(x)= \\dfrac{1}{2}\\) for \\(-1\\leq x \\leq 1\\). Define \\(Y=X^2\\)\n\nFind the pdf of \\(Y\\)"
  },
  {
    "objectID": "lessons/10_Transformations/10_Transformations.html#summary-of-nonlinear-transformations",
    "href": "lessons/10_Transformations/10_Transformations.html#summary-of-nonlinear-transformations",
    "title": "Lesson 10: Transformations",
    "section": "Summary of nonlinear transformations",
    "text": "Summary of nonlinear transformations\n\nNonlinear transformations can change the shape of a distribution\nAlways use the CDF method to find the pdf of a nonlinear transformation of a random variable\nRemember to carefully determine the possible values of the transformed random variable"
  },
  {
    "objectID": "lessons/10_Transformations/Old_notes/25_Joint_densities_muddy_points.html",
    "href": "lessons/10_Transformations/Old_notes/25_Joint_densities_muddy_points.html",
    "title": "Muddy Points",
    "section": "",
    "text": "Muddy points from Fall 2023:\n\n1. How do pdf, CDF, and probability interact with each other?\nLet’s say we have a pdf, \\(f_X(x) = \\dfrac{1}{9}x^2\\) for \\(0 \\leq x \\leq 3\\). This is just a function. The pdf is not used on its own to report any probability. We must integrate over the pdf to find a probability.\n\nlibrary(\"ggplot2\")\neq = function(x){(1/9)*x^2}\nggplot(data.frame(x=c(1, 50)), aes(x=x)) + \n  stat_function(fun=eq) +\n  xlab(\"x\") + ylab(\"pdf\") +\n  xlim(0,3)\n\n\n\n\n\n\n\n\nThe total area under the pdf is 1. This makes our pdf valid.\n\neq = function(x){(1/9)*x^2}\nggplot(data.frame(x=c(1, 50)), aes(x=x)) + \n  stat_function(fun=eq) +\n  xlab(\"x\") + ylab(\"pdf\") +\n  xlim(0,3) +\n  stat_function(fun=eq, \n                xlim = c(0, 3),\n                geom = \"area\", \n                aes(fill = \"red\")) +\n  theme(legend.position = \"none\") +\n  annotate(\"text\", x = 0.5, y = 0.7, label = \"AUC = 1\", color = \"black\")\n\n\n\n\n\n\n\n\nIf we only look at a proportion of the area under the pdf, then we start constructing our probabilities. For example, we can look at probability that we have a value between 0 and 1.5.\n\nggplot(data.frame(x=c(1, 50)), aes(x=x)) + \n  stat_function(fun=eq) +\n  xlab(\"x\") + ylab(\"pdf\") +\n  xlim(0,3) +\n  stat_function(fun=eq, \n                xlim = c(0, 1.5),\n                geom = \"area\", \n                aes(fill = \"blue\")) +\n  theme(legend.position = \"none\") +\n  annotate(\"text\", x = 0.5, y = 0.7, label = \"AUC = 0.125\", color = \"black\")\n\n\n\n\n\n\n\n\nInstead of calculating the EXACT probability for each value between 0 and 3, we can find the CDF of the pdf.\nThe CDF is: \\[\nF_X(x) = \\left\\{\n        \\begin{array}{ll}\n            0 & \\quad x&lt;3 \\quad \\\\\n            \\dfrac{1}{27}x^3 & \\quad 0 \\leq x \\leq 3\\quad \\\\\n            1 & \\quad x&gt;3 \\quad\n        \\end{array}\n    \\right.\n\\]\n\ncdf = function(x){(1/27)*x^3}\nggplot(data.frame(x=c(1, 50)), aes(x=x)) + \n  stat_function(fun=cdf) +\n  xlab(\"x\") + ylab(\"CDF\") +\n  xlim(0,3) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nWhen \\(x=1.5\\), we can calculate the probability using the CDF. Remember that \\(F_X(x) = P(X \\leq x)\\). So we can say \\(P(X \\leq 1.5) = F_X(1.5) = \\dfrac{1}{27}(1.5)^3\\), which equals 0.125.\n\ncdf = function(x){(1/27)*x^3}\nggplot(data.frame(x=c(1, 50)), aes(x=x)) + \n  stat_function(fun=cdf) +\n  xlab(\"x\") + ylab(\"CDF\") +\n  xlim(0,3) +\n  theme(legend.position = \"none\") +\n  geom_point(aes(x=1.5, y=.125), colour=\"blue\", size=3) +\n  annotate(\"text\", x = 0.5, y = 0.7, label = \"CDF = 0.125\", color = \"black\")\n\nWarning in geom_point(aes(x = 1.5, y = 0.125), colour = \"blue\", size = 3): All aesthetics have length 1, but the data has 2 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\nWe can also calculate the probability with an integral: \\(P(X \\leq 1.5) = \\displaystyle\\int_0^{1.5} \\dfrac{1}{9}x^2 dx\\).\nWe can also find the probability that X is between two numbers. \\(P(1\\leq X \\leq 1.5) = F_X(1.5) - F_X(1)\\) or \\(P(1\\leq X \\leq 1.5) = \\displaystyle\\int_1^{1.5} \\dfrac{1}{9}x^2 dx\\).\n\n\n2. Joint vs marginal vs conditional: How are we calculating the probability?\nIf we start at a joint probability \\(f_{X,Y}(x,y)\\)…. we can look at a few probabilities:\n\nJoint probability: \\(P(a \\leq X \\leq b, c \\leq Y \\leq d)\\)\n\\[P(a \\leq X \\leq b, c \\leq Y \\leq d) = \\displaystyle\\int_{x=a}^{x=b}\\displaystyle\\int_{y=c}^{y=d} f_{X,Y}(x,y) dydx\\]\nMarginal probability: \\(P(a \\leq X \\leq b)\\)\n\\[P(a \\leq X \\leq b) = \\displaystyle\\int_{x=a}^{x=b} f_{X}(x) dx\\]\nOR\n\\[P(a \\leq X \\leq b) = \\displaystyle\\int_{x=a}^{x=b}\\displaystyle\\int_{y=-\\inf}^{y=\\inf} f_{X,Y}(x,y) dydx\\]\nConditional probability: \\(P(a \\leq X \\leq b | Y = c)\\)\n\\[P(a \\leq X \\leq b | Y=c) = \\displaystyle\\int_{x=a}^{x=b} f_{X|Y}(x|y=c) dx\\]\nYou cannot calculate \\(P(a \\leq X \\leq b | Y = c)\\) by \\(\\dfrac{P(a \\leq X \\leq b, Y=c)}{P(Y = c)}\\) because \\(P(Y = c)\\) is 0. Instead, we need to find \\(f_{X|Y}(x|y=c)\\) by \\(\\dfrac{f_{X,Y}(x,y=c)}{f_{Y}(y=c)}\\) and THEN integrate over X.\n\n\n\n3. What are we actually finding by solving the double integral. In the first example, we found the probability was 1/16 after integrating but what does 1/16 mean in relation to the random variables X and Y?\nIt means that the volume contained by \\(0\\leq X \\leq 1\\), \\(0\\leq Y \\leq 1/2\\), and their joint pdf is 1/16 of the total volume contained by \\(0\\leq X \\leq 2\\), \\(0\\leq Y \\leq 1\\), and their joint pdf. The probability for a joint pdf is now a measure of the proportion of the volume.\nThis is not be confused with a probability from marginal pdf or pdf from one RV. The probability for marginal/single RV pdfs is the proportion of the area under the pdf for a specific range of values.\n\n\n4. Here’s a 3D plot of one of our joint pdf’s\n\\[\nf_{X,Y}(x,y) = 5e^{-x-3y} \\text{ for } 0 \\leq y \\leq x/2\n\\]\n\nlibrary(plotly)\n\nx = seq(0, 5, 0.1)\ny = seq(0, max(x)/2, 0.1/2)\nfn = expand.grid(x=x,y=y)\nfn$z = ifelse(fn$y&lt;fn$x/2, 5*exp( (-1)*fn$x - 3*fn$y), NA)\n\nz = matrix(fn$z, ncol = 51, nrow = 51, byrow = T)\n\nfig &lt;- plot_ly(x = x, y=y, z=z) %&gt;% add_surface()\n\nfig"
  },
  {
    "objectID": "lessons/10_Transformations/Old_notes/25_Joint_densities.html",
    "href": "lessons/10_Transformations/Old_notes/25_Joint_densities.html",
    "title": "Chapter 25: Joint densities",
    "section": "",
    "text": "Solve double integrals in our mini lesson!\nCalculate probabilities for a pair of continuous random variables\nCalculate a joint and marginal probability density function (pdf)\nCalculate a joint and marginal cumulative distribution function (CDF) from a pdf"
  },
  {
    "objectID": "lessons/10_Transformations/Old_notes/25_Joint_densities.html#learning-objectives",
    "href": "lessons/10_Transformations/Old_notes/25_Joint_densities.html#learning-objectives",
    "title": "Chapter 25: Joint densities",
    "section": "",
    "text": "Solve double integrals in our mini lesson!\nCalculate probabilities for a pair of continuous random variables\nCalculate a joint and marginal probability density function (pdf)\nCalculate a joint and marginal cumulative distribution function (CDF) from a pdf"
  },
  {
    "objectID": "lessons/10_Transformations/Old_notes/25_Joint_densities.html#double-integrals-mini-lesson-13",
    "href": "lessons/10_Transformations/Old_notes/25_Joint_densities.html#double-integrals-mini-lesson-13",
    "title": "Chapter 25: Joint densities",
    "section": "Double Integrals Mini Lesson (1/3)",
    "text": "Double Integrals Mini Lesson (1/3)\n\n\n\n\n\n\nDo this problem at home for extra practice. I’ll add the solution to the annotated notes!\n\n\n\n\n\n\n\nMini Lesson Example 1\n\n\nSolve the following integral: \\(\\displaystyle\\int_{2}^{3}\\displaystyle\\int_{0}^{1} xy dydx\\)"
  },
  {
    "objectID": "lessons/10_Transformations/Old_notes/25_Joint_densities.html#double-integrals-mini-lesson-23",
    "href": "lessons/10_Transformations/Old_notes/25_Joint_densities.html#double-integrals-mini-lesson-23",
    "title": "Chapter 25: Joint densities",
    "section": "Double Integrals Mini Lesson (2/3)",
    "text": "Double Integrals Mini Lesson (2/3)\n\n\n\n\n\n\nDo this problem at home for extra practice. I’ll add the solution to the annotated notes!\n\n\n\n\n\n\n\nMini Lesson Example 2\n\n\nSolve the following integral: \\(\\displaystyle\\int_{2}^{3}\\displaystyle\\int_{0}^{1} (x+y) dydx\\)"
  },
  {
    "objectID": "lessons/10_Transformations/Old_notes/25_Joint_densities.html#double-integrals-mini-lesson-33",
    "href": "lessons/10_Transformations/Old_notes/25_Joint_densities.html#double-integrals-mini-lesson-33",
    "title": "Chapter 25: Joint densities",
    "section": "Double Integrals Mini Lesson (3/3)",
    "text": "Double Integrals Mini Lesson (3/3)\n\n\n\n\n\n\nDo this problem at home for extra practice. I’ll add the solution to the annotated notes!\n\n\n\n\n\n\n\nMini Lesson Example 3\n\n\nSolve the following integral: \\(\\displaystyle\\int_{2}^{3}\\displaystyle\\int_{0}^{1} e^{x+y} dydx\\)"
  },
  {
    "objectID": "lessons/10_Transformations/Old_notes/25_Joint_densities.html#how-to-define-the-joint-pdf-for-continuous-rvs",
    "href": "lessons/10_Transformations/Old_notes/25_Joint_densities.html#how-to-define-the-joint-pdf-for-continuous-rvs",
    "title": "Chapter 25: Joint densities",
    "section": "How to define the joint pdf for continuous RVs?",
    "text": "How to define the joint pdf for continuous RVs?\n\n\nFor a single continuous RV \\(X\\) is a function \\(f_X(x)\\), such that for all real values \\(a,b\\) with \\(a \\leq b\\), \\[\\mathbb{P}(a \\leq X \\leq b) = \\int_a^b f_X(x)dx\\]\n\nFor two continuous RVs (\\(X\\) and \\(Y\\)), we can define the joint pdf, \\(f_{X,Y}(x,y)\\), such that for all real values \\(a,b, c, d\\) with \\(a \\leq b\\) and \\(c \\leq d\\), \\[\\mathbb{P}(a \\leq X \\leq b, c \\leq Y \\leq d) = \\int_a^b \\int_c^d f_{X,Y}(x,y)dydx\\]"
  },
  {
    "objectID": "lessons/10_Transformations/Old_notes/25_Joint_densities.html#important-properties-of-the-joint-pdf",
    "href": "lessons/10_Transformations/Old_notes/25_Joint_densities.html#important-properties-of-the-joint-pdf",
    "title": "Chapter 25: Joint densities",
    "section": "Important properties of the joint pdf",
    "text": "Important properties of the joint pdf\n\nNote that \\(f_{X,Y}(x,y)\\neq \\mathbb{P}(X=x, Y=y)\\)!!!\nIn order for \\(f_{X,Y}(x,y)\\) to be a pdf, it needs to satisfy the properties\n\n\\(f_{X,Y}(x,y)\\geq 0\\) for all \\(x,y\\)\n\\(\\displaystyle\\int_{-\\infty}^{\\infty}\\displaystyle\\int_{-\\infty}^{\\infty} f_{X,Y}(x,y)dxdy=1\\)"
  },
  {
    "objectID": "lessons/10_Transformations/Old_notes/25_Joint_densities.html#what-is-the-joint-cumulative-distribution-function",
    "href": "lessons/10_Transformations/Old_notes/25_Joint_densities.html#what-is-the-joint-cumulative-distribution-function",
    "title": "Chapter 25: Joint densities",
    "section": "What is the joint cumulative distribution function?",
    "text": "What is the joint cumulative distribution function?\n\n\nDefinition: Joint cumulative distribution function (Joint CDF)\n\n\nThe joint cumulative distribution function (cdf) of continuous random variables \\(X\\) and \\(Y\\), is the function \\(F_{X,Y}(x,y)\\), such that for all real values of \\(x\\) and \\(y\\), \\[F_{X,Y}(x,y)= \\mathbb{P}(X \\leq x, Y \\leq y) = \\int_{-\\infty}^x\\int_{-\\infty}^y f_{X,Y}(s,t)dtds\\]\n\n\nRemarks:\n\nThe definition above for \\(F_{X,Y}(x,y)\\) is a function of \\(x\\) and \\(y\\).\nThe joint cdf at the point \\((a,b)\\), is \\[F_{X,Y}(a,b) = \\mathbb{P}(X \\leq a, Y \\leq b) = \\int_{-\\infty}^a\\int_{-\\infty}^b f_{X,Y}(s,t)dtds\\]"
  },
  {
    "objectID": "lessons/10_Transformations/Old_notes/25_Joint_densities.html#what-are-the-marginal-pdfs",
    "href": "lessons/10_Transformations/Old_notes/25_Joint_densities.html#what-are-the-marginal-pdfs",
    "title": "Chapter 25: Joint densities",
    "section": "What are the marginal pdf’s?",
    "text": "What are the marginal pdf’s?\n\n\nDefinition: Marginal pdf’s\n\n\nSuppose \\(X\\) and \\(Y\\) are continuous r.v.’s, with joint pdf \\(f_{X,Y}(x,y)\\). Then the marginal probability density functions are \\[\\begin{aligned}\nf_X(x)&=& \\int_{-\\infty}^{\\infty} f_{X,Y}(x,y)dy\\\\\nf_Y(y)&=& \\int_{-\\infty}^{\\infty} f_{X,Y}(x,y)dx\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/10_Transformations/Old_notes/25_Joint_densities.html#common-steps-for-solving-problems",
    "href": "lessons/10_Transformations/Old_notes/25_Joint_densities.html#common-steps-for-solving-problems",
    "title": "Chapter 25: Joint densities",
    "section": "Common steps for solving problems",
    "text": "Common steps for solving problems\n\nSet up the domain of the pdf with a picture\nTranslate to needed integrands\n\nFor probability: shade in the area of interest, then translate\nFor expected value: translate domain\n\nSet up integral: \\(dxdy\\) or \\(dydx\\)?\nSolve integral!"
  },
  {
    "objectID": "lessons/10_Transformations/Old_notes/25_Joint_densities.html#example-of-joint-pdf",
    "href": "lessons/10_Transformations/Old_notes/25_Joint_densities.html#example-of-joint-pdf",
    "title": "Chapter 25: Joint densities",
    "section": "Example of joint pdf",
    "text": "Example of joint pdf\n\n\n\n\nExample 1.1\n\n\nLet \\(f_{X,Y}(x,y)= \\frac32 y^2\\), for \\(0 \\leq x \\leq 2, \\ 0 \\leq y \\leq 1\\).\n\nFind \\(\\mathbb{P}(0 \\leq X \\leq 1, 0 \\leq Y \\leq \\frac12)\\)."
  },
  {
    "objectID": "lessons/10_Transformations/Old_notes/25_Joint_densities.html#example-of-joint-pdf-1",
    "href": "lessons/10_Transformations/Old_notes/25_Joint_densities.html#example-of-joint-pdf-1",
    "title": "Chapter 25: Joint densities",
    "section": "Example of joint pdf",
    "text": "Example of joint pdf\n\n\n\n\nExample 1.2\n\n\nLet \\(f_{X,Y}(x,y)= \\frac32 y^2\\), for \\(0 \\leq x \\leq 2, \\ 0 \\leq y \\leq 1\\).\n\nFind \\(f_X(x)\\) and \\(f_Y(y)\\)."
  },
  {
    "objectID": "lessons/10_Transformations/Old_notes/25_Joint_densities.html#example-of-a-more-complicated-joint-pdf",
    "href": "lessons/10_Transformations/Old_notes/25_Joint_densities.html#example-of-a-more-complicated-joint-pdf",
    "title": "Chapter 25: Joint densities",
    "section": "Example of a more complicated joint pdf",
    "text": "Example of a more complicated joint pdf\n\n\n\n\n\n\nDo this problem at home for extra practice. I’ll add the solution to the annotated notes!\n\n\n\n\n\n\n\nExample 2.1\n\n\nLet \\(f_{X,Y}(x,y)= 2 e^{-(x+y)}\\), for \\(0 \\leq x \\leq y\\).\n\nFind \\(f_X(x)\\) and \\(f_Y(y)\\)."
  },
  {
    "objectID": "lessons/10_Transformations/Old_notes/25_Joint_densities.html#example-of-a-more-complicated-joint-pdf-1",
    "href": "lessons/10_Transformations/Old_notes/25_Joint_densities.html#example-of-a-more-complicated-joint-pdf-1",
    "title": "Chapter 25: Joint densities",
    "section": "Example of a more complicated joint pdf",
    "text": "Example of a more complicated joint pdf\n\n\n\n\n\n\nDo this problem at home for extra practice. I’ll add the solution to the annotated notes!\n\n\n\n\n\n\n\nExample 2.2\n\n\nLet \\(f_{X,Y}(x,y)= 2 e^{-(x+y)}\\), for \\(0 \\leq x \\leq y\\).\n\nFind \\(\\mathbb{P}(Y &lt; 3)\\)."
  },
  {
    "objectID": "lessons/10_Transformations/Old_notes/25_Joint_densities.html#lets-complicate-this-even-more",
    "href": "lessons/10_Transformations/Old_notes/25_Joint_densities.html#lets-complicate-this-even-more",
    "title": "Chapter 25: Joint densities",
    "section": "Let’s complicate this even more!",
    "text": "Let’s complicate this even more!\n\n\n\n\nExample 3.1\n\n\nLet \\(X\\) and \\(Y\\) have constant density on the square \\(0 \\leq X \\leq 4, 0 \\leq Y \\leq 4\\).\n\nFind \\(\\mathbb{P}(|X-Y| &lt; 2)\\)"
  },
  {
    "objectID": "lessons/10_Transformations/Old_notes/25_Joint_densities.html#finding-the-pdf-of-a-transformation",
    "href": "lessons/10_Transformations/Old_notes/25_Joint_densities.html#finding-the-pdf-of-a-transformation",
    "title": "Chapter 25: Joint densities",
    "section": "Finding the pdf of a transformation",
    "text": "Finding the pdf of a transformation\n\nLet \\(M\\) be a transformation of \\(X\\) and \\(Y\\)\nWhen we have a transformation of \\(X\\) and \\(Y\\), \\(M\\), we need to follow a specific process to find the pdf of \\(M\\)\n\nWe follow this process:\n\nStart with the joint pdf for \\(X\\) and \\(Y\\)\n\naka \\(f_{X,Y}(x, y)\\)\n\nTranslate the domain of \\(X\\) and \\(Y\\) to \\(M\\)\nFind the CDF of \\(M\\)\n\naka \\(F_M(m)\\) or \\(P(M \\leq m)\\)\n\nTake the derivative of the CDF of \\(M\\) to find the pdf of \\(M\\)\n\naka \\(f_M(m) = \\dfrac{d}{dm}F_M(m)\\)"
  },
  {
    "objectID": "lessons/10_Transformations/Old_notes/25_Joint_densities.html#lets-complicate-this-even-more-1",
    "href": "lessons/10_Transformations/Old_notes/25_Joint_densities.html#lets-complicate-this-even-more-1",
    "title": "Chapter 25: Joint densities",
    "section": "Let’s complicate this even more!",
    "text": "Let’s complicate this even more!\n\n\n\n\nExample 3.2\n\n\nLet \\(X\\) and \\(Y\\) have constant density on the square \\(0 \\leq X \\leq 4, 0 \\leq Y \\leq 4\\).\n\nLet \\(M = \\max(X,Y)\\). Find the pdf for \\(M\\), that is \\(f_M(m)\\)."
  },
  {
    "objectID": "lessons/10_Transformations/Old_notes/25_Joint_densities.html#lets-complicate-this-even-more-2",
    "href": "lessons/10_Transformations/Old_notes/25_Joint_densities.html#lets-complicate-this-even-more-2",
    "title": "Chapter 25: Joint densities",
    "section": "Let’s complicate this even more!",
    "text": "Let’s complicate this even more!\n\n\n\n\n\n\nDo this problem at home for extra practice. I’ll add the solution to the annotated notes!\n\n\n\n\n\n\n\nExample 3.3\n\n\nLet \\(X\\) and \\(Y\\) have constant density on the square \\(0 \\leq X \\leq 4, 0 \\leq Y \\leq 4\\).\n\nLet \\(Z = \\min(X,Y)\\). Find the pdf for \\(Z\\), that is \\(f_Z(z)\\)."
  },
  {
    "objectID": "lessons/10_Transformations/Old_notes/25_Joint_densities.html#lets-complicate-this-even-further",
    "href": "lessons/10_Transformations/Old_notes/25_Joint_densities.html#lets-complicate-this-even-further",
    "title": "Chapter 25: Joint densities",
    "section": "Let’s complicate this even further!",
    "text": "Let’s complicate this even further!\n\n\n\n\n\n\nDo this problem at home for extra practice. I’ll add the solution to the annotated notes!\n\n\n\n\n\n\n\nExample 4\n\n\nLet \\(X\\) and \\(Y\\) have joint density \\(f_{X,Y}(x,y)= \\frac85(x+y)\\) in the region \\(0 &lt; x &lt; 1,\\ \\frac12 &lt; y &lt;1\\). Find the pdf of the r.v. \\(Z\\), where \\(Z=XY\\)."
  },
  {
    "objectID": "lessons/13_Expected_Values/13_Expected_Values_muddy_points.html",
    "href": "lessons/13_Expected_Values/13_Expected_Values_muddy_points.html",
    "title": "Muddy Points",
    "section": "",
    "text": "Here’s a pretty good video on integration by parts!\nHere’s the Calc review muddy points with a few words on integration by parts.\n\n\n\nGot a little help from Chatgpt:\nThink back to our discrete example with the die. Our expected value was the weighted average of all the possible outcomes (weighted by their probability). So our expected value for discrete RVs will always be that weighted average: \\[\\mathbb{E}[X] = \\sum_{i=1}^n x_ip_X(x_i)\\]\nFor continuous RVs, they can take infinite possible values, so we cannot sum across the pdf the same way. We still want the weighted average, so we need to find a way to “sum” the weighted outcomes for the continuous RV, which translates to an integral.\nHere’s a fairly good explanation on StackExchange..)\n\n\n\nFinal example: Let \\(f_{X,Y}(x,y)= 2e^{-(x+y)}\\), for \\(0 \\leq x \\leq y\\). Find \\(\\mathbb{E}[X]\\).\nLet’s start with a plot for the domain. We have \\(0 \\leq x \\leq y\\), so we know that \\(x\\geq0\\) and because \\(y\\geq x\\), then \\(y\\geq0\\), so we’ll have a plot of all positive values of \\(x\\) and \\(y\\):\n\n\n\n\n\n\n\n\n\nOkay, now let’s add the information that \\(y \\geq x\\). We can look at the line, \\(y=x\\), and identify the area on the side of the line that upholds \\(y \\geq x\\):\n\n\n\n\n\n\n\n\n\nThe area above the line is where \\(y\\geq x\\):\n\n\n\n\n\n\n\n\n\nSo we need to find the bounds for the orange region in terms of \\(x\\) and \\(y\\).\nWhichever random variable is on the inner integral will need to incorporate the \\(y=x\\) line. If we integrate over \\(y\\) first, we will integrate from \\(y=x\\) to \\(y=\\infty\\). Once we have incorporated the line into our first integral, then we no longer need to worry about the \\(y=x\\) line. For \\(x\\), we can integrate from \\(x=0\\) to \\(x=\\infty\\)."
  },
  {
    "objectID": "lessons/13_Expected_Values/13_Expected_Values_muddy_points.html#fall-2025",
    "href": "lessons/13_Expected_Values/13_Expected_Values_muddy_points.html#fall-2025",
    "title": "Muddy Points",
    "section": "",
    "text": "Here’s a pretty good video on integration by parts!\nHere’s the Calc review muddy points with a few words on integration by parts.\n\n\n\nGot a little help from Chatgpt:\nThink back to our discrete example with the die. Our expected value was the weighted average of all the possible outcomes (weighted by their probability). So our expected value for discrete RVs will always be that weighted average: \\[\\mathbb{E}[X] = \\sum_{i=1}^n x_ip_X(x_i)\\]\nFor continuous RVs, they can take infinite possible values, so we cannot sum across the pdf the same way. We still want the weighted average, so we need to find a way to “sum” the weighted outcomes for the continuous RV, which translates to an integral.\nHere’s a fairly good explanation on StackExchange..)\n\n\n\nFinal example: Let \\(f_{X,Y}(x,y)= 2e^{-(x+y)}\\), for \\(0 \\leq x \\leq y\\). Find \\(\\mathbb{E}[X]\\).\nLet’s start with a plot for the domain. We have \\(0 \\leq x \\leq y\\), so we know that \\(x\\geq0\\) and because \\(y\\geq x\\), then \\(y\\geq0\\), so we’ll have a plot of all positive values of \\(x\\) and \\(y\\):\n\n\n\n\n\n\n\n\n\nOkay, now let’s add the information that \\(y \\geq x\\). We can look at the line, \\(y=x\\), and identify the area on the side of the line that upholds \\(y \\geq x\\):\n\n\n\n\n\n\n\n\n\nThe area above the line is where \\(y\\geq x\\):\n\n\n\n\n\n\n\n\n\nSo we need to find the bounds for the orange region in terms of \\(x\\) and \\(y\\).\nWhichever random variable is on the inner integral will need to incorporate the \\(y=x\\) line. If we integrate over \\(y\\) first, we will integrate from \\(y=x\\) to \\(y=\\infty\\). Once we have incorporated the line into our first integral, then we no longer need to worry about the \\(y=x\\) line. For \\(x\\), we can integrate from \\(x=0\\) to \\(x=\\infty\\)."
  },
  {
    "objectID": "lessons/13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/28_29_Joint_expected_value_Variance/28_29_Joint_expected_value_Variance.html",
    "href": "lessons/13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/28_29_Joint_expected_value_Variance/28_29_Joint_expected_value_Variance.html",
    "title": "Chapter 28: Revisiting Expected Values for Joint Distributions",
    "section": "",
    "text": "Calculate the mean (expected value) of a joint distribution of continuous RV"
  },
  {
    "objectID": "lessons/13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/28_29_Joint_expected_value_Variance/28_29_Joint_expected_value_Variance.html#learning-objectives",
    "href": "lessons/13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/28_29_Joint_expected_value_Variance/28_29_Joint_expected_value_Variance.html#learning-objectives",
    "title": "Chapter 28: Revisiting Expected Values for Joint Distributions",
    "section": "",
    "text": "Calculate the mean (expected value) of a joint distribution of continuous RV"
  },
  {
    "objectID": "lessons/13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/28_29_Joint_expected_value_Variance/28_29_Joint_expected_value_Variance.html#remark-on-expected-value-of-one-rv-from-joint-pdf",
    "href": "lessons/13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/28_29_Joint_expected_value_Variance/28_29_Joint_expected_value_Variance.html#remark-on-expected-value-of-one-rv-from-joint-pdf",
    "title": "Chapter 28: Revisiting Expected Values for Joint Distributions",
    "section": "Remark on expected value of one RV from joint pdf",
    "text": "Remark on expected value of one RV from joint pdf\nIf you are given \\(f_{X,Y}(x,y)\\) and want to calculate \\(\\mathbb{E}[X]\\), you have two options:\n\nFind \\(f_X(x)\\) and use it to calculate \\(\\mathbb{E}[X]\\).\nOr, calculate \\(\\mathbb{E}[X]\\) using the joint density: \\[\\mathbb{E}[X] = \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}x f_{X,Y}(x,y)dydx.\\]"
  },
  {
    "objectID": "lessons/13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/28_29_Joint_expected_value_Variance/28_29_Joint_expected_value_Variance.html#option-1-expected-value-from-a-joint-distribution",
    "href": "lessons/13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/28_29_Joint_expected_value_Variance/28_29_Joint_expected_value_Variance.html#option-1-expected-value-from-a-joint-distribution",
    "title": "Chapter 28: Revisiting Expected Values for Joint Distributions",
    "section": "Option 1: Expected value from a joint distribution",
    "text": "Option 1: Expected value from a joint distribution\n\n\n\n\nExample 3\n\n\nLet \\(f_{X,Y}(x,y)= 2e^{-(x+y)}\\), for \\(0 \\leq x \\leq y\\). Find \\(\\mathbb{E}[X]\\)."
  },
  {
    "objectID": "lessons/13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/28_29_Joint_expected_value_Variance/28_29_Joint_expected_value_Variance.html#option-2-expected-value-from-a-joint-distribution",
    "href": "lessons/13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/28_29_Joint_expected_value_Variance/28_29_Joint_expected_value_Variance.html#option-2-expected-value-from-a-joint-distribution",
    "title": "Chapter 28: Revisiting Expected Values for Joint Distributions",
    "section": "Option 2: Expected value from a joint distribution",
    "text": "Option 2: Expected value from a joint distribution\n\n\n\n\nExample 1\n\n\nLet \\(f_{X,Y}(x,y)= 2e^{-(x+y)}\\), for \\(0 \\leq x \\leq y\\). Find \\(\\mathbb{E}[X]\\)."
  },
  {
    "objectID": "lessons/13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/36_Sums_of_Independent_Normal_rv/36_Sums_of_Independent_Normal_rv_key_info.html",
    "href": "lessons/13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/36_Sums_of_Independent_Normal_rv/36_Sums_of_Independent_Normal_rv_key_info.html",
    "title": "Key Info",
    "section": "",
    "text": "Homework 7 presentations due soon!\n\nI will check your HW 7 solutions ASAP!\n\nDepending on how far we get today and Wednesday, I might cancel the 12/9 class\nAnything else?"
  },
  {
    "objectID": "lessons/13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/36_Sums_of_Independent_Normal_rv/36_Sums_of_Independent_Normal_rv_key_info.html#announcements",
    "href": "lessons/13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/36_Sums_of_Independent_Normal_rv/36_Sums_of_Independent_Normal_rv_key_info.html#announcements",
    "title": "Key Info",
    "section": "",
    "text": "Homework 7 presentations due soon!\n\nI will check your HW 7 solutions ASAP!\n\nDepending on how far we get today and Wednesday, I might cancel the 12/9 class\nAnything else?"
  },
  {
    "objectID": "lessons/13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/36_Sums_of_Independent_Normal_rv/36_Sums_of_Independent_Normal_rv_key_info.html#key-dates",
    "href": "lessons/13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/36_Sums_of_Independent_Normal_rv/36_Sums_of_Independent_Normal_rv_key_info.html#key-dates",
    "title": "Key Info",
    "section": "Key Dates",
    "text": "Key Dates\n\nThursday: HW 8 assignment due\nSunday: HW 7 presentations due at 11pm"
  },
  {
    "objectID": "lessons/13_Expected_Values/13_Expected_Values_key_info.html",
    "href": "lessons/13_Expected_Values/13_Expected_Values_key_info.html",
    "title": "Key Info",
    "section": "",
    "text": "Quiz 2 back: everyone got at least a 12/14! Woo!\n\nMost common mistake: Question 10 - the fact that \\(f_X(x)\\) can be greater than 1 is:\n\nWrong answer: Invalid, because a pdf (like a probability) can never be greater than 1\nCorrect answer: Valid, because the area under the curve from 0 to 1 is 1, even if the height of the function is greater than 1\n\n\nFeel free to use your no-questions-asked extensions on homework\n\nAnd remember that there is no grade penalty for turning in homework late\n\nCharles is not obligated to give you feedback though\n\n\nSadly, there is no audio in the class on 11/5\n\nNot sure if human error or computer issue"
  },
  {
    "objectID": "lessons/13_Expected_Values/13_Expected_Values_key_info.html#announcements",
    "href": "lessons/13_Expected_Values/13_Expected_Values_key_info.html#announcements",
    "title": "Key Info",
    "section": "",
    "text": "Quiz 2 back: everyone got at least a 12/14! Woo!\n\nMost common mistake: Question 10 - the fact that \\(f_X(x)\\) can be greater than 1 is:\n\nWrong answer: Invalid, because a pdf (like a probability) can never be greater than 1\nCorrect answer: Valid, because the area under the curve from 0 to 1 is 1, even if the height of the function is greater than 1\n\n\nFeel free to use your no-questions-asked extensions on homework\n\nAnd remember that there is no grade penalty for turning in homework late\n\nCharles is not obligated to give you feedback though\n\n\nSadly, there is no audio in the class on 11/5\n\nNot sure if human error or computer issue"
  },
  {
    "objectID": "lessons/13_Expected_Values/13_Expected_Values_key_info.html#key-dates",
    "href": "lessons/13_Expected_Values/13_Expected_Values_key_info.html#key-dates",
    "title": "Key Info",
    "section": "Key Dates",
    "text": "Key Dates\n\nHW 6 due this Sunday"
  },
  {
    "objectID": "lessons/04_Rules_of_prob/03_Independent_Events.html",
    "href": "lessons/04_Rules_of_prob/03_Independent_Events.html",
    "title": "Chapter 3: Independent Events",
    "section": "",
    "text": "Define independence of 2-3 events given probability notation\nCalculate whether two or more events are independent"
  },
  {
    "objectID": "lessons/04_Rules_of_prob/03_Independent_Events.html#learning-objectives",
    "href": "lessons/04_Rules_of_prob/03_Independent_Events.html#learning-objectives",
    "title": "Chapter 3: Independent Events",
    "section": "",
    "text": "Define independence of 2-3 events given probability notation\nCalculate whether two or more events are independent"
  },
  {
    "objectID": "lessons/04_Rules_of_prob/03_Independent_Events.html#where-are-we",
    "href": "lessons/04_Rules_of_prob/03_Independent_Events.html#where-are-we",
    "title": "Chapter 3: Independent Events",
    "section": "Where are we?",
    "text": "Where are we?"
  },
  {
    "objectID": "lessons/04_Rules_of_prob/03_Independent_Events.html#independent-events",
    "href": "lessons/04_Rules_of_prob/03_Independent_Events.html#independent-events",
    "title": "Chapter 3: Independent Events",
    "section": "Independent Events",
    "text": "Independent Events\n\n\nDefinition: Independence\n\n\nEvents \\(A\\) and \\(B\\) are independent if \\[\\mathbb{P}(A \\cap B) = \\mathbb{P}(A) \\cdot  \\mathbb{P}(B).\\]\n\n\nNotation: For shorthand, we sometimes write \\(A \\mathrel{\\unicode{x2AEB}} B,\\) to denote that \\(A\\) and \\(B\\) are independent events.\n \n\nAlso note: \\[\\begin{aligned} \\mathbb{P}(A \\cap B) = \\mathbb{P}(A) \\cdot  \\mathbb{P}(B) & \\implies A \\mathrel{\\unicode{x2AEB}} B \\\\\nA \\mathrel{\\unicode{x2AEB}} B & \\implies \\mathbb{P}(A \\cap B) = \\mathbb{P}(A) \\cdot  \\mathbb{P}(B) \\end{aligned}\\]"
  },
  {
    "objectID": "lessons/04_Rules_of_prob/03_Independent_Events.html#example-of-two-dice",
    "href": "lessons/04_Rules_of_prob/03_Independent_Events.html#example-of-two-dice",
    "title": "Chapter 3: Independent Events",
    "section": "Example of two dice",
    "text": "Example of two dice\n\n\nExample 1\n\n\nTwo dice (red and blue) are rolled. Let \\(A =\\) event a total of 7 appears, and \\(B =\\) event red die is a six. Are events \\(A\\) and \\(B\\) independent?"
  },
  {
    "objectID": "lessons/04_Rules_of_prob/03_Independent_Events.html#independence-of-3-events",
    "href": "lessons/04_Rules_of_prob/03_Independent_Events.html#independence-of-3-events",
    "title": "Chapter 3: Independent Events",
    "section": "Independence of 3 Events",
    "text": "Independence of 3 Events\n\n\nDefinition: Independence of 3 Events\n\n\nEvents \\(A\\), \\(B\\), and \\(C\\) are mutually independent if\n\n\n\\(\\mathbb{P}(A \\cap B) = \\mathbb{P}(A) \\cdot \\mathbb{P}(B)\\)\n\\(\\mathbb{P}(A \\cap C) = \\mathbb{P}(A) \\cdot \\mathbb{P}(C)\\)\n\\(\\mathbb{P}(B \\cap C) = \\mathbb{P}(B) \\cdot \\mathbb{P}(C)\\)\n\n\\(\\mathbb{P}(A \\cap B \\cap C) = \\mathbb{P}(A) \\cdot \\mathbb{P}(B) \\cdot \\mathbb{P}(C)\\)\n\n\n\nRemark:\nOn your homework you will show that \\((1) \\not \\Rightarrow (2)\\) and \\((2) \\not \\Rightarrow (1)\\)."
  },
  {
    "objectID": "lessons/04_Rules_of_prob/03_Independent_Events.html#probability-at-least-one-smoker",
    "href": "lessons/04_Rules_of_prob/03_Independent_Events.html#probability-at-least-one-smoker",
    "title": "Chapter 3: Independent Events",
    "section": "Probability at least one smoker",
    "text": "Probability at least one smoker\n\n\n\n\nExample 2\n\n\nSuppose you take a random sample of \\(n\\) people, of which people are smokers and non-smokers independently of each other. Let\n\n\\(A_i =\\) event person \\(i\\) is a smoker, for \\(i=1, \\ldots ,n\\), and\n\\(p_i =\\) probability person \\(i\\) is a smoker, for \\(i=1, \\ldots ,n\\).\n\nFind the probability that at least one person in the random sample is a smoker."
  },
  {
    "objectID": "lessons/04_Rules_of_prob/03_Independent_Events.html#building-geometric-series",
    "href": "lessons/04_Rules_of_prob/03_Independent_Events.html#building-geometric-series",
    "title": "Chapter 3: Independent Events",
    "section": "Building geometric series",
    "text": "Building geometric series\n\n\nExample 3\n\n\n\\(A, B,\\) and \\(C\\) toss a fair coin in order. The first to throw heads wins. What are their respective chances of winning?\n\n\nLet\n\n\\(A_H\\) and \\(A_T\\) be the events player A tosses heads and tails, respectively.\nSimilarly define \\(B_H\\), \\(B_T\\), \\(C_H\\), and \\(C_T\\)."
  },
  {
    "objectID": "lessons/04_Rules_of_prob/04_Rules_of_prob.html",
    "href": "lessons/04_Rules_of_prob/04_Rules_of_prob.html",
    "title": "Lesson 4: Rules of probability",
    "section": "",
    "text": "Define independence of 2-3 events and decide whether two or more events are independent\nDefine key facts for conditional probabilities and calculate conditional probabilities.\nCalculate the probability of an event using Bayes’ Theorem, Higher Order Multiplication Rule, and the Law of Total Probabilities\n\n\n\n\n\n\n\n\n\n\n\n\nClearly define your events of interest\nTranslate question to probability using defined events OR Venn Diagram\nAsk yourself:\n\nAre we sampling with or without replacement?\nDoes order matter?\n\nUse axioms, properties, partitions, facts, etc. to define the end probability calculation into smaller parts\n\nIf probabilities are given to you, Venn Diagrams may help you parse out the events and probability calculations\nIf you need to find probabilities with counting, pictures or diagrams might help here\n\nWrite out a concluding statement that gives the probability context\n(For own check) Make sure the calculated probability follows the axioms. Is is between 0 and 1?"
  },
  {
    "objectID": "lessons/04_Rules_of_prob/04_Rules_of_prob.html#general-process-for-probability-word-problems",
    "href": "lessons/04_Rules_of_prob/04_Rules_of_prob.html#general-process-for-probability-word-problems",
    "title": "Lesson 4: Rules of probability",
    "section": "",
    "text": "Clearly define your events of interest\nTranslate question to probability using defined events OR Venn Diagram\nAsk yourself:\n\nAre we sampling with or without replacement?\nDoes order matter?\n\nUse axioms, properties, partitions, facts, etc. to define the end probability calculation into smaller parts\n\nIf probabilities are given to you, Venn Diagrams may help you parse out the events and probability calculations\nIf you need to find probabilities with counting, pictures or diagrams might help here\n\nWrite out a concluding statement that gives the probability context\n(For own check) Make sure the calculated probability follows the axioms. Is is between 0 and 1?"
  },
  {
    "objectID": "lessons/04_Rules_of_prob/04_Rules_of_prob.html#independent-events",
    "href": "lessons/04_Rules_of_prob/04_Rules_of_prob.html#independent-events",
    "title": "Lesson 4: Rules of probability",
    "section": "Independent Events",
    "text": "Independent Events\n\n\nDefinition: Independence\n\n\nEvents \\(A\\) and \\(B\\) are independent if \\[\\mathbb{P}(A \\cap B) = \\mathbb{P}(A) \\cdot  \\mathbb{P}(B).\\]\n\n\nNotation: For shorthand, we sometimes write \\(A \\mathrel{\\unicode{x2AEB}} B,\\) to denote that \\(A\\) and \\(B\\) are independent events.\n \n\nAlso note: \\[\\begin{aligned} \\mathbb{P}(A \\cap B) = \\mathbb{P}(A) \\cdot  \\mathbb{P}(B) & \\implies A \\mathrel{\\unicode{x2AEB}} B \\\\\nA \\mathrel{\\unicode{x2AEB}} B & \\implies \\mathbb{P}(A \\cap B) = \\mathbb{P}(A) \\cdot  \\mathbb{P}(B) \\end{aligned}\\]"
  },
  {
    "objectID": "lessons/04_Rules_of_prob/04_Rules_of_prob.html#example-of-two-dice",
    "href": "lessons/04_Rules_of_prob/04_Rules_of_prob.html#example-of-two-dice",
    "title": "Lesson 4: Rules of probability",
    "section": "Example of two dice",
    "text": "Example of two dice\n\n\nExample 1\n\n\nTwo dice (green and blue) are rolled. Let \\(A =\\) event a total of 7 appears, and \\(B =\\) event green die is a six. Are events \\(A\\) and \\(B\\) independent?"
  },
  {
    "objectID": "lessons/04_Rules_of_prob/04_Rules_of_prob.html#example-of-two-dice-simulating-in-r-12",
    "href": "lessons/04_Rules_of_prob/04_Rules_of_prob.html#example-of-two-dice-simulating-in-r-12",
    "title": "Lesson 4: Rules of probability",
    "section": "Example of two dice: simulating in R (1/2)",
    "text": "Example of two dice: simulating in R (1/2)\n\n\nExample 1\n\n\nTwo dice (green and blue) are rolled. Let \\(A =\\) event a total of 7 appears, and \\(B =\\) event green die is a six. Are events \\(A\\) and \\(B\\) independent?\n\n\n\nset.seed(1002)\nreps = 10000\nrolls = replicate(reps, sample(x = 1:6, size = 2, replace = TRUE))\nrolls[, 1:10]\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n[1,]    4    5    5    5    6    5    2    4    3     4\n[2,]    1    4    6    3    1    1    5    5    6     3\n\nevent_A = ( rolls[1, ] + rolls[2, ] == 7 )\nhead(event_A, 10)\n\n [1] FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE FALSE  TRUE\n\nevent_B = ( rolls[1, ] == 6 )\nhead(event_B, 10)\n\n [1] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE"
  },
  {
    "objectID": "lessons/04_Rules_of_prob/04_Rules_of_prob.html#example-of-two-dice-simulating-in-r-12-1",
    "href": "lessons/04_Rules_of_prob/04_Rules_of_prob.html#example-of-two-dice-simulating-in-r-12-1",
    "title": "Lesson 4: Rules of probability",
    "section": "Example of two dice: simulating in R (1/2)",
    "text": "Example of two dice: simulating in R (1/2)\n\n\nExample 1\n\n\nTwo dice (green and blue) are rolled. Let \\(A =\\) event a total of 7 appears, and \\(B =\\) event green die is a six. Are events \\(A\\) and \\(B\\) independent?\n\n\n\n( sum(event_A) / reps ) * ( sum(event_B) / reps )\n\n[1] 0.0286608\n\nevent_A_and_B = ( rolls[1, ] + rolls[2, ] == 7 ) & ( rolls[1, ] == 6 )\nhead(event_A_and_B, 10)\n\n [1] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE\n\nsum(event_A_and_B) / reps\n\n[1] 0.0284"
  },
  {
    "objectID": "lessons/04_Rules_of_prob/04_Rules_of_prob.html#independence-of-3-events",
    "href": "lessons/04_Rules_of_prob/04_Rules_of_prob.html#independence-of-3-events",
    "title": "Lesson 4: Rules of probability",
    "section": "Independence of 3 Events",
    "text": "Independence of 3 Events\n\n\nDefinition: Independence of 3 Events\n\n\nEvents \\(A\\), \\(B\\), and \\(C\\) are mutually independent if\n\n\n\\(\\mathbb{P}(A \\cap B) = \\mathbb{P}(A) \\cdot \\mathbb{P}(B)\\)\n\\(\\mathbb{P}(A \\cap C) = \\mathbb{P}(A) \\cdot \\mathbb{P}(C)\\)\n\\(\\mathbb{P}(B \\cap C) = \\mathbb{P}(B) \\cdot \\mathbb{P}(C)\\)\n\n\\(\\mathbb{P}(A \\cap B \\cap C) = \\mathbb{P}(A) \\cdot \\mathbb{P}(B) \\cdot \\mathbb{P}(C)\\)\n\n\n\nRemark:\nOn your homework you will show that \\((1) \\not \\Rightarrow (2)\\) and \\((2) \\not \\Rightarrow (1)\\)."
  },
  {
    "objectID": "lessons/04_Rules_of_prob/04_Rules_of_prob.html#probability-at-least-one-smoker",
    "href": "lessons/04_Rules_of_prob/04_Rules_of_prob.html#probability-at-least-one-smoker",
    "title": "Lesson 4: Rules of probability",
    "section": "Probability at least one smoker",
    "text": "Probability at least one smoker\n\n\n\n\nExample 2\n\n\nSuppose you take a random sample of \\(n\\) people, of which people are smokers and non-smokers independently of each other. Let\n\n\\(A_i =\\) event person \\(i\\) is a smoker, for \\(i=1, \\ldots ,n\\), and\n\\(p_i =\\) probability person \\(i\\) is a smoker, for \\(i=1, \\ldots ,n\\).\n\nFind the probability that at least one person in the random sample is a smoker."
  },
  {
    "objectID": "lessons/04_Rules_of_prob/04_Rules_of_prob.html#building-geometric-series",
    "href": "lessons/04_Rules_of_prob/04_Rules_of_prob.html#building-geometric-series",
    "title": "Lesson 4: Rules of probability",
    "section": "Building geometric series",
    "text": "Building geometric series\n\n\nExample 3\n\n\n\\(A, B,\\) and \\(C\\) toss a fair coin in order. The first to throw heads wins. What are their respective chances of winning?\n\n\nLet\n\n\\(A_H\\) and \\(A_T\\) be the events player A tosses heads and tails, respectively.\nSimilarly define \\(B_H\\), \\(B_T\\), \\(C_H\\), and \\(C_T\\)."
  },
  {
    "objectID": "lessons/04_Rules_of_prob/04_Rules_of_prob.html#lets-revisit-our-deck-of-cards",
    "href": "lessons/04_Rules_of_prob/04_Rules_of_prob.html#lets-revisit-our-deck-of-cards",
    "title": "Lesson 4: Rules of probability",
    "section": "Let’s revisit our deck of cards",
    "text": "Let’s revisit our deck of cards\n\n\n\n\nExample 1\n\n\nSuppose we randomly draw 2 cards from a standard deck of cards. What is the probability that we draw a spade then a heart?\n\n\nLet\n\nLet \\(A =\\) event \\(1^{st}\\) card is spades\nLet \\(B =\\) event \\(2^{nd}\\) card is heart"
  },
  {
    "objectID": "lessons/04_Rules_of_prob/04_Rules_of_prob.html#conditional-probability-facts-12",
    "href": "lessons/04_Rules_of_prob/04_Rules_of_prob.html#conditional-probability-facts-12",
    "title": "Lesson 4: Rules of probability",
    "section": "Conditional Probability facts (1/2)",
    "text": "Conditional Probability facts (1/2)\n\n\n\n\nFact 1: General Multiplication Rule\n\n\n\\[\\mathbb{P}(A\\cap B)=\\mathbb{P}(A)\\cdot\\mathbb{P}(B|A)\\]\n\n\n\n\n\nFact 2: Conditional Probability Definition\n\n\n\\[\\mathbb{P}(A|B)=\\frac{\\mathbb{P}(A\\cap B)}{\\mathbb{P}(B)}\\]"
  },
  {
    "objectID": "lessons/04_Rules_of_prob/04_Rules_of_prob.html#conditional-probability-facts-22",
    "href": "lessons/04_Rules_of_prob/04_Rules_of_prob.html#conditional-probability-facts-22",
    "title": "Lesson 4: Rules of probability",
    "section": "Conditional Probability facts (2/2)",
    "text": "Conditional Probability facts (2/2)\n\n\n\n\nFact 3\n\n\nIf \\(A\\) and \\(B\\) are independent events (\\(A \\unicode{x2AEB}B\\)), then \\[\\mathbb{P}(A|B) = \\mathbb{P}(A)\\]\n\n\n\n\n\nFact 4\n\n\n\\(\\mathbb{P}(A|B)\\) is a probability, meaning that it satisfies the probability axioms. In particular, \\[\\mathbb{P}(A|B) + \\mathbb{P}(A^C|B) = 1\\]"
  },
  {
    "objectID": "lessons/04_Rules_of_prob/04_Rules_of_prob.html#conditional-probability-with-two-dice",
    "href": "lessons/04_Rules_of_prob/04_Rules_of_prob.html#conditional-probability-with-two-dice",
    "title": "Lesson 4: Rules of probability",
    "section": "Conditional probability with two dice",
    "text": "Conditional probability with two dice\n\n\n\n\nExample 3\n\n\nTwo dice (green and blue) are rolled. If the dice do not show the same face, what is the probability that one of the dice is a 1?"
  },
  {
    "objectID": "lessons/04_Rules_of_prob/04_Rules_of_prob.html#conditional-probability-with-two-dice-simulations",
    "href": "lessons/04_Rules_of_prob/04_Rules_of_prob.html#conditional-probability-with-two-dice-simulations",
    "title": "Lesson 4: Rules of probability",
    "section": "Conditional probability with two dice: simulations",
    "text": "Conditional probability with two dice: simulations\n\n\nExample 3\n\n\nTwo dice (green and blue) are rolled. If the dice do not show the same face, what is the probability that one of the dice is a 1?\n\n\n\nset.seed(1002)\nrolls = replicate(reps, sample(x = 1:6, size = 2, replace = TRUE))\nrolls[, 1:10]\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n[1,]    4    5    5    5    6    5    2    4    3     4\n[2,]    1    4    6    3    1    1    5    5    6     3\n\nevent_A = ( rolls[1, ] != rolls[2, ] )\nhead(event_A, 10)\n\n [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n\nevent_B = ( rolls[1, ] == 1 | rolls[2, ] == 1 )\nhead(event_B, 10)\n\n [1]  TRUE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE\n\nsum(event_B & event_A) / sum(event_A)\n\n[1] 0.3315328"
  },
  {
    "objectID": "lessons/04_Rules_of_prob/04_Rules_of_prob.html#monty-hall-problem",
    "href": "lessons/04_Rules_of_prob/04_Rules_of_prob.html#monty-hall-problem",
    "title": "Lesson 4: Rules of probability",
    "section": "Monty Hall Problem",
    "text": "Monty Hall Problem\nSurvivor Season 42\nWith the Wiki page on it!"
  },
  {
    "objectID": "lessons/04_Rules_of_prob/04_Rules_of_prob.html#bayes-rule-for-two-events",
    "href": "lessons/04_Rules_of_prob/04_Rules_of_prob.html#bayes-rule-for-two-events",
    "title": "Lesson 4: Rules of probability",
    "section": "Bayes’ Rule for two events",
    "text": "Bayes’ Rule for two events\n\nWe can use the conditional probability (\\(\\mathbb{P}(A|B)\\)) to get information on the flipped conditional probability (\\(\\mathbb{P}(B|A)\\))\n\n\n\n\n\nTheorem: Bayes’ Rule (for two events)\n\n\nFor any two events \\(A\\) and \\(B\\) with nonzero probabilties,\n\\[\\mathbb{P}(A| B) =\n\\frac{\\mathbb{P}(A) \\cdot \\mathbb{P}(B|A)}\n{\\mathbb{P}(B)}\\]"
  },
  {
    "objectID": "lessons/04_Rules_of_prob/04_Rules_of_prob.html#calculating-probability-with-higher-order-multiplication-rule",
    "href": "lessons/04_Rules_of_prob/04_Rules_of_prob.html#calculating-probability-with-higher-order-multiplication-rule",
    "title": "Lesson 4: Rules of probability",
    "section": "Calculating probability with Higher Order Multiplication Rule",
    "text": "Calculating probability with Higher Order Multiplication Rule\n\n\n\n\nExample 4\n\n\nSuppose we draw 5 cards from a standard shuffled deck of 52 cards. What is the probability of a flush, that is all the cards are of the same suit (including straight flushes)?\n\n\n\n\n\nHigher Order Multiplication Rule\n\n\n\\[\\begin{aligned} \\mathbb{P}(A_1\\cap & A_2 \\cap  \\ldots \\cap A_n)= \\\\ & \\mathbb{P}(A_1)\\cdot\\mathbb{P}(A_2|A_1) \\cdot \\\\\n& \\mathbb{P}(A_3|A_1A_2)\\ldots \\cdot\\mathbb{P}(A_n|A_1A_2\\ldots A_{n-1})\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/04_Rules_of_prob/04_Rules_of_prob.html#calculating-probability-with-law-of-total-probability",
    "href": "lessons/04_Rules_of_prob/04_Rules_of_prob.html#calculating-probability-with-law-of-total-probability",
    "title": "Lesson 4: Rules of probability",
    "section": "Calculating probability with Law of Total Probability",
    "text": "Calculating probability with Law of Total Probability\n\n\n\n\nExample 5\n\n\nSuppose 1% of people assigned female at birth (AFAB) and 5% of people assigned male at birth (AMAB) are color-blind. Assume person born is equally likely AFAB or AMAB (not including intersex). What is the probability that a person chosen at random is color-blind?\n\n\n\n\n\nLaw of Total Probability for 2 Events\n\n\nFor events \\(A\\) and \\(B\\),\n\\[%\\left(\n\\begin{array}{ccl}\n\\mathbb{P}(B)&=&\\mathbb{P}(B \\cap A) + \\mathbb{P}(B \\cap A^C)\\\\\n           &=& \\mathbb{P}(B|A) \\cdot \\mathbb{P}(A)+ \\mathbb{P}(B | A^C)\\cdot \\mathbb{P}(A^C)\n\\end{array}\n%\\right)\\]"
  },
  {
    "objectID": "lessons/04_Rules_of_prob/04_Rules_of_prob.html#general-law-of-total-proability",
    "href": "lessons/04_Rules_of_prob/04_Rules_of_prob.html#general-law-of-total-proability",
    "title": "Lesson 4: Rules of probability",
    "section": "General Law of Total Proability",
    "text": "General Law of Total Proability\n\n\nLaw of Total Probability (general)\n\n\nIf \\(\\{A_i\\}_{i=1}^{n} = \\{A_1, A_2, \\ldots, A_n\\}\\) form a partition of the sample space, then for event \\(B\\),\n\\[%\\left(\n\\begin{array}{ccl}\n\\mathbb{P}(B)&=& \\sum_{i=1}^{n} \\mathbb{P}(B \\cap A_i)\\\\\n           &=& \\sum_{i=1}^{n} \\mathbb{P}(B|A_i) \\cdot \\mathbb{P}(A_i)\n\\end{array}\n%\\right)\\]"
  },
  {
    "objectID": "lessons/04_Rules_of_prob/04_Rules_of_prob.html#calculating-probability-with-generalized-law-of-total-probability",
    "href": "lessons/04_Rules_of_prob/04_Rules_of_prob.html#calculating-probability-with-generalized-law-of-total-probability",
    "title": "Lesson 4: Rules of probability",
    "section": "Calculating probability with generalized Law of Total Probability",
    "text": "Calculating probability with generalized Law of Total Probability"
  },
  {
    "objectID": "lessons/04_Rules_of_prob/04_Rules_of_prob.html#calculating-probability-with-generalized-law-of-total-probability-1",
    "href": "lessons/04_Rules_of_prob/04_Rules_of_prob.html#calculating-probability-with-generalized-law-of-total-probability-1",
    "title": "Lesson 4: Rules of probability",
    "section": "Calculating probability with generalized Law of Total Probability",
    "text": "Calculating probability with generalized Law of Total Probability\n\n\n\n\nExample 3\n\n\nIndividuals are diagnosed with a particular type of cancer that can take on three different disease forms,* \\(D_1\\), \\(D_2\\), and \\(D_3\\). It is known that amongst people diagnosed with this particular type of cancer,\n\n20% of people will eventually be diagnosed with form \\(D_1\\),\n30% with form \\(D_2\\), and\n50% with form \\(D_3\\).\n\nThe probability of requiring chemotherapy (\\(C\\)) differs among the three forms of disease:\n\n80% with \\(D_1\\),\n30% with \\(D_2\\), and\n10% with \\(D_3\\).\n\nBased solely on the preliminary test of being diagnosed with the cancer, what is the probability of requiring chemotherapy (the event C)?\n\n\n\nSkipping in class! Let me know if you would like me to post solutions to this if you work through it!"
  },
  {
    "objectID": "lessons/04_Rules_of_prob/04_Rules_of_prob.html#lets-revisit-the-color-blind-example",
    "href": "lessons/04_Rules_of_prob/04_Rules_of_prob.html#lets-revisit-the-color-blind-example",
    "title": "Lesson 4: Rules of probability",
    "section": "Let’s revisit the color-blind example",
    "text": "Let’s revisit the color-blind example\n\n\n\n\nExample 4\n\n\nRecall the color-blind example (Example 2), where\n\na person is AMAB with probability 0.5,\nAMAB people are color-blind with probability 0.05, and\nall people are color-blind with probability 0.03.\n\nAssuming people are AMAB or AFAB, find the probability that a color-blind person is AMAB."
  },
  {
    "objectID": "lessons/04_Rules_of_prob/04_Rules_of_prob.html#calculate-probability-with-both-rules",
    "href": "lessons/04_Rules_of_prob/04_Rules_of_prob.html#calculate-probability-with-both-rules",
    "title": "Lesson 4: Rules of probability",
    "section": "Calculate probability with both rules",
    "text": "Calculate probability with both rules\n\n\n\n\nExample 6\n\n\nSuppose\n\n1% of people who are AFAB aged 40-50 years have breast cancer,\nan AFAB person with breast cancer has a 90% chance of a positive test from a mammogram, and\nan AFAB person has a 10% chance of a false-positive result from a mammogram.\n\nWhat is the probability that an AFAB person has breast cancer given that they just had a positive test?"
  },
  {
    "objectID": "lessons/04_Rules_of_prob/04_Rules_of_prob.html#bayes-rule",
    "href": "lessons/04_Rules_of_prob/04_Rules_of_prob.html#bayes-rule",
    "title": "Lesson 4: Rules of probability",
    "section": "Bayes’ Rule",
    "text": "Bayes’ Rule\n\n\nTheorem: Bayes’ Rule\n\n\nIf \\(\\{A_i\\}_{i=1}^{n}\\) form a partition of the sample space \\(S\\), with \\(\\mathbb{P}(A_i)&gt;0\\) for \\(i=1\\ldots n\\) and \\(\\mathbb{P}(B)&gt;0\\), then\n\\[\\mathbb{P}(A_j | B) =\n\\frac{\\mathbb{P}(B|A_j) \\cdot \\mathbb{P}(A_j)}\n{\\sum_{i=1}^{n} \\mathbb{P}(B|A_i) \\cdot \\mathbb{P}(A_i)}\\]"
  },
  {
    "objectID": "lessons/04_Rules_of_prob/04_Conditional_Probability.html",
    "href": "lessons/04_Rules_of_prob/04_Conditional_Probability.html",
    "title": "Chapter 4: Conditional Probability",
    "section": "",
    "text": "Use set process to calculate probability of event of interest\nCalculate the probability of an event occurring, given that another event occurred.\nDefine keys facts for conditional probabilities using notation."
  },
  {
    "objectID": "lessons/04_Rules_of_prob/04_Conditional_Probability.html#learning-objectives",
    "href": "lessons/04_Rules_of_prob/04_Conditional_Probability.html#learning-objectives",
    "title": "Chapter 4: Conditional Probability",
    "section": "",
    "text": "Use set process to calculate probability of event of interest\nCalculate the probability of an event occurring, given that another event occurred.\nDefine keys facts for conditional probabilities using notation."
  },
  {
    "objectID": "lessons/04_Rules_of_prob/04_Conditional_Probability.html#where-are-we",
    "href": "lessons/04_Rules_of_prob/04_Conditional_Probability.html#where-are-we",
    "title": "Chapter 4: Conditional Probability",
    "section": "Where are we?",
    "text": "Where are we?"
  },
  {
    "objectID": "lessons/04_Rules_of_prob/04_Conditional_Probability.html#general-process-for-probability-word-problems",
    "href": "lessons/04_Rules_of_prob/04_Conditional_Probability.html#general-process-for-probability-word-problems",
    "title": "Chapter 4: Conditional Probability",
    "section": "General Process for Probability Word Problems",
    "text": "General Process for Probability Word Problems\n\nClearly define your events of interest\nTranslate question to probability using defined events OR Venn Diagram\nAsk yourself:\n\nAre we sampling with or without replacement?\nDoes order matter?\n\nUse axioms, properties, partitions, facts, etc. to define the end probability calculation into smaller parts\n\nIf probabilities are given to you, Venn Diagrams may help you parse out the events and probability calculations\nIf you need to find probabilities with counting, pictures or diagrams might help here\n\nWrite out a concluding statement that gives the probability context\n(For own check) Make sure the calculated probability follows the axioms. Is is between 0 and 1?"
  },
  {
    "objectID": "lessons/04_Rules_of_prob/04_Conditional_Probability.html#lets-revisit-our-deck-of-cards",
    "href": "lessons/04_Rules_of_prob/04_Conditional_Probability.html#lets-revisit-our-deck-of-cards",
    "title": "Chapter 4: Conditional Probability",
    "section": "Let’s revisit our deck of cards",
    "text": "Let’s revisit our deck of cards\n\n\n\n\nExample 1\n\n\nSuppose we randomly draw 2 cards from a standard deck of cards. What is the probability that we draw a spade then a heart?\n\n\nLet\n\nLet \\(A =\\) event \\(1^{st}\\) card is spades\nLet \\(B =\\) event \\(2^{nd}\\) card is heart"
  },
  {
    "objectID": "lessons/04_Rules_of_prob/04_Conditional_Probability.html#conditional-probability-facts-12",
    "href": "lessons/04_Rules_of_prob/04_Conditional_Probability.html#conditional-probability-facts-12",
    "title": "Chapter 4: Conditional Probability",
    "section": "Conditional Probability facts (1/2)",
    "text": "Conditional Probability facts (1/2)\n\n\n\n\nFact 1: General Multiplication Rule\n\n\n\\[\\mathbb{P}(A\\cap B)=\\mathbb{P}(A)\\cdot\\mathbb{P}(B|A)\\]\n\n\n\n\n\nFact 2: Conditional Probability Definition\n\n\n\\[\\mathbb{P}(A|B)=\\frac{\\mathbb{P}(A\\cap B)}{\\mathbb{P}(B)}\\]"
  },
  {
    "objectID": "lessons/04_Rules_of_prob/04_Conditional_Probability.html#conditional-probability-facts-22",
    "href": "lessons/04_Rules_of_prob/04_Conditional_Probability.html#conditional-probability-facts-22",
    "title": "Chapter 4: Conditional Probability",
    "section": "Conditional Probability facts (2/2)",
    "text": "Conditional Probability facts (2/2)\n\n\n\n\nFact 3\n\n\nIf \\(A\\) and \\(B\\) are independent events (\\(A \\unicode{x2AEB}B\\)), then \\[\\mathbb{P}(A|B) = \\mathbb{P}(A)\\]\n\n\n\n\n\nFact 4\n\n\n\\(\\mathbb{P}(A|B)\\) is a probability, meaning that it satisfies the probability axioms. In particular, \\[\\mathbb{P}(A|B) + \\mathbb{P}(A^C|B) = 1\\]"
  },
  {
    "objectID": "lessons/04_Rules_of_prob/04_Conditional_Probability.html#monty-hall-problem",
    "href": "lessons/04_Rules_of_prob/04_Conditional_Probability.html#monty-hall-problem",
    "title": "Chapter 4: Conditional Probability",
    "section": "Monty Hall Problem",
    "text": "Monty Hall Problem\nSurvivor Season 42\nWith the Wiki page on it!"
  },
  {
    "objectID": "lessons/04_Rules_of_prob/04_Conditional_Probability.html#conditional-probability-with-two-dice",
    "href": "lessons/04_Rules_of_prob/04_Conditional_Probability.html#conditional-probability-with-two-dice",
    "title": "Chapter 4: Conditional Probability",
    "section": "Conditional probability with two dice",
    "text": "Conditional probability with two dice\n\n\n\n\nExample 2\n\n\nTwo dice (red and blue) are rolled. If the dice do not show the same face, what is the probability that one of the dice is a 1?"
  },
  {
    "objectID": "lessons/15_Discrete_RVs/15_Discrete_RVs_key_info.html",
    "href": "lessons/15_Discrete_RVs/15_Discrete_RVs_key_info.html",
    "title": "Key Info",
    "section": "",
    "text": "We will finish up the example from last class\nNo class on 12/1\n\nMajority voted for no class on 12/1\nVote 7 to 4\n\nBiostatistics tutoring starts today\n\nSchedule a meeting here\nOr email Charles directly"
  },
  {
    "objectID": "lessons/15_Discrete_RVs/15_Discrete_RVs_key_info.html#announcements",
    "href": "lessons/15_Discrete_RVs/15_Discrete_RVs_key_info.html#announcements",
    "title": "Key Info",
    "section": "",
    "text": "We will finish up the example from last class\nNo class on 12/1\n\nMajority voted for no class on 12/1\nVote 7 to 4\n\nBiostatistics tutoring starts today\n\nSchedule a meeting here\nOr email Charles directly"
  },
  {
    "objectID": "lessons/15_Discrete_RVs/15_Discrete_RVs_key_info.html#key-dates",
    "href": "lessons/15_Discrete_RVs/15_Discrete_RVs_key_info.html#key-dates",
    "title": "Key Info",
    "section": "Key Dates",
    "text": "Key Dates\n\nHW 7 dues Sunday 11/23\nQuiz 3 opens 12/3\nHW 8 due 12/7"
  },
  {
    "objectID": "lessons/15_Discrete_RVs/15_Discrete_RVs_muddy_points.html",
    "href": "lessons/15_Discrete_RVs/15_Discrete_RVs_muddy_points.html",
    "title": "Muddy Points",
    "section": "",
    "text": "Haha, so true. Hopefully more examples next class will be helpful.\n\n\n\nYes, it gets tricky especially when \\(X\\) can take infinite values. The good news is these are all very common random variables and there are many videos that go through the proofs.\nFor example, here is one from Khan Academy on the geometric distribution."
  },
  {
    "objectID": "lessons/15_Discrete_RVs/15_Discrete_RVs_muddy_points.html#fall-2025",
    "href": "lessons/15_Discrete_RVs/15_Discrete_RVs_muddy_points.html#fall-2025",
    "title": "Muddy Points",
    "section": "",
    "text": "Haha, so true. Hopefully more examples next class will be helpful.\n\n\n\nYes, it gets tricky especially when \\(X\\) can take infinite values. The good news is these are all very common random variables and there are many videos that go through the proofs.\nFor example, here is one from Khan Academy on the geometric distribution."
  },
  {
    "objectID": "lessons/15_Discrete_RVs/15_Discrete_RVs.html",
    "href": "lessons/15_Discrete_RVs/15_Discrete_RVs.html",
    "title": "Lesson 15: Some Important Discrete RVs",
    "section": "",
    "text": "Distinguish between Bernoulli, Binomial, Geometric, Hypergeometric, Discrete Uniform, Negative Binomial, and Poisson distributions when reading a world problem.\nIdentify the variable and the parameters in a world problem, and state what the variable and parameters mean.\nUse the formulas for the pmf/CDF, expected value, and variance to answer questions and find probabilities."
  },
  {
    "objectID": "lessons/15_Discrete_RVs/15_Discrete_RVs.html#properties-of-bernoulli-rvs",
    "href": "lessons/15_Discrete_RVs/15_Discrete_RVs.html#properties-of-bernoulli-rvs",
    "title": "Lesson 15: Some Important Discrete RVs",
    "section": "Properties of Bernoulli RVs",
    "text": "Properties of Bernoulli RVs\n\nScenario: One trial, with outcome success or failure\nShorthand: \\(X \\sim \\text{Bernoulli}(p)\\)\n\n\\[\nX = \\left\\{\n        \\begin{array}{ll}\n            1 & \\quad \\mathrm{with\\ probability}\\ p \\quad \\\\\n            0 & \\quad \\mathrm{with\\ probability}\\ 1-p \\quad\n        \\end{array}\n    \\right.\n\\]\n\\[\np_X(x) = P(X=x) = p^x(1-p)^{1-x} \\text{ for } x=0,1\n\\]\n\\[\\text{E}(X) = p\\]\n\\[\\text{Var}(X) = pq = p(1-p)\\]"
  },
  {
    "objectID": "lessons/15_Discrete_RVs/15_Discrete_RVs.html#bernoulli-example-1",
    "href": "lessons/15_Discrete_RVs/15_Discrete_RVs.html#bernoulli-example-1",
    "title": "Lesson 15: Some Important Discrete RVs",
    "section": "Bernoulli Example 1",
    "text": "Bernoulli Example 1\n\n\n\n\nExample 1\n\n\n\nWe roll a fair 6-sided die.\nWe get $1 if we roll a 5, and nothing otherwise.\nLet \\(X\\) be how much money we get.\nFind the mean and variance of \\(X\\)."
  },
  {
    "objectID": "lessons/15_Discrete_RVs/15_Discrete_RVs.html#properties-of-binomial-rvs",
    "href": "lessons/15_Discrete_RVs/15_Discrete_RVs.html#properties-of-binomial-rvs",
    "title": "Lesson 15: Some Important Discrete RVs",
    "section": "Properties of Binomial RVs",
    "text": "Properties of Binomial RVs\n\nScenario: There are \\(n\\) independent trials, each resulting in a success or failure, with constant probability, \\(p\\), in each trial. We are counting the number of successes (or failures).\nShorthand: \\(X \\sim \\text{Binomial}(n, p)\\)\n\n\\[\nX = \\text{Number of successes of } n \\text{ independent trials}\n\\]\n\\[\np_X(x) = P(X=x) = {n \\choose x}p^x(1-p)^{n-x} \\text{ for } x=0,1,2,  ..., n\n\\]\n\\[\\text{E}(X) = np\\] \\[\\text{Var}(X) = npq = np(1-p)\\]"
  },
  {
    "objectID": "lessons/15_Discrete_RVs/15_Discrete_RVs.html#our-beloved-fair-sided-die",
    "href": "lessons/15_Discrete_RVs/15_Discrete_RVs.html#our-beloved-fair-sided-die",
    "title": "Lesson 15: Some Important Discrete RVs",
    "section": "Our beloved fair-sided die",
    "text": "Our beloved fair-sided die\n\n\n\n\nExample 2\n\n\n\nSuppose we roll a fair 6-sided die 50 times.\nWe get $1 every time we roll a 5, and nothing otherwise.\nLet \\(X\\) be how much money we get on the 50 rolls.\nFind the mean and variance of \\(X\\)."
  },
  {
    "objectID": "lessons/15_Discrete_RVs/15_Discrete_RVs.html#geometric-rvs",
    "href": "lessons/15_Discrete_RVs/15_Discrete_RVs.html#geometric-rvs",
    "title": "Lesson 15: Some Important Discrete RVs",
    "section": "Geometric RVs",
    "text": "Geometric RVs\n\nScenario: There are repeated independent trials, each resulting in a success or failure, with constant probability of success for each trial. We are counting the number of trials until the first success.\nShorthand: \\(X \\sim \\text{Geo}(p)\\) or \\(X \\sim \\text{Geometric}(p)\\) or \\(X \\sim \\text{G}(p)\\)\n\n\n\n\n\n\n\n\n\\(X =\\) Number of trials needed for first success (count \\(x\\) includes the success)\n\\(X =\\) Number of failures before first success (count \\(x\\) does not include the success)\n\n\n\n\n$p _\nX( x ) = P(X=x) = (1-p)^{x-1}p$\nfor \\(x=1,2, 3,...\\)\n$$F_ X ( x\n) = P(Xx) = 1-(1-p)^x$$\nfor \\(x=1,2, 3,...\\)\n$ p _X (x)= P(X=x) = (1-p)^{x}p$\nfor \\(x=0, 1,2,...\\)\n$$F_X ( x )\n= P(Xx) = 1-(1-p)^{x+1}$$\nfor \\(x=0, 1,2,...\\)\n\n\n\\(E(X)=\\dfrac{1}{p}\\)\n\\(Var(X)= \\dfrac{1-p}{p^2}\\)\n\\(E(X)=\\dfrac{1-p}{p}\\)\n\\(Var(X) = \\dfrac{1-p}{p^2}\\)"
  },
  {
    "objectID": "lessons/15_Discrete_RVs/15_Discrete_RVs.html#bullseye-14",
    "href": "lessons/15_Discrete_RVs/15_Discrete_RVs.html#bullseye-14",
    "title": "Lesson 15: Some Important Discrete RVs",
    "section": "Bullseye (1/4)",
    "text": "Bullseye (1/4)\n\n\n\n\nExample 3\n\n\nWe throw darts at a dartboard until we hit the bullseye. Assume throws are independent and the probability of hitting the bullseye is 0.01 for each throw.\n\nWhat is the pmf for the number of throws needed to hit the bullseye?\nWhat are the mean and variance for the number of throws needed to hit the bullseye?\nFind the probability that our first bullseye:\n\nis on one of the first fifty tries\nis after the \\(50^{th}\\) try, given that it did not happen on the first 20 tries"
  },
  {
    "objectID": "lessons/15_Discrete_RVs/15_Discrete_RVs.html#bullseye-24",
    "href": "lessons/15_Discrete_RVs/15_Discrete_RVs.html#bullseye-24",
    "title": "Lesson 15: Some Important Discrete RVs",
    "section": "Bullseye (2/4)",
    "text": "Bullseye (2/4)\n\n\n\n\nExample 3\n\n\nWe throw darts at a dartboard until we hit the bullseye. Assume throws are independent and the probability of hitting the bullseye is 0.01 for each throw.\n\nWhat is the pmf for the number of throws needed to hit the bullseye?"
  },
  {
    "objectID": "lessons/15_Discrete_RVs/15_Discrete_RVs.html#bullseye-34",
    "href": "lessons/15_Discrete_RVs/15_Discrete_RVs.html#bullseye-34",
    "title": "Lesson 15: Some Important Discrete RVs",
    "section": "Bullseye (3/4)",
    "text": "Bullseye (3/4)\n\n\n\n\nExample 3\n\n\nWe throw darts at a dartboard until we hit the bullseye. Assume throws are independent and the probability of hitting the bullseye is 0.01 for each throw.\n\nWhat are the mean and variance for the number of throws needed to hit the bullseye?"
  },
  {
    "objectID": "lessons/15_Discrete_RVs/15_Discrete_RVs.html#bullseye-44",
    "href": "lessons/15_Discrete_RVs/15_Discrete_RVs.html#bullseye-44",
    "title": "Lesson 15: Some Important Discrete RVs",
    "section": "Bullseye (4/4)",
    "text": "Bullseye (4/4)\n\n\n\n\nExample 3\n\n\nWe throw darts at a dartboard until we hit the bullseye. Assume throws are independent and the probability of hitting the bullseye is 0.01 for each throw.\n\nFind the probability that our first bullseye:\n\nis on one of the first fifty tries\nis after the \\(50^{th}\\) try, given that it did not happen on the first 20 tries"
  },
  {
    "objectID": "lessons/15_Discrete_RVs/15_Discrete_RVs.html#memoryless-property-for-geometric-rvs",
    "href": "lessons/15_Discrete_RVs/15_Discrete_RVs.html#memoryless-property-for-geometric-rvs",
    "title": "Lesson 15: Some Important Discrete RVs",
    "section": "Memoryless property for Geometric RVs",
    "text": "Memoryless property for Geometric RVs\nIf we know \\(X\\) is greater than some number (aka given \\(X &gt;j\\)), then the probability of \\(X &gt; k+j\\) is just the probability that \\(X&gt;k\\).\n \n\\(P(X &gt; k+j |X &gt; j) = P(X &gt; k)\\)\n\\[ P(X &gt; k+j |X &gt; j) = \\dfrac{P(X&gt;k+j \\text{ and } X&gt;j)}{P(X&gt;j)} = \\dfrac{P(X&gt;k+j)}{P(X&gt;j)} = \\dfrac{(1-p)^{k+j}}{(1-p)^{j}} = (1-p)^{k} \\]"
  },
  {
    "objectID": "lessons/15_Discrete_RVs/15_Discrete_RVs.html#properties-of-negative-binomial-rvs",
    "href": "lessons/15_Discrete_RVs/15_Discrete_RVs.html#properties-of-negative-binomial-rvs",
    "title": "Lesson 15: Some Important Discrete RVs",
    "section": "Properties of Negative Binomial RVs",
    "text": "Properties of Negative Binomial RVs\n\nScenario: There are repeated independent trials, each resulting in a success or failure, with constant probability of success for each trial. We are counting the number of trials until the \\(r^{th}\\) success.\nShorthand: \\(X \\sim \\text{NegBin}(p, r)\\) or \\(X \\sim \\text{NB}(p, r)\\)\nNegative binomial is sum of \\(r\\) geometric distributions\n\n\\[\nX = \\text{Number of independent trials until } r^{th} \\text{ success}\n\\]\n\\[ p_X(x) = P(X=x) = {x-1 \\choose r-1}(1-p)^{x-r}p^r \\text{ for } x = r, r+1, r+2, ...\\]\n\\[ E(X) = \\dfrac{r}{p}\\]\n\\[Var(X) = \\dfrac{rq}{p^2} = \\dfrac{r(1-p)}{p^2}\\]"
  },
  {
    "objectID": "lessons/15_Discrete_RVs/15_Discrete_RVs.html#hitting-more-than-1-bullseye",
    "href": "lessons/15_Discrete_RVs/15_Discrete_RVs.html#hitting-more-than-1-bullseye",
    "title": "Lesson 15: Some Important Discrete RVs",
    "section": "Hitting more than 1 bullseye",
    "text": "Hitting more than 1 bullseye\n\n\n\n\nExample 1\n\n\nConsider again the bullseye example, where we throw darts at a dartboard until we hit the bullseye. Assume throws are independent and the probability of hitting the bullseye is 0.01 for each throw.\n\nWhat is the expected value and variance of the number of throws needed to hit 5 bullseyes?"
  },
  {
    "objectID": "lessons/15_Discrete_RVs/15_Discrete_RVs.html#hitting-more-than-1-bullseye-1",
    "href": "lessons/15_Discrete_RVs/15_Discrete_RVs.html#hitting-more-than-1-bullseye-1",
    "title": "Lesson 15: Some Important Discrete RVs",
    "section": "Hitting more than 1 bullseye",
    "text": "Hitting more than 1 bullseye\n\n\n\n\nExample 1\n\n\nConsider again the bullseye example, where we throw darts at a dartboard until we hit the bullseye. Assume throws are independent and the probability of hitting the bullseye is 0.01 for each throw.\n\nWhat is the probability that the \\(5^{th}\\) bullseye is on the \\(20^{th}\\) throw?"
  },
  {
    "objectID": "lessons/15_Discrete_RVs/15_Discrete_RVs.html#properties-of-poisson-rvs",
    "href": "lessons/15_Discrete_RVs/15_Discrete_RVs.html#properties-of-poisson-rvs",
    "title": "Lesson 15: Some Important Discrete RVs",
    "section": "Properties of Poisson RVs",
    "text": "Properties of Poisson RVs\n\nScenario: We are counting the number of successes in a fixed time period (or fixed space), which has a constant rate (\\(\\lambda\\)) of successes\nShorthand: \\(X \\sim \\text{Poisson}(\\lambda)\\) or \\(X \\sim \\text{Pois}(\\lambda)\\)\n\n\\[\nX = \\text{Number of successes in a given period}\n\\]\n\\[ p_X(x) = P(X=x) = \\dfrac{e^{-\\lambda}\\lambda^x}{x!} \\text{ for } x = 0, 1, 2,3, ...\\]\n\\[ \\text{E}(X) = \\lambda\\]\n\\[\\text{Var}(X) = \\lambda\\]"
  },
  {
    "objectID": "lessons/15_Discrete_RVs/15_Discrete_RVs.html#distinguishing-between-binomial-and-poisson-rvs",
    "href": "lessons/15_Discrete_RVs/15_Discrete_RVs.html#distinguishing-between-binomial-and-poisson-rvs",
    "title": "Lesson 15: Some Important Discrete RVs",
    "section": "Distinguishing between Binomial and Poisson RVs",
    "text": "Distinguishing between Binomial and Poisson RVs\n\nRecall that if \\(X\\sim \\text{Binomial}(n,p)\\), then\n\n\\(X\\) models the number of successes …\nin \\(n\\) independent (Bernoulli) trials …\nthat each have the same probability of success \\(p\\).\n\nPoisson RV’s are similar,\n\nexcept that instead of having \\(n\\) discrete independent trials,\nthere is a fixed time period (or space) during which the successes happen"
  },
  {
    "objectID": "lessons/15_Discrete_RVs/15_Discrete_RVs.html#examples-of-poisson-rvs",
    "href": "lessons/15_Discrete_RVs/15_Discrete_RVs.html#examples-of-poisson-rvs",
    "title": "Lesson 15: Some Important Discrete RVs",
    "section": "Examples of Poisson RVs",
    "text": "Examples of Poisson RVs\n\nNumber of visitors to an emergency room in an hour during a weekend night\nNumber of study participants enrolled in a study per week\nNumber of pedestrians walking through a square mile\nAny more?"
  },
  {
    "objectID": "lessons/15_Discrete_RVs/15_Discrete_RVs.html#emergency-room-visitors",
    "href": "lessons/15_Discrete_RVs/15_Discrete_RVs.html#emergency-room-visitors",
    "title": "Lesson 15: Some Important Discrete RVs",
    "section": "Emergency Room Visitors",
    "text": "Emergency Room Visitors\n\n\n\n\nExample 1\n\n\nSuppose an emergency room has an average of 50 visitors per day. Find the following probabilities.\n\nProbability of 30 visitors in a day.\nProbability of 8 visitors in an hour.\nProbability of at least 8 visitors in an hour."
  },
  {
    "objectID": "lessons/15_Discrete_RVs/15_Discrete_RVs.html#combining-independent-poisson-distributions",
    "href": "lessons/15_Discrete_RVs/15_Discrete_RVs.html#combining-independent-poisson-distributions",
    "title": "Lesson 15: Some Important Discrete RVs",
    "section": "Combining independent Poisson distributions",
    "text": "Combining independent Poisson distributions\n\n\nTheorem 1\n\n\nIf \\(X\\sim Pois(\\lambda_1)\\) and \\(Y\\sim Pois(\\lambda_2)\\) are independent of each other, then \\(Z=X+Y\\sim Pois(\\lambda_1 + \\lambda_2)\\)."
  },
  {
    "objectID": "lessons/15_Discrete_RVs/15_Discrete_RVs.html#two-emergency-rooms",
    "href": "lessons/15_Discrete_RVs/15_Discrete_RVs.html#two-emergency-rooms",
    "title": "Lesson 15: Some Important Discrete RVs",
    "section": "Two emergency rooms",
    "text": "Two emergency rooms\n\n\n\n\nExample 2\n\n\nSuppose emergency room 1 has an average of 50 visitors per day, and emergency room 2 has an average of 70 visitors per day, independently of each other. What is the probability distribution to model of the total number of visitors to both?"
  },
  {
    "objectID": "lessons/15_Discrete_RVs/15_Discrete_RVs.html#poisson-approximation-of-the-binomial",
    "href": "lessons/15_Discrete_RVs/15_Discrete_RVs.html#poisson-approximation-of-the-binomial",
    "title": "Lesson 15: Some Important Discrete RVs",
    "section": "Poisson Approximation of the Binomial",
    "text": "Poisson Approximation of the Binomial\nBoth Poisson and Binomial RV’s are counting the number of successes\n\nIf for a Binomial RV\n\nthe number of trials \\(n\\) is very large, and\nthe probability of success \\(p\\) is close to 0 or 1,\n\nThen the Poisson distribution can be used to approximate Binomial probabilities\n\nand we use \\(\\lambda = np\\)\n\nRule of thumb: We can use the Poisson approximation when \\(\\dfrac{1}{10} \\leq np(1-p) \\leq 10\\)"
  },
  {
    "objectID": "lessons/15_Discrete_RVs/15_Discrete_RVs.html#medical-lab-errors",
    "href": "lessons/15_Discrete_RVs/15_Discrete_RVs.html#medical-lab-errors",
    "title": "Lesson 15: Some Important Discrete RVs",
    "section": "Medical lab errors",
    "text": "Medical lab errors\n\n\n\n\nExample 3\n\n\nSuppose that in the long run, errors in a medical testing lab are made 0.1% of the time. Find the probability that fewer than 4 mistakes are made in the next 2,000 tests.\n\nFind the probability using the Binomial distribution.\nApproximate the probability in part (1) using the Poisson distribution.\n\n\n\n\nTo do for extra practice - will also see a similar problem in BSTA 511"
  },
  {
    "objectID": "lessons/15_Discrete_RVs/15_Discrete_RVs.html#hypergeometric-rvs",
    "href": "lessons/15_Discrete_RVs/15_Discrete_RVs.html#hypergeometric-rvs",
    "title": "Lesson 15: Some Important Discrete RVs",
    "section": "Hypergeometric RVs",
    "text": "Hypergeometric RVs\n\nScenario: There are a fixed number of successes and failures (which are known in advance), from which we make \\(n\\) draws without replacement. We are counting the number of successes from the \\(n\\) trials.\n\nThere is a finite population of \\(N\\) items\nEach item in the population is either a success or a failure, and there are \\(M\\) successes total.\nWe randomly select (sample) \\(n\\) items from the population without replacement\n\nShorthand: \\(X \\sim \\text{Hypergeo}(M, N, n)\\)\n\n\n\n\\[\nX = \\text{Number of successes in } n \\text{ draws}\n\\]\n\\[\np_X(x) = P(X=x) = \\dfrac{{M \\choose x}{N-M \\choose n-x}}{{N \\choose n}}\n\\] \\[\\text{ for } x \\text{ integer-valued } \\\\ \\max(0, n-(N-M)) \\leq x \\leq \\min(n, M)\\]\n\n\\[\\text{E}(X) =\\dfrac{nM}{N}\\]\n\\[\\text{Var}(X) = n \\dfrac{M}{N} \\bigg(1- \\dfrac{M}{N} \\bigg)\\bigg(\\dfrac{N-n}{N-1} \\bigg)\\]"
  },
  {
    "objectID": "lessons/15_Discrete_RVs/15_Discrete_RVs.html#wolf-population",
    "href": "lessons/15_Discrete_RVs/15_Discrete_RVs.html#wolf-population",
    "title": "Lesson 15: Some Important Discrete RVs",
    "section": "Wolf population",
    "text": "Wolf population\n\n\n\n\nExample 4\n\n\nA wildlife biologist is using mark-recapture to research a wolf population. Suppose a specific study region is known to have 24 wolves, of which 11 have already been tagged. If 5 wolves are randomly captured, what is the probability that 3 of them have already been tagged?"
  },
  {
    "objectID": "lessons/15_Discrete_RVs/15_Discrete_RVs.html#binomial-approximation-of-the-hypergeometric-rv",
    "href": "lessons/15_Discrete_RVs/15_Discrete_RVs.html#binomial-approximation-of-the-hypergeometric-rv",
    "title": "Lesson 15: Some Important Discrete RVs",
    "section": "Binomial approximation of the hypergeometric RV",
    "text": "Binomial approximation of the hypergeometric RV\nSuppose a hypergeometric RV \\(X\\) has the following properties:\n\nthe population size \\(N\\) is really big,\nthe number of successes \\(M\\) in the population is relatively large,\n\n\\(\\frac{M}{N}\\) shouldn’t be close to 0 or 1\n\nand the number of items \\(n\\) selected is small\nRule of thumb: \\(\\dfrac{n}{N}&lt;0.05\\) or \\(N&gt;20n\\)\n\nThen, in this case, making \\(n\\) draws from the population doesn’t change the probability of success much, and the hypergeometric RV. can be approximated by a binomial RV"
  },
  {
    "objectID": "lessons/15_Discrete_RVs/15_Discrete_RVs.html#wolf-population-revisited",
    "href": "lessons/15_Discrete_RVs/15_Discrete_RVs.html#wolf-population-revisited",
    "title": "Lesson 15: Some Important Discrete RVs",
    "section": "Wolf population revisited",
    "text": "Wolf population revisited\n\n\n\n\nExample 5\n\n\nSuppose a specific study region is known to have 2400 wolves, of which 1100 have already been tagged.\n\nIf 50 wolves are randomly captured, what is the probability that 20 of them have already been tagged?\nApproximate the probability in part (1) using the binomial distribution."
  },
  {
    "objectID": "lessons/15_Discrete_RVs/15_Discrete_RVs.html#discrete-uniform-rvs",
    "href": "lessons/15_Discrete_RVs/15_Discrete_RVs.html#discrete-uniform-rvs",
    "title": "Lesson 15: Some Important Discrete RVs",
    "section": "Discrete Uniform RVs",
    "text": "Discrete Uniform RVs\n\nScenario: There are \\(N\\) possible outcomes, which are all equally likely.\nShorthand: \\(X \\sim \\text{Uniform}(N)\\)\n\n\\[\nX = \\text{Outcome of interest, with } x=1, 2, ..., N\n\\]\n\\[\np_X(x) = P(X=x) = \\dfrac{1}{N} \\text{ for } x=1, 2, 3, ..., N\n\\]\n\\[\\text{E}(X) =\\dfrac{N+1}{2}\\]\n\\[\\text{Var}(X) = \\dfrac{N^2 -1}{12}\\]"
  },
  {
    "objectID": "lessons/15_Discrete_RVs/15_Discrete_RVs.html#what-discrete-uniform-rvs-have-we-seen-already",
    "href": "lessons/15_Discrete_RVs/15_Discrete_RVs.html#what-discrete-uniform-rvs-have-we-seen-already",
    "title": "Lesson 15: Some Important Discrete RVs",
    "section": "What discrete uniform RVs have we seen already?",
    "text": "What discrete uniform RVs have we seen already?\n\n\n\n\nExample 6\n\n\nExamples of discrete uniform RVs"
  },
  {
    "objectID": "lessons/15_Discrete_RVs/15_Discrete_RVs.html#all-the-r-code-for-these",
    "href": "lessons/15_Discrete_RVs/15_Discrete_RVs.html#all-the-r-code-for-these",
    "title": "Lesson 15: Some Important Discrete RVs",
    "section": "All the R code for these!",
    "text": "All the R code for these!\nCheck out this page with all the different functions for distributions in R\nExample of R commands for hypergeometric distribution with their input and output:\n\n\n\n\n\n\n\nR code\nWhat does it return?\n\n\n\n\nrhyper()\nreturns sample of random v ariables with specified dist ribution\n\n\ndhyper()\nreturns value of probability density at certain point of the dist ribution\n\n\nphyper()\nreturns cumulative pro bability of getting certain point (or less) of the normal dist ribution\n\n\nqhyper()\nreturns inverse CDF corresponding to desired quantile"
  },
  {
    "objectID": "lessons/04_Rules_of_prob/04_Rules_of_prob_muddy_points.html",
    "href": "lessons/04_Rules_of_prob/04_Rules_of_prob_muddy_points.html",
    "title": "Muddy Points",
    "section": "",
    "text": "When trying to figure out what theorem/rule/property to use, the most helpful thing is to (1) write the probability statement for your answer and (2) write the information you know in probability statements. Then you want to examine all the rules/theorems/properties we learned to see which can help you connect the information you know to the probability statement for the question."
  },
  {
    "objectID": "lessons/04_Rules_of_prob/04_Rules_of_prob_muddy_points.html#fall-2025",
    "href": "lessons/04_Rules_of_prob/04_Rules_of_prob_muddy_points.html#fall-2025",
    "title": "Muddy Points",
    "section": "",
    "text": "When trying to figure out what theorem/rule/property to use, the most helpful thing is to (1) write the probability statement for your answer and (2) write the information you know in probability statements. Then you want to examine all the rules/theorems/properties we learned to see which can help you connect the information you know to the probability statement for the question."
  },
  {
    "objectID": "lessons/04_Rules_of_prob/04_Rules_of_prob_muddy_points.html#fall-2024",
    "href": "lessons/04_Rules_of_prob/04_Rules_of_prob_muddy_points.html#fall-2024",
    "title": "Muddy Points",
    "section": "Fall 2024",
    "text": "Fall 2024\n\n1. How do I know if two events are independent?\nMy main piece of advice for independence is to rely on the math to show it, not logic! Don’t go into a problem thinking “Logically, these two events are independent.” If the problem does not say “Assume independence,” then we need to show it mathematically.\nSame goes with the definition of independence. When we say “knowing the outcome of one provides no information about the outcome of the other,” we once again need to show this mathematically, not just using logic. We translate the previous statement to \\[P(A) = P(A|B)\\] And we need to show this mathematically!\n\n\n2. Disjoint vs. Independent Events\nHere is a pretty good video breaking down disjoint (mutually exclusive) events and independent events. It includes examples as well."
  },
  {
    "objectID": "lessons/04_Rules_of_prob/05_Bayes_Theorem.html",
    "href": "lessons/04_Rules_of_prob/05_Bayes_Theorem.html",
    "title": "Chapter 5: Bayes’ Theorem",
    "section": "",
    "text": "Calculate conditional probability of an event using Bayes’ Theorem\nUtilize additional probability rules in probability calculations, specifically the Higher Order Multiplication Rule and the Law of Total Probabilities"
  },
  {
    "objectID": "lessons/04_Rules_of_prob/05_Bayes_Theorem.html#learning-objectives",
    "href": "lessons/04_Rules_of_prob/05_Bayes_Theorem.html#learning-objectives",
    "title": "Chapter 5: Bayes’ Theorem",
    "section": "",
    "text": "Calculate conditional probability of an event using Bayes’ Theorem\nUtilize additional probability rules in probability calculations, specifically the Higher Order Multiplication Rule and the Law of Total Probabilities"
  },
  {
    "objectID": "lessons/04_Rules_of_prob/05_Bayes_Theorem.html#where-are-we",
    "href": "lessons/04_Rules_of_prob/05_Bayes_Theorem.html#where-are-we",
    "title": "Chapter 5: Bayes’ Theorem",
    "section": "Where are we?",
    "text": "Where are we?"
  },
  {
    "objectID": "lessons/04_Rules_of_prob/05_Bayes_Theorem.html#introduction",
    "href": "lessons/04_Rules_of_prob/05_Bayes_Theorem.html#introduction",
    "title": "Chapter 5: Bayes’ Theorem",
    "section": "Introduction",
    "text": "Introduction\n\nSo we learned about conditional probabilities\n\nWe learned how the occurrence of event A affects event B (B conditional on A)\n\nCan we figure out information on how the occurrence of event B affects event A?\nWe can use the conditional probability (\\(\\mathbb{P}(A|B)\\)) to get information on the flipped conditional probability (\\(\\mathbb{P}(B|A)\\))"
  },
  {
    "objectID": "lessons/04_Rules_of_prob/05_Bayes_Theorem.html#bayes-rule-for-two-events",
    "href": "lessons/04_Rules_of_prob/05_Bayes_Theorem.html#bayes-rule-for-two-events",
    "title": "Chapter 5: Bayes’ Theorem",
    "section": "Bayes’ Rule for two events",
    "text": "Bayes’ Rule for two events\n\n\n\n\nTheorem: Bayes’ Rule (for two events)\n\n\nFor any two events \\(A\\) and \\(B\\) with nonzero probabilties,\n\\[\\mathbb{P}(A| B) =\n\\frac{\\mathbb{P}(A) \\cdot \\mathbb{P}(B|A)}\n{\\mathbb{P}(B)}\\]"
  },
  {
    "objectID": "lessons/04_Rules_of_prob/05_Bayes_Theorem.html#calculating-probability-with-higher-order-multiplication-rule",
    "href": "lessons/04_Rules_of_prob/05_Bayes_Theorem.html#calculating-probability-with-higher-order-multiplication-rule",
    "title": "Chapter 5: Bayes’ Theorem",
    "section": "Calculating probability with Higher Order Multiplication Rule",
    "text": "Calculating probability with Higher Order Multiplication Rule\n\n\n\n\nExample 1\n\n\nSuppose we draw 5 cards from a standard shuffled deck of 52 cards. What is the probability of a flush, that is all the cards are of the same suit (including straight flushes)?\n\n\n\n\n\nHigher Order Multiplication Rule\n\n\n\\[\\mathbb{P}(A_1\\cap A_2 \\cap  \\ldots \\cap A_n)=\\mathbb{P}(A_1)\\cdot\\mathbb{P}(A_2|A_1) \\cdot \\\\\n\\mathbb{P}(A_3|A_1A_2)\\ldots \\cdot\\mathbb{P}(A_n|A_1A_2\\ldots A_{n-1})\\]"
  },
  {
    "objectID": "lessons/04_Rules_of_prob/05_Bayes_Theorem.html#calculating-probability-with-law-of-total-probability",
    "href": "lessons/04_Rules_of_prob/05_Bayes_Theorem.html#calculating-probability-with-law-of-total-probability",
    "title": "Chapter 5: Bayes’ Theorem",
    "section": "Calculating probability with Law of Total Probability",
    "text": "Calculating probability with Law of Total Probability\n\n\n\n\nExample 2\n\n\nSuppose 1% of people assigned female at birth (AFAB) and 5% of people assigned male at birth (AMAB) are color-blind. Assume person born is equally likely AFAB or AMAB (not including intersex). What is the probability that a person chosen at random is color-blind?\n\n\n\n\n\nLaw of Total Probability for 2 Events\n\n\nFor events \\(A\\) and \\(B\\),\n\\[%\\left(\n\\begin{array}{ccl}\n\\mathbb{P}(B)&=&\\mathbb{P}(B \\cap A) + \\mathbb{P}(B \\cap A^C)\\\\\n           &=& \\mathbb{P}(B|A) \\cdot \\mathbb{P}(A)+ \\mathbb{P}(B | A^C)\\cdot \\mathbb{P}(A^C)\n\\end{array}\n%\\right)\\]"
  },
  {
    "objectID": "lessons/04_Rules_of_prob/05_Bayes_Theorem.html#general-law-of-total-proability",
    "href": "lessons/04_Rules_of_prob/05_Bayes_Theorem.html#general-law-of-total-proability",
    "title": "Chapter 5: Bayes’ Theorem",
    "section": "General Law of Total Proability",
    "text": "General Law of Total Proability\n\n\nLaw of Total Probability (general)\n\n\nIf \\(\\{A_i\\}_{i=1}^{n} = \\{A_1, A_2, \\ldots, A_n\\}\\) form a partition of the sample space, then for event \\(B\\),\n\\[%\\left(\n\\begin{array}{ccl}\n\\mathbb{P}(B)&=& \\sum_{i=1}^{n} \\mathbb{P}(B \\cap A_i)\\\\\n           &=& \\sum_{i=1}^{n} \\mathbb{P}(B|A_i) \\cdot \\mathbb{P}(A_i)\n\\end{array}\n%\\right)\\]"
  },
  {
    "objectID": "lessons/04_Rules_of_prob/05_Bayes_Theorem.html#calculating-probability-with-generalized-law-of-total-probability",
    "href": "lessons/04_Rules_of_prob/05_Bayes_Theorem.html#calculating-probability-with-generalized-law-of-total-probability",
    "title": "Chapter 5: Bayes’ Theorem",
    "section": "Calculating probability with generalized Law of Total Probability",
    "text": "Calculating probability with generalized Law of Total Probability"
  },
  {
    "objectID": "lessons/04_Rules_of_prob/05_Bayes_Theorem.html#calculating-probability-with-generalized-law-of-total-probability-1",
    "href": "lessons/04_Rules_of_prob/05_Bayes_Theorem.html#calculating-probability-with-generalized-law-of-total-probability-1",
    "title": "Chapter 5: Bayes’ Theorem",
    "section": "Calculating probability with generalized Law of Total Probability",
    "text": "Calculating probability with generalized Law of Total Probability\n\n\n\n\nExample 3\n\n\nIndividuals are diagnosed with a particular type of cancer that can take on three different disease forms,* \\(D_1\\), \\(D_2\\), and \\(D_3\\). It is known that amongst people diagnosed with this particular type of cancer,\n\n20% of people will eventually be diagnosed with form \\(D_1\\),\n30% with form \\(D_2\\), and\n50% with form \\(D_3\\).\n\nThe probability of requiring chemotherapy (\\(C\\)) differs among the three forms of disease:\n\n80% with \\(D_1\\),\n30% with \\(D_2\\), and\n10% with \\(D_3\\).\n\nBased solely on the preliminary test of being diagnosed with the cancer, what is the probability of requiring chemotherapy (the event C)?\n\n\n\nSkipping in class! Let me know if you would like me to post solutions to this if you work through it!"
  },
  {
    "objectID": "lessons/04_Rules_of_prob/05_Bayes_Theorem.html#lets-revisit-the-color-blind-example",
    "href": "lessons/04_Rules_of_prob/05_Bayes_Theorem.html#lets-revisit-the-color-blind-example",
    "title": "Chapter 5: Bayes’ Theorem",
    "section": "Let’s revisit the color-blind example",
    "text": "Let’s revisit the color-blind example\n\n\n\n\nExample 4\n\n\nRecall the color-blind example (Example 2), where\n\na person is AMAB with probability 0.5,\nAMAB people are color-blind with probability 0.05, and\nall people are color-blind with probability 0.03.\n\nAssuming people are AMAB or AFAB, find the probability that a color-blind person is AMAB."
  },
  {
    "objectID": "lessons/04_Rules_of_prob/05_Bayes_Theorem.html#calculate-probability-with-both-rules",
    "href": "lessons/04_Rules_of_prob/05_Bayes_Theorem.html#calculate-probability-with-both-rules",
    "title": "Chapter 5: Bayes’ Theorem",
    "section": "Calculate probability with both rules",
    "text": "Calculate probability with both rules\n\n\n\n\nExample 5\n\n\nSuppose\n\n1% of people who are AFAB aged 40-50 years have breast cancer,\nan AFAB person with breast cancer has a 90% chance of a positive test from a mammogram, and\nan AFAB person has a 10% chance of a false-positive result from a mammogram.\n\nWhat is the probability that an AFAB person has breast cancer given that they just had a positive test?"
  },
  {
    "objectID": "lessons/04_Rules_of_prob/05_Bayes_Theorem.html#bayes-rule",
    "href": "lessons/04_Rules_of_prob/05_Bayes_Theorem.html#bayes-rule",
    "title": "Chapter 5: Bayes’ Theorem",
    "section": "Bayes’ Rule",
    "text": "Bayes’ Rule\n\n\nTheorem: Bayes’ Rule\n\n\nIf \\(\\{A_i\\}_{i=1}^{n}\\) form a partition of the sample space \\(S\\), with \\(\\mathbb{P}(A_i)&gt;0\\) for \\(i=1\\ldots n\\) and \\(\\mathbb{P}(B)&gt;0\\), then\n\\[\\mathbb{P}(A_j | B) =\n\\frac{\\mathbb{P}(B|A_j) \\cdot \\mathbb{P}(A_j)}\n{\\sum_{i=1}^{n} \\mathbb{P}(B|A_i) \\cdot \\mathbb{P}(A_i)}\\]"
  },
  {
    "objectID": "lessons/04_Rules_of_prob/04_Rules_of_prob_key_info.html",
    "href": "lessons/04_Rules_of_prob/04_Rules_of_prob_key_info.html",
    "title": "Key Info",
    "section": "",
    "text": "No office hours from me this week - I have a standing appointment\n\nBut we will start next week!\n\nDrop deadline with 100% refund: this Friday, 10/10\nNew AI Video Tutorials! If anyone is looking for course materials on the responsible use of generative AI tools such as ChatGPT, I wanted to highlight that the OHSU library has released a new set of AI video tutorials!\n\nCreated by librarian David Carson, this series of short videos (3-4 minutes each) is based on the Kickstart Your Writing with AI workshop taught with writing specialist Zoe Speidel. Each video is standalone and can be viewed in any order, so faculty can share and reuse whichever are most relevant. The topics include effective prompting, AI and publishing, and how to cite or disclose AI. The videos are designed to be flexible resources and are available to embed directly into Sakai. This series is available on the Library’s Generative AI student guide."
  },
  {
    "objectID": "lessons/04_Rules_of_prob/04_Rules_of_prob_key_info.html#announcements",
    "href": "lessons/04_Rules_of_prob/04_Rules_of_prob_key_info.html#announcements",
    "title": "Key Info",
    "section": "",
    "text": "No office hours from me this week - I have a standing appointment\n\nBut we will start next week!\n\nDrop deadline with 100% refund: this Friday, 10/10\nNew AI Video Tutorials! If anyone is looking for course materials on the responsible use of generative AI tools such as ChatGPT, I wanted to highlight that the OHSU library has released a new set of AI video tutorials!\n\nCreated by librarian David Carson, this series of short videos (3-4 minutes each) is based on the Kickstart Your Writing with AI workshop taught with writing specialist Zoe Speidel. Each video is standalone and can be viewed in any order, so faculty can share and reuse whichever are most relevant. The topics include effective prompting, AI and publishing, and how to cite or disclose AI. The videos are designed to be flexible resources and are available to embed directly into Sakai. This series is available on the Library’s Generative AI student guide."
  },
  {
    "objectID": "lessons/04_Rules_of_prob/04_Rules_of_prob_key_info.html#key-dates",
    "href": "lessons/04_Rules_of_prob/04_Rules_of_prob_key_info.html#key-dates",
    "title": "Key Info",
    "section": "Key Dates",
    "text": "Key Dates\n\nHW 1 Assignment due this Sunday at 11pm"
  },
  {
    "objectID": "lessons/13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/36_Sums_of_Independent_Normal_rv/36_Sums_of_Independent_Normal_rv.html",
    "href": "lessons/13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/36_Sums_of_Independent_Normal_rv/36_Sums_of_Independent_Normal_rv.html",
    "title": "Chapter 36: Sums of Independent Normal RVs",
    "section": "",
    "text": "Calculate probability of a sample mean using a Normally distributed population"
  },
  {
    "objectID": "lessons/13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/36_Sums_of_Independent_Normal_rv/36_Sums_of_Independent_Normal_rv.html#learning-objectives",
    "href": "lessons/13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/36_Sums_of_Independent_Normal_rv/36_Sums_of_Independent_Normal_rv.html#learning-objectives",
    "title": "Chapter 36: Sums of Independent Normal RVs",
    "section": "",
    "text": "Calculate probability of a sample mean using a Normally distributed population"
  },
  {
    "objectID": "lessons/13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/36_Sums_of_Independent_Normal_rv/36_Sums_of_Independent_Normal_rv.html#sum-of-normal-rvs",
    "href": "lessons/13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/36_Sums_of_Independent_Normal_rv/36_Sums_of_Independent_Normal_rv.html#sum-of-normal-rvs",
    "title": "Chapter 36: Sums of Independent Normal RVs",
    "section": "Sum of Normal RVs",
    "text": "Sum of Normal RVs\n\n\nTheorem 1\n\n\nLet \\(X\\sim N(\\mu, \\sigma^2)\\), and let \\(Y=aX+b\\), where \\(a\\) and \\(b\\) are constants. Then \\[Y \\sim N(a\\mu+b, a^2\\sigma^2)\\]\n\n\n\n\nTheorem 2\n\n\nLet \\(X_i \\sim N(\\mu_i, \\sigma_i^2)\\) be independent normal rv’s, for \\(i=1,2,\\ldots,n\\). Then \\[\\sum_{i=1}^n X_i \\sim N\\Bigg(\\sum_{i=1}^n \\mu_i , \\sum_{i=1}^n \\sigma^2_i\\Bigg)\\]"
  },
  {
    "objectID": "lessons/13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/36_Sums_of_Independent_Normal_rv/36_Sums_of_Independent_Normal_rv.html#special-cases",
    "href": "lessons/13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/36_Sums_of_Independent_Normal_rv/36_Sums_of_Independent_Normal_rv.html#special-cases",
    "title": "Chapter 36: Sums of Independent Normal RVs",
    "section": "Special Cases",
    "text": "Special Cases\n\nLet \\(X_i \\sim N(\\mu, \\sigma^2)\\) be iid normal rv’s, for \\(i=1,2,\\ldots,n\\). Then \\[\\sum_{i=1}^n X_i \\sim N\\big(n\\mu, n \\sigma^2\\big)\\]\nLet \\(X_i \\sim N(\\mu, \\sigma^2)\\) be iid normal rv’s, for \\(i=1,2,\\ldots,n\\). Then \\[\\bar{X}=\\frac{\\sum_{i=1}^n X_i}{n} \\sim N\\big(\\mu, \\sigma^2 / n\\big)\\]\nLet \\(X\\sim N(\\mu_X,\\sigma_X^2)\\), and \\(Y\\sim N(\\mu_Y,\\sigma_Y^2)\\). Then \\[X-Y \\sim N\\big(\\mu_X - \\mu_Y, \\sigma^2_X + \\sigma^2_Y \\big)\\]"
  },
  {
    "objectID": "lessons/13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/36_Sums_of_Independent_Normal_rv/36_Sums_of_Independent_Normal_rv.html#detecting-and-solving-sums-of-normal-rvs-from-a-word-problem",
    "href": "lessons/13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/36_Sums_of_Independent_Normal_rv/36_Sums_of_Independent_Normal_rv.html#detecting-and-solving-sums-of-normal-rvs-from-a-word-problem",
    "title": "Chapter 36: Sums of Independent Normal RVs",
    "section": "Detecting and solving sums of Normal RVs from a word problem",
    "text": "Detecting and solving sums of Normal RVs from a word problem\n\n\n\n\nExample 1\n\n\nGlaucoma is an eye disease that is manifested by high intraocular pressure (IOP). The distribution of IOP in the general population is approximately normal with mean 16 mmHg and standard deviation 3 mmHg.\n\nSuppose a patient has 40 IOP readings. What is the probability that their average reading is greater than 20.32 mmHg, assuming their eyes are healthy?\nRepeat the previous question for a patient with 10 IOP readings."
  },
  {
    "objectID": "lessons/13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/11_Expected_Values_of_Sums_of_rvs.html",
    "href": "lessons/13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/11_Expected_Values_of_Sums_of_rvs.html",
    "title": "Chapter 11: Expected Values of Sums of Discrete RVs",
    "section": "",
    "text": "Calculate the mean (expected value) of sums of discrete random variables"
  },
  {
    "objectID": "lessons/13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/11_Expected_Values_of_Sums_of_rvs.html#learning-objectives",
    "href": "lessons/13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/11_Expected_Values_of_Sums_of_rvs.html#learning-objectives",
    "title": "Chapter 11: Expected Values of Sums of Discrete RVs",
    "section": "",
    "text": "Calculate the mean (expected value) of sums of discrete random variables"
  },
  {
    "objectID": "lessons/13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/11_Expected_Values_of_Sums_of_rvs.html#where-are-we",
    "href": "lessons/13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/11_Expected_Values_of_Sums_of_rvs.html#where-are-we",
    "title": "Chapter 11: Expected Values of Sums of Discrete RVs",
    "section": "Where are we?",
    "text": "Where are we?"
  },
  {
    "objectID": "lessons/13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/11_Expected_Values_of_Sums_of_rvs.html#revisiting-our-two-card-draw",
    "href": "lessons/13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/11_Expected_Values_of_Sums_of_rvs.html#revisiting-our-two-card-draw",
    "title": "Chapter 11: Expected Values of Sums of Discrete RVs",
    "section": "Revisiting our two card draw",
    "text": "Revisiting our two card draw\n\n\n\n\nExample 1\n\n\nSuppose you draw 2 cards from a standard deck of cards with replacement. Let \\(X\\) be the number of hearts you draw. Find \\(\\mathbb{E}[X]\\).\n\n\n\n\nRecall Binomial RV with \\(n=2\\):\n\\[p_X(x) = {2 \\choose x}p^x(1-p)^{2-x} \\text{  for } x = 0, 1, 2\\]"
  },
  {
    "objectID": "lessons/13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/11_Expected_Values_of_Sums_of_rvs.html#what-if-we-draw-a-lot-of-cards",
    "href": "lessons/13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/11_Expected_Values_of_Sums_of_rvs.html#what-if-we-draw-a-lot-of-cards",
    "title": "Chapter 11: Expected Values of Sums of Discrete RVs",
    "section": "What if we draw A LOT of cards?",
    "text": "What if we draw A LOT of cards?\n\n\n\n\nExample 2\n\n\nWhat is the expected number of hearts in Example 1 if you draw 200 cards?\n\n\n\n\nRecall Binomial RV with \\(n=200\\):\n\\[p_X(x) = {200 \\choose x}p^x(1-p)^{200-x}\\] \\[\\text{  for } x = 0, 1, 2, ..., 200\\]"
  },
  {
    "objectID": "lessons/13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/11_Expected_Values_of_Sums_of_rvs.html#sum-of-discrete-rvs",
    "href": "lessons/13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/11_Expected_Values_of_Sums_of_rvs.html#sum-of-discrete-rvs",
    "title": "Chapter 11: Expected Values of Sums of Discrete RVs",
    "section": "Sum of discrete RVs",
    "text": "Sum of discrete RVs\n\n\nTheorem 11.1: Sum of discrete RVs\n\n\nFor discrete r.v.’s \\(X_i\\) and constants \\(a_i\\), \\(i=1,2,\\dots, n\\), \\[\\mathbb{E}\\Bigg[\\sum_{i=1}^n a_iX_i\\Bigg] = \\sum_{i=1}^n a_i\\mathbb{E}[X_i] .\\] Remark: The theorem holds for infinitely r.v.’s \\(X_i\\) as well.\n\n\n\nFor two RVs, \\(X\\) and \\(Y\\):\n\nWe can say \\(E[X+Y] = E[X] + E[Y]\\)\n… and constant numbers \\(a\\) and \\(b\\), we can also say \\(E[aX+bY] = aE[X] + bE[Y]\\)\nWe can also also say \\(E[X-Y] = E[X] - E[Y]\\), since \\(b=-1\\)"
  },
  {
    "objectID": "lessons/13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/11_Expected_Values_of_Sums_of_rvs.html#corollaries-from-thm-11.1",
    "href": "lessons/13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/11_Expected_Values_of_Sums_of_rvs.html#corollaries-from-thm-11.1",
    "title": "Chapter 11: Expected Values of Sums of Discrete RVs",
    "section": "Corollaries from Thm 11.1",
    "text": "Corollaries from Thm 11.1\n\n\n\n\nCorollary 11.1.1\n\n\nFor a discrete r.v. \\(X\\), and constants \\(a\\) and \\(b\\), \\[\\mathbb{E}[aX+b] = a\\mathbb{E}[X] + b.\\]\n\n\n\n\n\nCorollary 11.1.2\n\n\nIf \\(X_i\\), \\(i=1,2,\\dots, n\\), are identically distributed r.v.’s, then \\[\\mathbb{E}\\bigg[\\sum_{i=1}^n X_i\\bigg] = n\\mathbb{E}[X_1] .\\]"
  },
  {
    "objectID": "lessons/13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/11_Expected_Values_of_Sums_of_rvs.html#cost-of-hotel-rooms",
    "href": "lessons/13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/11_Expected_Values_of_Sums_of_rvs.html#cost-of-hotel-rooms",
    "title": "Chapter 11: Expected Values of Sums of Discrete RVs",
    "section": "Cost of hotel rooms",
    "text": "Cost of hotel rooms\n\n\n\n\nExample 4\n\n\nA tour group is planning a visit to the city of Minneapolis and needs to book 30 hotel rooms. The average price of a room is $200. In addition, there is a 10% tourism tax for each room. What is the expected cost for the 30 hotel rooms?"
  },
  {
    "objectID": "lessons/13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/28_29_Joint_expected_value_Variance/28_29_Joint_expected_value_Variance_muddy_points.html",
    "href": "lessons/13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/28_29_Joint_expected_value_Variance/28_29_Joint_expected_value_Variance_muddy_points.html",
    "title": "Muddy Points",
    "section": "",
    "text": "Muddy points from Fall 2023:\n\n2. How do we set the bounds on a double integral?\nWhen the domain of the RVs are not dependent on each other, then we use the bounds as is. In example 2 of Chapter 26 notes, we have the joint pdf \\(f_{X,Y}(x,y)= 18 x^2 y^5\\), for\\(0 \\leq x \\leq 1, \\ 0 \\leq y \\leq 1\\). If we wanted to calculate something like \\(E(X)\\), then we could use the bounds as they are. Below is the domain for \\(x\\) and \\(y\\):\n\n\n\n\n\nHere is the integral for the expected value where we integrate over the whole domain of \\(x\\) and \\(y\\):\n\\[ E(X) = \\displaystyle\\int_0^1 \\displaystyle\\int_0^1 x (18 x^2 y^5 )dy dx \\]If we want to find the probability \\(P(0.25 \\leq X \\leq 0.5, 0.5 \\leq Y \\leq 0.75)\\), then we can look at the specific values of the probability:\n\n\n\n\n\nNote the blue lines above indicate how we integrate over \\(y\\) first from 0.5 to 0.75 and the green lines indicate how integrate over \\(x\\) first from 0.25 to 0.5. It seems like we’ve integrated over an area that isn’t within our specified probability. However, the integrated area is ONLY the overlap of the \\(x\\) and \\(y\\) bounds for the probability.\n\\[P(0.25 \\leq X \\leq 0.5, 0.5 \\leq Y \\leq 0.75) = \\displaystyle\\int_{0.25}^{0.5} \\displaystyle\\int_{0.5}^{0.75} 18 x^2 y^5 dy dx\\]\nLet’s use the same pdf, but now the domain of the two RVs is dependent on one another. We have the joint pdf \\(f_{X,Y}(x,y)= 18 x^2 y^5\\), for\\(0 \\leq x \\leq y\\leq1\\)\nIf we wanted to calculate something like \\(E(X)\\), then we need to account for fact that \\(x\\) must be less than of equal to \\(y\\). We can look back at the domain for this:\n\n\n\n\n\nNote the blue lines above still indicate how we integrate over \\(y\\) first from \\(x\\) to 1, and the green lines indicate how integrate over \\(x\\) first from 0 to 1. Once again, it seems like we’ve integrated over an area that isn’t within the domain. However, the integrated area is ONLY the overlap of the \\(x\\) and \\(y\\) bounds. Thus, once we’ve restricted \\(y\\) to the area between \\(x\\) and 1, we no longer need to restrict \\(x\\) to the are of 0 to \\(y\\).\n\\[ E(X) = \\displaystyle\\int_0^1 \\displaystyle\\int_x^1 x (18 x^2 y^5 )dy dx \\] If we want to find the probability \\(P(0.25 \\leq X \\leq 0.5, 0.5 \\leq Y \\leq 0.75)\\), then we should look back at our domain. For now, we are focusing on the orange area:\n\n\n\n\n\nBecause the orange area is totally within our domain, we can leave our integral our bounds as the exact values we specified:\n\\[P(0.25 \\leq X \\leq 0.5, 0.5 \\leq Y \\leq 0.75) = \\displaystyle\\int_{0.25}^{0.5} \\displaystyle\\int_{0.5}^{0.75} 18 x^2 y^5 dy dx\\]\nHowever, if we want the probability \\(P(0.5 \\leq X \\leq 0.75, 0.5 \\leq Y \\leq 0.75)\\), we would focus on the pink area above. We would limit one of our integrals to the \\(y=x\\) equation:\n\\[P(0.5 \\leq X \\leq 0.75, 0.5 \\leq Y \\leq 0.75) = \\displaystyle\\int_{0.5}^{0.75} \\displaystyle\\int_{x}^{0.75} 18 x^2 y^5 dy dx\\]\nOR\n\\[P(0.5 \\leq X \\leq 0.75, 0.5 \\leq Y \\leq 0.75) = \\displaystyle\\int_{0.5}^{0.75} \\displaystyle\\int_{0.5}^{y} 18 x^2 y^5 dx dy\\]\nThe key to these probabilities is that the bounds with the other variable is on the inside integral! Otherwise we end up with a answer that includes a RV."
  },
  {
    "objectID": "lessons/13_Expected_Values/13_Expected_Values.html",
    "href": "lessons/13_Expected_Values/13_Expected_Values.html",
    "title": "Lesson 13: Expected Values",
    "section": "",
    "text": "Define the expected value for discrete and continuous RVs\nCalculate the expected value (mean) of a single discrete RV\nCalculate the expected value (mean) of a single continuous RV\nCalculate expected value for the sum of discrete or continuous RVs\nCalculate expected value for joint densities (continuous RVs)"
  },
  {
    "objectID": "lessons/13_Expected_Values/13_Expected_Values.html#our-good-and-fair-friend-the-6-sided-die",
    "href": "lessons/13_Expected_Values/13_Expected_Values.html#our-good-and-fair-friend-the-6-sided-die",
    "title": "Lesson 13: Expected Values",
    "section": "Our good and fair friend, the 6-sided die",
    "text": "Our good and fair friend, the 6-sided die\n\n\n\n\nExample 1\n\n\nSuppose you roll a fair 6-sided die. What value do you expect to get?"
  },
  {
    "objectID": "lessons/13_Expected_Values/13_Expected_Values.html#what-is-an-expected-value",
    "href": "lessons/13_Expected_Values/13_Expected_Values.html#what-is-an-expected-value",
    "title": "Lesson 13: Expected Values",
    "section": "What is an expected value?",
    "text": "What is an expected value?\n\n\n\n\nDefinition: Expected value of discrete RV\n\n\nThe expected value of a discrete RV \\(X\\) that takes on values \\(x_1, x_2, \\ldots, x_n\\) is \\[\\mathbb{E}[X] = \\sum_{i=1}^n x_ip_X(x_i)\\] where \\(n\\) can be \\(\\infty\\)\n\n\n\n\n\nDefinition: Expected value of continuous RV\n\n\nThe expected value of a continuous RV \\(X\\) is \\[\\mathbb{E}[X] = \\displaystyle\\int_{-\\infty}^\\infty xf_X(x) dx\\] where we adjust the integrand based on the bounds of \\(X\\)\n\n\n\n\n\nExpected values are not necessarily an actual outcome\n\nIn previous example, we cannot roll a 3.5\nIt could be that our expected value is not in the sample space (\\(E(X) \\notin S\\))"
  },
  {
    "objectID": "lessons/13_Expected_Values/13_Expected_Values.html#our-good-and-not-so-fair-friend-the-6-sided-die",
    "href": "lessons/13_Expected_Values/13_Expected_Values.html#our-good-and-not-so-fair-friend-the-6-sided-die",
    "title": "Lesson 13: Expected Values",
    "section": "Our good and not-so-fair friend, the 6-sided die",
    "text": "Our good and not-so-fair friend, the 6-sided die\n\n\n\n\nExample 2\n\n\nSuppose the die is 6-sided, but not fair. And the probabilities of each side is distributed as:\n\n\n\n\\(x\\)\n\\(p_X(x)\\)\n\n\n\n\n1\n0.10\n\n\n2\n0.05\n\n\n3\n0.02\n\n\n4\n0.30\n\n\n5\n0.50\n\n\n6\n0.03\n\n\n\nWhat value do you expect to get on a roll?"
  },
  {
    "objectID": "lessons/13_Expected_Values/13_Expected_Values.html#expected-value-of-a-bernoulli-distribution",
    "href": "lessons/13_Expected_Values/13_Expected_Values.html#expected-value-of-a-bernoulli-distribution",
    "title": "Lesson 13: Expected Values",
    "section": "Expected value of a Bernoulli distribution",
    "text": "Expected value of a Bernoulli distribution\n\n\n\n\nExample 3\n\n\nSuppose \\[X = \\left\\{\n        \\begin{array}{ll}\n            1 & \\quad \\mathrm{with\\ probability}\\ p \\quad\\mathrm{(success)}\\\\\n            0 & \\quad \\mathrm{with\\ probability}\\ 1-p \\quad\\mathrm{(failure)}\n        \\end{array}\n    \\right.\\] Find the expected value of \\(X\\)."
  },
  {
    "objectID": "lessons/13_Expected_Values/13_Expected_Values.html#lets-slightly-change-our-random-variable",
    "href": "lessons/13_Expected_Values/13_Expected_Values.html#lets-slightly-change-our-random-variable",
    "title": "Lesson 13: Expected Values",
    "section": "Let’s slightly change our random variable",
    "text": "Let’s slightly change our random variable\n\n\n\n\nExample 4\n\n\nSuppose \\[X = \\left\\{\n        \\begin{array}{ll}\n            1 & \\quad \\mathrm{with\\ probability}\\ p \\\\\n            -1 & \\quad \\mathrm{with\\ probability}\\ 1-p\n        \\end{array}\n    \\right.\\] Find the expected value of \\(X\\)."
  },
  {
    "objectID": "lessons/13_Expected_Values/13_Expected_Values.html#bullseye",
    "href": "lessons/13_Expected_Values/13_Expected_Values.html#bullseye",
    "title": "Lesson 13: Expected Values",
    "section": "Bullseye! 🎯",
    "text": "Bullseye! 🎯\n\n\n\n\nExample 5\n\n\nSuppose I throw darts at a dartboard until I hit the bullseye, and that my probability of hitting the bullseye is \\(p\\). Suppose further that all of my throws are independent, and that the probability of a bullseye never changes, no matter how many times I throw a dart. How many times should I expect to have to throw the dart until I hit the bullseye?"
  },
  {
    "objectID": "lessons/13_Expected_Values/13_Expected_Values.html#ghost",
    "href": "lessons/13_Expected_Values/13_Expected_Values.html#ghost",
    "title": "Lesson 13: Expected Values",
    "section": "Ghost! 👻",
    "text": "Ghost! 👻\n\n\n\n\nExample 6\n\n\nA ghost is trick-or-treating. It comes to a house where it is known that there are 30 candies in the bag and only one is a watermelon Jolly Rancher, which is the ghost’s favorite. The ghost takes pieces of candy without replacement until it gets the watermelon Jolly Rancher. How many pieces of candy do we expect the ghost to take?\n\n\nCan we model this with a distribution?"
  },
  {
    "objectID": "lessons/13_Expected_Values/13_Expected_Values.html#some-remarks-on-last-two-examples",
    "href": "lessons/13_Expected_Values/13_Expected_Values.html#some-remarks-on-last-two-examples",
    "title": "Lesson 13: Expected Values",
    "section": "Some remarks on last two examples",
    "text": "Some remarks on last two examples\nBoth examples are repeated random processes. They are fundamentally different though:\n\nThe bullseye example is “with replacement” since the probability of success remains constant.\nThe ghost trick-or-treating example is without replacement, and thus the probability of success changes with each trial."
  },
  {
    "objectID": "lessons/13_Expected_Values/13_Expected_Values.html#expected-value-of-the-uniform-distribution",
    "href": "lessons/13_Expected_Values/13_Expected_Values.html#expected-value-of-the-uniform-distribution",
    "title": "Lesson 13: Expected Values",
    "section": "Expected Value of the Uniform Distribution",
    "text": "Expected Value of the Uniform Distribution\n\n\n\n\nExample 5\n\n\nLet \\(f_X(x)= \\frac{1}{b-a}\\), for \\(a \\leq x \\leq b\\). Find \\(\\mathbb{E}[X]\\)."
  },
  {
    "objectID": "lessons/13_Expected_Values/13_Expected_Values.html#expected-value-of-the-exponential-distribution",
    "href": "lessons/13_Expected_Values/13_Expected_Values.html#expected-value-of-the-exponential-distribution",
    "title": "Lesson 13: Expected Values",
    "section": "Expected Value of the Exponential Distribution",
    "text": "Expected Value of the Exponential Distribution\n\n\n\n\nExample 6\n\n\nLet \\(f_X(x)= \\lambda e^{-\\lambda x}\\), for \\(x &gt; 0\\) and \\(\\lambda&gt; 0\\). Find \\(\\mathbb{E}[X]\\).\n\n\n\n\nIntegrating by Parts\n\n\n\\(\\displaystyle\\int_a^b u dv = uv\\bigg|^b_a - \\displaystyle\\int_a^b vdu\\)"
  },
  {
    "objectID": "lessons/13_Expected_Values/13_Expected_Values.html#revisiting-our-two-card-draw",
    "href": "lessons/13_Expected_Values/13_Expected_Values.html#revisiting-our-two-card-draw",
    "title": "Lesson 13: Expected Values",
    "section": "Revisiting our two card draw",
    "text": "Revisiting our two card draw\n\n\n\n\nExample 7\n\n\nSuppose you draw 2 cards from a standard deck of cards with replacement. Let \\(X\\) be the number of hearts you draw. Find \\(\\mathbb{E}[X]\\).\n\n\n\n\nRecall Binomial RV with \\(n=2\\):\n\\[p_X(x) = {2 \\choose x}p^x(1-p)^{2-x} \\text{  for } x = 0, 1, 2\\]"
  },
  {
    "objectID": "lessons/13_Expected_Values/13_Expected_Values.html#what-if-we-draw-a-lot-of-cards",
    "href": "lessons/13_Expected_Values/13_Expected_Values.html#what-if-we-draw-a-lot-of-cards",
    "title": "Lesson 13: Expected Values",
    "section": "What if we draw A LOT of cards?",
    "text": "What if we draw A LOT of cards?\n\n\n\n\nExample 8\n\n\nWhat is the expected number of hearts in Example 1 if you draw 200 cards?\n\n\n\n\nRecall Binomial RV with \\(n=200\\):\n\\[p_X(x) = {200 \\choose x}p^x(1-p)^{200-x}\\] \\[\\text{  for } x = 0, 1, 2, ..., 200\\]"
  },
  {
    "objectID": "lessons/13_Expected_Values/13_Expected_Values.html#sums-of-random-variables",
    "href": "lessons/13_Expected_Values/13_Expected_Values.html#sums-of-random-variables",
    "title": "Lesson 13: Expected Values",
    "section": "Sums of Random Variables",
    "text": "Sums of Random Variables\n\n\nTheorem: Sum of random variables\n\n\nFor RVs (discrete or continuous) \\(X_i\\) and constants \\(a_i\\), \\(i=1,2,\\dots, n\\), \\[\\mathbb{E}\\Bigg[\\sum_{i=1}^n a_iX_i\\Bigg] = \\sum_{i=1}^n a_i\\mathbb{E}[X_i] .\\] Remark: The theorem holds for infinitely RV’s \\(X_i\\) as well.\n\n\n\nFor two RVs, \\(X\\) and \\(Y\\):\n\nWe can say \\(E[X+Y] = E[X] + E[Y]\\)\n… and constant numbers \\(a\\) and \\(b\\), we can also say \\(E[aX+bY] = aE[X] + bE[Y]\\)\nWe can also also say \\(E[X-Y] = E[X] - E[Y]\\), since \\(b=-1\\)"
  },
  {
    "objectID": "lessons/13_Expected_Values/13_Expected_Values.html#corollaries-from-theorem",
    "href": "lessons/13_Expected_Values/13_Expected_Values.html#corollaries-from-theorem",
    "title": "Lesson 13: Expected Values",
    "section": "Corollaries from Theorem",
    "text": "Corollaries from Theorem\n\n\n\n\nFunction with two constants\n\n\nFor a RV \\(X\\), and constants \\(a\\) and \\(b\\), \\[\\mathbb{E}[aX+b] = a\\mathbb{E}[X] + b.\\]\n\n\n\n\n\nExpected value of sum of identically distributed RVs\n\n\nIf \\(X_i\\), \\(i=1,2,\\dots, n\\), are identically distributed RV’s, then \\[\\mathbb{E}\\bigg[\\sum_{i=1}^n X_i\\bigg] = n\\mathbb{E}[X_1] .\\]"
  },
  {
    "objectID": "lessons/13_Expected_Values/13_Expected_Values.html#cost-of-hotel-rooms",
    "href": "lessons/13_Expected_Values/13_Expected_Values.html#cost-of-hotel-rooms",
    "title": "Lesson 13: Expected Values",
    "section": "Cost of hotel rooms",
    "text": "Cost of hotel rooms\n\n\n\n\nExample 8\n\n\nA tour group is planning a visit to the city of Minneapolis and needs to book 30 hotel rooms. The average price of a room is $200. In addition, there is a 10% tourism tax for each room. What is the expected cost for the 30 hotel rooms?"
  },
  {
    "objectID": "lessons/13_Expected_Values/13_Expected_Values.html#expected-value-of-one-rv-from-joint-pdf",
    "href": "lessons/13_Expected_Values/13_Expected_Values.html#expected-value-of-one-rv-from-joint-pdf",
    "title": "Lesson 13: Expected Values",
    "section": "Expected value of one RV from joint pdf",
    "text": "Expected value of one RV from joint pdf\nIf you have a joint distribution \\(f_{X,Y}(x,y)\\) and want to calculate \\(\\mathbb{E}[X]\\), you have two options:\n\nFind \\(f_X(x)\\) and use it to calculate \\(\\mathbb{E}[X]\\).\nCalculate \\(\\mathbb{E}[X]\\) using the joint density: \\[\\mathbb{E}[X] = \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}x f_{X,Y}(x,y)dydx.\\]\n\nYou can do the same for \\(\\mathbb{E}[Y]\\)!"
  },
  {
    "objectID": "lessons/13_Expected_Values/13_Expected_Values.html#option-1-find-marginal-first",
    "href": "lessons/13_Expected_Values/13_Expected_Values.html#option-1-find-marginal-first",
    "title": "Lesson 13: Expected Values",
    "section": "Option 1: Find marginal first",
    "text": "Option 1: Find marginal first\n\n\n\n\nExample 9\n\n\nLet \\(f_{X,Y}(x,y)= 2e^{-(x+y)}\\), for \\(0 \\leq x \\leq y\\). Find \\(\\mathbb{E}[X]\\).\n\n\n\nDo this one at home by finding \\(f_X(x)\\) then \\(\\mathbb{E}[X] = \\displaystyle\\int_{-\\infty}^\\infty xf_X(x) dx\\). See if you get the same result as next page’s answer!"
  },
  {
    "objectID": "lessons/13_Expected_Values/13_Expected_Values.html#option-2-expected-value-from-a-joint-distribution",
    "href": "lessons/13_Expected_Values/13_Expected_Values.html#option-2-expected-value-from-a-joint-distribution",
    "title": "Lesson 13: Expected Values",
    "section": "Option 2: Expected value from a joint distribution",
    "text": "Option 2: Expected value from a joint distribution\n\n\n\n\nExample 9\n\n\nLet \\(f_{X,Y}(x,y)= 2e^{-(x+y)}\\), for \\(0 \\leq x \\leq y\\). Find \\(\\mathbb{E}[X]\\)."
  },
  {
    "objectID": "lessons/10_Transformations/Old_notes/25_Joint_densities_key_info.html",
    "href": "lessons/10_Transformations/Old_notes/25_Joint_densities_key_info.html",
    "title": "Key Info",
    "section": "",
    "text": "Grading HW 6 assignment today!\n\nFeel free to turn in solutions later than 11/24 since I took so long to grade\n\nI made quite a few changes to the homeworks\n\nHW 10 is gone\nHW 9 is optional\nInstead of a HW assignment due over late November break\n\nHW 8 will include material from Week 8 and 9 and be due 12/5\n\n\nI have quite a bit of grading to catch up on!\nRealizing I never gave you mid-quarter feedback forms…\n\nLet’s chat about this and our assessment breakdown in the coming weeks\n\nAnything else?"
  },
  {
    "objectID": "lessons/10_Transformations/Old_notes/25_Joint_densities_key_info.html#announcements",
    "href": "lessons/10_Transformations/Old_notes/25_Joint_densities_key_info.html#announcements",
    "title": "Key Info",
    "section": "",
    "text": "Grading HW 6 assignment today!\n\nFeel free to turn in solutions later than 11/24 since I took so long to grade\n\nI made quite a few changes to the homeworks\n\nHW 10 is gone\nHW 9 is optional\nInstead of a HW assignment due over late November break\n\nHW 8 will include material from Week 8 and 9 and be due 12/5\n\n\nI have quite a bit of grading to catch up on!\nRealizing I never gave you mid-quarter feedback forms…\n\nLet’s chat about this and our assessment breakdown in the coming weeks\n\nAnything else?"
  },
  {
    "objectID": "lessons/10_Transformations/Old_notes/25_Joint_densities_key_info.html#key-dates",
    "href": "lessons/10_Transformations/Old_notes/25_Joint_densities_key_info.html#key-dates",
    "title": "Key Info",
    "section": "Key Dates",
    "text": "Key Dates\n\nThursday: HW 7 due\nSunday: HW 6 solutions due"
  },
  {
    "objectID": "lessons/10_Transformations/10_Transformations_key_info.html",
    "href": "lessons/10_Transformations/10_Transformations_key_info.html",
    "title": "Key Info",
    "section": "",
    "text": "Quiz 1 grades look pretty good! Everyone got score above 11/15\n\nFeedback released at 12:10pm today\nWill go over questions next class"
  },
  {
    "objectID": "lessons/10_Transformations/10_Transformations_key_info.html#announcements",
    "href": "lessons/10_Transformations/10_Transformations_key_info.html#announcements",
    "title": "Key Info",
    "section": "",
    "text": "Quiz 1 grades look pretty good! Everyone got score above 11/15\n\nFeedback released at 12:10pm today\nWill go over questions next class"
  },
  {
    "objectID": "lessons/10_Transformations/10_Transformations_key_info.html#key-dates",
    "href": "lessons/10_Transformations/10_Transformations_key_info.html#key-dates",
    "title": "Key Info",
    "section": "Key Dates",
    "text": "Key Dates\n\nHomework 4 due 11/02 at 11pm"
  },
  {
    "objectID": "lessons/12_Independence_Conditioning/12_Independence_conditioning_key_info.html",
    "href": "lessons/12_Independence_Conditioning/12_Independence_conditioning_key_info.html",
    "title": "Key Info",
    "section": "",
    "text": "Did everyone turn in their midquarter feedback?\nQuiz 2 opens on Wednesday\n\nLessons 7-9: pmfs, pdfs, and CDFs\n\nFinish up example from last class"
  },
  {
    "objectID": "lessons/12_Independence_Conditioning/12_Independence_conditioning_key_info.html#announcements",
    "href": "lessons/12_Independence_Conditioning/12_Independence_conditioning_key_info.html#announcements",
    "title": "Key Info",
    "section": "",
    "text": "Did everyone turn in their midquarter feedback?\nQuiz 2 opens on Wednesday\n\nLessons 7-9: pmfs, pdfs, and CDFs\n\nFinish up example from last class"
  },
  {
    "objectID": "lessons/12_Independence_Conditioning/12_Independence_conditioning_key_info.html#key-dates",
    "href": "lessons/12_Independence_Conditioning/12_Independence_conditioning_key_info.html#key-dates",
    "title": "Key Info",
    "section": "Key Dates",
    "text": "Key Dates\n\nHomework 5 due this Sunday\nQuiz 2 due this Sunday\n\nOpens Wednesday!"
  },
  {
    "objectID": "lessons/12_Independence_Conditioning/12_Independence_conditioning.html",
    "href": "lessons/12_Independence_Conditioning/12_Independence_conditioning.html",
    "title": "Lesson 12: Independence and Conditioning",
    "section": "",
    "text": "Identify the formula for joint distributions for independent RVs and conditional distributions (PMFs/PDFs)\nFind conditional pmf from a joint pmf and check if two RVs are independent.\nConstruct a joint distribution for two independent continuous RVs from their marginal distributions.\nCalculate conditional probabilities and distributions for continuous RVs."
  },
  {
    "objectID": "lessons/12_Independence_Conditioning/12_Independence_conditioning.html#how-do-we-represent-conditional-pmfspdfs",
    "href": "lessons/12_Independence_Conditioning/12_Independence_conditioning.html#how-do-we-represent-conditional-pmfspdfs",
    "title": "Lesson 12: Independence and Conditioning",
    "section": "How do we represent conditional pmfs/pdfs?",
    "text": "How do we represent conditional pmfs/pdfs?\nFor events:\n\\[P(A | B) = \\dfrac{P(A \\cap B)}{P(B)}\\]\n\n\n\n\nFor discrete RVs:\n\n\n\\[p_{X|Y}(x|y) = P(X=x|Y=y) = \\dfrac{p_{X,Y}(x,y)}{p_Y(y)}\\] \\[p_{Y|X}(y|x) = P(Y=y|X=x) = \\dfrac{p_{X,Y}(x,y)}{p_X(x)}\\]\nif denominator is greater than 0 (\\(p_Y(y) &gt; 0\\) or \\(p_X(x) &gt; 0\\))\n\n\n\n\n\n\n\nFor continuous RVs:\n\n\n\\[f_{X|Y}(x|y) = \\dfrac{f_{X,Y}(x,y)}{f_Y(y)}\\] \\[f_{Y|X}(y|x) = \\dfrac{f_{X,Y}(x,y)}{f_X(x)}\\]\nif denominator is greater than 0 (\\(f_Y(y) &gt; 0\\) or \\(f_X(x) &gt; 0\\))"
  },
  {
    "objectID": "lessons/12_Independence_Conditioning/12_Independence_conditioning.html#how-do-we-represent-independent-rvs-in-a-joint-pmfpdf",
    "href": "lessons/12_Independence_Conditioning/12_Independence_conditioning.html#how-do-we-represent-independent-rvs-in-a-joint-pmfpdf",
    "title": "Lesson 12: Independence and Conditioning",
    "section": "How do we represent independent RVs in a joint pmf/pdf?",
    "text": "How do we represent independent RVs in a joint pmf/pdf?\nWhat do we know about independence for events?\nFor events: If \\(A \\perp B\\)\n\\[P(A \\cap B) = P(A)P(B)\\] \\[P(A|B) = P(A)\\]\n\n\n\n\nFor discrete RVs: If \\(X \\perp Y\\)\n\n\n\\[p_{X,Y}(x,y) = p_{X}(x)p_{Y}(y)\\] \\[F_{X,Y}(x,y) = F_{X}(x)F_{Y}(y)\\] \\[p_{X|Y}(x|y) = p_{X}(x)\\] \\[p_{Y|X}(y|x) = p_{Y}(y)\\]\n\n\n\n\n\n\n\nFor continuous RVs: If \\(X \\perp Y\\)\n\n\n\\[f_{X,Y}(x,y) = f_{X}(x)f_{Y}(y)\\] \\[F_{X,Y}(x,y) = F_{X}(x)F_{Y}(y)\\] \\[f_{X|Y}(x|y) = f_{X}(x)\\] \\[f_{Y|X}(y|x) = f_{Y}(y)\\]"
  },
  {
    "objectID": "lessons/12_Independence_Conditioning/12_Independence_conditioning.html#remember-our-probability-rules-must-hold-for-these",
    "href": "lessons/12_Independence_Conditioning/12_Independence_conditioning.html#remember-our-probability-rules-must-hold-for-these",
    "title": "Lesson 12: Independence and Conditioning",
    "section": "Remember: our probability rules must hold for these!",
    "text": "Remember: our probability rules must hold for these!\n\n\n\n\nFor discrete RVs\n\n\nFor a valid joint pmf, we need:\n\n\\(0 \\geq p_{X,Y}(x,y) \\leq 1\\) for all \\(x, y\\)\n\\(\\sum \\limits_{\\{all\\ x\\}} \\sum \\limits_{\\{all\\ y\\}} p_{X,Y}(x,y)=1\\)\n\nFor a valid conditional pmf, we need:\n\n\\(0 \\geq p_{X|Y}(x|y) \\leq 1\\) for all \\(x, y\\)\n\\(\\sum \\limits_{\\{all\\ x\\}} p_{X|Y}(x|y)=1\\)\n\n\n\n\n\n\n\n\nFor continuous RVs\n\n\nFor a valid joint pdf, we need:\n\n\\(f_{X,Y}(x,y)\\geq 0\\) for all \\(x,y\\)\n\\(\\displaystyle\\int_{-\\infty}^{\\infty}\\displaystyle\\int_{-\\infty}^{\\infty} f_{X,Y}(x,y)dxdy=1\\)\n\nFor a valid conditional pdf, we need:\n\n\\(f_{X|Y}(x|y)\\geq 0\\) for all \\(x\\) and \\(y\\)\n\\(\\displaystyle\\int_{-\\infty}^{\\infty} f_{X|Y}(x|y)dx =1\\)"
  },
  {
    "objectID": "lessons/12_Independence_Conditioning/12_Independence_conditioning.html#extra-notes",
    "href": "lessons/12_Independence_Conditioning/12_Independence_conditioning.html#extra-notes",
    "title": "Lesson 12: Independence and Conditioning",
    "section": "Extra notes",
    "text": "Extra notes\n\nIf \\(X_1, X_2, …, X_n\\) are independent\n\n\\[p_{X_1, X_2, …, X_n}(x_1, x_2, …, x_n) = P(X_1=x_1, X_2=x_2, …, X_n=x_n)=\\prod\\limits_{i=1}^np_{X_i}(x_i)\\] \\[f_{X_1, X_2, …, X_n}(x_1, x_2, …, x_n) = \\prod\\limits_{i=1}^nf_{X_i}(x_i)\\] \\[F_{X_1, X_2, …, X_n}(x_1, x_2, …, x_n) = P(X_1\\leq x_1, X_2\\leq x_2, …, X_n\\leq x_n)=\\prod\\limits_{i=1}^nP(X_i \\leq x_i) = \\prod\\limits_{i=1}^nF_{X_i}(x_i)\\]\n\nDon’t forget, you can manipulate the conditional density to get the joint: \\[f_{X,Y}(x,y)= f_{X|Y}(x|y)f_Y(y)\\]"
  },
  {
    "objectID": "lessons/12_Independence_Conditioning/12_Independence_conditioning.html#last-class-joint-distribution-for-two-discrete-random-variables-12",
    "href": "lessons/12_Independence_Conditioning/12_Independence_conditioning.html#last-class-joint-distribution-for-two-discrete-random-variables-12",
    "title": "Lesson 12: Independence and Conditioning",
    "section": "Last class: joint distribution for two discrete random variables (1/2)",
    "text": "Last class: joint distribution for two discrete random variables (1/2)\n\n\n\n\nExample 1\n\n\nLet \\(X\\) and \\(Y\\) be two random draws from a box containing balls labelled 1, 2, and 3 without replacement.\n\nFind \\(p_{X|Y}(x|y)\\)."
  },
  {
    "objectID": "lessons/12_Independence_Conditioning/12_Independence_conditioning.html#last-class-joint-distribution-for-two-discrete-random-variables-22",
    "href": "lessons/12_Independence_Conditioning/12_Independence_conditioning.html#last-class-joint-distribution-for-two-discrete-random-variables-22",
    "title": "Lesson 12: Independence and Conditioning",
    "section": "Last class: joint distribution for two discrete random variables (2/2)",
    "text": "Last class: joint distribution for two discrete random variables (2/2)\n\n\n\n\nExample 1\n\n\nLet \\(X\\) and \\(Y\\) be two random draws from a box containing balls labelled 1, 2, and 3 without replacement.\n\nAre \\(X\\) and \\(Y\\) independent? Why or why not?\n\n\n\n\n\nRemark:\n\nTo show that \\(X\\) and \\(Y\\) are not independent, we just need to find one counter example\nHowever, to show that they are independent, we need to verify this for all possible pairs of \\(x\\) and \\(y\\)"
  },
  {
    "objectID": "lessons/12_Independence_Conditioning/12_Independence_conditioning.html#constructing-a-joint-pdf-from-two-independent-continuous-rvs",
    "href": "lessons/12_Independence_Conditioning/12_Independence_conditioning.html#constructing-a-joint-pdf-from-two-independent-continuous-rvs",
    "title": "Lesson 12: Independence and Conditioning",
    "section": "Constructing a joint pdf from two independent, continuous RVs",
    "text": "Constructing a joint pdf from two independent, continuous RVs\n\n\n\n\nExample 1.1\n\n\nLet \\(X\\) and \\(Y\\) be independent r.v.’s with \\(f_X(x)= \\frac12\\), for \\(0 \\leq x \\leq 2\\) and \\(f_Y(y)= 3y^2\\), for \\(0 \\leq y \\leq 1\\).\n\nFind \\(f_{X,Y}(x,y)\\)"
  },
  {
    "objectID": "lessons/12_Independence_Conditioning/12_Independence_conditioning.html#probability-from-joint-pdf-from-two-independent-continuous-rvs",
    "href": "lessons/12_Independence_Conditioning/12_Independence_conditioning.html#probability-from-joint-pdf-from-two-independent-continuous-rvs",
    "title": "Lesson 12: Independence and Conditioning",
    "section": "Probability from joint pdf from two independent, continuous RVs",
    "text": "Probability from joint pdf from two independent, continuous RVs\n\n\n\n\nExample 1.2\n\n\nLet \\(X\\) and \\(Y\\) be independent r.v.’s with \\(f_X(x)= \\frac12\\), for \\(0 \\leq x \\leq 2\\) and \\(f_Y(y)= 3y^2\\), for \\(0 \\leq y \\leq 1\\).\n\nFind \\(\\mathbb{P}(0 \\leq X \\leq 1, 0 \\leq Y \\leq \\frac12)\\)"
  },
  {
    "objectID": "lessons/12_Independence_Conditioning/12_Independence_conditioning.html#showing-independence-from-joint-pdf",
    "href": "lessons/12_Independence_Conditioning/12_Independence_conditioning.html#showing-independence-from-joint-pdf",
    "title": "Lesson 12: Independence and Conditioning",
    "section": "Showing independence from joint pdf",
    "text": "Showing independence from joint pdf\n\n\n\n\nExample 2.1\n\n\nLet \\(f_{X,Y}(x,y)= 18 x^2 y^5\\), for \\(0 \\leq x \\leq 1, \\ 0 \\leq y \\leq 1\\).\n\nAre \\(X\\) and \\(Y\\) independent?"
  },
  {
    "objectID": "lessons/12_Independence_Conditioning/12_Independence_conditioning.html#finding-cdf-from-two-independent-rvs",
    "href": "lessons/12_Independence_Conditioning/12_Independence_conditioning.html#finding-cdf-from-two-independent-rvs",
    "title": "Lesson 12: Independence and Conditioning",
    "section": "Finding CDF from two independent RVs",
    "text": "Finding CDF from two independent RVs\n\n\n\n\nExample 2.2\n\n\nLet \\(f_{X,Y}(x,y)= 18 x^2 y^5\\), for \\(0 \\leq x \\leq 1, \\ 0 \\leq y \\leq 1\\).\n\nFind \\(F_{X,Y}(x,y)\\)."
  },
  {
    "objectID": "lessons/12_Independence_Conditioning/12_Independence_conditioning.html#showing-independence-from-joint-pdf-1",
    "href": "lessons/12_Independence_Conditioning/12_Independence_conditioning.html#showing-independence-from-joint-pdf-1",
    "title": "Lesson 12: Independence and Conditioning",
    "section": "Showing independence from joint pdf",
    "text": "Showing independence from joint pdf\n\n\n\n\n\n\nDo this problem at home for extra practice. The solution is available in Meike’s video!\n\n\n\n\n\n\n\nExample 3\n\n\nLet \\(f_{X,Y}(x,y)= 2 e^{-(x+y)}\\), for \\(0 \\leq x \\leq y\\). Are \\(X\\) and \\(Y\\) independent?"
  },
  {
    "objectID": "lessons/12_Independence_Conditioning/12_Independence_conditioning.html#final-statement-on-independence",
    "href": "lessons/12_Independence_Conditioning/12_Independence_conditioning.html#final-statement-on-independence",
    "title": "Lesson 12: Independence and Conditioning",
    "section": "Final statement on independence",
    "text": "Final statement on independence\n\nIf \\(f_{X,Y}(x,y)= g(x)h(y)\\), where \\(g(x)\\) and \\(h(y)\\) are pdf’s, then \\(X\\) and \\(Y\\) are independent.\n\nThe domain of the joint pdf needs to be independent as well!!\n\n\n       \n\nIf \\(F_{X,Y}(x,y)= G(x)H(y)\\), where \\(G(x)\\) and \\(H(y)\\) are cdf’s, then \\(X\\) and \\(Y\\) are independent.\n\nThe domain of the joint CDF needs to be independent as well!!\n\n\n       \n\nMake sure that:\n\n\\(X\\) domain does NOT depend on \\(Y\\)\n\\(Y\\) domain does NOT depend on \\(X\\)"
  },
  {
    "objectID": "lessons/12_Independence_Conditioning/12_Independence_conditioning.html#example-starting-from-a-joint-pdf-first-try",
    "href": "lessons/12_Independence_Conditioning/12_Independence_conditioning.html#example-starting-from-a-joint-pdf-first-try",
    "title": "Lesson 12: Independence and Conditioning",
    "section": "Example starting from a joint pdf: first try!",
    "text": "Example starting from a joint pdf: first try!\n\n\n\n\nExample 1.1\n\n\nLet \\(f_{X,Y}(x,y)= 5 e^{-x-3y}\\), for \\(0 &lt; y &lt; \\frac{x}{2}\\).\n\nFind \\(\\mathbb{P}(2&lt;X&lt;10|Y=4)\\)"
  },
  {
    "objectID": "lessons/12_Independence_Conditioning/12_Independence_conditioning.html#example-starting-from-a-joint-pdf-second-try-12",
    "href": "lessons/12_Independence_Conditioning/12_Independence_conditioning.html#example-starting-from-a-joint-pdf-second-try-12",
    "title": "Lesson 12: Independence and Conditioning",
    "section": "Example starting from a joint pdf: second try! (1/2)",
    "text": "Example starting from a joint pdf: second try! (1/2)\n\n\n\n\nExample 1.1\n\n\nLet \\(f_{X,Y}(x,y)= 5 e^{-x-3y}\\), for \\(0 &lt; y &lt; \\frac{x}{2}\\).\n\nFind \\(\\mathbb{P}(2&lt;X&lt;10|Y=4)\\)"
  },
  {
    "objectID": "lessons/12_Independence_Conditioning/12_Independence_conditioning.html#example-starting-from-a-joint-pdf-second-try-22",
    "href": "lessons/12_Independence_Conditioning/12_Independence_conditioning.html#example-starting-from-a-joint-pdf-second-try-22",
    "title": "Lesson 12: Independence and Conditioning",
    "section": "Example starting from a joint pdf: second try! (2/2)",
    "text": "Example starting from a joint pdf: second try! (2/2)"
  },
  {
    "objectID": "lessons/12_Independence_Conditioning/12_Independence_conditioning.html#example-starting-from-a-joint-pdf",
    "href": "lessons/12_Independence_Conditioning/12_Independence_conditioning.html#example-starting-from-a-joint-pdf",
    "title": "Lesson 12: Independence and Conditioning",
    "section": "Example starting from a joint pdf",
    "text": "Example starting from a joint pdf\n\n\n\n\n\n\nDo this problem at home for extra practice. The solution is available in Meike’s video!\n\n\n\n\n\n\n\nExample 1.2\n\n\nLet \\(f_{X,Y}(x,y)= 5 e^{-x-3y}\\), for \\(0 &lt; y &lt; \\frac{x}{2}\\).\n\nFind \\(\\mathbb{P}(X&gt;20 |Y=5)\\)"
  },
  {
    "objectID": "lessons/12_Independence_Conditioning/12_Independence_conditioning.html#finding-probability-with-conditional-domain-and-pdf",
    "href": "lessons/12_Independence_Conditioning/12_Independence_conditioning.html#finding-probability-with-conditional-domain-and-pdf",
    "title": "Lesson 12: Independence and Conditioning",
    "section": "Finding probability with conditional domain and pdf",
    "text": "Finding probability with conditional domain and pdf\n\n\n\n\n\n\nDo this problem at home for extra practice. The solution is available in Meike’s video!\n\n\n\n\n\n\n\nExample 2\n\n\nRandomly choose a point \\(X\\) from the interval \\([0,1]\\), and given \\(X=x\\), randomly choose a point \\(Y\\) from \\([0,x]\\). Find \\(\\mathbb{P}(0 &lt; Y &lt; \\frac14)\\)."
  },
  {
    "objectID": "lessons/01_Probability/01_Probability_muddy_points.html",
    "href": "lessons/01_Probability/01_Probability_muddy_points.html",
    "title": "Muddy Points",
    "section": "",
    "text": "In class, we were wondering why/if \\(2^{|S|}\\) is the general formula for calculating the total number of possible events. We were specifically wondering if the \\(2\\) came from the fact that we had two options (heads and tails) for our outcome. Let’s work through the example of a 6-sided die to explain this further. The sample space is \\(S=\\{1, 2, 3, 4, 5, 6\\}\\). So is the total number of possible events \\(2^6\\) or \\(6^6\\) or something else? We can actually think about an event by using an indicator variable for each outcome of the sample space. An indicator variable is just a way to give us a yes/no answer to a question. So in this case, we are wondering: is this outcome a part of our event? If our event is \\(\\{1\\}\\) then for the outcome \\(1\\), the answer is “yes, the outcome is part of the event. For outcomes \\(2-6\\), the answer is”no, the outcome is not apart of the event.”\n\nFor each outcome, we have a “yes” or “no” answer. We can look at another example of an event. Let’s say our event is rolling an even number:\n\nFor \\(2\\), \\(4\\), and \\(6\\), the answer is “yes.” We can define the indicator variable for whether an outcome is in an event or not. The indicator gives a 1 or 0 for yes and no respectively.\n\nAs stated above, the \\(2\\) in \\(2^6\\) comes from the \\(2\\) options from our indicator. Each side has two options, and there are \\(6\\) sides. Thus, \\(2^6\\) possible events.\n\n\n\nI think this will become clearer when we start thinking about events in the context of probability. When we think of events outside of probability, we may think of something we actually do or something that happens, like going to a concert or coming to class or missing the streetcar. In this case, we think of the event as the single thing (out of all the options) that actually occured. For example, if I’m taking the streetcar to class, I can think of two definitive options of what might occur: I miss the streetcar or I get on the streetcar. Only one of these things can occur, which I may call an event colloquially.\nIt is important to make the distinction with events defined within probability. Events are not necessarily a single thing that occurred. Instead it can be a collection of things that may occur. In the example of the streetcar, I can define my event to include both options. Thus, my event is that I make the streetcar or I miss it. Both of these things cannot happen simultaneously, but if I want to calculate the probability that I miss or make the streetcar, then it is helpful to have the event defined.\n\n\nAn outcome is a single result. The two options in the above example, missing the streetcar or getting on the streetcar, are two potential outcomes. Events are the collection of 0, 1, or more outcomes. So the possible events are: the empty set, missing the streetcar, getting on the streetcar, or the set of missing the streetcar and getting on the streetcar."
  },
  {
    "objectID": "lessons/01_Probability/01_Probability_muddy_points.html#fall-2024",
    "href": "lessons/01_Probability/01_Probability_muddy_points.html#fall-2024",
    "title": "Muddy Points",
    "section": "",
    "text": "In class, we were wondering why/if \\(2^{|S|}\\) is the general formula for calculating the total number of possible events. We were specifically wondering if the \\(2\\) came from the fact that we had two options (heads and tails) for our outcome. Let’s work through the example of a 6-sided die to explain this further. The sample space is \\(S=\\{1, 2, 3, 4, 5, 6\\}\\). So is the total number of possible events \\(2^6\\) or \\(6^6\\) or something else? We can actually think about an event by using an indicator variable for each outcome of the sample space. An indicator variable is just a way to give us a yes/no answer to a question. So in this case, we are wondering: is this outcome a part of our event? If our event is \\(\\{1\\}\\) then for the outcome \\(1\\), the answer is “yes, the outcome is part of the event. For outcomes \\(2-6\\), the answer is”no, the outcome is not apart of the event.”\n\nFor each outcome, we have a “yes” or “no” answer. We can look at another example of an event. Let’s say our event is rolling an even number:\n\nFor \\(2\\), \\(4\\), and \\(6\\), the answer is “yes.” We can define the indicator variable for whether an outcome is in an event or not. The indicator gives a 1 or 0 for yes and no respectively.\n\nAs stated above, the \\(2\\) in \\(2^6\\) comes from the \\(2\\) options from our indicator. Each side has two options, and there are \\(6\\) sides. Thus, \\(2^6\\) possible events.\n\n\n\nI think this will become clearer when we start thinking about events in the context of probability. When we think of events outside of probability, we may think of something we actually do or something that happens, like going to a concert or coming to class or missing the streetcar. In this case, we think of the event as the single thing (out of all the options) that actually occured. For example, if I’m taking the streetcar to class, I can think of two definitive options of what might occur: I miss the streetcar or I get on the streetcar. Only one of these things can occur, which I may call an event colloquially.\nIt is important to make the distinction with events defined within probability. Events are not necessarily a single thing that occurred. Instead it can be a collection of things that may occur. In the example of the streetcar, I can define my event to include both options. Thus, my event is that I make the streetcar or I miss it. Both of these things cannot happen simultaneously, but if I want to calculate the probability that I miss or make the streetcar, then it is helpful to have the event defined.\n\n\nAn outcome is a single result. The two options in the above example, missing the streetcar or getting on the streetcar, are two potential outcomes. Events are the collection of 0, 1, or more outcomes. So the possible events are: the empty set, missing the streetcar, getting on the streetcar, or the set of missing the streetcar and getting on the streetcar."
  },
  {
    "objectID": "lessons/01_Probability/01_Probability_key_info.html",
    "href": "lessons/01_Probability/01_Probability_key_info.html",
    "title": "Key Info",
    "section": "",
    "text": "Slack is working for a few people??\n\nhttps://join.slack.com/t/bsta550/shared_invite/zt-2qtfo9s8j-j6ozEaYdT7yy1KuHoT9Mhw"
  },
  {
    "objectID": "lessons/01_Probability/01_Probability_key_info.html#announcements",
    "href": "lessons/01_Probability/01_Probability_key_info.html#announcements",
    "title": "Key Info",
    "section": "",
    "text": "Slack is working for a few people??\n\nhttps://join.slack.com/t/bsta550/shared_invite/zt-2qtfo9s8j-j6ozEaYdT7yy1KuHoT9Mhw"
  },
  {
    "objectID": "lessons/07_pmfs/Untitled.html",
    "href": "lessons/07_pmfs/Untitled.html",
    "title": "Untitled",
    "section": "",
    "text": "Let’s write the pdf for a binomial distribution: \\[f(x) = \\binom{n}{x} p^x (1-p)^{n-x}\\] where \\(x = 0, 1, 2, \\ldots, n\\).\nLet’s take the integral of this function from \\(-\\infty\\) to \\(\\infty\\):\n\\[\nF_X(x) = \\int_{-\\infty}^{\\infty} f(x) \\, dx\n\\]"
  },
  {
    "objectID": "lessons/07_pmfs/07_pmfs.html",
    "href": "lessons/07_pmfs/07_pmfs.html",
    "title": "Lesson 7: Probability Mass Functions (pmf’s)",
    "section": "",
    "text": "Identify a probability mass function (pmf) from past simulations\nIdentify a binomial random variable and its parameters from a word problem\nUse R to calculate probabilities and simulate binomial random variables"
  },
  {
    "objectID": "lessons/07_pmfs/07_pmfs.html#from-lesson-2-types-of-random-variables",
    "href": "lessons/07_pmfs/07_pmfs.html#from-lesson-2-types-of-random-variables",
    "title": "Lesson 7: Probability Mass Functions (pmf’s)",
    "section": "From Lesson 2: Types of random variables",
    "text": "From Lesson 2: Types of random variables\nThere are two types of random variables:\n\n\n\n\n\nDiscrete random variables (RVs): the set of possible values is either finite or can be put into a countably infinite list\n\nYou could theoretically list the specific possible outcomes that the variable can take\nIf you sum the rolls of three dice, you must get a whole number. For example, you can’t get any number between 3 and 4.\n\n\n\n\n\n\n\n\nContinuous random variables (RVs): take on values from continuous intervals, or unions of continuous intervals\n\nVariable takes on a range of values, but there are infinitely possible values within the range\nIf you keep track of the time you sleep, you can sleep for 8 hours or 7.9 hours or 7.99 hours or 7.999 hours …\n\n\n\n\n\n\n\nDiscrete random variables (RVs) are a little easier to simulate right now\n\nWe will only do discrete RVs today"
  },
  {
    "objectID": "lessons/07_pmfs/07_pmfs.html#what-is-a-probability-mass-function",
    "href": "lessons/07_pmfs/07_pmfs.html#what-is-a-probability-mass-function",
    "title": "Lesson 7: Probability Mass Functions (pmf’s)",
    "section": "What is a probability mass function?",
    "text": "What is a probability mass function?\n\n\nDefinition: probability distribution or probability mass function (pmf)\n\n\nThe probability distribution or probability mass function (pmf) of a discrete r.v. \\(X\\) is defined for every number \\(x\\) by \\[p_X(x) = \\mathbb{P}(X=x) = \\mathbb{P}(\\mathrm{all }\\ \\omega\\in S:X(\\omega) = x)\\]"
  },
  {
    "objectID": "lessons/07_pmfs/07_pmfs.html#from-lesson-2-simulating-two-rolls-12",
    "href": "lessons/07_pmfs/07_pmfs.html#from-lesson-2-simulating-two-rolls-12",
    "title": "Lesson 7: Probability Mass Functions (pmf’s)",
    "section": "From Lesson 2: Simulating two rolls (1/2)",
    "text": "From Lesson 2: Simulating two rolls (1/2)\n\n\nExample: Simulating Two Rolls of a Fair Four-Sided Die\n\n\nWe’re going to roll two four-sided die. Let \\(X\\) be the sum of two rolls. How would we simulate \\(X\\)?\n\n\n\nreps &lt;- 100000\nsimulations &lt;- replicate(reps, sample(x = 1:4, size = 2, replace = TRUE))\n\n\nLet’s show the first 14 simulations\n\n\nsimulations[, 1:14]\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14]\n[1,]    3    2    1    1    1    3    3    4    1     4     4     2     3     3\n[2,]    4    2    2    3    4    1    4    2    1     1     4     4     3     3\n\n\n\n\\(X\\) is the sum of the two rolls: we could calculate that for each column\n\n\nX_simulated &lt;- apply(simulations, 2, sum)\nX_simulated[1:14]\n\n [1] 7 4 3 4 5 4 7 6 2 5 8 6 6 6"
  },
  {
    "objectID": "lessons/07_pmfs/07_pmfs.html#from-lesson-2-simulating-two-rolls-22",
    "href": "lessons/07_pmfs/07_pmfs.html#from-lesson-2-simulating-two-rolls-22",
    "title": "Lesson 7: Probability Mass Functions (pmf’s)",
    "section": "From Lesson 2: Simulating two rolls (2/2)",
    "text": "From Lesson 2: Simulating two rolls (2/2)\n\n\n\n\nPlot simulated distribution of X\nX_df &lt;- as.data.frame(X_simulated) %&gt;%\n  rename(X = X_simulated)\n\nggplot(X_df, aes(x = X, after_stat(density))) +\n  geom_histogram(binwidth = 1, color = \"black\", fill = \"lightblue\") +\n  scale_x_continuous(breaks = seq(2, 8, by = 1)) +\n  labs(title = \"Simulated Distribution of X (Sum of Two Rolls)\",\n       x = \"Value of X\",\n       y = \"Approximated probablity\")\n\n\n\n\n\n\n\n\n\n\n\nFor the RV \\(X\\), we can find the probability for each possible value, \\(P(X=x) = p_X(x)\\): \\[\np_X(x) =\n\\begin{cases}\n\\frac{4-|x-5|}{16}, & x = 2, 3, 4, 5, 6,7, 8,\\\\\n0, & \\text{otherwise}\n\\end{cases}\n\\]"
  },
  {
    "objectID": "lessons/07_pmfs/07_pmfs.html#remarks-on-the-pmf",
    "href": "lessons/07_pmfs/07_pmfs.html#remarks-on-the-pmf",
    "title": "Lesson 7: Probability Mass Functions (pmf’s)",
    "section": "Remarks on the pmf",
    "text": "Remarks on the pmf\n\n\nProperties of pmf\n\n\nA pmf \\(p_X(x)\\) must satisfy the following properties:\n\n\\(0 \\leq p_X(x) \\leq 1\\) for all \\(x\\)\n\\(\\sum \\limits_{\\{all\\ x\\}}p_X(x)=1\\)\n\n\n\n\nSome distributions depend on parameters\n\nEach value of a parameter gives a different pmf\nIn previous example, the number of dice rolled was a parameter\n\nWe rolled 2 dice\nIf we rolled 4 dice, we’d get a different pmf!\n\nThe collection of all pmf’s for different values of the parameters is called a family of pmf’s"
  },
  {
    "objectID": "lessons/07_pmfs/07_pmfs.html#binomial-random-variables",
    "href": "lessons/07_pmfs/07_pmfs.html#binomial-random-variables",
    "title": "Lesson 7: Probability Mass Functions (pmf’s)",
    "section": "Binomial random variables",
    "text": "Binomial random variables\n\nOne specific type of discrete random variable is a binomial random variable\n\n\n\nBinomial random variable\n\n\n\n\\(X\\) is a binomial random variable if it represents the number of successes in \\(n\\) independent replications (or trials) of an experiment where\n\nEach replicate has two possible outcomes: either success or failure\nThe probability of success is \\(p\\)\nThe probability of failure is \\(q=1-p\\)\n\n\n\n\n\nA binomial random variable takes on values \\(0, 1, 2, \\dots, n\\).\nIf a r.v. \\(X\\) is modeled by a Binomial distribution, then we write in shorthand \\(X \\sim \\text{Binom}(n,p)\\)\nQuick example: The number of heads in 3 tosses of a fair coin is a binomial random variable with parameters \\(n = 3\\) and \\(p = 0.5\\)."
  },
  {
    "objectID": "lessons/07_pmfs/07_pmfs.html#binomial-family-of-distributions",
    "href": "lessons/07_pmfs/07_pmfs.html#binomial-family-of-distributions",
    "title": "Lesson 7: Probability Mass Functions (pmf’s)",
    "section": "Binomial family of distributions",
    "text": "Binomial family of distributions\n\n\nDistribution (or pmf) of a Binomial random variable\n\n\nLet \\(X\\) be the total number of successes in \\(n\\) independent trials, each with probability \\(p\\) of a success. Then probability of observing exactly \\(k\\) successes in \\(n\\) independent trials is\n\\[P(X = x) = \\binom{n}{x} p^x (1-p)^{n-x},  x= 0, 1, 2, \\dots, n \\]\n\n\n\nThe parameters of a binomial distribution are \\(p\\) and \\(n\\)."
  },
  {
    "objectID": "lessons/07_pmfs/07_pmfs.html#binomial-distribution-r-commands",
    "href": "lessons/07_pmfs/07_pmfs.html#binomial-distribution-r-commands",
    "title": "Lesson 7: Probability Mass Functions (pmf’s)",
    "section": "Binomial distribution: R commands",
    "text": "Binomial distribution: R commands\nR commands with their input and output:\n\n\n\n\n\n\n\n\n\nR code\nWhat does it return?\n\n\n\n\nrbinom()\nreturns sample of random variables with specified binomial distribution\n\n\ndbinom()\nreturns probability of getting certain number of successes\n\n\npbinom()\nreturns cumulative probability of getting certain number or less successes\n\n\nqbinom()\nreturns number of successes corresponding to desired quantile"
  },
  {
    "objectID": "lessons/07_pmfs/07_pmfs.html#falls-in-older-adults-15",
    "href": "lessons/07_pmfs/07_pmfs.html#falls-in-older-adults-15",
    "title": "Lesson 7: Probability Mass Functions (pmf’s)",
    "section": "Falls in Older Adults (1/5)",
    "text": "Falls in Older Adults (1/5)\n\n\n\n\nExample 1: Falls in Older Adults\n\n\nA major public health concern is falls among older adults (age 65+). National data suggests that 25% of older adults will experience at least one fall within a given year. A community health program is tracking a random group of \\(n = 8\\) older adults for one year. Assume the likelihood of falling is independent from person to person.\nLet \\(X\\) be the random variable representing the number of individuals in this group who experience at least one fall.\n\nWhat is the sample space for the random variable \\(X\\)?\nWrite the probability mass function (pmf) for \\(X\\).\nUse R to calculate the probability for each possible value of \\(X\\).\nMake a bar plot of the pmf.\nSimulate \\(X\\) for 10000 groups and plot the approximated pmf."
  },
  {
    "objectID": "lessons/07_pmfs/07_pmfs.html#falls-in-older-adults-25",
    "href": "lessons/07_pmfs/07_pmfs.html#falls-in-older-adults-25",
    "title": "Lesson 7: Probability Mass Functions (pmf’s)",
    "section": "Falls in Older Adults (2/5)",
    "text": "Falls in Older Adults (2/5)\n\n\n\n\nExample 1: Falls in Older Adults\n\n\n\nWhat is the sample space for the random variable \\(X\\)?"
  },
  {
    "objectID": "lessons/07_pmfs/07_pmfs.html#falls-in-older-adults-35",
    "href": "lessons/07_pmfs/07_pmfs.html#falls-in-older-adults-35",
    "title": "Lesson 7: Probability Mass Functions (pmf’s)",
    "section": "Falls in Older Adults (3/5)",
    "text": "Falls in Older Adults (3/5)\n\n\n\n\nExample 1: Falls in Older Adults\n\n\n\nWrite the probability mass function (pmf) for \\(X\\).\n\n\n\n\n\n\\[P(X = x) = \\binom{n}{x} p^x (1-p)^{n-x},  x= 0, 1, 2, \\dots, n \\]"
  },
  {
    "objectID": "lessons/07_pmfs/07_pmfs.html#falls-in-older-adults-46",
    "href": "lessons/07_pmfs/07_pmfs.html#falls-in-older-adults-46",
    "title": "Lesson 7: Probability Mass Functions (pmf’s)",
    "section": "Falls in Older Adults (4/6)",
    "text": "Falls in Older Adults (4/6)\n\n\n\n\nExample 1: Falls in Older Adults\n\n\n\nUse R to calculate the probability for each possible value of \\(X\\).\n\n\n\n\nn = 8\np = 0.25\n\ndbinom(0, size = n, prob = p)  #P(X=0)\n\n[1] 0.1001129\n\nfalls &lt;- tibble(\n  x = 0:n,\n  prob = dbinom(x, size = n, prob = p)\n)\n\n\n \n \n\nfalls\n\n# A tibble: 9 × 2\n      x      prob\n  &lt;int&gt;     &lt;dbl&gt;\n1     0 0.100    \n2     1 0.267    \n3     2 0.311    \n4     3 0.208    \n5     4 0.0865   \n6     5 0.0231   \n7     6 0.00385  \n8     7 0.000366 \n9     8 0.0000153"
  },
  {
    "objectID": "lessons/07_pmfs/07_pmfs.html#falls-in-older-adults-56",
    "href": "lessons/07_pmfs/07_pmfs.html#falls-in-older-adults-56",
    "title": "Lesson 7: Probability Mass Functions (pmf’s)",
    "section": "Falls in Older Adults (5/6)",
    "text": "Falls in Older Adults (5/6)\n\n\n\n\nExample 1: Falls in Older Adults\n\n\n\nMake a bar plot of the pmf.\n\n\n\n\n\n\nlibrary(ggplot2)\n\nggplot(falls, aes(x = x, y = prob)) +\n  geom_col() + \n  labs(\n    title = \"Probability mass function (pmf) of X\",\n    x = \"Number of adults (x)\",\n    y = \"Probability\"\n  )"
  },
  {
    "objectID": "lessons/07_pmfs/07_pmfs.html#falls-in-older-adults-66",
    "href": "lessons/07_pmfs/07_pmfs.html#falls-in-older-adults-66",
    "title": "Lesson 7: Probability Mass Functions (pmf’s)",
    "section": "Falls in Older Adults (6/6)",
    "text": "Falls in Older Adults (6/6)\n\n\n\n\nExample 1: Falls in Older Adults\n\n\n\nSimulate \\(X\\) for 10000 groups and plot the approximated pmf.\n\n\n\n\nset.seed(4764)\nreps = 10000\n\nsims = rbinom(n = reps, \n              size = n, \n              prob = p)\n\nsims %&gt;% head(., 14)\n\n [1] 2 1 2 1 3 3 2 2 4 2 3 0 2 0\n\nfalls2 &lt;- tibble(x = 0:n) %&gt;%\n  rowwise() %&gt;%\n  mutate(prob = sum(sims == x) / reps)\n\n\n\nggplot(falls2, aes(x = x, y = prob)) +\n  geom_col() + \n  labs(\n    title = \"Approximate probability mass function (pmf) of X\",\n    x = \"Number of adults (x)\",\n    y = \"Approximate Probability\"\n  )"
  },
  {
    "objectID": "lessons/14_Variance/14_Variance_key_info.html",
    "href": "lessons/14_Variance/14_Variance_key_info.html",
    "title": "Key Info and Announcements",
    "section": "",
    "text": "Homework 5\n\nCharles said it looks good! Impressed with everyone’s work on questions 3 and 4!\n\nQuestion for today’s exit ticket\n\nHow would you feel if we cancelled class on 12/1 and covered MGFs only in one class?\n\nMain reason I cover MGFs is to prepare you for the next course (BSSTA 551), but I talked to Jessica and she does not think they are needed for her class…\n\n\n\n\n\nWe have had a little delay with onboarding new peer-tutors this year, but Charles Parker will be available starting November 17th to support folks with biostatistics!\nUnlike TAs, tutors are not closely associated with any particular course sections or professors, nor do they have any grading responsibilities. Consider meeting with a tutor when you feel stuck on a project, need help with R coding, want to review course concepts, or simply complete assignments in a social learning environment. There is no cost to use tutoring. ​\nCharles is looking forward to working with you in a one-to-one or group setting, depending on your needs. Starting November 17th, you will be able to schedule to meet with them. \nYou can contact the Academic Success Center with questions learningsupport@ohsu.edu.​"
  },
  {
    "objectID": "lessons/14_Variance/14_Variance_key_info.html#announcements",
    "href": "lessons/14_Variance/14_Variance_key_info.html#announcements",
    "title": "Key Info and Announcements",
    "section": "",
    "text": "Homework 5\n\nCharles said it looks good! Impressed with everyone’s work on questions 3 and 4!\n\nQuestion for today’s exit ticket\n\nHow would you feel if we cancelled class on 12/1 and covered MGFs only in one class?\n\nMain reason I cover MGFs is to prepare you for the next course (BSSTA 551), but I talked to Jessica and she does not think they are needed for her class…\n\n\n\n\n\nWe have had a little delay with onboarding new peer-tutors this year, but Charles Parker will be available starting November 17th to support folks with biostatistics!\nUnlike TAs, tutors are not closely associated with any particular course sections or professors, nor do they have any grading responsibilities. Consider meeting with a tutor when you feel stuck on a project, need help with R coding, want to review course concepts, or simply complete assignments in a social learning environment. There is no cost to use tutoring. ​\nCharles is looking forward to working with you in a one-to-one or group setting, depending on your needs. Starting November 17th, you will be able to schedule to meet with them. \nYou can contact the Academic Success Center with questions learningsupport@ohsu.edu.​"
  },
  {
    "objectID": "lessons/14_Variance/14_Variance_key_info.html#key-dates",
    "href": "lessons/14_Variance/14_Variance_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates\n\nHomework 6 due 11/16\nHomework 7 due 11/23"
  },
  {
    "objectID": "lessons/14_Variance/old_lessons/29_Variance_and_Sums_of_rv.html",
    "href": "lessons/14_Variance/old_lessons/29_Variance_and_Sums_of_rv.html",
    "title": "Chapter 29: Variance of Continuous Random Variables",
    "section": "",
    "text": "Calculate expected value of functions of RVs\nCalculate variance of RVs"
  },
  {
    "objectID": "lessons/14_Variance/old_lessons/29_Variance_and_Sums_of_rv.html#learning-objectives",
    "href": "lessons/14_Variance/old_lessons/29_Variance_and_Sums_of_rv.html#learning-objectives",
    "title": "Chapter 29: Variance of Continuous Random Variables",
    "section": "",
    "text": "Calculate expected value of functions of RVs\nCalculate variance of RVs"
  },
  {
    "objectID": "lessons/14_Variance/old_lessons/29_Variance_and_Sums_of_rv.html#expected-value-of-a-function-of-a-continuous-rv",
    "href": "lessons/14_Variance/old_lessons/29_Variance_and_Sums_of_rv.html#expected-value-of-a-function-of-a-continuous-rv",
    "title": "Chapter 29: Variance of Continuous Random Variables",
    "section": "Expected value of a function of a continuous RV",
    "text": "Expected value of a function of a continuous RV\n\n\nHow do we calculate the expected value of a function of a discrete RV or joint RVs?\n   \nFor discrete RVs:\n\\[\\mathbb{E}[g(X)] = \\sum_{\\{all\\ x\\}}\\ g(x) p_X(x).\\] \\[\\mathbb{E}[g(X, Y)] = \\sum_{\\{all\\ x\\}}\\sum_{\\{all\\ y\\}}\\ g(x,y) p_{X,Y}(x,y).\\]\n\n\n\nHow do we calculate the expected value of a function of a continuous RV or joint RVs?\n   \nFor continuous RVs:"
  },
  {
    "objectID": "lessons/14_Variance/old_lessons/29_Variance_and_Sums_of_rv.html#important-properties-of-expected-values-of-functions-of-continuous-rvs",
    "href": "lessons/14_Variance/old_lessons/29_Variance_and_Sums_of_rv.html#important-properties-of-expected-values-of-functions-of-continuous-rvs",
    "title": "Chapter 29: Variance of Continuous Random Variables",
    "section": "Important properties of expected values of functions of continuous RVs",
    "text": "Important properties of expected values of functions of continuous RVs\n\n\n\n\nFunction of RV with two constants\n\n\n\\(\\mathbb{E}[aX+b] =a\\mathbb{E}[X]+b\\)\n\n\n\n\nFunction of two RVs added\n\n\n\\(\\mathbb{E}[X+Y] =\\mathbb{E}[X]+\\mathbb{E}[Y]\\)\n\n\n\n\nExpected value of sum of independent RVs pt 1\n\n\nIf \\(X_1, X_2, \\ldots X_n\\) are continuous RVs and \\(a_1, a_2, \\ldots a_n\\) are constants, then \\[\\mathbb{E}\\Bigg[\\sum_{i=1}^{n} a_i X_i\\Bigg] = \\sum_{i=1}^{n}a_i\\mathbb{E}[X_i]\\]\n\n\n\n\n\nExpected value of multiplication of function of independent RVs\n\n\nIf \\(X\\) and \\(Y\\) are independent continuous RVs, and \\(g\\) and \\(h\\) are functions, then \\[\\mathbb{E}[g(X)h(Y)] =\\mathbb{E}[g(X)]\\mathbb{E}[h(Y)]\\]\n\n\n\n\nExpected value of multiplication of independent RVs\n\n\nIf \\(X\\) and \\(Y\\) are independent continuous RVs, then \\[\\mathbb{E}[XY] = \\mathbb{E}[X] \\mathbb{E}[Y] \\]"
  },
  {
    "objectID": "lessons/14_Variance/old_lessons/29_Variance_and_Sums_of_rv.html#variance-of-continuous-rvs",
    "href": "lessons/14_Variance/old_lessons/29_Variance_and_Sums_of_rv.html#variance-of-continuous-rvs",
    "title": "Chapter 29: Variance of Continuous Random Variables",
    "section": "Variance of continuous RVs",
    "text": "Variance of continuous RVs\n\n\nHow do we calculate the variance of a discrete RV?\n   \nFor discrete RVs:\n\\[\n\\begin{align}\nVar(X) & =\n\\mathbb{E}[(X-\\mu_X)^2] \\\\ & = \\mathbb{E}[(X-\\mathbb{E}[X])^2] \\\\ &= \\mathbb{E}[X^2]-(\\mathbb{E}[X])^2 \\\\ & = \\sum_{\\{all\\ x\\}}(x-\\mu_x)^2 p_{X}(x)\n\\end{align}\n\\]\n\n\n\nHow do we calculate the variance of a continuous RV?\n    \nFor continuous RVs:"
  },
  {
    "objectID": "lessons/14_Variance/old_lessons/29_Variance_and_Sums_of_rv.html#variance-of-an-uniform-distribution",
    "href": "lessons/14_Variance/old_lessons/29_Variance_and_Sums_of_rv.html#variance-of-an-uniform-distribution",
    "title": "Chapter 29: Variance of Continuous Random Variables",
    "section": "Variance of an Uniform distribution",
    "text": "Variance of an Uniform distribution\n\n\n\n\nExample 2\n\n\nLet \\(f_X(x)= \\frac{1}{b-a}\\), for \\(a \\leq x \\leq b\\). Find \\(Var[X]\\)."
  },
  {
    "objectID": "lessons/14_Variance/old_lessons/29_Variance_and_Sums_of_rv.html#variance-of-exponential-distribution",
    "href": "lessons/14_Variance/old_lessons/29_Variance_and_Sums_of_rv.html#variance-of-exponential-distribution",
    "title": "Chapter 29: Variance of Continuous Random Variables",
    "section": "Variance of exponential distribution",
    "text": "Variance of exponential distribution\n\n\nIn the homework:\n\n\nExample 3\n\n\nLet \\(f_X(x)= \\lambda e^{-\\lambda x}\\), for \\(x &gt; 0\\) and \\(\\lambda&gt; 0\\). Find \\(Var[X]\\)."
  },
  {
    "objectID": "lessons/14_Variance/old_lessons/29_Variance_and_Sums_of_rv.html#important-properties-of-variances-of-continuous-rvs",
    "href": "lessons/14_Variance/old_lessons/29_Variance_and_Sums_of_rv.html#important-properties-of-variances-of-continuous-rvs",
    "title": "Chapter 29: Variance of Continuous Random Variables",
    "section": "Important properties of variances of continuous RVs",
    "text": "Important properties of variances of continuous RVs\n\n\n\n\nFunction of RV with two constants\n\n\n\\[Var[aX+b] = a^2Var[X]\\]\n\n\n\n\n\nVariance of sum of independent RVs pt 1\n\n\nIf \\(X_1, X_2, \\ldots X_n\\) are independent continuous RVs and \\(a_1, a_2, \\ldots a_n\\) are constants, then \\[Var\\Bigg(\\sum_{i=1}^{n} a_i X_i\\Bigg) =\\sum_{i=1}^{n} a^2_i Var(X_i)\\]\n\n\n\n\nVariance of sum of independent RVs pt 2\n\n\nIf \\(X_1, X_2, \\ldots X_n\\) are independent continuous RVs, then \\[Var\\Bigg(\\sum_{i=1}^{n} X_i\\Bigg) = \\sum_{i=1}^{n} Var(X_i)\\]"
  },
  {
    "objectID": "lessons/14_Variance/old_lessons/29_Variance_and_Sums_of_rv.html#find-the-mean-and-sd-from-word-problem",
    "href": "lessons/14_Variance/old_lessons/29_Variance_and_Sums_of_rv.html#find-the-mean-and-sd-from-word-problem",
    "title": "Chapter 29: Variance of Continuous Random Variables",
    "section": "Find the mean and sd from word problem",
    "text": "Find the mean and sd from word problem\n\n\n\n\nExample 4\n\n\nA machine manufactures cubes with a side length that varies uniformly from 1 to 2 inches. Assume the sides of the base and height are equal. The cost to make a cube is 10 ¢ per cubic inch, and 5 ¢ cents for the general cost per cube. Find the mean and standard deviation of the cost to make 10 cubes."
  },
  {
    "objectID": "lessons/00_Intro/00_Intro.html",
    "href": "lessons/00_Intro/00_Intro.html",
    "title": "Welcome to BSTA 551!",
    "section": "",
    "text": "Call me “Jessica,” “Dr. M,” “Professor Minnier [MIN-ee-ay],” or any combo!\nAssociate Professor of Biostatistics\n \nThis is a “newish” class to me\nTaught 552 Math/Stats II (RIP) for 9 years; Intro to R course developed & taught 3 years\nIntro\n\nSPH, Knight Cancer Institute, risk prediction, ’omics"
  },
  {
    "objectID": "lessons/00_Intro/00_Intro.html#jessica-minnier-sheher",
    "href": "lessons/00_Intro/00_Intro.html#jessica-minnier-sheher",
    "title": "Welcome to BSTA 551!",
    "section": "Jessica Minnier (she/her)",
    "text": "Jessica Minnier (she/her)\n\n\n\nCall me “Jessica,” “Dr. M,” “Professor Minnier [MIN-ee-ay],” or any combo!\nAssociate Professor of Biostatistics\n \nThis is a “newish” class to me\nTaught 552 Math/Stats II (RIP) for 9 years; Intro to R course developed & taught 3 years\nIntro\n\nSPH, Knight Cancer Institute, risk prediction, ’omics"
  },
  {
    "objectID": "lessons/00_Intro/00_Intro.html#some-important-tasks",
    "href": "lessons/00_Intro/00_Intro.html#some-important-tasks",
    "title": "Welcome to BSTA 551!",
    "section": "Some important tasks",
    "text": "Some important tasks\n\nStar the class website: https://nwakim.github.io/BSTA_550_25F/\n\n \n\nComplete Homework 0 by this Thursday at 11pm!\n\nIncludes office hours set up, attendance prefereance, and homework due date decision\nThink about what day of the week you would like your homeworks due\n\n\n \n\nHighly suggest that you make an appointment with a learning specialist through Student Academic Success Center!"
  },
  {
    "objectID": "lessons/00_Intro/00_Intro.html#lets-visit-the-website-homepage",
    "href": "lessons/00_Intro/00_Intro.html#lets-visit-the-website-homepage",
    "title": "Welcome to BSTA 551!",
    "section": "Let’s visit the website: Homepage",
    "text": "Let’s visit the website: Homepage"
  },
  {
    "objectID": "lessons/00_Intro/00_Intro.html#lets-visit-the-website-syllabus-12",
    "href": "lessons/00_Intro/00_Intro.html#lets-visit-the-website-syllabus-12",
    "title": "Welcome to BSTA 551!",
    "section": "Let’s visit the website: Syllabus (1/2)",
    "text": "Let’s visit the website: Syllabus (1/2)\n\nCourse learning objectives\nTextbook online! (different than last year)\nResources: PennState STAT 414 site!\nR: you will get a lot of help in BSTA 511 and we will use some!\nAssessments and grade breakdowns\n\nMostly homework + quizzes\n\nFeedback from you to me: in the form of exit tickets, midterm feedback, and final course eval\nHow to succeed in this course: resources and assignments explained\nLate work policy / Attendance policy\nChatGPT and other AI technology\nCourse expectations: a few ways that I will show you respect and commitment to you as students\n\nAnd a few ways I expect from you!\n\nCommunicating with me: give me 24 hours to reply M-F\n\nI try really hard to keep emails from taking over my life"
  },
  {
    "objectID": "lessons/00_Intro/00_Intro.html#lets-visit-the-website-syllabus-22",
    "href": "lessons/00_Intro/00_Intro.html#lets-visit-the-website-syllabus-22",
    "title": "Welcome to BSTA 551!",
    "section": "Let’s visit the website: Syllabus (2/2)",
    "text": "Let’s visit the website: Syllabus (2/2)"
  },
  {
    "objectID": "lessons/00_Intro/00_Intro.html#lets-visit-the-website-schedule-12",
    "href": "lessons/00_Intro/00_Intro.html#lets-visit-the-website-schedule-12",
    "title": "Welcome to BSTA 551!",
    "section": "Let’s visit the website: Schedule (1/2)",
    "text": "Let’s visit the website: Schedule (1/2)\n\nWeeks, class info, homeworks, labs"
  },
  {
    "objectID": "lessons/00_Intro/00_Intro.html#lets-visit-the-website-schedule-22",
    "href": "lessons/00_Intro/00_Intro.html#lets-visit-the-website-schedule-22",
    "title": "Welcome to BSTA 551!",
    "section": "Let’s visit the website: Schedule (2/2)",
    "text": "Let’s visit the website: Schedule (2/2)\n\n\n\n\nKey Info\nI will post announcements and other important class related info here. For example, if I change a due date or discuss a common mistake in homework, I will put it here.\n\n\n\nSlides HTML\nThese are the basic slides that will open in your browser.\n\n\n\nSlides PDF\nThese are the slides in pdf form for easy note taking. I’m not always the best at posting these before class, so make sure you know how to save your own copy of pdf slides!\n\n\n\nSlides Notes\nThese are the annotated slides in pdf form. In class, I add my own notes to slides. After class, I will post them here.\n\n\n\nExit tix\nThese are links to that day’s exit ticket.\n\n\n\nRecording\nI record our classes. This will be a link to the OneDrive folder containing this recording.\n\n\n\nMuddy Points\nYou will have a chance to ask questions about class in your exit tickets. If I notice a trend in confusion, I will add explanations to these “Muddy Points”"
  },
  {
    "objectID": "lessons/00_Intro/00_Intro.html#lets-visit-the-website-search",
    "href": "lessons/00_Intro/00_Intro.html#lets-visit-the-website-search",
    "title": "Welcome to BSTA 551!",
    "section": "Let’s visit the website: Search",
    "text": "Let’s visit the website: Search"
  },
  {
    "objectID": "lessons/00_Intro/00_Intro.html#lets-visit-the-website-homework",
    "href": "lessons/00_Intro/00_Intro.html#lets-visit-the-website-homework",
    "title": "Welcome to BSTA 551!",
    "section": "Let’s visit the website: Homework!",
    "text": "Let’s visit the website: Homework!"
  },
  {
    "objectID": "lessons/00_Intro/00_Intro.html#decision-on-homework-due-dates",
    "href": "lessons/00_Intro/00_Intro.html#decision-on-homework-due-dates",
    "title": "Welcome to BSTA 551!",
    "section": "Decision on Homework due dates",
    "text": "Decision on Homework due dates\n\nI have some set due dates in the schedule\nPlease look at your other classes, your calendar, etc\nConsider what day of the week you would like to turn in your assignments\nQuestion in HW 0 to cast your vote and share your opinion"
  },
  {
    "objectID": "lessons/00_Intro/00_Intro.html#structure-for-this-course",
    "href": "lessons/00_Intro/00_Intro.html#structure-for-this-course",
    "title": "Welcome to BSTA 551!",
    "section": "Structure for this course",
    "text": "Structure for this course\n\nLearning the basic tools to understand statistics\n\nThis is the first quarter that I am adding simulations\nSo I rearranged a lot of the topics!\n\n\n \n\nIt is going to feel useless at times, but I swear it is not!\n\n \n\nThis class will help you build a toolbox that allows you to analyze data while understanding the inner theory at play\nYou can use probability and simulations to change your analysis as you need"
  },
  {
    "objectID": "lessons/00_Intro/00_Intro.html#what-we-will-cover",
    "href": "lessons/00_Intro/00_Intro.html#what-we-will-cover",
    "title": "Welcome to BSTA 551!",
    "section": "What we will cover",
    "text": "What we will cover"
  },
  {
    "objectID": "lessons/11_Joint_distributions/Old_notes/25_Joint_densities.html",
    "href": "lessons/11_Joint_distributions/Old_notes/25_Joint_densities.html",
    "title": "Chapter 25: Joint densities",
    "section": "",
    "text": "Solve double integrals in our mini lesson!\nCalculate probabilities for a pair of continuous random variables\nCalculate a joint and marginal probability density function (pdf)\nCalculate a joint and marginal cumulative distribution function (CDF) from a pdf"
  },
  {
    "objectID": "lessons/11_Joint_distributions/Old_notes/25_Joint_densities.html#learning-objectives",
    "href": "lessons/11_Joint_distributions/Old_notes/25_Joint_densities.html#learning-objectives",
    "title": "Chapter 25: Joint densities",
    "section": "",
    "text": "Solve double integrals in our mini lesson!\nCalculate probabilities for a pair of continuous random variables\nCalculate a joint and marginal probability density function (pdf)\nCalculate a joint and marginal cumulative distribution function (CDF) from a pdf"
  },
  {
    "objectID": "lessons/11_Joint_distributions/Old_notes/25_Joint_densities.html#double-integrals-mini-lesson-13",
    "href": "lessons/11_Joint_distributions/Old_notes/25_Joint_densities.html#double-integrals-mini-lesson-13",
    "title": "Chapter 25: Joint densities",
    "section": "Double Integrals Mini Lesson (1/3)",
    "text": "Double Integrals Mini Lesson (1/3)\n\n\n\n\n\n\nDo this problem at home for extra practice. I’ll add the solution to the annotated notes!\n\n\n\n\n\n\n\nMini Lesson Example 1\n\n\nSolve the following integral: \\(\\displaystyle\\int_{2}^{3}\\displaystyle\\int_{0}^{1} xy dydx\\)"
  },
  {
    "objectID": "lessons/11_Joint_distributions/Old_notes/25_Joint_densities.html#double-integrals-mini-lesson-23",
    "href": "lessons/11_Joint_distributions/Old_notes/25_Joint_densities.html#double-integrals-mini-lesson-23",
    "title": "Chapter 25: Joint densities",
    "section": "Double Integrals Mini Lesson (2/3)",
    "text": "Double Integrals Mini Lesson (2/3)\n\n\n\n\n\n\nDo this problem at home for extra practice. I’ll add the solution to the annotated notes!\n\n\n\n\n\n\n\nMini Lesson Example 2\n\n\nSolve the following integral: \\(\\displaystyle\\int_{2}^{3}\\displaystyle\\int_{0}^{1} (x+y) dydx\\)"
  },
  {
    "objectID": "lessons/11_Joint_distributions/Old_notes/25_Joint_densities.html#double-integrals-mini-lesson-33",
    "href": "lessons/11_Joint_distributions/Old_notes/25_Joint_densities.html#double-integrals-mini-lesson-33",
    "title": "Chapter 25: Joint densities",
    "section": "Double Integrals Mini Lesson (3/3)",
    "text": "Double Integrals Mini Lesson (3/3)\n\n\n\n\n\n\nDo this problem at home for extra practice. I’ll add the solution to the annotated notes!\n\n\n\n\n\n\n\nMini Lesson Example 3\n\n\nSolve the following integral: \\(\\displaystyle\\int_{2}^{3}\\displaystyle\\int_{0}^{1} e^{x+y} dydx\\)"
  },
  {
    "objectID": "lessons/11_Joint_distributions/Old_notes/25_Joint_densities.html#how-to-define-the-joint-pdf-for-continuous-rvs",
    "href": "lessons/11_Joint_distributions/Old_notes/25_Joint_densities.html#how-to-define-the-joint-pdf-for-continuous-rvs",
    "title": "Chapter 25: Joint densities",
    "section": "How to define the joint pdf for continuous RVs?",
    "text": "How to define the joint pdf for continuous RVs?\n\n\nFor a single continuous RV \\(X\\) is a function \\(f_X(x)\\), such that for all real values \\(a,b\\) with \\(a \\leq b\\), \\[\\mathbb{P}(a \\leq X \\leq b) = \\int_a^b f_X(x)dx\\]\n\nFor two continuous RVs (\\(X\\) and \\(Y\\)), we can define the joint pdf, \\(f_{X,Y}(x,y)\\), such that for all real values \\(a,b, c, d\\) with \\(a \\leq b\\) and \\(c \\leq d\\), \\[\\mathbb{P}(a \\leq X \\leq b, c \\leq Y \\leq d) = \\int_a^b \\int_c^d f_{X,Y}(x,y)dydx\\]"
  },
  {
    "objectID": "lessons/11_Joint_distributions/Old_notes/25_Joint_densities.html#important-properties-of-the-joint-pdf",
    "href": "lessons/11_Joint_distributions/Old_notes/25_Joint_densities.html#important-properties-of-the-joint-pdf",
    "title": "Chapter 25: Joint densities",
    "section": "Important properties of the joint pdf",
    "text": "Important properties of the joint pdf\n\nNote that \\(f_{X,Y}(x,y)\\neq \\mathbb{P}(X=x, Y=y)\\)!!!\nIn order for \\(f_{X,Y}(x,y)\\) to be a pdf, it needs to satisfy the properties\n\n\\(f_{X,Y}(x,y)\\geq 0\\) for all \\(x,y\\)\n\\(\\displaystyle\\int_{-\\infty}^{\\infty}\\displaystyle\\int_{-\\infty}^{\\infty} f_{X,Y}(x,y)dxdy=1\\)"
  },
  {
    "objectID": "lessons/11_Joint_distributions/Old_notes/25_Joint_densities.html#what-is-the-joint-cumulative-distribution-function",
    "href": "lessons/11_Joint_distributions/Old_notes/25_Joint_densities.html#what-is-the-joint-cumulative-distribution-function",
    "title": "Chapter 25: Joint densities",
    "section": "What is the joint cumulative distribution function?",
    "text": "What is the joint cumulative distribution function?\n\n\nDefinition: Joint cumulative distribution function (Joint CDF)\n\n\nThe joint cumulative distribution function (cdf) of continuous random variables \\(X\\) and \\(Y\\), is the function \\(F_{X,Y}(x,y)\\), such that for all real values of \\(x\\) and \\(y\\), \\[F_{X,Y}(x,y)= \\mathbb{P}(X \\leq x, Y \\leq y) = \\int_{-\\infty}^x\\int_{-\\infty}^y f_{X,Y}(s,t)dtds\\]\n\n\nRemarks:\n\nThe definition above for \\(F_{X,Y}(x,y)\\) is a function of \\(x\\) and \\(y\\).\nThe joint cdf at the point \\((a,b)\\), is \\[F_{X,Y}(a,b) = \\mathbb{P}(X \\leq a, Y \\leq b) = \\int_{-\\infty}^a\\int_{-\\infty}^b f_{X,Y}(s,t)dtds\\]"
  },
  {
    "objectID": "lessons/11_Joint_distributions/Old_notes/25_Joint_densities.html#what-are-the-marginal-pdfs",
    "href": "lessons/11_Joint_distributions/Old_notes/25_Joint_densities.html#what-are-the-marginal-pdfs",
    "title": "Chapter 25: Joint densities",
    "section": "What are the marginal pdf’s?",
    "text": "What are the marginal pdf’s?\n\n\nDefinition: Marginal pdf’s\n\n\nSuppose \\(X\\) and \\(Y\\) are continuous r.v.’s, with joint pdf \\(f_{X,Y}(x,y)\\). Then the marginal probability density functions are \\[\\begin{aligned}\nf_X(x)&=& \\int_{-\\infty}^{\\infty} f_{X,Y}(x,y)dy\\\\\nf_Y(y)&=& \\int_{-\\infty}^{\\infty} f_{X,Y}(x,y)dx\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/11_Joint_distributions/Old_notes/25_Joint_densities.html#common-steps-for-solving-problems",
    "href": "lessons/11_Joint_distributions/Old_notes/25_Joint_densities.html#common-steps-for-solving-problems",
    "title": "Chapter 25: Joint densities",
    "section": "Common steps for solving problems",
    "text": "Common steps for solving problems\n\nSet up the domain of the pdf with a picture\nTranslate to needed integrands\n\nFor probability: shade in the area of interest, then translate\nFor expected value: translate domain\n\nSet up integral: \\(dxdy\\) or \\(dydx\\)?\nSolve integral!"
  },
  {
    "objectID": "lessons/11_Joint_distributions/Old_notes/25_Joint_densities.html#example-of-joint-pdf",
    "href": "lessons/11_Joint_distributions/Old_notes/25_Joint_densities.html#example-of-joint-pdf",
    "title": "Chapter 25: Joint densities",
    "section": "Example of joint pdf",
    "text": "Example of joint pdf\n\n\n\n\nExample 1.1\n\n\nLet \\(f_{X,Y}(x,y)= \\frac32 y^2\\), for \\(0 \\leq x \\leq 2, \\ 0 \\leq y \\leq 1\\).\n\nFind \\(\\mathbb{P}(0 \\leq X \\leq 1, 0 \\leq Y \\leq \\frac12)\\)."
  },
  {
    "objectID": "lessons/11_Joint_distributions/Old_notes/25_Joint_densities.html#example-of-joint-pdf-1",
    "href": "lessons/11_Joint_distributions/Old_notes/25_Joint_densities.html#example-of-joint-pdf-1",
    "title": "Chapter 25: Joint densities",
    "section": "Example of joint pdf",
    "text": "Example of joint pdf\n\n\n\n\nExample 1.2\n\n\nLet \\(f_{X,Y}(x,y)= \\frac32 y^2\\), for \\(0 \\leq x \\leq 2, \\ 0 \\leq y \\leq 1\\).\n\nFind \\(f_X(x)\\) and \\(f_Y(y)\\)."
  },
  {
    "objectID": "lessons/11_Joint_distributions/Old_notes/25_Joint_densities.html#example-of-a-more-complicated-joint-pdf",
    "href": "lessons/11_Joint_distributions/Old_notes/25_Joint_densities.html#example-of-a-more-complicated-joint-pdf",
    "title": "Chapter 25: Joint densities",
    "section": "Example of a more complicated joint pdf",
    "text": "Example of a more complicated joint pdf\n\n\n\n\n\n\nDo this problem at home for extra practice. I’ll add the solution to the annotated notes!\n\n\n\n\n\n\n\nExample 2.1\n\n\nLet \\(f_{X,Y}(x,y)= 2 e^{-(x+y)}\\), for \\(0 \\leq x \\leq y\\).\n\nFind \\(f_X(x)\\) and \\(f_Y(y)\\)."
  },
  {
    "objectID": "lessons/11_Joint_distributions/Old_notes/25_Joint_densities.html#example-of-a-more-complicated-joint-pdf-1",
    "href": "lessons/11_Joint_distributions/Old_notes/25_Joint_densities.html#example-of-a-more-complicated-joint-pdf-1",
    "title": "Chapter 25: Joint densities",
    "section": "Example of a more complicated joint pdf",
    "text": "Example of a more complicated joint pdf\n\n\n\n\n\n\nDo this problem at home for extra practice. I’ll add the solution to the annotated notes!\n\n\n\n\n\n\n\nExample 2.2\n\n\nLet \\(f_{X,Y}(x,y)= 2 e^{-(x+y)}\\), for \\(0 \\leq x \\leq y\\).\n\nFind \\(\\mathbb{P}(Y &lt; 3)\\)."
  },
  {
    "objectID": "lessons/11_Joint_distributions/Old_notes/25_Joint_densities.html#lets-complicate-this-even-more",
    "href": "lessons/11_Joint_distributions/Old_notes/25_Joint_densities.html#lets-complicate-this-even-more",
    "title": "Chapter 25: Joint densities",
    "section": "Let’s complicate this even more!",
    "text": "Let’s complicate this even more!\n\n\n\n\nExample 3.1\n\n\nLet \\(X\\) and \\(Y\\) have constant density on the square \\(0 \\leq X \\leq 4, 0 \\leq Y \\leq 4\\).\n\nFind \\(\\mathbb{P}(|X-Y| &lt; 2)\\)"
  },
  {
    "objectID": "lessons/11_Joint_distributions/Old_notes/25_Joint_densities.html#finding-the-pdf-of-a-transformation",
    "href": "lessons/11_Joint_distributions/Old_notes/25_Joint_densities.html#finding-the-pdf-of-a-transformation",
    "title": "Chapter 25: Joint densities",
    "section": "Finding the pdf of a transformation",
    "text": "Finding the pdf of a transformation\n\nLet \\(M\\) be a transformation of \\(X\\) and \\(Y\\)\nWhen we have a transformation of \\(X\\) and \\(Y\\), \\(M\\), we need to follow a specific process to find the pdf of \\(M\\)\n\nWe follow this process:\n\nStart with the joint pdf for \\(X\\) and \\(Y\\)\n\naka \\(f_{X,Y}(x, y)\\)\n\nTranslate the domain of \\(X\\) and \\(Y\\) to \\(M\\)\nFind the CDF of \\(M\\)\n\naka \\(F_M(m)\\) or \\(P(M \\leq m)\\)\n\nTake the derivative of the CDF of \\(M\\) to find the pdf of \\(M\\)\n\naka \\(f_M(m) = \\dfrac{d}{dm}F_M(m)\\)"
  },
  {
    "objectID": "lessons/11_Joint_distributions/Old_notes/25_Joint_densities.html#lets-complicate-this-even-more-1",
    "href": "lessons/11_Joint_distributions/Old_notes/25_Joint_densities.html#lets-complicate-this-even-more-1",
    "title": "Chapter 25: Joint densities",
    "section": "Let’s complicate this even more!",
    "text": "Let’s complicate this even more!\n\n\n\n\nExample 3.2\n\n\nLet \\(X\\) and \\(Y\\) have constant density on the square \\(0 \\leq X \\leq 4, 0 \\leq Y \\leq 4\\).\n\nLet \\(M = \\max(X,Y)\\). Find the pdf for \\(M\\), that is \\(f_M(m)\\)."
  },
  {
    "objectID": "lessons/11_Joint_distributions/Old_notes/25_Joint_densities.html#lets-complicate-this-even-more-2",
    "href": "lessons/11_Joint_distributions/Old_notes/25_Joint_densities.html#lets-complicate-this-even-more-2",
    "title": "Chapter 25: Joint densities",
    "section": "Let’s complicate this even more!",
    "text": "Let’s complicate this even more!\n\n\n\n\n\n\nDo this problem at home for extra practice. I’ll add the solution to the annotated notes!\n\n\n\n\n\n\n\nExample 3.3\n\n\nLet \\(X\\) and \\(Y\\) have constant density on the square \\(0 \\leq X \\leq 4, 0 \\leq Y \\leq 4\\).\n\nLet \\(Z = \\min(X,Y)\\). Find the pdf for \\(Z\\), that is \\(f_Z(z)\\)."
  },
  {
    "objectID": "lessons/11_Joint_distributions/Old_notes/25_Joint_densities.html#lets-complicate-this-even-further",
    "href": "lessons/11_Joint_distributions/Old_notes/25_Joint_densities.html#lets-complicate-this-even-further",
    "title": "Chapter 25: Joint densities",
    "section": "Let’s complicate this even further!",
    "text": "Let’s complicate this even further!\n\n\n\n\n\n\nDo this problem at home for extra practice. I’ll add the solution to the annotated notes!\n\n\n\n\n\n\n\nExample 4\n\n\nLet \\(X\\) and \\(Y\\) have joint density \\(f_{X,Y}(x,y)= \\frac85(x+y)\\) in the region \\(0 &lt; x &lt; 1,\\ \\frac12 &lt; y &lt;1\\). Find the pdf of the r.v. \\(Z\\), where \\(Z=XY\\)."
  },
  {
    "objectID": "lessons/11_Joint_distributions/Old_notes/25_Joint_densities_muddy_points.html",
    "href": "lessons/11_Joint_distributions/Old_notes/25_Joint_densities_muddy_points.html",
    "title": "Muddy Points",
    "section": "",
    "text": "Muddy points from Fall 2023:\n\n1. How do pdf, CDF, and probability interact with each other?\nLet’s say we have a pdf, \\(f_X(x) = \\dfrac{1}{9}x^2\\) for \\(0 \\leq x \\leq 3\\). This is just a function. The pdf is not used on its own to report any probability. We must integrate over the pdf to find a probability.\n\nlibrary(\"ggplot2\")\neq = function(x){(1/9)*x^2}\nggplot(data.frame(x=c(1, 50)), aes(x=x)) + \n  stat_function(fun=eq) +\n  xlab(\"x\") + ylab(\"pdf\") +\n  xlim(0,3)\n\n\n\n\n\n\n\n\nThe total area under the pdf is 1. This makes our pdf valid.\n\neq = function(x){(1/9)*x^2}\nggplot(data.frame(x=c(1, 50)), aes(x=x)) + \n  stat_function(fun=eq) +\n  xlab(\"x\") + ylab(\"pdf\") +\n  xlim(0,3) +\n  stat_function(fun=eq, \n                xlim = c(0, 3),\n                geom = \"area\", \n                aes(fill = \"red\")) +\n  theme(legend.position = \"none\") +\n  annotate(\"text\", x = 0.5, y = 0.7, label = \"AUC = 1\", color = \"black\")\n\n\n\n\n\n\n\n\nIf we only look at a proportion of the area under the pdf, then we start constructing our probabilities. For example, we can look at probability that we have a value between 0 and 1.5.\n\nggplot(data.frame(x=c(1, 50)), aes(x=x)) + \n  stat_function(fun=eq) +\n  xlab(\"x\") + ylab(\"pdf\") +\n  xlim(0,3) +\n  stat_function(fun=eq, \n                xlim = c(0, 1.5),\n                geom = \"area\", \n                aes(fill = \"blue\")) +\n  theme(legend.position = \"none\") +\n  annotate(\"text\", x = 0.5, y = 0.7, label = \"AUC = 0.125\", color = \"black\")\n\n\n\n\n\n\n\n\nInstead of calculating the EXACT probability for each value between 0 and 3, we can find the CDF of the pdf.\nThe CDF is: \\[\nF_X(x) = \\left\\{\n        \\begin{array}{ll}\n            0 & \\quad x&lt;3 \\quad \\\\\n            \\dfrac{1}{27}x^3 & \\quad 0 \\leq x \\leq 3\\quad \\\\\n            1 & \\quad x&gt;3 \\quad\n        \\end{array}\n    \\right.\n\\]\n\ncdf = function(x){(1/27)*x^3}\nggplot(data.frame(x=c(1, 50)), aes(x=x)) + \n  stat_function(fun=cdf) +\n  xlab(\"x\") + ylab(\"CDF\") +\n  xlim(0,3) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nWhen \\(x=1.5\\), we can calculate the probability using the CDF. Remember that \\(F_X(x) = P(X \\leq x)\\). So we can say \\(P(X \\leq 1.5) = F_X(1.5) = \\dfrac{1}{27}(1.5)^3\\), which equals 0.125.\n\ncdf = function(x){(1/27)*x^3}\nggplot(data.frame(x=c(1, 50)), aes(x=x)) + \n  stat_function(fun=cdf) +\n  xlab(\"x\") + ylab(\"CDF\") +\n  xlim(0,3) +\n  theme(legend.position = \"none\") +\n  geom_point(aes(x=1.5, y=.125), colour=\"blue\", size=3) +\n  annotate(\"text\", x = 0.5, y = 0.7, label = \"CDF = 0.125\", color = \"black\")\n\nWarning in geom_point(aes(x = 1.5, y = 0.125), colour = \"blue\", size = 3): All aesthetics have length 1, but the data has 2 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\nWe can also calculate the probability with an integral: \\(P(X \\leq 1.5) = \\displaystyle\\int_0^{1.5} \\dfrac{1}{9}x^2 dx\\).\nWe can also find the probability that X is between two numbers. \\(P(1\\leq X \\leq 1.5) = F_X(1.5) - F_X(1)\\) or \\(P(1\\leq X \\leq 1.5) = \\displaystyle\\int_1^{1.5} \\dfrac{1}{9}x^2 dx\\).\n\n\n2. Joint vs marginal vs conditional: How are we calculating the probability?\nIf we start at a joint probability \\(f_{X,Y}(x,y)\\)…. we can look at a few probabilities:\n\nJoint probability: \\(P(a \\leq X \\leq b, c \\leq Y \\leq d)\\)\n\\[P(a \\leq X \\leq b, c \\leq Y \\leq d) = \\displaystyle\\int_{x=a}^{x=b}\\displaystyle\\int_{y=c}^{y=d} f_{X,Y}(x,y) dydx\\]\nMarginal probability: \\(P(a \\leq X \\leq b)\\)\n\\[P(a \\leq X \\leq b) = \\displaystyle\\int_{x=a}^{x=b} f_{X}(x) dx\\]\nOR\n\\[P(a \\leq X \\leq b) = \\displaystyle\\int_{x=a}^{x=b}\\displaystyle\\int_{y=-\\inf}^{y=\\inf} f_{X,Y}(x,y) dydx\\]\nConditional probability: \\(P(a \\leq X \\leq b | Y = c)\\)\n\\[P(a \\leq X \\leq b | Y=c) = \\displaystyle\\int_{x=a}^{x=b} f_{X|Y}(x|y=c) dx\\]\nYou cannot calculate \\(P(a \\leq X \\leq b | Y = c)\\) by \\(\\dfrac{P(a \\leq X \\leq b, Y=c)}{P(Y = c)}\\) because \\(P(Y = c)\\) is 0. Instead, we need to find \\(f_{X|Y}(x|y=c)\\) by \\(\\dfrac{f_{X,Y}(x,y=c)}{f_{Y}(y=c)}\\) and THEN integrate over X.\n\n\n\n3. What are we actually finding by solving the double integral. In the first example, we found the probability was 1/16 after integrating but what does 1/16 mean in relation to the random variables X and Y?\nIt means that the volume contained by \\(0\\leq X \\leq 1\\), \\(0\\leq Y \\leq 1/2\\), and their joint pdf is 1/16 of the total volume contained by \\(0\\leq X \\leq 2\\), \\(0\\leq Y \\leq 1\\), and their joint pdf. The probability for a joint pdf is now a measure of the proportion of the volume.\nThis is not be confused with a probability from marginal pdf or pdf from one RV. The probability for marginal/single RV pdfs is the proportion of the area under the pdf for a specific range of values.\n\n\n4. Here’s a 3D plot of one of our joint pdf’s\n\\[\nf_{X,Y}(x,y) = 5e^{-x-3y} \\text{ for } 0 \\leq y \\leq x/2\n\\]\n\nlibrary(plotly)\n\nx = seq(0, 5, 0.1)\ny = seq(0, max(x)/2, 0.1/2)\nfn = expand.grid(x=x,y=y)\nfn$z = ifelse(fn$y&lt;fn$x/2, 5*exp( (-1)*fn$x - 3*fn$y), NA)\n\nz = matrix(fn$z, ncol = 51, nrow = 51, byrow = T)\n\nfig &lt;- plot_ly(x = x, y=y, z=z) %&gt;% add_surface()\n\nfig"
  },
  {
    "objectID": "lessons/11_Joint_distributions/11_Joint_distributions_key_info.html",
    "href": "lessons/11_Joint_distributions/11_Joint_distributions_key_info.html",
    "title": "Key Info",
    "section": "",
    "text": "Midterm feedback!\nHomework\n\nYou NEED to show your work! I am asking Charles to give half credit if you do not show sufficient work\nIf you have an html file and images of work, please insert the image into your html!\n\nHomework 2 feedback from Charles\n\nPeople forgot to use combinations for binary distribution\nSome people didn’t get all of the combinations for question 3\nSome didn’t know how to break up the venn diagram to find each prob. Or label it correctly\nSome people inserted \\(A \\cup B\\), \\(A \\cup C\\), \\(B \\cup C\\) into the equation for \\(A \\cup B \\cup C\\) …somehow it gets the same probability for A&B&C even though that the wrong way to do it"
  },
  {
    "objectID": "lessons/11_Joint_distributions/11_Joint_distributions_key_info.html#announcements",
    "href": "lessons/11_Joint_distributions/11_Joint_distributions_key_info.html#announcements",
    "title": "Key Info",
    "section": "",
    "text": "Midterm feedback!\nHomework\n\nYou NEED to show your work! I am asking Charles to give half credit if you do not show sufficient work\nIf you have an html file and images of work, please insert the image into your html!\n\nHomework 2 feedback from Charles\n\nPeople forgot to use combinations for binary distribution\nSome people didn’t get all of the combinations for question 3\nSome didn’t know how to break up the venn diagram to find each prob. Or label it correctly\nSome people inserted \\(A \\cup B\\), \\(A \\cup C\\), \\(B \\cup C\\) into the equation for \\(A \\cup B \\cup C\\) …somehow it gets the same probability for A&B&C even though that the wrong way to do it"
  },
  {
    "objectID": "lessons/11_Joint_distributions/11_Joint_distributions_key_info.html#key-dates",
    "href": "lessons/11_Joint_distributions/11_Joint_distributions_key_info.html#key-dates",
    "title": "Key Info",
    "section": "Key Dates",
    "text": "Key Dates\n\nHW 04 due this Sunday!\nMidterm feedback due Sunday as well!"
  },
  {
    "objectID": "lessons/11_Joint_distributions/11_Joint_distributions.html",
    "href": "lessons/11_Joint_distributions/11_Joint_distributions.html",
    "title": "Lesson 11: Joint distributions",
    "section": "",
    "text": "Define joint and marginal distributions for discrete and continuous random variables\nCalculate or find joint and marginal probabilities, pmf’s, and CDF’s for discrete random variables\nCalculate or find joint and marginal probabilities, pdf’s, and CDF’s for continuous random variables\nExtra practice on your own: solve double integrals in a mini lesson"
  },
  {
    "objectID": "lessons/11_Joint_distributions/11_Joint_distributions.html#what-is-a-joint-distribution",
    "href": "lessons/11_Joint_distributions/11_Joint_distributions.html#what-is-a-joint-distribution",
    "title": "Lesson 11: Joint distributions",
    "section": "What is a joint distribution?",
    "text": "What is a joint distribution?\n\n\n\n\nDefinition: joint pmf\n\n\nThe joint pmf of a pair of discrete RV’s \\(X\\) and \\(Y\\) is \\[\\begin{aligned}\np_{X,Y}(x,y) = & \\mathbb{P}(X=x \\cap Y=y) \\\\ = & \\mathbb{P}(X=x, Y=y)\n\\end{aligned}\\]\n\n\n\n\n\nDefinition: joint pdf\n\n\nThe joint pdf for two continuous RVs (\\(X\\) and \\(Y\\)) is \\(f_{X,Y}(x,y)\\), such that we have the following joint probability: \\[\\begin{aligned}\n\\mathbb{P}(a \\leq X \\leq b, & c \\leq Y \\leq d) = \\\\ & \\int_a^b \\int_c^d f_{X,Y}(x,y)dydx\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/11_Joint_distributions/11_Joint_distributions.html#important-properties-of-joint-distributions",
    "href": "lessons/11_Joint_distributions/11_Joint_distributions.html#important-properties-of-joint-distributions",
    "title": "Lesson 11: Joint distributions",
    "section": "Important properties of joint distributions",
    "text": "Important properties of joint distributions\n\n\n\n\nProperties of joint pmf’s\n\n\n\nA joint pmf \\(p_{X,Y}(x,y)\\) must satisfy the following properties:\n\n\\(0 \\geq p_{X,Y}(x,y) \\leq 1\\) for all \\(x, y\\)\n\n \n\n\\(\\sum \\limits_{\\{all\\ x\\}} \\sum \\limits_{\\{all\\ y\\}} p_{X,Y}(x,y)=1\\)\n\n\n\n\n\n\n\nProperties of joint pdf’s\n\n\n\nA joint pdf \\(f_{X,Y}(x,y)\\) must satisfy the following properties:\n\n\\(f_{X,Y}(x,y)\\geq 0\\) for all \\(x,y\\)\n\n \n\n\\(\\displaystyle\\int_{-\\infty}^{\\infty}\\displaystyle\\int_{-\\infty}^{\\infty} f_{X,Y}(x,y)dxdy=1\\)\n\n\n \n\nRemember that \\(f_{X,Y}(x,y)\\neq \\mathbb{P}(X=x, Y=y)\\)!!!"
  },
  {
    "objectID": "lessons/11_Joint_distributions/11_Joint_distributions.html#marginal-distributions",
    "href": "lessons/11_Joint_distributions/11_Joint_distributions.html#marginal-distributions",
    "title": "Lesson 11: Joint distributions",
    "section": "Marginal distributions",
    "text": "Marginal distributions\n\n\n\n\nMarginal pmf’s\n\n\nSuppose \\(X\\) and \\(Y\\) are discrete RV’s, with joint pmf \\(p_{X,Y}(x,y)\\). Then the marginal probability mass functions are\n\\[p_X(x) = \\sum \\limits_{\\{all\\ y\\}} p_{X,Y}(x,y)\\]\n\\[p_Y(y) = \\sum \\limits_{\\{all\\ x\\}} p_{X,Y}(x,y)\\]\n\n\n\n\n\nMarginal pdf’s\n\n\nSuppose \\(X\\) and \\(Y\\) are continuous RV’s, with joint pdf \\(f_{X,Y}(x,y)\\). Then the marginal probability density functions are \\[\\begin{aligned}\nf_X(x)&=& \\int_{-\\infty}^{\\infty} f_{X,Y}(x,y)dy\\\\\nf_Y(y)&=& \\int_{-\\infty}^{\\infty} f_{X,Y}(x,y)dx\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/11_Joint_distributions/11_Joint_distributions.html#joint-cumulative-distribution-functions-cdfs",
    "href": "lessons/11_Joint_distributions/11_Joint_distributions.html#joint-cumulative-distribution-functions-cdfs",
    "title": "Lesson 11: Joint distributions",
    "section": "Joint cumulative distribution functions (CDFs)",
    "text": "Joint cumulative distribution functions (CDFs)\n\n\n\n\nJoint CDF for discrete RVs\n\n\nThe joint CDF of a pair of discrete RV’s \\(X\\) and \\(Y\\) is \\[\\begin{aligned}\nF_{X,Y}(x,y) = &\\mathbb{P}(X \\leq x\\ and\\ Y \\leq y) \\\\ = &\\mathbb{P}(X \\leq x, Y \\leq y)\n\\end{aligned}\\]\n\n\n\n\n\nJoint CDF for continuous RVs\n\n\nThe joint CDF of continuous random variables \\(X\\) and \\(Y\\), is the function \\(F_{X,Y}(x,y)\\), such that for all real values of \\(x\\) and \\(y\\), \\[\\begin{aligned}\nF_{X,Y}(x,y)= \\mathbb{P}(X \\leq x, & Y \\leq y) = \\\\\n& \\int_{-\\infty}^x\\int_{-\\infty}^y f_{X,Y}(s,t)dtds\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/11_Joint_distributions/11_Joint_distributions.html#joint-distribution-for-two-discrete-random-variables-15",
    "href": "lessons/11_Joint_distributions/11_Joint_distributions.html#joint-distribution-for-two-discrete-random-variables-15",
    "title": "Lesson 11: Joint distributions",
    "section": "Joint distribution for two discrete random variables (1/5)",
    "text": "Joint distribution for two discrete random variables (1/5)\n\n\n\n\nExample 1\n\n\nLet \\(X\\) and \\(Y\\) be two random draws from a box containing balls labelled 1, 2, and 3 without replacement.\n\nFind \\(p_{X,Y}(x,y)\\)\nFind \\(\\mathbb{P}(X+Y=3)\\)\nFind \\(\\mathbb{P}(Y = 1)\\)\nFind \\(\\mathbb{P}(Y \\leq 2)\\)\nFind the joint CDF \\(F_{X,Y}(x,y)\\) for the joint pmf \\(p_{X,Y}(x,y)\\)\nFind the marginal CDFs \\(F_{X}(x)\\) and \\(F_{Y}(y)\\)"
  },
  {
    "objectID": "lessons/11_Joint_distributions/11_Joint_distributions.html#joint-distribution-for-two-discrete-random-variables-25",
    "href": "lessons/11_Joint_distributions/11_Joint_distributions.html#joint-distribution-for-two-discrete-random-variables-25",
    "title": "Lesson 11: Joint distributions",
    "section": "Joint distribution for two discrete random variables (2/5)",
    "text": "Joint distribution for two discrete random variables (2/5)\n\n\n\n\nExample 1\n\n\nLet \\(X\\) and \\(Y\\) be two random draws from a box containing balls labelled 1, 2, and 3 without replacement.\n\nFind \\(p_{X,Y}(x,y)\\)\nFind \\(\\mathbb{P}(X+Y=3)\\)"
  },
  {
    "objectID": "lessons/11_Joint_distributions/11_Joint_distributions.html#joint-distribution-for-two-discrete-random-variables-35",
    "href": "lessons/11_Joint_distributions/11_Joint_distributions.html#joint-distribution-for-two-discrete-random-variables-35",
    "title": "Lesson 11: Joint distributions",
    "section": "Joint distribution for two discrete random variables (3/5)",
    "text": "Joint distribution for two discrete random variables (3/5)\n\n\n\n\nExample 1\n\n\nLet \\(X\\) and \\(Y\\) be two random draws from a box containing balls labelled 1, 2, and 3 without replacement.\n\nFind \\(\\mathbb{P}(Y = 1)\\)\nFind \\(\\mathbb{P}(Y \\leq 2)\\)"
  },
  {
    "objectID": "lessons/11_Joint_distributions/11_Joint_distributions.html#joint-distribution-for-two-discrete-random-variables-45",
    "href": "lessons/11_Joint_distributions/11_Joint_distributions.html#joint-distribution-for-two-discrete-random-variables-45",
    "title": "Lesson 11: Joint distributions",
    "section": "Joint distribution for two discrete random variables (4/5)",
    "text": "Joint distribution for two discrete random variables (4/5)\n\n\n\n\nExample 1\n\n\nLet \\(X\\) and \\(Y\\) be two random draws from a box containing balls labelled 1, 2, and 3 without replacement.\n\nFind the joint CDF \\(F_{X,Y}(x,y)\\) for the joint pmf \\(p_{X,Y}(x,y)\\)"
  },
  {
    "objectID": "lessons/11_Joint_distributions/11_Joint_distributions.html#joint-distribution-for-two-discrete-random-variables-55",
    "href": "lessons/11_Joint_distributions/11_Joint_distributions.html#joint-distribution-for-two-discrete-random-variables-55",
    "title": "Lesson 11: Joint distributions",
    "section": "Joint distribution for two discrete random variables (5/5)",
    "text": "Joint distribution for two discrete random variables (5/5)\n\n\n\n\nExample 1\n\n\nLet \\(X\\) and \\(Y\\) be two random draws from a box containing balls labelled 1, 2, and 3 without replacement.\n\nFind the marginal CDFs \\(F_{X}(x)\\) and \\(F_{Y}(y)\\)"
  },
  {
    "objectID": "lessons/11_Joint_distributions/11_Joint_distributions.html#quick-remarks-on-the-joint-and-marginal-cdf",
    "href": "lessons/11_Joint_distributions/11_Joint_distributions.html#quick-remarks-on-the-joint-and-marginal-cdf",
    "title": "Lesson 11: Joint distributions",
    "section": "Quick remarks on the joint and marginal CDF",
    "text": "Quick remarks on the joint and marginal CDF\n\n\\(F_X(x)\\): right most columns of the CDF table (where the \\(Y\\) values are largest)\n\\(F_Y(y)\\): bottom row of the table (where X values are largest)\n\\(F_X(x)=\\lim\\limits_{y\\rightarrow\\infty}F_{X, Y}(x,y)\\)\n\\(F_Y(y)=\\lim\\limits_{x\\rightarrow\\infty}F_{X, Y}(x,y)\\)"
  },
  {
    "objectID": "lessons/11_Joint_distributions/11_Joint_distributions.html#common-steps-for-joint-pdfs-and-cdfs",
    "href": "lessons/11_Joint_distributions/11_Joint_distributions.html#common-steps-for-joint-pdfs-and-cdfs",
    "title": "Lesson 11: Joint distributions",
    "section": "Common steps for joint pdfs and CDFs",
    "text": "Common steps for joint pdfs and CDFs\n\nSet up the domain of the pdf with a picture\n\n \n\nTranslate to needed integrands\n\nFor probability: shade in the area of interest, then translate\nFor expected value: translate domain\n\n\n \n\nSet up integral: \\(dxdy\\) or \\(dydx\\)?\n\n \n\nSolve integral!"
  },
  {
    "objectID": "lessons/11_Joint_distributions/11_Joint_distributions.html#example-2-joint-pdf-12",
    "href": "lessons/11_Joint_distributions/11_Joint_distributions.html#example-2-joint-pdf-12",
    "title": "Lesson 11: Joint distributions",
    "section": "Example 2: Joint pdf (1/2)",
    "text": "Example 2: Joint pdf (1/2)\n\n\n\n\nExample 2.1\n\n\nLet \\(f_{X,Y}(x,y)= \\frac32 y^2\\), for \\(0 \\leq x \\leq 2, \\ 0 \\leq y \\leq 1\\).\n\nFind \\(\\mathbb{P}(0 \\leq X \\leq 1, 0 \\leq Y \\leq \\frac12)\\)"
  },
  {
    "objectID": "lessons/11_Joint_distributions/11_Joint_distributions.html#example-2-joint-pdf-22",
    "href": "lessons/11_Joint_distributions/11_Joint_distributions.html#example-2-joint-pdf-22",
    "title": "Lesson 11: Joint distributions",
    "section": "Example 2: Joint pdf (2/2)",
    "text": "Example 2: Joint pdf (2/2)\n\n\n\n\nExample 2.2\n\n\nLet \\(f_{X,Y}(x,y)= \\frac32 y^2\\), for \\(0 \\leq x \\leq 2, \\ 0 \\leq y \\leq 1\\).\n\nFind \\(f_X(x)\\) and \\(f_Y(y)\\)."
  },
  {
    "objectID": "lessons/11_Joint_distributions/11_Joint_distributions.html#example-with-more-complicated-pdf-12",
    "href": "lessons/11_Joint_distributions/11_Joint_distributions.html#example-with-more-complicated-pdf-12",
    "title": "Lesson 11: Joint distributions",
    "section": "Example with more complicated pdf (1/2)",
    "text": "Example with more complicated pdf (1/2)\n\n\n\n\n\n\nDo this problem at home for extra practice. I’ll add the solution to the annotated notes!\n\n\n\n\n\n\n\nExample 3.1\n\n\nLet \\(f_{X,Y}(x,y)= 2 e^{-(x+y)}\\), for \\(0 \\leq x \\leq y\\).\n\nFind \\(f_X(x)\\) and \\(f_Y(y)\\)."
  },
  {
    "objectID": "lessons/11_Joint_distributions/11_Joint_distributions.html#example-with-more-complicated-pdf-22",
    "href": "lessons/11_Joint_distributions/11_Joint_distributions.html#example-with-more-complicated-pdf-22",
    "title": "Lesson 11: Joint distributions",
    "section": "Example with more complicated pdf (2/2)",
    "text": "Example with more complicated pdf (2/2)\n\n\n\n\n\n\nDo this problem at home for extra practice. I’ll add the solution to the annotated notes!\n\n\n\n\n\n\n\nExample 3.2\n\n\nLet \\(f_{X,Y}(x,y)= 2 e^{-(x+y)}\\), for \\(0 \\leq x \\leq y\\).\n\nFind \\(\\mathbb{P}(Y &lt; 3)\\)."
  },
  {
    "objectID": "lessons/11_Joint_distributions/11_Joint_distributions.html#recall-finding-the-pdf-of-a-transformation",
    "href": "lessons/11_Joint_distributions/11_Joint_distributions.html#recall-finding-the-pdf-of-a-transformation",
    "title": "Lesson 11: Joint distributions",
    "section": "Recall: Finding the pdf of a transformation",
    "text": "Recall: Finding the pdf of a transformation\n\nLet \\(M\\) be a transformation of \\(X\\) and \\(Y\\): \\(M = g(X, Y)\\)\nWhen we have a transformation of \\(X\\) and \\(Y\\), \\(M\\), we need to follow the CDF method to find the pdf of \\(M\\)\n\nWe follow CDF method:\n\nStart with the joint pdf for \\(X\\) and \\(Y\\)\n\naka \\(f_{X,Y}(x, y)\\)\n\nTranslate the domain of \\(X\\) and \\(Y\\) to \\(M\\): find possible values of \\(M\\)\nFind the CDF of \\(M\\)\n\naka \\(F_M(m) = P(M \\leq m) = P(g(X, Y) \\leq m)\\)\n\nTake the derivative of the CDF of \\(M\\) with respect to \\(m\\) to find the pdf of \\(M\\)\n\naka \\(f_M(m) = \\dfrac{d}{dm}F_M(m)\\)"
  },
  {
    "objectID": "lessons/11_Joint_distributions/11_Joint_distributions.html#example-of-a-joint-pdf-with-a-transformation-13",
    "href": "lessons/11_Joint_distributions/11_Joint_distributions.html#example-of-a-joint-pdf-with-a-transformation-13",
    "title": "Lesson 11: Joint distributions",
    "section": "Example of a joint pdf with a transformation (1/3)",
    "text": "Example of a joint pdf with a transformation (1/3)\n\n\n\n\nExample 4.1\n\n\nLet \\(X\\) and \\(Y\\) have constant density on the square \\(0 \\leq X \\leq 4, 0 \\leq Y \\leq 4\\).\n\nFind \\(\\mathbb{P}(|X-Y| &lt; 2)\\)"
  },
  {
    "objectID": "lessons/11_Joint_distributions/11_Joint_distributions.html#example-of-a-joint-pdf-with-a-transformation-23",
    "href": "lessons/11_Joint_distributions/11_Joint_distributions.html#example-of-a-joint-pdf-with-a-transformation-23",
    "title": "Lesson 11: Joint distributions",
    "section": "Example of a joint pdf with a transformation (2/3)",
    "text": "Example of a joint pdf with a transformation (2/3)\n\n\n\n\nExample 4.2\n\n\nLet \\(X\\) and \\(Y\\) have constant density on the square \\(0 \\leq X \\leq 4, 0 \\leq Y \\leq 4\\).\n\nLet \\(M = \\max(X,Y)\\). Find the pdf for \\(M\\), that is \\(f_M(m)\\)"
  },
  {
    "objectID": "lessons/11_Joint_distributions/11_Joint_distributions.html#example-of-a-joint-pdf-with-a-transformation-33",
    "href": "lessons/11_Joint_distributions/11_Joint_distributions.html#example-of-a-joint-pdf-with-a-transformation-33",
    "title": "Lesson 11: Joint distributions",
    "section": "Example of a joint pdf with a transformation (3/3)",
    "text": "Example of a joint pdf with a transformation (3/3)\n\n\n\n\nExample 4.3\n\n\nLet \\(X\\) and \\(Y\\) have constant density on the square \\(0 \\leq X \\leq 4, 0 \\leq Y \\leq 4\\).\n\nLet \\(Z = \\min(X,Y)\\). Find the pdf for \\(Z\\), that is \\(f_Z(z)\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDo this problem at home for extra practice. I’ll add the solution to the annotated notes!"
  },
  {
    "objectID": "lessons/11_Joint_distributions/11_Joint_distributions.html#last-example-for-home-more-complicated-transformation",
    "href": "lessons/11_Joint_distributions/11_Joint_distributions.html#last-example-for-home-more-complicated-transformation",
    "title": "Lesson 11: Joint distributions",
    "section": "Last example for home: more complicated transformation",
    "text": "Last example for home: more complicated transformation\n\n\n\n\nExample 5\n\n\nLet \\(X\\) and \\(Y\\) have joint density \\(f_{X,Y}(x,y)= \\frac85(x+y)\\) in the region \\(0 &lt; x &lt; 1,\\ \\frac12 &lt; y &lt;1\\). Find the pdf of the RV \\(Z\\), where \\(Z=XY\\)."
  },
  {
    "objectID": "lessons/11_Joint_distributions/11_Joint_distributions.html#double-integrals-mini-lesson-13",
    "href": "lessons/11_Joint_distributions/11_Joint_distributions.html#double-integrals-mini-lesson-13",
    "title": "Lesson 11: Joint distributions",
    "section": "Double Integrals Mini Lesson (1/3)",
    "text": "Double Integrals Mini Lesson (1/3)\n\n\n\n\n\n\nDo this problem at home for extra practice. I’ll add the solution to the annotated notes!\n\n\n\n\n\n\n\nMini Lesson Example 1\n\n\nSolve the following integral: \\(\\displaystyle\\int_{2}^{3}\\displaystyle\\int_{0}^{1} xy dydx\\)"
  },
  {
    "objectID": "lessons/11_Joint_distributions/11_Joint_distributions.html#double-integrals-mini-lesson-23",
    "href": "lessons/11_Joint_distributions/11_Joint_distributions.html#double-integrals-mini-lesson-23",
    "title": "Lesson 11: Joint distributions",
    "section": "Double Integrals Mini Lesson (2/3)",
    "text": "Double Integrals Mini Lesson (2/3)\n\n\n\n\n\n\nDo this problem at home for extra practice. I’ll add the solution to the annotated notes!\n\n\n\n\n\n\n\nMini Lesson Example 2\n\n\nSolve the following integral: \\(\\displaystyle\\int_{2}^{3}\\displaystyle\\int_{0}^{1} (x+y) dydx\\)"
  },
  {
    "objectID": "lessons/11_Joint_distributions/11_Joint_distributions.html#double-integrals-mini-lesson-33",
    "href": "lessons/11_Joint_distributions/11_Joint_distributions.html#double-integrals-mini-lesson-33",
    "title": "Lesson 11: Joint distributions",
    "section": "Double Integrals Mini Lesson (3/3)",
    "text": "Double Integrals Mini Lesson (3/3)\n\n\n\n\n\n\nDo this problem at home for extra practice. I’ll add the solution to the annotated notes!\n\n\n\n\n\n\n\nMini Lesson Example 3\n\n\nSolve the following integral: \\(\\displaystyle\\int_{2}^{3}\\displaystyle\\int_{0}^{1} e^{x+y} dydx\\)"
  },
  {
    "objectID": "lessons/11_Joint_distributions/11_Joint_distributions_review.html",
    "href": "lessons/11_Joint_distributions/11_Joint_distributions_review.html",
    "title": "Lesson 11: Joint distributions",
    "section": "",
    "text": "Let \\(M\\) be a transformation of \\(X\\) and \\(Y\\): \\(M = g(X, Y)\\)\nWhen we have a transformation of \\(X\\) and \\(Y\\), \\(M\\), we need to follow the CDF method to find the pdf of \\(M\\)\n\nWe follow CDF method:\n\nStart with the joint pdf for \\(X\\) and \\(Y\\)\n\naka \\(f_{X,Y}(x, y)\\)\n\nTranslate the domain of \\(X\\) and \\(Y\\) to \\(M\\): find possible values of \\(M\\)\nFind the CDF of \\(M\\)\n\naka \\(F_M(m) = P(M \\leq m) = P(g(X, Y) \\leq m)\\)\n\nTake the derivative of the CDF of \\(M\\) with respect to \\(m\\) to find the pdf of \\(M\\)\n\naka \\(f_M(m) = \\dfrac{d}{dm}F_M(m)\\)"
  },
  {
    "objectID": "lessons/11_Joint_distributions/11_Joint_distributions_review.html#recall-finding-the-pdf-of-a-transformation",
    "href": "lessons/11_Joint_distributions/11_Joint_distributions_review.html#recall-finding-the-pdf-of-a-transformation",
    "title": "Lesson 11: Joint distributions",
    "section": "",
    "text": "Let \\(M\\) be a transformation of \\(X\\) and \\(Y\\): \\(M = g(X, Y)\\)\nWhen we have a transformation of \\(X\\) and \\(Y\\), \\(M\\), we need to follow the CDF method to find the pdf of \\(M\\)\n\nWe follow CDF method:\n\nStart with the joint pdf for \\(X\\) and \\(Y\\)\n\naka \\(f_{X,Y}(x, y)\\)\n\nTranslate the domain of \\(X\\) and \\(Y\\) to \\(M\\): find possible values of \\(M\\)\nFind the CDF of \\(M\\)\n\naka \\(F_M(m) = P(M \\leq m) = P(g(X, Y) \\leq m)\\)\n\nTake the derivative of the CDF of \\(M\\) with respect to \\(m\\) to find the pdf of \\(M\\)\n\naka \\(f_M(m) = \\dfrac{d}{dm}F_M(m)\\)"
  },
  {
    "objectID": "lessons/11_Joint_distributions/11_Joint_distributions_review.html#example-of-a-joint-pdf-with-a-transformation-13",
    "href": "lessons/11_Joint_distributions/11_Joint_distributions_review.html#example-of-a-joint-pdf-with-a-transformation-13",
    "title": "Lesson 11: Joint distributions",
    "section": "Example of a joint pdf with a transformation (1/3)",
    "text": "Example of a joint pdf with a transformation (1/3)\n\n\n\n\nExample 4.1\n\n\nLet \\(X\\) and \\(Y\\) have constant density on the square \\(0 \\leq X \\leq 4, 0 \\leq Y \\leq 4\\).\n\nFind \\(\\mathbb{P}(|X-Y| &lt; 2)\\)"
  },
  {
    "objectID": "lessons/11_Joint_distributions/11_Joint_distributions_review.html#example-of-a-joint-pdf-with-a-transformation-23",
    "href": "lessons/11_Joint_distributions/11_Joint_distributions_review.html#example-of-a-joint-pdf-with-a-transformation-23",
    "title": "Lesson 11: Joint distributions",
    "section": "Example of a joint pdf with a transformation (2/3)",
    "text": "Example of a joint pdf with a transformation (2/3)\n\n\nExample 4.2\n\n\nLet \\(X\\) and \\(Y\\) have constant density on the square \\(0 \\leq X \\leq 4, 0 \\leq Y \\leq 4\\).\n\nLet \\(M = \\max(X,Y)\\). Find the pdf for \\(M\\), that is \\(f_M(m)\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\nf_{X,Y}(x,y) = \\dfrac{1}{16} \\text{ for } 0 \\leq x \\leq 4, 0 \\leq y \\leq 4\n\\]\nPlot of \\(f_{X,Y}(x,y)\\):\n\n\n\n\n\n\nFor first problem, we want:\n\n\n\n\n\n\nNow I want to show the domain for the max, M:"
  },
  {
    "objectID": "lessons/11_Joint_distributions/11_Joint_distributions_review.html#example-of-a-joint-pdf-with-a-transformation-33",
    "href": "lessons/11_Joint_distributions/11_Joint_distributions_review.html#example-of-a-joint-pdf-with-a-transformation-33",
    "title": "Lesson 11: Joint distributions",
    "section": "Example of a joint pdf with a transformation (3/3)",
    "text": "Example of a joint pdf with a transformation (3/3)\n\n\n\n\nExample 4.3\n\n\nLet \\(X\\) and \\(Y\\) have constant density on the square \\(0 \\leq X \\leq 4, 0 \\leq Y \\leq 4\\).\n\nLet \\(Z = \\min(X,Y)\\). Find the pdf for \\(Z\\), that is \\(f_Z(z)\\)."
  },
  {
    "objectID": "lessons/11_Joint_distributions/11_Joint_distributions_review.html#last-example-for-home-more-complicated-transformation",
    "href": "lessons/11_Joint_distributions/11_Joint_distributions_review.html#last-example-for-home-more-complicated-transformation",
    "title": "Lesson 11: Joint distributions",
    "section": "Last example for home: more complicated transformation",
    "text": "Last example for home: more complicated transformation\n\n\n\n\nExample 5\n\n\nLet \\(X\\) and \\(Y\\) have joint density \\(f_{X,Y}(x,y)= \\frac85(x+y)\\) in the region \\(0 &lt; x &lt; 1,\\ \\frac12 &lt; y &lt;1\\). Find the pdf of the RV \\(Z\\), where \\(Z=XY\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can look at the joint pdf of X and Y:\n\n\n\n\n\n\nAnd let’s look at \\(P(Z \\leq 0.5)\\) aka \\(P(XY \\leq 0.5)\\) to get a better sense of the volume needed:\n\n\n\n\n\n\nNow I want to show the domain for Z:\n\n\n\n\n\n\nNow I want to show the domain for Z:"
  },
  {
    "objectID": "lessons/09_CDFs/09_CDFs.html",
    "href": "lessons/09_CDFs/09_CDFs.html",
    "title": "Lesson 9: Cumulative distribution functions (CDFs)",
    "section": "",
    "text": "Understand the definition of cumulative distribution functions (CDFs) for discrete and continuous random variables.\nCompute CDFs for given probability mass functions (pmfs).\nCompute CDFs for given probability density functions (pdfs) and pdfs from CDFs."
  },
  {
    "objectID": "lessons/09_CDFs/09_CDFs.html#what-is-a-cumulative-distribution-function",
    "href": "lessons/09_CDFs/09_CDFs.html#what-is-a-cumulative-distribution-function",
    "title": "Lesson 9: Cumulative distribution functions (CDFs)",
    "section": "What is a cumulative distribution function?",
    "text": "What is a cumulative distribution function?\n\n\n\n\nCumulative distribution function (CDF) for discrete random variable\n\n\nThe cumulative distribution function (cdf) of a discrete RV \\(X\\) with pmf \\(p_X(x)\\), is defined for every value \\(x\\) by \\[F_X(x) = \\mathbb{P}(X \\leq x) = \\sum \\limits_{\\{all\\ y:\\ y\\leq x\\}}p_X(y)\\]\n\n\n\n\\(F(x)\\) is increasing or flat (never decreasing)\n\\(\\min\\limits_x F(x) = 0\\)\n\\(\\max\\limits_xF(x)=1\\)\nCDF is a step function\n\n\n\n\nCumulative distribution function (CDF) for continuous random variable\n\n\nThe cumulative distribution function (cdf) of a continuous RV \\(X\\), is the function \\(F_X(x)\\), such that for all real values of \\(x\\), \\[F_X(x)= \\mathbb{P}(X \\leq x) = \\int_{-\\infty}^x f_X(s)ds\\]\n\n\nRemarks: In general, \\(F_X(x)\\) is increasing and\n\n\\(\\lim_{x\\rightarrow -\\infty} F_X(x)= 0\\)\n\\(\\lim_{x\\rightarrow \\infty} F_X(x)= 1\\)\n\\(P(X &gt; a) = 1 - P(X \\leq a) = 1 - F_X(a)\\)\n\\(P(a \\leq X \\leq b) = F_X(b) - F_X(a)\\)"
  },
  {
    "objectID": "lessons/09_CDFs/09_CDFs.html#how-to-define-cdfs-for-discrete-and-continuous-rvs",
    "href": "lessons/09_CDFs/09_CDFs.html#how-to-define-cdfs-for-discrete-and-continuous-rvs",
    "title": "Lesson 9: Cumulative distribution functions (CDFs)",
    "section": "How to define CDFs for discrete and continuous RVs?",
    "text": "How to define CDFs for discrete and continuous RVs?\n\n\nDiscrete RV \\(X\\):\n\n\n\n\n\n\n\n\n\n\npmf: \\(p_X(x) = P(X=x)\\)\nCDF: \\(F_X(x) = P(X \\leq x) = \\sum\\limits_{\\{all\\ y:\\ y\\leq x\\}} p_X(y)\\)\n\n\nContinuous RV \\(X\\):\n\n\n\n\n\n\n\n\n\n\ndensity: \\(f_X(x)\\)\nprobability: \\(P(a \\leq X \\leq b) = \\int_a^b f_X(x)dx\\)\nCDF: \\(F_X(x) = P(X \\leq x) = \\int_{-\\infty}^x f_X(s)ds\\)"
  },
  {
    "objectID": "lessons/09_CDFs/09_CDFs.html#falls-in-older-adults-revisited-15",
    "href": "lessons/09_CDFs/09_CDFs.html#falls-in-older-adults-revisited-15",
    "title": "Lesson 9: Cumulative distribution functions (CDFs)",
    "section": "Falls in Older Adults Revisited (1/5)",
    "text": "Falls in Older Adults Revisited (1/5)\n\n\n\n\nExample 1: Falls in Older Adults\n\n\nA major public health concern is falls among older adults (age 65+). National data suggests that 25% of older adults will experience at least one fall within a given year. A community health program is tracking a random group of \\(n = 8\\) older adults for one year. Assume the likelihood of falling is independent from person to person.\nLet \\(X\\) be the random variable representing the number of individuals in this group who experience at least one fall.\n\nWrite the CDF of \\(X\\) and make a table of values.\nUse R to calculate the cumulative probability for each possible value of \\(X\\).\nPlot the CDF of \\(X\\).\nSimulate \\(X\\) for 10000 groups and plot the approximated CDF.\n\n\n\n\n\nRecall our pmf: \\[P(X = x) = \\binom{8}{x} 0.25^x 0.75^{8-x},  x= 0, 1, 2, \\dots, 8 \\]"
  },
  {
    "objectID": "lessons/09_CDFs/09_CDFs.html#falls-in-older-adults-revisited-25",
    "href": "lessons/09_CDFs/09_CDFs.html#falls-in-older-adults-revisited-25",
    "title": "Lesson 9: Cumulative distribution functions (CDFs)",
    "section": "Falls in Older Adults Revisited (2/5)",
    "text": "Falls in Older Adults Revisited (2/5)\n\n\n\n\nExample 1: Falls in Older Adults\n\n\n\nWrite the CDF of \\(X\\).\n\n\n\n\n\nRecall our pmf: \\[P(X = x) = \\binom{8}{x} 0.25^x 0.75^{8-x},  x= 0, 1, 2, \\dots, 8 \\]\n\\[F_X(x) = P(X \\leq x) = \\sum \\limits_{k=0}^{x} \\binom{8}{k} 0.25^y 0.75^{8-k}\\]"
  },
  {
    "objectID": "lessons/09_CDFs/09_CDFs.html#falls-in-older-adults-revisited-35",
    "href": "lessons/09_CDFs/09_CDFs.html#falls-in-older-adults-revisited-35",
    "title": "Lesson 9: Cumulative distribution functions (CDFs)",
    "section": "Falls in Older Adults Revisited (3/5)",
    "text": "Falls in Older Adults Revisited (3/5)\n\n\n\n\nExample 1: Falls in Older Adults\n\n\n\nUse R to calculate the cumulative probability for each possible value of \\(X\\).\n\n\n\n\nn = 8\np = 0.25\n\nfalls_cdf &lt;- tibble(\n  x = 0:n,\n  c_prob = pbinom(x, size = n, prob = p)\n)\n\n\n \n \n\nfalls_cdf\n\n# A tibble: 9 × 2\n      x c_prob\n  &lt;int&gt;  &lt;dbl&gt;\n1     0  0.100\n2     1  0.367\n3     2  0.679\n4     3  0.886\n5     4  0.973\n6     5  0.996\n7     6  1.00 \n8     7  1.00 \n9     8  1"
  },
  {
    "objectID": "lessons/09_CDFs/09_CDFs.html#falls-in-older-adults-revisited-45",
    "href": "lessons/09_CDFs/09_CDFs.html#falls-in-older-adults-revisited-45",
    "title": "Lesson 9: Cumulative distribution functions (CDFs)",
    "section": "Falls in Older Adults Revisited (4/5)",
    "text": "Falls in Older Adults Revisited (4/5)\n\n\nExample 1: Falls in Older Adults\n\n\n\nPlot the CDF of \\(X\\).\n\n\n\n\nggplot(\n  falls_cdf,\n  aes(x = x, y = c_prob)\n       ) +\n  geom_step(\n    size = 1, \n    color = \"black\"\n    ) +\n  labs(\n    x = \"Number of Falls\",\n    y = \"Cumulative Probability\",\n    title = \"CDF of X\"\n    )"
  },
  {
    "objectID": "lessons/09_CDFs/09_CDFs.html#falls-in-older-adults-revisited-45-1",
    "href": "lessons/09_CDFs/09_CDFs.html#falls-in-older-adults-revisited-45-1",
    "title": "Lesson 9: Cumulative distribution functions (CDFs)",
    "section": "Falls in Older Adults Revisited (4/5)",
    "text": "Falls in Older Adults Revisited (4/5)\n\n\n\n\nExample 1: Falls in Older Adults\n\n\n\nSimulate \\(X\\) for 10000 groups and plot the approximated CDF.\n\n\n\n\nset.seed(4764)\nreps = 10000\n\nsims = rbinom(n = reps, \n              size = n, \n              prob = p)\n\nsims %&gt;% head(., 14)\n\n [1] 2 1 2 1 3 3 2 2 4 2 3 0 2 0\n\nfalls2 &lt;- tibble(x = 0:n) %&gt;%\n  rowwise() %&gt;%\n  mutate(c_prob = sum(sims &lt;= x) / reps)\n\n\n\nggplot(falls2, aes(x = x, y = c_prob)) +\n  geom_step(size = 1, color = \"black\") +\n  labs(\n    title = \"Approximate CDF of X\",\n    x = \"Number of adults (x)\",\n    y = \"Approximate Cumulative Probability\"\n  )"
  },
  {
    "objectID": "lessons/09_CDFs/09_CDFs.html#lets-demonstrate-the-cdf-with-an-example",
    "href": "lessons/09_CDFs/09_CDFs.html#lets-demonstrate-the-cdf-with-an-example",
    "title": "Lesson 9: Cumulative distribution functions (CDFs)",
    "section": "Let’s demonstrate the CDF with an example",
    "text": "Let’s demonstrate the CDF with an example\n\n\n\n\nExample 2\n\n\nLet \\(f_X(x)= 2\\), for \\(2.5 \\leq x \\leq 3\\). Find \\(F_X(x)\\)."
  },
  {
    "objectID": "lessons/09_CDFs/09_CDFs.html#derivatives-of-the-cdf",
    "href": "lessons/09_CDFs/09_CDFs.html#derivatives-of-the-cdf",
    "title": "Lesson 9: Cumulative distribution functions (CDFs)",
    "section": "Derivatives of the CDF",
    "text": "Derivatives of the CDF\n\n\nTheorem 1\n\n\nIf \\(X\\) is a continuous random variable with pdf \\(f_X(x)\\) and cdf \\(F_X(x)\\), then for all real values of \\(x\\) at which \\(F'_X(x)\\) exists, \\[\\frac{d}{dx} F_X(x)= F'_X(x) = f_X(x)\\]"
  },
  {
    "objectID": "lessons/09_CDFs/09_CDFs.html#finding-the-pdf-from-a-cdf",
    "href": "lessons/09_CDFs/09_CDFs.html#finding-the-pdf-from-a-cdf",
    "title": "Lesson 9: Cumulative distribution functions (CDFs)",
    "section": "Finding the PDF from a CDF",
    "text": "Finding the PDF from a CDF\n\n\n\n\nExample 3\n\n\nLet \\(X\\) be a RV with cdf \\[F_X(x)= \\left\\{\n        \\begin{array}{ll}\n            0 & \\quad x &lt; 2.5 \\\\\n            2x-5 & \\quad 2.5 \\leq x \\leq 3 \\\\\n            1 & \\quad x &gt; 3\n        \\end{array}\n    \\right.\\] Find the pdf \\(f_X(x)\\)."
  },
  {
    "objectID": "lessons/09_CDFs/09_CDFs.html#lets-go-through-another-example-17",
    "href": "lessons/09_CDFs/09_CDFs.html#lets-go-through-another-example-17",
    "title": "Lesson 9: Cumulative distribution functions (CDFs)",
    "section": "Let’s go through another example (1/7)",
    "text": "Let’s go through another example (1/7)\n\n\n\n\nExample 4\n\n\nLet \\(X\\) be a RV with pdf \\(f_X(x)= 2e^{-2x}\\), for \\(x&gt;0\\).\n\nShow \\(f_X(x)\\) is a pdf.\nFind \\(\\mathbb{P}(1 \\leq X \\leq 3)\\).\nFind \\(F_X(x)\\).\nGiven \\(F_X(x)\\), find \\(f_X(x)\\).\nFind \\(\\mathbb{P}(X \\geq 1 | X \\leq 3)\\).\nFind the median of the distribution of \\(X\\)."
  },
  {
    "objectID": "lessons/09_CDFs/09_CDFs.html#lets-go-through-another-example-27",
    "href": "lessons/09_CDFs/09_CDFs.html#lets-go-through-another-example-27",
    "title": "Lesson 9: Cumulative distribution functions (CDFs)",
    "section": "Let’s go through another example (2/7)",
    "text": "Let’s go through another example (2/7)\n\n\n\n\nExample 4.1\n\n\nLet \\(X\\) be a RV with pdf \\(f_X(x)= 2e^{-2x}\\), for \\(x&gt;0\\).\n\nShow \\(f_X(x)\\) is a pdf."
  },
  {
    "objectID": "lessons/09_CDFs/09_CDFs.html#lets-go-through-another-example-37",
    "href": "lessons/09_CDFs/09_CDFs.html#lets-go-through-another-example-37",
    "title": "Lesson 9: Cumulative distribution functions (CDFs)",
    "section": "Let’s go through another example (3/7)",
    "text": "Let’s go through another example (3/7)\n\n\n\n\n\n\nDo this problem at home for extra practice.\n\n\n\n\n\n\n\nExample 4.2\n\n\nLet \\(X\\) be a RV with pdf \\(f_X(x)= 2e^{-2x}\\), for \\(x&gt;0\\).\n\nFind \\(\\mathbb{P}(1 \\leq X \\leq 3)\\)."
  },
  {
    "objectID": "lessons/09_CDFs/09_CDFs.html#lets-go-through-another-example-47",
    "href": "lessons/09_CDFs/09_CDFs.html#lets-go-through-another-example-47",
    "title": "Lesson 9: Cumulative distribution functions (CDFs)",
    "section": "Let’s go through another example (4/7)",
    "text": "Let’s go through another example (4/7)\n\n\n\n\nExample 4.3\n\n\nLet \\(X\\) be a RV with pdf \\(f_X(x)= 2e^{-2x}\\), for \\(x&gt;0\\).\n\nFind \\(F_X(x)\\)."
  },
  {
    "objectID": "lessons/09_CDFs/09_CDFs.html#lets-go-through-another-example-57",
    "href": "lessons/09_CDFs/09_CDFs.html#lets-go-through-another-example-57",
    "title": "Lesson 9: Cumulative distribution functions (CDFs)",
    "section": "Let’s go through another example (5/7)",
    "text": "Let’s go through another example (5/7)\n\n\n\n\n\n\nDo this problem at home for extra practice.\n\n\n\n\n\n\n\nExample 4.4\n\n\nLet \\(X\\) be a RV with pdf \\(f_X(x)= 2e^{-2x}\\), for \\(x&gt;0\\).\n\nGiven \\(F_X(x)\\), find \\(f_X(x)\\)."
  },
  {
    "objectID": "lessons/09_CDFs/09_CDFs.html#lets-go-through-another-example-67",
    "href": "lessons/09_CDFs/09_CDFs.html#lets-go-through-another-example-67",
    "title": "Lesson 9: Cumulative distribution functions (CDFs)",
    "section": "Let’s go through another example (6/7)",
    "text": "Let’s go through another example (6/7)\n\n\n\n\nExample 4.5\n\n\nLet \\(X\\) be a RV with pdf \\(f_X(x)= 2e^{-2x}\\), for \\(x&gt;0\\).\n\nFind \\(\\mathbb{P}(X \\geq 1 | X \\leq 3)\\)."
  },
  {
    "objectID": "lessons/09_CDFs/09_CDFs.html#lets-go-through-another-example-77",
    "href": "lessons/09_CDFs/09_CDFs.html#lets-go-through-another-example-77",
    "title": "Lesson 9: Cumulative distribution functions (CDFs)",
    "section": "Let’s go through another example (7/7)",
    "text": "Let’s go through another example (7/7)\n\n\n\n\nExample 4.6\n\n\nLet \\(X\\) be a RV with pdf \\(f_X(x)= 2e^{-2x}\\), for \\(x&gt;0\\).\n\nFind the median of the distribution of \\(X\\)."
  },
  {
    "objectID": "lessons/17_Central_Limit_Theorem/17_Central_Limit_Theorem.html",
    "href": "lessons/17_Central_Limit_Theorem/17_Central_Limit_Theorem.html",
    "title": "Lesson 17: Central Limit Theorem",
    "section": "",
    "text": "Calculate probability of a sample mean using a population mean and variance with unknown distribution\nUse the Central Limit Theorem to construct the Normal approximation of the Binomial and Poisson distributions\n\n\n\n\n\nTheorem 1: Central Limit Theorem (CLT)\n\n\nLet \\(X_i\\) be iid rv’s with common mean \\(\\mu\\) and variance \\(\\sigma^2\\), for \\(i=1,2,\\ldots,n\\). Then \\[\\sum_{i=1}^n X_i \\rightarrow \\text{N}(n\\mu, n\\sigma^2)\\]\n\n\n\n\n\n\n\nCorollary 1\n\n\nLet \\(X_i\\) be iid rv’s with common mean \\(\\mu\\) and variance \\(\\sigma^2\\), for \\(i=1,2,\\ldots,n\\). Then \\[\\overline{X}=\\frac{\\sum_{i=1}^n X_i}{n}  \\rightarrow \\text{N}\\Bigg(\\mu, \\dfrac{\\sigma^2}{n}\\Bigg)\\]\n\n\n\n\n\n\n\n\n\nExample 1\n\n\nAccording to a large US study, the mean resting heart rate of adult women is about 74 beats per minutes (bpm), with standard deviation 13 bpm (NHANES 2003-2004).\n\nFind the probability that the average resting heart rate for a random sample of 36 adult women is more than 3 bpm away from the mean.\nRepeat the previous question for a single adult woman.\n\n\n\n\n\n\n\n\n\n\n\n\nExample 2\n\n\nLet \\(X_i \\sim Exp(\\lambda)\\) be iid RVs for \\(i=1,2,\\ldots,n\\). Then \\[\\sum_{i=1}^n X_i \\rightarrow\\]\n\n\n\n\n\n\n\n\n\n\nBinomial rv’s: Let \\(X \\sim Bin(n,p)\\)\n\n\\(X = \\displaystyle\\sum_{i=1}^n X_i\\), where \\(X_i\\) are iid \\(\\text{Bernoulli}(p)\\)\nRule of thumb: \\(np\\geq10\\) and \\(n(1-p)\\geq 10\\) to use Normal approximation\n\n\n\n\n\n\nPoisson rv’s: Let \\(X \\sim Poisson(\\lambda)\\)\n\n\\(X = \\displaystyle\\sum_{i=1}^n X_i\\), where \\(X_i\\) are iid \\(\\text{Poiss}(1)\\)\nRecall from Chapter 18 that if \\(X_i \\sim Poiss(\\lambda_i)\\) and \\(X_i\\) independent, then \\(\\sum_{i=1}^n X_i \\sim Poiss(\\sum_{i=1}^n \\lambda_i)\\)\nRule of thumb: \\(\\lambda \\geq10\\) to use Normal approximation\n\n\n\n\n\n\n\n\n\nExample 3\n\n\nSuppose that the probability of developing a specific type of breast cancer in women aged 40-49 is 0.001. Assume the occurrences of cancer are independent. Suppose you have data from a random sample of 20,000 women aged 40-49.\n\nHow many of the 20,000 women would you expect to develop this type of breast cancer, and what is the standard deviation?\nFind the exact probability that more than 15 of the 20,000 women will develop this type of breast cancer.\nUse the CLT to find the approximate probability that more than 15 of the 20,000 women will develop this type of breast cancer.\nUse the CLT to approximate the following probabilities, where \\(X\\) is the number of women that will develop this type of breast cancer.\n\n\\(\\mathbb{P}(15 \\leq X \\leq 22)\\)\n\\(\\mathbb{P}(X &gt; 20)\\)\n\\(\\mathbb{P}(X &lt; 20)\\)\n\nFind the approximate probability that more than 15 of the 20,000 women will develop this type of breast cancer - not using the CLT!\nUse the CLT to approximate the approximate probability in the previous question!\n\n\n\n\n\n\n\n\nExample 3\n\n\nSuppose that the probability of developing a specific type of breast cancer in women aged 40-49 is 0.001. Assume the occurrences of cancer are independent. Suppose you have data from a random sample of 20,000 women aged 40-49.\n\nHow many of the 20,000 women would you expect to develop this type of breast cancer, and what is the standard deviation?\n\n\n\n\n\n\n\n\nExample 3\n\n\nSuppose that the probability of developing a specific type of breast cancer in women aged 40-49 is 0.001. Assume the occurrences of cancer are independent. Suppose you have data from a random sample of 20,000 women aged 40-49.\n\nFind the exact probability that more than 15 of the 20,000 women will develop this type of breast cancer.\n\n\n\n\n\n\n\n\nExample 3\n\n\nSuppose that the probability of developing a specific type of breast cancer in women aged 40-49 is 0.001. Assume the occurrences of cancer are independent. Suppose you have data from a random sample of 20,000 women aged 40-49.\n\nUse the CLT to find the approximate probability that more than 15 of the 20,000 women will develop this type of breast cancer.\n\n\n\n\n\n\n\n\nExample 3\n\n\nSuppose that the probability of developing a specific type of breast cancer in women aged 40-49 is 0.001. Assume the occurrences of cancer are independent. Suppose you have data from a random sample of 20,000 women aged 40-49.\n\nUse the CLT to approximate the following probabilities, where \\(X\\) is the number of women that will develop this type of breast cancer.\n\n\\(\\mathbb{P}(15 \\leq X \\leq 22)\\)\n\\(\\mathbb{P}(X &gt; 20)\\)\n\\(\\mathbb{P}(X &lt; 20)\\)\n\n\n\n\n\n\n\n\n\nExample 3\n\n\nSuppose that the probability of developing a specific type of breast cancer in women aged 40-49 is 0.001. Assume the occurrences of cancer are independent. Suppose you have data from a random sample of 20,000 women aged 40-49.\n\nFind the approximate probability that more than 15 of the 20,000 women will develop this type of breast cancer - not using the CLT!\n\n\n\n\n\n\n\n\nExample 3\n\n\nSuppose that the probability of developing a specific type of breast cancer in women aged 40-49 is 0.001. Assume the occurrences of cancer are independent. Suppose you have data from a random sample of 20,000 women aged 40-49.\n\nUse the CLT to approximate the approximate probability in the previous question!"
  },
  {
    "objectID": "lessons/17_Central_Limit_Theorem/17_Central_Limit_Theorem.html#the-central-limit-theorem",
    "href": "lessons/17_Central_Limit_Theorem/17_Central_Limit_Theorem.html#the-central-limit-theorem",
    "title": "Lesson 17: Central Limit Theorem",
    "section": "",
    "text": "Theorem 1: Central Limit Theorem (CLT)\n\n\nLet \\(X_i\\) be iid rv’s with common mean \\(\\mu\\) and variance \\(\\sigma^2\\), for \\(i=1,2,\\ldots,n\\). Then \\[\\sum_{i=1}^n X_i \\rightarrow \\text{N}(n\\mu, n\\sigma^2)\\]"
  },
  {
    "objectID": "lessons/17_Central_Limit_Theorem/17_Central_Limit_Theorem.html#extension-of-the-clt",
    "href": "lessons/17_Central_Limit_Theorem/17_Central_Limit_Theorem.html#extension-of-the-clt",
    "title": "Lesson 17: Central Limit Theorem",
    "section": "",
    "text": "Corollary 1\n\n\nLet \\(X_i\\) be iid rv’s with common mean \\(\\mu\\) and variance \\(\\sigma^2\\), for \\(i=1,2,\\ldots,n\\). Then \\[\\overline{X}=\\frac{\\sum_{i=1}^n X_i}{n}  \\rightarrow \\text{N}\\Bigg(\\mu, \\dfrac{\\sigma^2}{n}\\Bigg)\\]"
  },
  {
    "objectID": "lessons/17_Central_Limit_Theorem/17_Central_Limit_Theorem.html#example-of-corollary-in-use",
    "href": "lessons/17_Central_Limit_Theorem/17_Central_Limit_Theorem.html#example-of-corollary-in-use",
    "title": "Lesson 17: Central Limit Theorem",
    "section": "",
    "text": "Example 1\n\n\nAccording to a large US study, the mean resting heart rate of adult women is about 74 beats per minutes (bpm), with standard deviation 13 bpm (NHANES 2003-2004).\n\nFind the probability that the average resting heart rate for a random sample of 36 adult women is more than 3 bpm away from the mean.\nRepeat the previous question for a single adult woman."
  },
  {
    "objectID": "lessons/17_Central_Limit_Theorem/17_Central_Limit_Theorem.html#example-of-clt-for-exponential-distribution",
    "href": "lessons/17_Central_Limit_Theorem/17_Central_Limit_Theorem.html#example-of-clt-for-exponential-distribution",
    "title": "Lesson 17: Central Limit Theorem",
    "section": "",
    "text": "Example 2\n\n\nLet \\(X_i \\sim Exp(\\lambda)\\) be iid RVs for \\(i=1,2,\\ldots,n\\). Then \\[\\sum_{i=1}^n X_i \\rightarrow\\]"
  },
  {
    "objectID": "lessons/17_Central_Limit_Theorem/17_Central_Limit_Theorem.html#clt-for-discrete-rvs",
    "href": "lessons/17_Central_Limit_Theorem/17_Central_Limit_Theorem.html#clt-for-discrete-rvs",
    "title": "Lesson 17: Central Limit Theorem",
    "section": "",
    "text": "Binomial rv’s: Let \\(X \\sim Bin(n,p)\\)\n\n\\(X = \\displaystyle\\sum_{i=1}^n X_i\\), where \\(X_i\\) are iid \\(\\text{Bernoulli}(p)\\)\nRule of thumb: \\(np\\geq10\\) and \\(n(1-p)\\geq 10\\) to use Normal approximation\n\n\n\n\n\n\nPoisson rv’s: Let \\(X \\sim Poisson(\\lambda)\\)\n\n\\(X = \\displaystyle\\sum_{i=1}^n X_i\\), where \\(X_i\\) are iid \\(\\text{Poiss}(1)\\)\nRecall from Chapter 18 that if \\(X_i \\sim Poiss(\\lambda_i)\\) and \\(X_i\\) independent, then \\(\\sum_{i=1}^n X_i \\sim Poiss(\\sum_{i=1}^n \\lambda_i)\\)\nRule of thumb: \\(\\lambda \\geq10\\) to use Normal approximation"
  },
  {
    "objectID": "lessons/17_Central_Limit_Theorem/17_Central_Limit_Theorem.html#larger-example-17",
    "href": "lessons/17_Central_Limit_Theorem/17_Central_Limit_Theorem.html#larger-example-17",
    "title": "Lesson 17: Central Limit Theorem",
    "section": "",
    "text": "Example 3\n\n\nSuppose that the probability of developing a specific type of breast cancer in women aged 40-49 is 0.001. Assume the occurrences of cancer are independent. Suppose you have data from a random sample of 20,000 women aged 40-49.\n\nHow many of the 20,000 women would you expect to develop this type of breast cancer, and what is the standard deviation?\nFind the exact probability that more than 15 of the 20,000 women will develop this type of breast cancer.\nUse the CLT to find the approximate probability that more than 15 of the 20,000 women will develop this type of breast cancer.\nUse the CLT to approximate the following probabilities, where \\(X\\) is the number of women that will develop this type of breast cancer.\n\n\\(\\mathbb{P}(15 \\leq X \\leq 22)\\)\n\\(\\mathbb{P}(X &gt; 20)\\)\n\\(\\mathbb{P}(X &lt; 20)\\)\n\nFind the approximate probability that more than 15 of the 20,000 women will develop this type of breast cancer - not using the CLT!\nUse the CLT to approximate the approximate probability in the previous question!"
  },
  {
    "objectID": "lessons/17_Central_Limit_Theorem/17_Central_Limit_Theorem.html#larger-example-27",
    "href": "lessons/17_Central_Limit_Theorem/17_Central_Limit_Theorem.html#larger-example-27",
    "title": "Lesson 17: Central Limit Theorem",
    "section": "",
    "text": "Example 3\n\n\nSuppose that the probability of developing a specific type of breast cancer in women aged 40-49 is 0.001. Assume the occurrences of cancer are independent. Suppose you have data from a random sample of 20,000 women aged 40-49.\n\nHow many of the 20,000 women would you expect to develop this type of breast cancer, and what is the standard deviation?"
  },
  {
    "objectID": "lessons/17_Central_Limit_Theorem/17_Central_Limit_Theorem.html#larger-example-37",
    "href": "lessons/17_Central_Limit_Theorem/17_Central_Limit_Theorem.html#larger-example-37",
    "title": "Lesson 17: Central Limit Theorem",
    "section": "",
    "text": "Example 3\n\n\nSuppose that the probability of developing a specific type of breast cancer in women aged 40-49 is 0.001. Assume the occurrences of cancer are independent. Suppose you have data from a random sample of 20,000 women aged 40-49.\n\nFind the exact probability that more than 15 of the 20,000 women will develop this type of breast cancer."
  },
  {
    "objectID": "lessons/17_Central_Limit_Theorem/17_Central_Limit_Theorem.html#larger-example-47",
    "href": "lessons/17_Central_Limit_Theorem/17_Central_Limit_Theorem.html#larger-example-47",
    "title": "Lesson 17: Central Limit Theorem",
    "section": "",
    "text": "Example 3\n\n\nSuppose that the probability of developing a specific type of breast cancer in women aged 40-49 is 0.001. Assume the occurrences of cancer are independent. Suppose you have data from a random sample of 20,000 women aged 40-49.\n\nUse the CLT to find the approximate probability that more than 15 of the 20,000 women will develop this type of breast cancer."
  },
  {
    "objectID": "lessons/17_Central_Limit_Theorem/17_Central_Limit_Theorem.html#larger-example-57",
    "href": "lessons/17_Central_Limit_Theorem/17_Central_Limit_Theorem.html#larger-example-57",
    "title": "Lesson 17: Central Limit Theorem",
    "section": "",
    "text": "Example 3\n\n\nSuppose that the probability of developing a specific type of breast cancer in women aged 40-49 is 0.001. Assume the occurrences of cancer are independent. Suppose you have data from a random sample of 20,000 women aged 40-49.\n\nUse the CLT to approximate the following probabilities, where \\(X\\) is the number of women that will develop this type of breast cancer.\n\n\\(\\mathbb{P}(15 \\leq X \\leq 22)\\)\n\\(\\mathbb{P}(X &gt; 20)\\)\n\\(\\mathbb{P}(X &lt; 20)\\)"
  },
  {
    "objectID": "lessons/17_Central_Limit_Theorem/17_Central_Limit_Theorem.html#larger-example-67",
    "href": "lessons/17_Central_Limit_Theorem/17_Central_Limit_Theorem.html#larger-example-67",
    "title": "Lesson 17: Central Limit Theorem",
    "section": "",
    "text": "Example 3\n\n\nSuppose that the probability of developing a specific type of breast cancer in women aged 40-49 is 0.001. Assume the occurrences of cancer are independent. Suppose you have data from a random sample of 20,000 women aged 40-49.\n\nFind the approximate probability that more than 15 of the 20,000 women will develop this type of breast cancer - not using the CLT!"
  },
  {
    "objectID": "lessons/17_Central_Limit_Theorem/17_Central_Limit_Theorem.html#larger-example-77",
    "href": "lessons/17_Central_Limit_Theorem/17_Central_Limit_Theorem.html#larger-example-77",
    "title": "Lesson 17: Central Limit Theorem",
    "section": "",
    "text": "Example 3\n\n\nSuppose that the probability of developing a specific type of breast cancer in women aged 40-49 is 0.001. Assume the occurrences of cancer are independent. Suppose you have data from a random sample of 20,000 women aged 40-49.\n\nUse the CLT to approximate the approximate probability in the previous question!"
  },
  {
    "objectID": "lessons/02_Simulations/02_Simulations.html",
    "href": "lessons/02_Simulations/02_Simulations.html",
    "title": "Lesson 2: Introduction to Simulations",
    "section": "",
    "text": "Describe random variables and distinguish between discrete and continuous types\nExplain the role of simulation in approximating probabilities and distributions\nUse R to run simulations of discrete random variables\nApply and interpret the four-step simulation process"
  },
  {
    "objectID": "lessons/02_Simulations/02_Simulations.html#recall-outcomes-events-sample-spaces",
    "href": "lessons/02_Simulations/02_Simulations.html#recall-outcomes-events-sample-spaces",
    "title": "Lesson 2: Introduction to Simulations",
    "section": "Recall: Outcomes, events, sample spaces",
    "text": "Recall: Outcomes, events, sample spaces\n\n\nDefinition: Outcome\n\n\nThe possible results in a random phenomenon.\n\n\n\n\nDefinition: Sample Space\n\n\nThe sample space \\(S\\) is the set of all outcomes\n\n\n\n\nDefinition: Event\n\n\nAn event is a collection of some outcomes. An event can include multiple outcomes or no outcomes (a subset of the sample space).\n\n\nWhen thinking about events, think about outcomes that you might be asking the probability of. For example, what is the probability that you get a heads or a tails in one flip? (Answer: 1)"
  },
  {
    "objectID": "lessons/02_Simulations/02_Simulations.html#we-need-to-understand-random-variables",
    "href": "lessons/02_Simulations/02_Simulations.html#we-need-to-understand-random-variables",
    "title": "Lesson 2: Introduction to Simulations",
    "section": "We need to understand random variables",
    "text": "We need to understand random variables\n\n\nDefinition: Random Variable\n\n\nFor a given sample space \\(S\\), a random variable (r.v.) is a function whose domain is \\(S\\) and whose range is the set of real numbers \\(\\mathbb{R}\\). A random variable assigns a real number to each outcome in the sample space.\n\n\n\nA random variable’s value is completely determined by the outcome \\(\\omega\\), where \\(\\omega \\in S\\)\n\nWhat is random is the outcome \\(\\omega\\)\n\nA random variable is a function from the sample space (with outcomes \\(\\omega\\)) to the set of real numbers\n\nWe typically write \\(X(\\omega)\\) (or \\(X\\) for short), where \\(X\\) is our random variable\n\nThus, we can take our sample space (all outcomes) and make functional transformations to it"
  },
  {
    "objectID": "lessons/02_Simulations/02_Simulations.html#the-cool-and-tricky-thing-about-random-variables",
    "href": "lessons/02_Simulations/02_Simulations.html#the-cool-and-tricky-thing-about-random-variables",
    "title": "Lesson 2: Introduction to Simulations",
    "section": "The cool (and tricky) thing about random variables",
    "text": "The cool (and tricky) thing about random variables\nDo you remember our coin example from Lesson 1? We tossed one or two coins.\n\nFor each coin, the sample space is heads and tails (\\(S = \\{H,T\\}\\))\nIf we want the sample space for both coins in order, then we have combinations (\\(S=\\{(H,H), (H,T), (T,H), (T,T)\\}\\))\n\n \nWe make the random variable a function of the sample space.\n\nFor one coin toss, we can say random variable \\(X\\) is \\(1\\) if we toss a heads (\\(\\omega = \\text{H}\\)) and \\(X=0\\) if we get a tails\nFor the two coins, we can say \\(X\\) is the count of heads, so if \\(\\omega = \\text{(H, T)}\\), then \\(X=1\\)"
  },
  {
    "objectID": "lessons/02_Simulations/02_Simulations.html#types-of-random-variables",
    "href": "lessons/02_Simulations/02_Simulations.html#types-of-random-variables",
    "title": "Lesson 2: Introduction to Simulations",
    "section": "Types of random variables",
    "text": "Types of random variables\nThere are two types of random variables:\n\n\n\n\n\nDiscrete random variables (RVs): the set of possible values is either finite or can be put into a countably infinite list\n\nYou could theoretically list the specific possible outcomes that the variable can take\nIf you sum the rolls of three dice, you must get a whole number. For example, you can’t get any number between 3 and 4.\n\n\n\n\n\n\n\n\nContinuous random variables (RVs): take on values from continuous intervals, or unions of continuous intervals\n\nVariable takes on a range of values, but there are infinitely possible values within the range\nIf you keep track of the time you sleep, you can sleep for 8 hours or 7.9 hours or 7.99 hours or 7.999 hours …\n\n\n\n\n\n\n\nDiscrete random variables (RVs) are a little easier to simulate right now\n\nWe will only do discrete RVs today"
  },
  {
    "objectID": "lessons/02_Simulations/02_Simulations.html#what-is-a-simulation",
    "href": "lessons/02_Simulations/02_Simulations.html#what-is-a-simulation",
    "title": "Lesson 2: Introduction to Simulations",
    "section": "What is a simulation?",
    "text": "What is a simulation?\nA probability model for a random phenomenon includes a sample space, events, random variables, and a probability measure.\n\n\nSimulation\n\n\nSimulation involves using a probability model to artificially recreate a random phenomenon, many times, usually using a computer.\n\n\nWe simulate outcomes and values of random variables according to the model’s assumptions."
  },
  {
    "objectID": "lessons/02_Simulations/02_Simulations.html#the-foundation-relative-frequencies",
    "href": "lessons/02_Simulations/02_Simulations.html#the-foundation-relative-frequencies",
    "title": "Lesson 2: Introduction to Simulations",
    "section": "The Foundation: Relative Frequencies",
    "text": "The Foundation: Relative Frequencies\n\nProbabilities can be interpreted as long-run relative frequencies\n\n \n\nBy simulating a random phenomenon a large number of times, we can approximate the probability of an event by calculating the relative frequency of its occurrence\n\nBasically, out of all the trials we run, how many times did the event happen?\n\n\n \n\nSimulation is a powerful tool to approximate a few things:\n\nProbabilities\nDistributions of random variables\nLong-run averages"
  },
  {
    "objectID": "lessons/02_Simulations/02_Simulations.html#we-saw-an-example-of-long-run-relative-frequency-in-our-coin-flip",
    "href": "lessons/02_Simulations/02_Simulations.html#we-saw-an-example-of-long-run-relative-frequency-in-our-coin-flip",
    "title": "Lesson 2: Introduction to Simulations",
    "section": "We saw an example of long-run relative frequency in our coin flip",
    "text": "We saw an example of long-run relative frequency in our coin flip\nIn Lesson 1, we flipped a coin 100 times and recorded the proportion of heads.\n\nWe tossed 50 heads out of the 100 flips\nOur long-run frequency was \\(50/100 = 0.5\\), which approximated the probability of getting a head on any one flip"
  },
  {
    "objectID": "lessons/02_Simulations/02_Simulations.html#tactile-simulations",
    "href": "lessons/02_Simulations/02_Simulations.html#tactile-simulations",
    "title": "Lesson 2: Introduction to Simulations",
    "section": "Tactile simulations",
    "text": "Tactile simulations\n\nWe’ve already seen coin flips!\nWe can also use cards, dice, and other objects to simulate discrete random variables\n\n \n\nOther common method: A box model uses a box/hat/bucket of “tickets” with labels to represent possible outcomes\n\nAllows us to increase the number of “tickets” with appropriate labels\nCoin flip as box model: A box with two tickets (H and T).\n90% free throw shooter: A box with 10 tickets (9 “make” and 1 “miss”).\nDraws can be with replacement (e.g., coin flips) or without replacement (e.g., dealing a poker hand)."
  },
  {
    "objectID": "lessons/02_Simulations/02_Simulations.html#example-to-build-our-simulation-skills",
    "href": "lessons/02_Simulations/02_Simulations.html#example-to-build-our-simulation-skills",
    "title": "Lesson 2: Introduction to Simulations",
    "section": "Example to build our simulation skills",
    "text": "Example to build our simulation skills\n\n\nExample: Simulating Two Rolls of a Fair Four-Sided Die\n\n\nWe’re going to roll two four-sided die. Let \\(X\\) be the sum of two rolls, and \\(Y\\) be the larger of the two rolls. How would we simulate \\(X\\) and \\(Y\\) separately?\n\n\n \n\nNote: this example is not asking for a probability!\n\nWe can simulate a random variable and looks at its distribution without calculating any probabilities.\n\n\n \n\nWe will focus on simulating \\(X\\) first\n\n \nLet’s build up some coding tools to do this!"
  },
  {
    "objectID": "lessons/02_Simulations/02_Simulations.html#how-do-we-simulate-something-like-a-single-dice-roll",
    "href": "lessons/02_Simulations/02_Simulations.html#how-do-we-simulate-something-like-a-single-dice-roll",
    "title": "Lesson 2: Introduction to Simulations",
    "section": "How do we simulate something like a single dice roll?",
    "text": "How do we simulate something like a single dice roll?\n\nWe can also use R to sample from the box or spinner\nThe sample() function is a powerful tool for simulating draws from a box model.\nFor example, we can simulate a coin flip\n\nWhat is x?\nWhat is size?\n\n\n\nsample(x = c(\"H\", \"T\"), size = 1)\n\n[1] \"T\"\n\n\n \n\nOr a dice roll\n\n\nsample(x = c(1, 2, 3, 4), size = 1)\n\n[1] 1"
  },
  {
    "objectID": "lessons/02_Simulations/02_Simulations.html#what-if-we-have-multiple-rolls-at-once",
    "href": "lessons/02_Simulations/02_Simulations.html#what-if-we-have-multiple-rolls-at-once",
    "title": "Lesson 2: Introduction to Simulations",
    "section": "What if we have multiple rolls at once?",
    "text": "What if we have multiple rolls at once?\n\nWe can set size to be larger than 1 to simulate multiple draws at once\n\n\nsample(x = c(\"H\", \"T\"), size = 5, replace = TRUE)\n\n[1] \"T\" \"T\" \"T\" \"H\" \"T\"\n\n\n\nWe can simulate our example of the two four-sided dice\n\n\nsample(x = c(1, 2, 3, 4), size = 2, replace = TRUE)\n\n[1] 3 3\n\n\n \n\nWhat happens if we set replace = FALSE?\n\n\nsample(x = c(1, 2, 3, 4), size = 2, replace = FALSE)\n\n[1] 4 2\n\nsample(x = c(1, 2, 3, 4), size = 4, replace = FALSE)\n\n[1] 2 1 3 4"
  },
  {
    "objectID": "lessons/02_Simulations/02_Simulations.html#can-we-start-to-simulate-many-rolls-of-two-dice",
    "href": "lessons/02_Simulations/02_Simulations.html#can-we-start-to-simulate-many-rolls-of-two-dice",
    "title": "Lesson 2: Introduction to Simulations",
    "section": "Can we start to simulate many rolls of two dice?",
    "text": "Can we start to simulate many rolls of two dice?\n\n\nExample: Simulating Two Rolls of a Fair Four-Sided Die\n\n\nWe’re going to roll two four-sided dice. Let \\(X\\) be the sum of two rolls, and \\(Y\\) be the larger of the two rolls. How would we simulate \\(X\\) and \\(Y\\) separately?\n\n\n\nWe’ve seen how to simulate a single pair of rolls\n\n\nrolls &lt;- sample(x = c(1, 2, 3, 4), size = 2, replace = TRUE)\n\n\nWe can use the replicate() function to repeat this process many times (we’ll do 10)\n\n\nreps &lt;- 10\nreplicate(reps, sample(x = 1:4, size = 2, replace = TRUE))\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n[1,]    2    3    3    3    3    3    3    1    3     3\n[2,]    1    1    2    2    2    1    4    1    4     2"
  },
  {
    "objectID": "lessons/02_Simulations/02_Simulations.html#we-need-more-reps-for-long-run-relative-frequencies",
    "href": "lessons/02_Simulations/02_Simulations.html#we-need-more-reps-for-long-run-relative-frequencies",
    "title": "Lesson 2: Introduction to Simulations",
    "section": "We need more reps for long-run relative frequencies",
    "text": "We need more reps for long-run relative frequencies\n\nreps &lt;- 10000\nsimulations &lt;- replicate(reps, sample(x = 1:4, size = 2, replace = TRUE))\n\n\nLet’s show the first 14 simulations\n\n\nsimulations[, 1:14]\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14]\n[1,]    4    1    2    2    4    2    1    2    4     1     4     2     2     3\n[2,]    4    4    1    3    1    1    2    3    2     4     1     2     2     2\n\n\n \n\n\\(X\\) is the sum of the two rolls: we could calculate that for each column\n\n\nX_simulated &lt;- apply(simulations, 2, sum)\nX_simulated[1:14]\n\n [1] 8 5 3 5 5 3 3 5 6 5 5 4 4 5"
  },
  {
    "objectID": "lessons/02_Simulations/02_Simulations.html#we-can-look-at-the-plot-of-random-variable-x",
    "href": "lessons/02_Simulations/02_Simulations.html#we-can-look-at-the-plot-of-random-variable-x",
    "title": "Lesson 2: Introduction to Simulations",
    "section": "We can look at the plot of random variable \\(X\\)",
    "text": "We can look at the plot of random variable \\(X\\)"
  },
  {
    "objectID": "lessons/02_Simulations/02_Simulations.html#if-we-want-to-calculate-something-else-we-can",
    "href": "lessons/02_Simulations/02_Simulations.html#if-we-want-to-calculate-something-else-we-can",
    "title": "Lesson 2: Introduction to Simulations",
    "section": "If we want to calculate something else, we can!",
    "text": "If we want to calculate something else, we can!\n\n\n\nAverage:\n\n\nmean(X_simulated)\n\n[1] 5.0044\n\n\n\nStandard deviation:\n\n\nsd(X_simulated)\n\n[1] 1.574303\n\n\n\nProbability that \\(X=5\\):\n\n\nsum(X_simulated == 5) / reps\n\n[1] 0.2468\n\n\n\nProbability that \\(X&lt;3\\):\n\n\nsum(X_simulated &lt; 3) / reps\n\n[1] 0.0584\n\n\n\n \n \n \n \n \n \n \n\n\nProbabilities (relative frequencies) are calculated by:\n\nsumming the number of times an event occurs and\ndividing by the total number of simulations (reps)"
  },
  {
    "objectID": "lessons/02_Simulations/02_Simulations.html#steps-of-a-simulation",
    "href": "lessons/02_Simulations/02_Simulations.html#steps-of-a-simulation",
    "title": "Lesson 2: Introduction to Simulations",
    "section": "4 (S)teps of a Simulation",
    "text": "4 (S)teps of a Simulation\n\n\n\nSet up\n\n\n\nDefine the probability space and related random variables and events, including assumptions.\n\n\n\n\n\nSimulate\n\n\n\nRun the simulation to generate outcomes according to the assumptions.\n\n\n\n\n\nSummarize\n\n\n\nAnalyze the output using plots and summary statistics like relative frequencies and averages.\n\n\n\n\n\nSensitivity analysis\n\n\n\nInvestigate how results change when assumptions or parameters of the model are altered."
  },
  {
    "objectID": "lessons/02_Simulations/02_Simulations.html#example-dice-rolls",
    "href": "lessons/02_Simulations/02_Simulations.html#example-dice-rolls",
    "title": "Lesson 2: Introduction to Simulations",
    "section": "Example: Dice Rolls",
    "text": "Example: Dice Rolls\n\n\nExample: Simulating Two Rolls of a Fair Four-Sided Die\n\n\nWe’re going to roll two four-sided die. Let \\(X\\) be the sum of two rolls, and \\(Y\\) be the larger of the two rolls. How would we simulate \\(X\\) and \\(Y\\) separately?\n\n\n\nUse the steps to run simulation for for \\(Y\\) now"
  },
  {
    "objectID": "lessons/02_Simulations/02_Simulations.html#section",
    "href": "lessons/02_Simulations/02_Simulations.html#section",
    "title": "Lesson 2: Introduction to Simulations",
    "section": "",
    "text": "Set up\n\n\n\nDefine the probability space and related random variables and events, including assumptions.\n\n\n\nRandom variable: \\(Y(\\omega)\\) is the larger of the two rolls in outcome \\(\\omega\\)\nGoal: simulate \\(Y\\), the larger of two rolls of a fair four-sided die\nSample space: all possible outcomes of rolling two four-sided die\n\nNot necessary, but helpful to define the sample space\n\n\n\\[\\begin{aligned} S =  \\{ &(1,1), (1,2), (1,3), (1,4), (2,1), (2,2), (2,3), (2,4),\\\\ & (3,1), (3,2), (3,3), (3,4), (4,1), (4,2), (4,3), (4,4)\\} \\end{aligned}\\]\n\nAssumptions: each die is fair and rolls are independent\n\nEach outcome in \\(S\\) is equally likely with probability \\(1/16\\)"
  },
  {
    "objectID": "lessons/02_Simulations/02_Simulations.html#section-1",
    "href": "lessons/02_Simulations/02_Simulations.html#section-1",
    "title": "Lesson 2: Introduction to Simulations",
    "section": "",
    "text": "Simulate\n\n\n\nRun the simulation to generate outcomes according to the assumptions.\n\n\n\nreps &lt;- 10000\nsimulations &lt;- replicate(reps, sample(x = 1:4, size = 2, replace = TRUE))\nY_simulated &lt;- apply(simulations, 2, max)\n\n \n\nWe can look at the first 30 simulations\n\n\nY_simulated[1:30]\n\n [1] 3 4 4 4 2 4 4 4 2 3 4 4 1 4 4 3 4 4 2 4 4 4 1 4 2 2 4 4 3 3"
  },
  {
    "objectID": "lessons/02_Simulations/02_Simulations.html#section-2",
    "href": "lessons/02_Simulations/02_Simulations.html#section-2",
    "title": "Lesson 2: Introduction to Simulations",
    "section": "",
    "text": "Summarize\n\n\n\nAnalyze the output using plots and summary statistics like relative frequencies and averages.\n\n\n\n\n\n\nShow/Hide Code for plotting Y\nY_df &lt;- as.data.frame(Y_simulated) %&gt;%\n  rename(Y = Y_simulated)\n\nggplot(Y_df, aes(x = Y)) +\n  geom_histogram(binwidth = 1, color = \"black\", fill = \"#B3C8BF\") +\n  scale_x_continuous(breaks = seq(1, 4, by = 1)) +\n  labs(title = \"Simulated Distribution of Y (Larger of Two Rolls)\",\n       x = \"Value of Y\",\n       y = \"Frequency\") +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(size = 20), \n    axis.text.y = element_text(size = 20),\n    axis.title.x = element_text(size = 20),\n    axis.title.y = element_text(size = 20), \n    plot.title = element_text(size = 20)\n    )\n\n\n\n\n\n\n\n\n\n\nIf the problem asked us for something else, we could compute it:\n\nAverage:\n\n\nmean(Y_simulated)\n\n[1] 3.1199\n\n\n\nProbability that \\(Y=1\\):\n\n\nsum(Y_simulated == 1) / reps\n\n[1] 0.0634\n\n\n\nProbability that \\(Y&gt;3\\):\n\n\nsum(Y_simulated &gt; 3) / reps\n\n[1] 0.4356"
  },
  {
    "objectID": "lessons/02_Simulations/02_Simulations.html#section-3",
    "href": "lessons/02_Simulations/02_Simulations.html#section-3",
    "title": "Lesson 2: Introduction to Simulations",
    "section": "",
    "text": "Sensitivity analysis\n\n\n\nInvestigate how results change when assumptions or parameters of the model are altered.\n\n\nWhat if we rolled three die instead of two?\n\n\n\nreps &lt;- 10000\ndie &lt;- 3\nsimulations &lt;- replicate(\n  reps, \n  sample(x = 1:4, \n         size = die, \n         replace = TRUE)\n)\nsimulations[, 1:6]\n\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,]    1    3    1    4    2    3\n[2,]    4    1    2    1    2    3\n[3,]    4    4    4    4    3    3\n\nY_simulated &lt;- apply(simulations, 2, max)\n\n\n\n\nShow/Hide Code for plotting Y\nY_df &lt;- as.data.frame(Y_simulated) %&gt;%\n  rename(Y = Y_simulated)\n\nggplot(Y_df, aes(x = Y)) +\n  geom_histogram(binwidth = 1, color = \"black\", fill = \"#B3C8BF\") +\n  scale_x_continuous(breaks = seq(1, 4, by = 1)) +\n  labs(title = \"Simulated Distribution of Y (Larger of Two Rolls)\",\n       x = \"Value of Y\",\n       y = \"Frequency\") +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(size = 20), \n    axis.text.y = element_text(size = 20),\n    axis.title.x = element_text(size = 20),\n    axis.title.y = element_text(size = 20), \n    plot.title = element_text(size = 20)\n    )"
  },
  {
    "objectID": "lessons/02_Simulations/02_Simulations_muddy_points.html",
    "href": "lessons/02_Simulations/02_Simulations_muddy_points.html",
    "title": "Muddy Points",
    "section": "",
    "text": "When we say “with replacement,” it means that after we select an item (like drawing a card from a deck), we put it back before selecting again. This means that the probability of selecting that item remains the same for each draw. For example, if you draw a card from a standard deck of 52 cards and then put it back, the probability of drawing any specific card (like an Ace) remains 1/52 for each draw.\nOn the other hand, “without replacement” means that once an item is selected, it is not put back. This means that the probability of selecting that item changes for subsequent draws because there are fewer items left to choose from. For example, if you draw a card from a deck and do not put it back, the probability of drawing an Ace on the next draw changes because there are now only 51 cards left in the deck.\nIt’s less about the importance of replacement, but that we accurately represent whatever scenario we want to simulate!"
  },
  {
    "objectID": "lessons/02_Simulations/02_Simulations_muddy_points.html#fall-2025",
    "href": "lessons/02_Simulations/02_Simulations_muddy_points.html#fall-2025",
    "title": "Muddy Points",
    "section": "",
    "text": "When we say “with replacement,” it means that after we select an item (like drawing a card from a deck), we put it back before selecting again. This means that the probability of selecting that item remains the same for each draw. For example, if you draw a card from a standard deck of 52 cards and then put it back, the probability of drawing any specific card (like an Ace) remains 1/52 for each draw.\nOn the other hand, “without replacement” means that once an item is selected, it is not put back. This means that the probability of selecting that item changes for subsequent draws because there are fewer items left to choose from. For example, if you draw a card from a deck and do not put it back, the probability of drawing an Ace on the next draw changes because there are now only 51 cards left in the deck.\nIt’s less about the importance of replacement, but that we accurately represent whatever scenario we want to simulate!"
  },
  {
    "objectID": "lessons/03_Lang_prob/03_Lang_prob_muddy_points.html",
    "href": "lessons/03_Lang_prob/03_Lang_prob_muddy_points.html",
    "title": "Muddy Points",
    "section": "",
    "text": "See below for questions on De Morgan’s laws and propositions from last year."
  },
  {
    "objectID": "lessons/03_Lang_prob/03_Lang_prob_muddy_points.html#fall-2025",
    "href": "lessons/03_Lang_prob/03_Lang_prob_muddy_points.html#fall-2025",
    "title": "Muddy Points",
    "section": "",
    "text": "See below for questions on De Morgan’s laws and propositions from last year."
  },
  {
    "objectID": "lessons/03_Lang_prob/03_Lang_prob_muddy_points.html#fall-2024",
    "href": "lessons/03_Lang_prob/03_Lang_prob_muddy_points.html#fall-2024",
    "title": "Muddy Points",
    "section": "Fall 2024",
    "text": "Fall 2024\n\n3. Confusion on De Morgan’s law and the high blood pressure example\nThis is in reference to the Lesson 3 notes on “BP example variation (3/3)” slide. I explained the event that at least one subject does not have high blood pressure using a venn diagram. In this venn diagram, I assumed \\(n=4\\), and I wanted to show that the union of complements is equal to the complement of unions: \\(\\bigcup\\limits_{i=1}^{n}H_i^C = \\Big(\\bigcap\\limits_{i=1}^{n}H_i\\Big)^C\\), which is De Morgan’s 2nd Law.\n\n Now we can look at \\(\\bigcup\\limits_{i=1}^{4}H_i^C\\). We first need to define \\(H_i^c\\)\n\n\n\n\n\n4. Proofs of propositions\nFurther explanations of the propositions can be found in the textbook from pages 24-27. For many of the explanations in class, I was working to produce a union of disjoint events, so that the probability could easily be calculated. Proposition 3 and 4 were specifically mentioned, so I will include some writing notes on them here:\n\nProposition 3\nIf \\(A \\subseteq B\\), then \\(\\mathbb{P}(A) \\leq \\mathbb{P}(B)\\)\nIn this proposition, I want to define event \\(B\\) as a union of disjoint events so that I can show \\(P(B)\\) is the sum of \\(P(A)\\) and some greater-than-or-equal-to 0 probability event. If the following is my venn diagram of A and B:\n\n\n\n\n\nThen I can define B as the union of disjoint events: \nIf we then take the probability of each side of the equation \\(B = A \\cup (A^c \\cap B)\\), we get \\[P(B) = P\\big(A \\cup (A^c \\cap B)\\big)\\]\nSince events \\(A\\) and \\(A^c \\cap B\\) are disjoint, the probability of their union is just: \\[P(A) + P(A^c \\cap B)\\]\nThus, our equation is now \\[P(B) = P(A) + P(A^c \\cap B)\\]\nFrom Axiom 1, we know for event \\(A^c \\cap B\\), \\(P(A^c \\cap B) \\geq 0\\).\nSo the probability of event B is the sum of the probability of event A and an event that is \\(\\geq\\) 0. This means \\(P(B) \\geq P(A)\\).\n\n\nProposition 4\n\\(\\mathbb{P}(A \\cup B) = \\mathbb{P}(A) + \\mathbb{P}(B) - \\mathbb{P}(A \\cap B)\\)\n\n\nFrom the pictures above, we can see some similar disjoint events.\nIf we look back at \\(A \\cup B\\), we can start manipulating the right side of the equation:\n\n \n\n\n\n5. Example at end of Chapter 2 slides (Venn Diagram)\nI will post this in the previous week’s Muddy Points as well. Please follow this link for my work through of the example. And here is the PDF with my work.\nSub-question: why don’t we just multiply the probability of A and B to get the intersection? This is a specific property of probability when A and B are independent. Only when A and B are independent can we conclude that \\(P(A \\cap B) = P(A)P(B)\\).\n\n\n6. Partition of events\nWe’ve been working with event partitions throughout Chapter 2, but we have not formally identified them. Partitions are advantageous to define for two reasons:\n\nThe partitions may be easier to calculate. We can then use the partitions to reconstruct other probabilities that may be more difficult to calculate\nPartitions have nice properties as a consequence of being disjoint. For example, the probability of the union of partitions is the sum of the probabilities across each partition: \\[P\\bigg(\\bigcup_{i=1}^n A_i\\bigg) = P(A_1)P(A_2)P(A_3) \\cdot \\cdot \\cdot P(A_n)\\]"
  },
  {
    "objectID": "lessons/08_pdfs/24_01_Continuous_rv_key_info.html",
    "href": "lessons/08_pdfs/24_01_Continuous_rv_key_info.html",
    "title": "Key Info",
    "section": "",
    "text": "Graded: HW 4 assignment and HW 3 solutions\nCalendly is up!!"
  },
  {
    "objectID": "lessons/08_pdfs/24_01_Continuous_rv_key_info.html#announcements",
    "href": "lessons/08_pdfs/24_01_Continuous_rv_key_info.html#announcements",
    "title": "Key Info",
    "section": "",
    "text": "Graded: HW 4 assignment and HW 3 solutions\nCalendly is up!!"
  },
  {
    "objectID": "lessons/08_pdfs/24_01_Continuous_rv_key_info.html#key-dates",
    "href": "lessons/08_pdfs/24_01_Continuous_rv_key_info.html#key-dates",
    "title": "Key Info",
    "section": "Key Dates",
    "text": "Key Dates\n\nThursday: HW 5 due\nSunday: HW 4 solutions due\nWeek of 11/11\n\nMake a meeting with me\n11/12: recordings due if you are making that\nNow on Sakai"
  },
  {
    "objectID": "lessons/08_pdfs/08_pdfs.html",
    "href": "lessons/08_pdfs/08_pdfs.html",
    "title": "Lesson 8: Probability density functions (PDFs)",
    "section": "",
    "text": "Distinguish between discrete and continuous random variables.\nCalculate probabilities for continuous random variables.\nUse R to simulate known continuous distributions."
  },
  {
    "objectID": "lessons/08_pdfs/08_pdfs.html#discrete-vs.-continuous-rvs",
    "href": "lessons/08_pdfs/08_pdfs.html#discrete-vs.-continuous-rvs",
    "title": "Lesson 8: Probability density functions (PDFs)",
    "section": "Discrete vs. Continuous RVs",
    "text": "Discrete vs. Continuous RVs\n\n\n\nFor a discrete RV, the set of possible values is either finite or can be put into a countably infinite list.\n \nContinuous RVs take on values from continuous intervals, or unions of continuous intervals\n\n\n\n\n\nFigure from Introduction to Probability TB (pg. 301)"
  },
  {
    "objectID": "lessons/08_pdfs/08_pdfs.html#how-to-define-probabilities-for-continuous-rvs",
    "href": "lessons/08_pdfs/08_pdfs.html#how-to-define-probabilities-for-continuous-rvs",
    "title": "Lesson 8: Probability density functions (PDFs)",
    "section": "How to define probabilities for continuous RVs?",
    "text": "How to define probabilities for continuous RVs?\n\n\nDiscrete RV \\(X\\):\n\n\n\n\n\n\n\n\n\n\npmf: \\(p_X(x) = P(X=x)\\)\n\n\nContinuous RV \\(X\\):\n\n\n\n\n\n\n\n\n\n\ndensity: \\(f_X(x)\\)\nprobability: \\(P(a \\leq X \\leq b) = \\int_a^b f_X(x)dx\\)"
  },
  {
    "objectID": "lessons/08_pdfs/08_pdfs.html#what-is-a-probability-density-function",
    "href": "lessons/08_pdfs/08_pdfs.html#what-is-a-probability-density-function",
    "title": "Lesson 8: Probability density functions (PDFs)",
    "section": "What is a probability density function?",
    "text": "What is a probability density function?\n\n\nProbability density function\n\n\nThe probability distribution, or probability density function (pdf), of a continuous random variable \\(X\\) is a function \\(f_X(x)\\), such that for all real values \\(a,b\\) with \\(a \\leq b\\),\n\\[\\mathbb{P}(a \\leq X \\leq b) = \\int_a^b f_X(x)dx\\]\n\n\nRemarks:\n\nNote that \\(f_X(x) \\neq \\mathbb{P}(X=x)\\)!!!\nIn order for \\(f_X(x)\\) to be a pdf, it needs to satisfy the properties\n\n\\(f_X(x) \\geq 0\\) for all \\(x\\)\n\\(\\int_{-\\infty}^{\\infty} f_X(x)dx=1\\)"
  },
  {
    "objectID": "lessons/08_pdfs/08_pdfs.html#lets-demonstrate-the-pdf-with-an-example-15",
    "href": "lessons/08_pdfs/08_pdfs.html#lets-demonstrate-the-pdf-with-an-example-15",
    "title": "Lesson 8: Probability density functions (PDFs)",
    "section": "Let’s demonstrate the PDF with an example (1/5)",
    "text": "Let’s demonstrate the PDF with an example (1/5)\n\n\n\n\nExample 1.1\n\n\nLet \\(f_X(x)= 2\\), for \\(a \\leq x \\leq 3\\).\n\nFind the value of \\(a\\) so that \\(f_X(x)\\) is a pdf."
  },
  {
    "objectID": "lessons/08_pdfs/08_pdfs.html#lets-demonstrate-the-pdf-with-an-example-25",
    "href": "lessons/08_pdfs/08_pdfs.html#lets-demonstrate-the-pdf-with-an-example-25",
    "title": "Lesson 8: Probability density functions (PDFs)",
    "section": "Let’s demonstrate the PDF with an example (2/5)",
    "text": "Let’s demonstrate the PDF with an example (2/5)\n\n\n\n\nExample 1.2\n\n\nLet \\(f_X(x)= 2\\), for \\(a \\leq x \\leq 3\\).\n\nFind \\(\\mathbb{P}(2.7 \\leq X \\leq 2.9)\\)."
  },
  {
    "objectID": "lessons/08_pdfs/08_pdfs.html#lets-demonstrate-the-pdf-with-an-example-35",
    "href": "lessons/08_pdfs/08_pdfs.html#lets-demonstrate-the-pdf-with-an-example-35",
    "title": "Lesson 8: Probability density functions (PDFs)",
    "section": "Let’s demonstrate the PDF with an example (3/5)",
    "text": "Let’s demonstrate the PDF with an example (3/5)\n\n\n\n\nExample 1.3\n\n\nLet \\(f_X(x)= 2\\), for \\(a \\leq x \\leq 3\\).\n\nFind \\(\\mathbb{P}(2.7 &lt; X \\leq 2.9)\\)."
  },
  {
    "objectID": "lessons/08_pdfs/08_pdfs.html#lets-demonstrate-the-pdf-with-an-example-45",
    "href": "lessons/08_pdfs/08_pdfs.html#lets-demonstrate-the-pdf-with-an-example-45",
    "title": "Lesson 8: Probability density functions (PDFs)",
    "section": "Let’s demonstrate the PDF with an example (4/5)",
    "text": "Let’s demonstrate the PDF with an example (4/5)\n\n\n\n\nExample 1.4\n\n\nLet \\(f_X(x)= 2\\), for \\(a \\leq x \\leq 3\\).\n\nFind \\(\\mathbb{P}(X = 2.9)\\)."
  },
  {
    "objectID": "lessons/08_pdfs/08_pdfs.html#lets-demonstrate-the-pdf-with-an-example-55",
    "href": "lessons/08_pdfs/08_pdfs.html#lets-demonstrate-the-pdf-with-an-example-55",
    "title": "Lesson 8: Probability density functions (PDFs)",
    "section": "Let’s demonstrate the PDF with an example (5/5)",
    "text": "Let’s demonstrate the PDF with an example (5/5)\n\n\n\n\nExample 1.5\n\n\nLet \\(f_X(x)= 2\\), for \\(a \\leq x \\leq 3\\).\n\nFind \\(\\mathbb{P}(X \\leq 2.8)\\)."
  },
  {
    "objectID": "lessons/08_pdfs/08_pdfs.html#use-r-to-simulate-known-distributions",
    "href": "lessons/08_pdfs/08_pdfs.html#use-r-to-simulate-known-distributions",
    "title": "Lesson 8: Probability density functions (PDFs)",
    "section": "Use R to simulate known distributions",
    "text": "Use R to simulate known distributions\n\nWe can use R to simulate continuous random variables and visualize their distributions\nFor example, we can simulate a uniform distribution between 2.5 and 3\n\n\nuniform = tibble(\n  x = runif(n=10000, min=2.5, max=3)\n)\n\nggplot(uniform, \n       aes(x = x, \n           y = after_stat(density))) +\n  geom_histogram( binwidth = 0.001) + \n  geom_abline(intercept = 2, slope = 0) + \n  labs(\n    title = \"Probability density function (pdf) of X\",\n    x = \"x\",\n    y = \"pdf\"\n  )"
  },
  {
    "objectID": "lessons/08_pdfs/08_pdfs.html#use-r-to-simulate-any-continuous-distribution",
    "href": "lessons/08_pdfs/08_pdfs.html#use-r-to-simulate-any-continuous-distribution",
    "title": "Lesson 8: Probability density functions (PDFs)",
    "section": "Use R to simulate any continuous distribution",
    "text": "Use R to simulate any continuous distribution\n\nWe will discuss other ways to simulate continuous distributions once we cover cumulative distribution functions (CDFs) and inverse CDFs"
  },
  {
    "objectID": "lessons/18_Moment_Generating_Functions/18_Moment_Generating_Functions.html",
    "href": "lessons/18_Moment_Generating_Functions/18_Moment_Generating_Functions.html",
    "title": "Lesson 18: Moment Generating Functions",
    "section": "",
    "text": "Learn the definition of a moment-generating function.\nFind the moment-generating function of a random variable.\nUse a moment-generating function to find the mean and variance of a random variable."
  },
  {
    "objectID": "lessons/18_Moment_Generating_Functions/18_Moment_Generating_Functions.html#learning-objectives",
    "href": "lessons/18_Moment_Generating_Functions/18_Moment_Generating_Functions.html#learning-objectives",
    "title": "Lesson 18: Moment Generating Functions",
    "section": "",
    "text": "Learn the definition of a moment-generating function.\nFind the moment-generating function of a random variable.\nUse a moment-generating function to find the mean and variance of a random variable."
  },
  {
    "objectID": "lessons/18_Moment_Generating_Functions/18_Moment_Generating_Functions.html#where-are-we",
    "href": "lessons/18_Moment_Generating_Functions/18_Moment_Generating_Functions.html#where-are-we",
    "title": "Lesson 18: Moment Generating Functions",
    "section": "Where are we?",
    "text": "Where are we?"
  },
  {
    "objectID": "lessons/18_Moment_Generating_Functions/18_Moment_Generating_Functions.html#what-are-moments",
    "href": "lessons/18_Moment_Generating_Functions/18_Moment_Generating_Functions.html#what-are-moments",
    "title": "Lesson 18: Moment Generating Functions",
    "section": "What are moments?",
    "text": "What are moments?\n\n\nDefinition 1\n\n\nThe \\(j^{th}\\) moment of a r.v. \\(X\\) is \\(\\mathbb{E}[X^j]\\)"
  },
  {
    "objectID": "lessons/18_Moment_Generating_Functions/18_Moment_Generating_Functions.html#okay-but-what-are-they",
    "href": "lessons/18_Moment_Generating_Functions/18_Moment_Generating_Functions.html#okay-but-what-are-they",
    "title": "Lesson 18: Moment Generating Functions",
    "section": "Okay, but what are they?",
    "text": "Okay, but what are they?\n\n\nExample 1\n\n\n\\(1^{st}-4^{th}\\) moments\n\n\n\n1st moment:\n\n \n\n2nd moment:\n\n \n\n3rd moment:\n\n \n\n4th moment:"
  },
  {
    "objectID": "lessons/18_Moment_Generating_Functions/18_Moment_Generating_Functions.html#what-is-a-moment-generating-function-mgf",
    "href": "lessons/18_Moment_Generating_Functions/18_Moment_Generating_Functions.html#what-is-a-moment-generating-function-mgf",
    "title": "Lesson 18: Moment Generating Functions",
    "section": "What is a moment generating function (MGF)??",
    "text": "What is a moment generating function (MGF)??\n\n\nDefinition 3\n\n\nIf \\(X\\) is a r.v., then the moment generating function (MGF) associated with \\(X\\) is: \\[M_X(t)= \\mathbb{E}[e^{tX}]\\]\n\n\nRemarks\n\n\n\nFor a discrete r.v., the MGF of \\(X\\) is \\[M_X(t)= \\mathbb{E}[e^{tX}]=\\sum_{all \\ x}e^{tx}p_X(x)\\]\nFor a continuous r.v., the MGF of \\(X\\) is \\[M_X(t)= \\mathbb{E}[e^{tX}]=\\int_{-\\infty}^{\\infty}e^{tx}f_X(x)dx\\]\n\n\n\nThe MGF \\(M_X(t)\\) is a function of \\(t\\), not of \\(X\\), and it might not be defined (i.e. finite) for all values of \\(t\\). We just need it to be defined for \\(t=0\\)."
  },
  {
    "objectID": "lessons/18_Moment_Generating_Functions/18_Moment_Generating_Functions.html#example",
    "href": "lessons/18_Moment_Generating_Functions/18_Moment_Generating_Functions.html#example",
    "title": "Lesson 18: Moment Generating Functions",
    "section": "Example",
    "text": "Example\n\n\n\n\nExample 4\n\n\nWhat is \\(M_X(t)\\) for \\(t=0\\)?"
  },
  {
    "objectID": "lessons/18_Moment_Generating_Functions/18_Moment_Generating_Functions.html#how-do-mgfs-give-us-moments",
    "href": "lessons/18_Moment_Generating_Functions/18_Moment_Generating_Functions.html#how-do-mgfs-give-us-moments",
    "title": "Lesson 18: Moment Generating Functions",
    "section": "How do MGFs give us moments?",
    "text": "How do MGFs give us moments?\n\n\nTheorem 5\n\n\nThe moment generating function uniquely specifies a probability distribution. AKA all moments can be found from the MGF through its derivatives at \\(t=0\\).\n\n\n\n\nTheorem 6\n\n\n\\[\\mathbb{E}[X^r] = M_X^{(r)}(0)\\]\n\\((r)\\) in this equation is the \\(r\\)th derivative with respect to \\(t\\). We calculate the derivative at \\(t=0\\)\n\n\n\nWhen \\(r=1\\), we are taking the first derivative\nWhen \\(r=4\\), we are taking the fourth derivative"
  },
  {
    "objectID": "lessons/18_Moment_Generating_Functions/18_Moment_Generating_Functions.html#using-the-mgf-to-uniquely-describe-a-probability-distribution",
    "href": "lessons/18_Moment_Generating_Functions/18_Moment_Generating_Functions.html#using-the-mgf-to-uniquely-describe-a-probability-distribution",
    "title": "Lesson 18: Moment Generating Functions",
    "section": "Using the MGF to uniquely describe a probability distribution",
    "text": "Using the MGF to uniquely describe a probability distribution\n\n\n\n\nExample 7\n\n\nLet \\(X \\sim Poisson(\\lambda)\\)\n\nFind the MGF of \\(X\\)\nFind \\(\\mathbb{E}[X]\\)\nFind \\(Var(X)\\)"
  },
  {
    "objectID": "lessons/18_Moment_Generating_Functions/18_Moment_Generating_Functions.html#theorem",
    "href": "lessons/18_Moment_Generating_Functions/18_Moment_Generating_Functions.html#theorem",
    "title": "Lesson 18: Moment Generating Functions",
    "section": "Theorem",
    "text": "Theorem\nRemark: Finding the mean and variance is sometimes easier with the following trick\n\n\nTheorem 8\n\n\nLet \\(R_X(t) = \\ln[M_X(t)]\\). Then,\n\\[\\mu = \\mathbb{E}[X] = R_X'(0) \\text{, and}\\] \\[\\sigma^2 = Var(X) = R_X''(0)\\]\n\n\n\nProof."
  },
  {
    "objectID": "lessons/18_Moment_Generating_Functions/18_Moment_Generating_Functions.html#using-r_xt-to-uniquely-describe-a-probability-distribution",
    "href": "lessons/18_Moment_Generating_Functions/18_Moment_Generating_Functions.html#using-r_xt-to-uniquely-describe-a-probability-distribution",
    "title": "Lesson 18: Moment Generating Functions",
    "section": "Using \\(R_X(t)\\) to uniquely describe a probability distribution",
    "text": "Using \\(R_X(t)\\) to uniquely describe a probability distribution\n\n\n\n\nExample 9\n\n\nLet \\(X \\sim Poisson(\\lambda)\\).\n\nFind \\(\\mathbb{E}[X]\\) using \\(R_X(t)\\)\nFind \\(Var(X)\\) using \\(R_X(t)\\)"
  },
  {
    "objectID": "lessons/18_Moment_Generating_Functions/18_Moment_Generating_Functions.html#using-the-mgf-to-uniquely-describe-the-standard-normal-distribution",
    "href": "lessons/18_Moment_Generating_Functions/18_Moment_Generating_Functions.html#using-the-mgf-to-uniquely-describe-the-standard-normal-distribution",
    "title": "Lesson 18: Moment Generating Functions",
    "section": "Using the MGF to uniquely describe the standard normal distribution",
    "text": "Using the MGF to uniquely describe the standard normal distribution\n\n\n\n\nExample 10\n\n\nLet \\(Z\\) be a standard normal random variable, i.e. \\(Z \\sim N(0,1)\\).\n\nFind the MGF of \\(Z\\)\nFind \\(\\mathbb{E}[Z]\\)\nFind \\(Var(Z)\\)"
  },
  {
    "objectID": "lessons/18_Moment_Generating_Functions/18_Moment_Generating_Functions.html#using-the-mgf-to-uniquely-describe-the-standard-normal-distribution-1",
    "href": "lessons/18_Moment_Generating_Functions/18_Moment_Generating_Functions.html#using-the-mgf-to-uniquely-describe-the-standard-normal-distribution-1",
    "title": "Lesson 18: Moment Generating Functions",
    "section": "Using the MGF to uniquely describe the standard normal distribution",
    "text": "Using the MGF to uniquely describe the standard normal distribution"
  },
  {
    "objectID": "lessons/18_Moment_Generating_Functions/18_Moment_Generating_Functions.html#mgfs-of-sums-of-independent-rvs",
    "href": "lessons/18_Moment_Generating_Functions/18_Moment_Generating_Functions.html#mgfs-of-sums-of-independent-rvs",
    "title": "Lesson 18: Moment Generating Functions",
    "section": "MGFs of sums of independent RV’s",
    "text": "MGFs of sums of independent RV’s\n\n\nTheorem 9\n\n\nIf \\(X\\) and \\(Y\\) are independent RV’s with respective MGFs \\(M_X(t)\\) and \\(M_Y(t)\\), then\n\\[M_{X+Y}(t) = E[e^{t(X+Y)}] = E[e^{tX} e^{tY}] = E[e^{tX}]E[e^{tY}]=M_{X}(t)M_{Y}(t)\\]"
  },
  {
    "objectID": "lessons/18_Moment_Generating_Functions/18_Moment_Generating_Functions.html#main-takeaways",
    "href": "lessons/18_Moment_Generating_Functions/18_Moment_Generating_Functions.html#main-takeaways",
    "title": "Lesson 18: Moment Generating Functions",
    "section": "Main takeaways",
    "text": "Main takeaways\n\nMGFs are a purely mathematically definition\n\nWe can’t really relate it to our real world analysis\n\nThey are helpful mathematically because they are unique to a probability distribution\n\nWe can find the unique MGF from for a probability distribution\nAnd we can find a distribution from an MGF\n\nMGFs can sometimes make it easier to find the mean and variance of an RV\nMGFs are most helpful when we are finding a joint distribution that is a sum or transformation of two RV’s\n\nMake the calculation easier!\n\nMGFs are often used to prove certain distribution are sums of other ones!"
  },
  {
    "objectID": "lessons/18_Moment_Generating_Functions/18_Moment_Generating_Functions.html#more-resources",
    "href": "lessons/18_Moment_Generating_Functions/18_Moment_Generating_Functions.html#more-resources",
    "title": "Lesson 18: Moment Generating Functions",
    "section": "More resources",
    "text": "More resources\n\nhttps://online.stat.psu.edu/stat414/book/export/html/676\nhttps://www.youtube.com/watch/ez_vq23xWrQ\nhttps://www.youtube.com/watch/2p9J9ChTeFI\nhttps://www.youtube.com/watch/A5bWU8xcQkE\nhttps://www.youtube.com/watch/QeUrTGFTFm4\nhttps://www.youtube.com/watch/HhrkwyyRtgI"
  },
  {
    "objectID": "lessons/16_Cont_RVs/16_Cont_RVs_key_info.html",
    "href": "lessons/16_Cont_RVs/16_Cont_RVs_key_info.html",
    "title": "Key Info",
    "section": "",
    "text": "We will finish up the example from last class\nNo class on 11/26 and 12/1\nBiostatistics tutoring starts today\n\nSchedule a meeting here\nOr email Charles directly\n\nHomework 6\n\nLooks pretty good!\nPlease attempt all problems! This is how you get practice."
  },
  {
    "objectID": "lessons/16_Cont_RVs/16_Cont_RVs_key_info.html#announcements",
    "href": "lessons/16_Cont_RVs/16_Cont_RVs_key_info.html#announcements",
    "title": "Key Info",
    "section": "",
    "text": "We will finish up the example from last class\nNo class on 11/26 and 12/1\nBiostatistics tutoring starts today\n\nSchedule a meeting here\nOr email Charles directly\n\nHomework 6\n\nLooks pretty good!\nPlease attempt all problems! This is how you get practice."
  },
  {
    "objectID": "lessons/16_Cont_RVs/16_Cont_RVs_key_info.html#key-dates",
    "href": "lessons/16_Cont_RVs/16_Cont_RVs_key_info.html#key-dates",
    "title": "Key Info",
    "section": "Key Dates",
    "text": "Key Dates\n\nHW 7 due Sunday 11/23\nQuiz 3 opens 12/3\nHW 8 due 12/7"
  },
  {
    "objectID": "lessons/16_Cont_RVs/16_Cont_RVs.html",
    "href": "lessons/16_Cont_RVs/16_Cont_RVs.html",
    "title": "Lesson 16: Some Important Continuous RVs",
    "section": "",
    "text": "Distinguish between Uniform, Exponential, Gamma, and Normal distributions when reading a word problem.\nIdentify the variable and the parameters in a word problem, and state what the variable and parameters mean.\nUse the formulas for the pdf/CDF, expected value, and variance to answer questions and find probabilities."
  },
  {
    "objectID": "lessons/16_Cont_RVs/16_Cont_RVs.html#properties-of-continuous-uniform-rvs",
    "href": "lessons/16_Cont_RVs/16_Cont_RVs.html#properties-of-continuous-uniform-rvs",
    "title": "Lesson 16: Some Important Continuous RVs",
    "section": "Properties of continuous uniform RVs",
    "text": "Properties of continuous uniform RVs\n\nScenario: Events are equally likely to happen anywhere or anytime in an interval of values\nShorthand: \\(X \\sim \\text{U}[a,b]\\)\n\n\\[\nf_X(x) = \\dfrac{1}{b-a}, \\text{ for }a \\leq x \\leq b\n\\]\n\\[\nF_X(x) = \\left\\{\n        \\begin{array}{ll}\n            0 & \\quad x&lt;a \\quad \\\\\n            \\dfrac{x-a}{b-a} & \\quad a \\leq x \\leq b\\quad \\\\\n            1 & \\quad x&gt;b \\quad\n        \\end{array}\n    \\right.\n\\]\n\\[\\text{E}(X) = \\dfrac{a+b}{2} \\text{, } \\text{ Var}(X) = \\dfrac{(b-a)^2}{12}\\]"
  },
  {
    "objectID": "lessons/16_Cont_RVs/16_Cont_RVs.html#identifying-continuous-uniform-rv-from-word-problems",
    "href": "lessons/16_Cont_RVs/16_Cont_RVs.html#identifying-continuous-uniform-rv-from-word-problems",
    "title": "Lesson 16: Some Important Continuous RVs",
    "section": "Identifying continuous uniform RV from word problems",
    "text": "Identifying continuous uniform RV from word problems\n\nLook for some indication that all events are equally likely\n\nCould also say “uniformly distributed”\n\nLook for an interval\n\nTime example: Costumer in your store will approach the cash register in next 30 minutes. Approaching the register throughout the 30 minutes is equally likely.\nLength example: You have a 12 inch string that you need to cut. You are equally likely to cut anywhere on the string.\n\nDifferent than the discrete uniform\n\nDiscrete usually includes a countable number of events that are equally likely\nContinuous is not countable\n\nExact time and length can be measured with infinite decimal places"
  },
  {
    "objectID": "lessons/16_Cont_RVs/16_Cont_RVs.html#helpful-r-code",
    "href": "lessons/16_Cont_RVs/16_Cont_RVs.html#helpful-r-code",
    "title": "Lesson 16: Some Important Continuous RVs",
    "section": "Helpful R code",
    "text": "Helpful R code\nLet’s say we’re looking at equally likely arrival times between 10 am and 11 am.\n\nIf we want to know the probability that someone arrives at 10:30am or earlier:\n\npunif(q = 30, min = 0, max = 60)\n\n[1] 0.5\n\n\nIf we want to know the time, say \\(t\\), where the probability of arriving at \\(t\\) or earlier is 0.35:\n\nqunif(p = 0.35, min = 0, max = 60)\n\n[1] 21\n\n\nIf we want to know the probability that someone arrives between 10:14 and 10:16 am:\n\npunif(q = 16, min = 0, max = 60) - punif(q = 14, min = 0, max = 60)\n\n[1] 0.03333333\n\n\nIf we want to sample 20 arrival times from the distribution:\n\nrunif(n = 20, min = 0, max = 60)\n\n [1] 57.254766 34.877635  9.463305  9.299211 44.853180 24.713062  7.637811\n [8] 56.432479 57.911681 26.541762 41.756752 32.239442 54.968374  4.029851\n[15] 33.138835 40.718886  5.610676 19.739926 21.084020 36.677654"
  },
  {
    "objectID": "lessons/16_Cont_RVs/16_Cont_RVs.html#bird-on-a-wire-tb-31.5",
    "href": "lessons/16_Cont_RVs/16_Cont_RVs.html#bird-on-a-wire-tb-31.5",
    "title": "Lesson 16: Some Important Continuous RVs",
    "section": "Bird on a wire (TB 31.5)",
    "text": "Bird on a wire (TB 31.5)\n\n\n\n\nExample 1\n\n\nA bird lands at a location that is Uniformly distributed along an electrical wire of length 150 feet. The wire is stretched tightly between two poles. What is the probability that the bird is 20 feet or less from one or the other of the poles?"
  },
  {
    "objectID": "lessons/16_Cont_RVs/16_Cont_RVs.html#properties-of-exponential-rvs",
    "href": "lessons/16_Cont_RVs/16_Cont_RVs.html#properties-of-exponential-rvs",
    "title": "Lesson 16: Some Important Continuous RVs",
    "section": "Properties of exponential RVs",
    "text": "Properties of exponential RVs\n\nScenario: Modeling the time until the next (first) event\nContinuous analog to the geometric distribution!\nShorthand: \\(X \\sim \\text{Exp}(\\lambda)\\)\n\n\\[\nf_X(x) = \\lambda e^{-\\lambda x}\\text{ for } x&gt;0, \\lambda&gt;0\n\\]\n\\[\nF_X(x) = \\left\\{\n        \\begin{array}{ll}\n            0 & \\quad x&lt;0 \\quad \\\\\n            1 - e^{-\\lambda x} & \\quad x\\geq0 \\\\\n        \\end{array}\n    \\right.\n\\]\n\\[\\text{E}(X) = \\dfrac{1}{\\lambda}\\] \\[\\text{Var}(X) = \\dfrac{1}{\\lambda^2}\\]"
  },
  {
    "objectID": "lessons/16_Cont_RVs/16_Cont_RVs.html#memoryless-property",
    "href": "lessons/16_Cont_RVs/16_Cont_RVs.html#memoryless-property",
    "title": "Lesson 16: Some Important Continuous RVs",
    "section": "Memoryless Property",
    "text": "Memoryless Property\n   \nIf \\(b&gt;0\\),\n\\[P(X &gt; a +b | X&gt; a) = P(X &gt; b)\\]\n     \n\nThis can be interpreted as:\n\nIf you have waited \\(a\\) seconds (or any other measure of time) without a success\nThen the probability that you have to wait \\(b\\) more seconds is the same as as the probability of waiting \\(b\\) seconds initially."
  },
  {
    "objectID": "lessons/16_Cont_RVs/16_Cont_RVs.html#identifying-exponential-rv-from-word-problems",
    "href": "lessons/16_Cont_RVs/16_Cont_RVs.html#identifying-exponential-rv-from-word-problems",
    "title": "Lesson 16: Some Important Continuous RVs",
    "section": "Identifying exponential RV from word problems",
    "text": "Identifying exponential RV from word problems\n\nLook for time between events/successes\nLook for a rate of the events over time period\nHow does it differ from the geometric distribution?\n\nGeometric is number of trials until first success\nExponential is time until first success\n\nRelation to the Poisson distribution?\n\nWhen the time between arrivals is exponential, the number of arrivals in a fixed time interval is Poisson with the mean \\(\\lambda\\)"
  },
  {
    "objectID": "lessons/16_Cont_RVs/16_Cont_RVs.html#helpful-r-code-1",
    "href": "lessons/16_Cont_RVs/16_Cont_RVs.html#helpful-r-code-1",
    "title": "Lesson 16: Some Important Continuous RVs",
    "section": "Helpful R code",
    "text": "Helpful R code\nLet’s say we’re sitting at the bus stop, measuring the time until our bus arrives. We know the bus comes every 10 minutes on average.\n\nIf we want to know the probability that the bus arrives in the next 5 minutes:\n\npexp(q = 5, rate = 1/10)\n\n[1] 0.3934693\n\n\nIf we want to know the time, say \\(t\\), where the probability of the bus arriving at \\(t\\) or earlier is 0.35:\n\nqexp(p = 0.35, rate = 1/10)\n\n[1] 4.307829\n\n\nIf we want to know the probability that the bus arrives between 3 and 5 minutes:\n\npexp(q = 5, rate = 1/10) - pexp(q = 3, rate = 1/10)\n\n[1] 0.1342876\n\n\nIf we want to sample 20 bus arrival times from the distribution:\n\nrexp(n = 20, rate = 1/10)\n\n [1]  3.27079121  9.21887422 23.15584516  1.43197788  7.13709980  2.46155182\n [7]  1.25854357 16.49368095  2.07284506  9.61934986  1.72675516  9.66994375\n[13]  0.60112111  8.14090359 27.93188929  0.02541741 22.76283795  4.51165567\n[19] 15.79182468  2.31727717"
  },
  {
    "objectID": "lessons/16_Cont_RVs/16_Cont_RVs.html#transformation-of-independent-exponential-rvs",
    "href": "lessons/16_Cont_RVs/16_Cont_RVs.html#transformation-of-independent-exponential-rvs",
    "title": "Lesson 16: Some Important Continuous RVs",
    "section": "Transformation of independent exponential RVs",
    "text": "Transformation of independent exponential RVs\n\n\nRevisit after joint notes:\n\n\nExample 2\n\n\nLet \\(X_i \\sim \\textrm{Exp}(\\lambda_i)\\) be independent RVs, for \\(i=1 \\ldots n\\). Find the pdf for the first of the arrival times."
  },
  {
    "objectID": "lessons/16_Cont_RVs/16_Cont_RVs.html#properties-of-gamma-rvs",
    "href": "lessons/16_Cont_RVs/16_Cont_RVs.html#properties-of-gamma-rvs",
    "title": "Lesson 16: Some Important Continuous RVs",
    "section": "Properties of gamma RVs",
    "text": "Properties of gamma RVs\n\nScenario: Modeling the time until the \\(r^{th}\\) event.\nContinuous analog to the Negative Binomial distribution\nShorthand: \\(X \\sim \\text{Gamma}(r, \\lambda)\\) or \\(X \\sim \\text{Gamma}(\\alpha, \\beta)\\)\n\n\\[\nf_X(x) = \\dfrac{\\lambda^r}{\\Gamma(r)}x^{r-1} e^{-\\lambda x}\\text{ for } x&gt;0, \\lambda&gt;0, \\Gamma(r) = (r-1)!\n\\]\n\\[\nF_X(x) = \\left\\{\n        \\begin{array}{ll}\n            0 & \\quad x&lt;0 \\quad \\\\\n            1 - e^{-\\lambda x}\\displaystyle\\sum_{j=0}^{r-1}\\dfrac{(\\lambda x)^j}{j!}  & \\quad x\\geq0 \\\\\n        \\end{array}\n    \\right.\n\\]\n\\[\\text{E}(X) = \\dfrac{r}{\\lambda}\\text{, }\\text{ Var}(X) = \\dfrac{r}{\\lambda^2}\\]\nCommon to see \\(\\alpha = r\\) and \\(\\beta = \\lambda\\)"
  },
  {
    "objectID": "lessons/16_Cont_RVs/16_Cont_RVs.html#identifying-gamma-rv-from-word-problems",
    "href": "lessons/16_Cont_RVs/16_Cont_RVs.html#identifying-gamma-rv-from-word-problems",
    "title": "Lesson 16: Some Important Continuous RVs",
    "section": "Identifying gamma RV from word problems",
    "text": "Identifying gamma RV from word problems\n\nGamma distribution with \\(r=1\\) is same as exponential\n\nJust like Negative Binomial with \\(r=1\\) is same as the geometric distribution\n\nSimilar to exponential\n\nLook for time between or until events/successes\n\nBUT now we are measuring time until more than 1 success\n\nLook for a rate of the events over time period"
  },
  {
    "objectID": "lessons/16_Cont_RVs/16_Cont_RVs.html#helpful-r-code-2",
    "href": "lessons/16_Cont_RVs/16_Cont_RVs.html#helpful-r-code-2",
    "title": "Lesson 16: Some Important Continuous RVs",
    "section": "Helpful R code",
    "text": "Helpful R code\nLet’s say we’re sitting at the bus stop, measuring the time until 4 buses arrive. We know the bus comes every 10 minutes on average.\n\nIf we want to know the probability that the 4 buses arrive in the next 50 minutes:\n\n\n\n\npgamma(q = 50, rate = 1/10, shape = 4)\n\n[1] 0.7349741\n\n\n\n\npgamma(q = 50, scale = 10, shape = 4)\n\n[1] 0.7349741\n\n\n\n\n\nIf we want to know the time, say \\(t\\), where the probability of the 4 buses arriving at \\(t\\) or earlier is 0.35:\n\nqgamma(p = 0.35, rate = 1/10, shape = 4)\n\n[1] 29.87645\n\n\nIf we want to know the probability that the 4 buses arrives between 30 and 50 minutes:\n\npgamma(q = 50, scale = 10, shape = 4) - pgamma(q = 30, scale = 10, shape = 4)\n\n[1] 0.382206\n\n\nIf we want to sample 20 arrival times for the 4 buses:\n\nrgamma(n = 20, scale = 10, shape = 4)\n\n [1] 67.04894 14.44306 43.05435 12.51608 30.58018 34.94098 28.34891 26.01258\n [9] 37.60928 24.73837 32.30761 45.71904 54.58035 41.08169 18.69450 28.78482\n[17] 51.66387 40.08829 56.18500 22.85659"
  },
  {
    "objectID": "lessons/16_Cont_RVs/16_Cont_RVs.html#remarks",
    "href": "lessons/16_Cont_RVs/16_Cont_RVs.html#remarks",
    "title": "Lesson 16: Some Important Continuous RVs",
    "section": "Remarks",
    "text": "Remarks\n\nThe parameter \\(r\\) in a Gamma(\\(r\\),\\(\\lambda\\)) distribution does NOT need to be a positive integer\n\n\\(r\\) is usually a positive integer\n\nWhen \\(r\\) is a positive integer, the distribution is sometimes called an Erlang(\\(r\\),\\(\\lambda\\)) distribution\n \n \nWhen \\(r\\) is any positive real number, we have a general gamma distribution that is usually instead parameterized by \\(\\alpha&gt;0\\) and \\(\\beta&gt;0\\), where:\n\n\\(\\alpha = \\text{shape parameter}\\) : same as \\(k\\), the total number of events we must witness\n\nIn R code example: 4 buses to wait for\n\n\\(\\beta = \\text{scale parameter}\\) : same as \\(\\lambda\\), the rate parameter\n\nIn R code example: 1 bus per 10 minutes (1/10)"
  },
  {
    "objectID": "lessons/16_Cont_RVs/16_Cont_RVs.html#sending-money-orders",
    "href": "lessons/16_Cont_RVs/16_Cont_RVs.html#sending-money-orders",
    "title": "Lesson 16: Some Important Continuous RVs",
    "section": "Sending money orders",
    "text": "Sending money orders\n\n\n\n\nExample 3\n\n\nOn average, someone sends a money order once per 15 minutes. What is the probability someone sends 10 money orders in less than 3 hours?"
  },
  {
    "objectID": "lessons/16_Cont_RVs/16_Cont_RVs.html#additional-resource",
    "href": "lessons/16_Cont_RVs/16_Cont_RVs.html#additional-resource",
    "title": "Lesson 16: Some Important Continuous RVs",
    "section": "Additional Resource",
    "text": "Additional Resource\n\nAnother helpful site with R code: https://rpubs.com/mpfoley73/459051"
  },
  {
    "objectID": "lessons/16_Cont_RVs/16_Cont_RVs.html#properties-of-normal-rvs",
    "href": "lessons/16_Cont_RVs/16_Cont_RVs.html#properties-of-normal-rvs",
    "title": "Lesson 16: Some Important Continuous RVs",
    "section": "Properties of Normal RVs",
    "text": "Properties of Normal RVs\n\nNo scenario description here because the Normal distribution is so universal\n\nCentral Limit Theorem (next class) makes it applicable to many types of events\n\nShorthand: \\(X \\sim \\text{Normal}(\\mu, \\sigma^2)\\)\n\n\\[\nf_X(x) = \\dfrac{1}{\\sqrt{2\\pi \\sigma^2}}e^{-(x-\\mu)^2/(2\\sigma^2)} \\text{, for} -\\infty &lt; x &lt; \\infty\n\\]\n\\[\\text{E}(X) = \\mu \\] \\[\\text{Var}(X) = \\sigma^2\\]"
  },
  {
    "objectID": "lessons/16_Cont_RVs/16_Cont_RVs.html#helpful-r-code-3",
    "href": "lessons/16_Cont_RVs/16_Cont_RVs.html#helpful-r-code-3",
    "title": "Lesson 16: Some Important Continuous RVs",
    "section": "Helpful R code",
    "text": "Helpful R code\nLet’s say we’re measuring the high temperature today. The average high temperature on this day across many, many years is 50 degrees with a standard deviation of 4 degrees.\n\nIf we want to know the probability that the high temperature is below 45 degrees:\n\npnorm(q = 45, mean = 50, sd = 4)\n\n[1] 0.1056498\n\n\nIf we want to know the temoerature, say \\(t\\), where the probability of that the temperature is at \\(t\\) or lower is 0.35:\n\nqnorm(p = 0.35, mean = 50, sd = 4)\n\n[1] 48.45872\n\n\nIf we want to know the probability that the temperature is between 45 and 50 degrees:\n\npnorm(q = 50, mean = 50, sd = 4) - pnorm(q = 45, mean = 50, sd = 4)\n\n[1] 0.3943502\n\n\nIf we want to sample 20 days’ temperature (over the years) from the distribution:\n\nrnorm(n = 20, mean = 50, sd = 4)\n\n [1] 51.25746 45.35334 49.66657 51.78714 47.45682 53.39772 52.86730 47.12661\n [9] 49.79689 52.23243 50.95174 58.17850 51.36292 46.18399 52.27518 53.80362\n[17] 51.33378 49.25484 54.50188 55.77698"
  },
  {
    "objectID": "lessons/16_Cont_RVs/16_Cont_RVs.html#movie-night-while-studying",
    "href": "lessons/16_Cont_RVs/16_Cont_RVs.html#movie-night-while-studying",
    "title": "Lesson 16: Some Important Continuous RVs",
    "section": "Movie night while studying",
    "text": "Movie night while studying\n\n\n\n\nExample 4\n\n\nChildren’s movies run an average of 98 minutes with a standard deviation of 10 minutes. You check out a random movie from the library to entertain your kids so you can study for your test. Assume that your kids will be occupied for the entire length of the movie.\n\nWhat is the probability that your kids will be occupied for at least the 2 hours you would like to study?\nWhat is range for the bottom quartile (lowest 25%) of time they will be occupied?"
  },
  {
    "objectID": "lessons/16_Cont_RVs/16_Cont_RVs.html#standard-normal-distribution",
    "href": "lessons/16_Cont_RVs/16_Cont_RVs.html#standard-normal-distribution",
    "title": "Lesson 16: Some Important Continuous RVs",
    "section": "Standard Normal Distribution",
    "text": "Standard Normal Distribution\n\\[ Z \\sim \\text{Normal}(\\mu = 0, \\sigma^2 = 1)\\]\n\nUsed to be more helpful when computing was not as advanced\n\nUse tables of the standard normal\nYou can convert any normal distribution to a standard normal through transformation\n\n\\(Z = \\dfrac{X - \\mu_X}{\\sigma_X}\\)\n\nComes from \\(X = \\sigma_X Z + \\mu_X\\)\nSince \\(\\sigma_X\\) and \\(\\mu_X\\) are constants, then \\(E(X) = \\mu_X\\) and \\(SD(X) = \\sigma_X\\)"
  },
  {
    "objectID": "lessons/16_Cont_RVs/16_Cont_RVs.html#chi-squared-distribution",
    "href": "lessons/16_Cont_RVs/16_Cont_RVs.html#chi-squared-distribution",
    "title": "Lesson 16: Some Important Continuous RVs",
    "section": "Chi-squared distribution",
    "text": "Chi-squared distribution\n\n\n\nIf \\(Z \\sim \\text{Normal}(0,1)\\), then \\(X = Z^2\\) has a Chi-squared distribution with 1 degree of freedom\n\nShorthand: \\(X \\sim \\chi^2_{(1)}\\)\n\nIf \\(Z_1, Z_2, \\ldots, Z_n\\) are independent standard normal RVs, then\n\\[X = \\sum_{i=1}^n Z_i^2\\]\nhas a Chi-squared distribution with \\(n\\) degrees of freedom\n\nShorthand: \\(X \\sim \\chi^2_{(n)}\\)"
  },
  {
    "objectID": "lessons/06_Calculus_review/06_Calculus_review.html",
    "href": "lessons/06_Calculus_review/06_Calculus_review.html",
    "title": "Lesson 6: Calculus Review",
    "section": "",
    "text": "Find derivatives of continuous functions with one variable\nFind antiderivatives and integrals of functions with one variable"
  },
  {
    "objectID": "lessons/06_Calculus_review/06_Calculus_review.html#learning-objectives",
    "href": "lessons/06_Calculus_review/06_Calculus_review.html#learning-objectives",
    "title": "Lesson 6: Calculus Review",
    "section": "",
    "text": "Find derivatives of continuous functions with one variable\nFind antiderivatives and integrals of functions with one variable"
  },
  {
    "objectID": "lessons/06_Calculus_review/06_Calculus_review.html#where-are-we",
    "href": "lessons/06_Calculus_review/06_Calculus_review.html#where-are-we",
    "title": "Lesson 6: Calculus Review",
    "section": "Where are we?",
    "text": "Where are we?"
  },
  {
    "objectID": "lessons/06_Calculus_review/06_Calculus_review.html#find-the-derivative-of-the-following-function",
    "href": "lessons/06_Calculus_review/06_Calculus_review.html#find-the-derivative-of-the-following-function",
    "title": "Lesson 6: Calculus Review",
    "section": "Find the derivative of the following function",
    "text": "Find the derivative of the following function\n\n\n\n\nExample 1.1\n\n\n\\(f(x) = 2\\)\n\n\n\n\nDerivative of a constant\n\n\n\\[\\dfrac{d}{dx} c = 0\\]"
  },
  {
    "objectID": "lessons/06_Calculus_review/06_Calculus_review.html#find-the-derivative-of-the-following-function-1",
    "href": "lessons/06_Calculus_review/06_Calculus_review.html#find-the-derivative-of-the-following-function-1",
    "title": "Lesson 6: Calculus Review",
    "section": "Find the derivative of the following function",
    "text": "Find the derivative of the following function\n\n\n\n\nExample 1.2\n\n\n\\(f(x) = 2x\\)"
  },
  {
    "objectID": "lessons/06_Calculus_review/06_Calculus_review.html#find-the-derivative-of-the-following-function-2",
    "href": "lessons/06_Calculus_review/06_Calculus_review.html#find-the-derivative-of-the-following-function-2",
    "title": "Lesson 6: Calculus Review",
    "section": "Find the derivative of the following function",
    "text": "Find the derivative of the following function\n\n\n\n\nExample 1.3\n\n\n\\(f(x) = 2x+2\\)"
  },
  {
    "objectID": "lessons/06_Calculus_review/06_Calculus_review.html#find-the-derivative-of-the-following-function-3",
    "href": "lessons/06_Calculus_review/06_Calculus_review.html#find-the-derivative-of-the-following-function-3",
    "title": "Lesson 6: Calculus Review",
    "section": "Find the derivative of the following function",
    "text": "Find the derivative of the following function\n\n\n\n\nExample 1.4\n\n\n\\(f(x) = x^2\\)\n\n\n\n\nDerivative of \\(x\\) to a constant\n\n\n\\[\\dfrac{d}{dx} x^n = nx^{n-1}\\]"
  },
  {
    "objectID": "lessons/06_Calculus_review/06_Calculus_review.html#find-the-derivative-of-the-following-function-4",
    "href": "lessons/06_Calculus_review/06_Calculus_review.html#find-the-derivative-of-the-following-function-4",
    "title": "Lesson 6: Calculus Review",
    "section": "Find the derivative of the following function",
    "text": "Find the derivative of the following function\n\n\n\n\nExample 1.5\n\n\n\\(f(x) = 3\\sqrt{x}+\\frac2x+5\\)"
  },
  {
    "objectID": "lessons/06_Calculus_review/06_Calculus_review.html#find-the-derivative-of-the-following-function-5",
    "href": "lessons/06_Calculus_review/06_Calculus_review.html#find-the-derivative-of-the-following-function-5",
    "title": "Lesson 6: Calculus Review",
    "section": "Find the derivative of the following function",
    "text": "Find the derivative of the following function\n\n\n\n\nExample 1.6\n\n\n\\(f(x) = e^x\\)\n\n\n\n\nDerivative of exponential function\n\n\n\\(\\dfrac{d}{dx} e^x = e^x\\)"
  },
  {
    "objectID": "lessons/06_Calculus_review/06_Calculus_review.html#find-the-derivative-of-the-following-function-6",
    "href": "lessons/06_Calculus_review/06_Calculus_review.html#find-the-derivative-of-the-following-function-6",
    "title": "Lesson 6: Calculus Review",
    "section": "Find the derivative of the following function",
    "text": "Find the derivative of the following function\n\n\n\n\nExample 1.7\n\n\n\\(f(x) = \\ln(x)\\)\n\n\n\n\nDerivative of logarithm\n\n\n\\(\\dfrac{d}{dx} ln(x) = \\dfrac{1}{x}\\)"
  },
  {
    "objectID": "lessons/06_Calculus_review/06_Calculus_review.html#find-the-derivative-of-the-following-function-7",
    "href": "lessons/06_Calculus_review/06_Calculus_review.html#find-the-derivative-of-the-following-function-7",
    "title": "Lesson 6: Calculus Review",
    "section": "Find the derivative of the following function",
    "text": "Find the derivative of the following function\n\n\n\n\nExample 1.8\n\n\n\\(f(x) = x^2 e^x\\)\n\n\n\n\nProduct Rule\n\n\n\\(\\dfrac{d}{dx} f(x)g(x) = f'(x)g(x) + f(x)g'(x)\\)"
  },
  {
    "objectID": "lessons/06_Calculus_review/06_Calculus_review.html#find-the-derivative-of-the-following-function-8",
    "href": "lessons/06_Calculus_review/06_Calculus_review.html#find-the-derivative-of-the-following-function-8",
    "title": "Lesson 6: Calculus Review",
    "section": "Find the derivative of the following function",
    "text": "Find the derivative of the following function\n\n\n\n\nExample 1.9\n\n\n\\(f(x) = \\dfrac{x^5}{2x+7}\\)\n\n\n\n\nQuotient Rule\n\n\n\\(\\dfrac{d}{dx} \\dfrac{f(x)}{g(x)} = \\dfrac{g(x)f'(x) - f(x)g'(x)}{\\big(g(x)\\big)^2}\\)"
  },
  {
    "objectID": "lessons/06_Calculus_review/06_Calculus_review.html#find-the-derivative-of-the-following-function-9",
    "href": "lessons/06_Calculus_review/06_Calculus_review.html#find-the-derivative-of-the-following-function-9",
    "title": "Lesson 6: Calculus Review",
    "section": "Find the derivative of the following function",
    "text": "Find the derivative of the following function\n\n\n\n\nExample 1.10\n\n\n\\(f(x) = e^{-2x+7}\\)\n\n\n\n\nChain Rule\n\n\n\\(\\dfrac{d}{dx} f\\big(g(x)\\big)= f'\\big(g(x)\\big)g'(x)\\)"
  },
  {
    "objectID": "lessons/06_Calculus_review/06_Calculus_review.html#find-the-derivative-of-the-following-function-10",
    "href": "lessons/06_Calculus_review/06_Calculus_review.html#find-the-derivative-of-the-following-function-10",
    "title": "Lesson 6: Calculus Review",
    "section": "Find the derivative of the following function",
    "text": "Find the derivative of the following function\n\n\n\n\nExample 1.11\n\n\n\\(f(x) = \\ln(x^2)\\)"
  },
  {
    "objectID": "lessons/06_Calculus_review/06_Calculus_review.html#find-the-antiderivative-of-the-following-function",
    "href": "lessons/06_Calculus_review/06_Calculus_review.html#find-the-antiderivative-of-the-following-function",
    "title": "Lesson 6: Calculus Review",
    "section": "Find the antiderivative of the following function",
    "text": "Find the antiderivative of the following function\n\n\n\n\nExample 2.1\n\n\n\\(f(x) = 2\\)"
  },
  {
    "objectID": "lessons/06_Calculus_review/06_Calculus_review.html#find-the-antiderivative-of-the-following-function-1",
    "href": "lessons/06_Calculus_review/06_Calculus_review.html#find-the-antiderivative-of-the-following-function-1",
    "title": "Lesson 6: Calculus Review",
    "section": "Find the antiderivative of the following function",
    "text": "Find the antiderivative of the following function\n\n\n\n\nExample 2.2\n\n\n\\(f(x) = x\\)\n\n\n\n\nIntegration of x to a constant\n\n\n\\(\\displaystyle\\int x^n dx = \\dfrac{x^{n+1}}{n+1} + c\\)"
  },
  {
    "objectID": "lessons/06_Calculus_review/06_Calculus_review.html#find-the-antiderivative-of-the-following-function-2",
    "href": "lessons/06_Calculus_review/06_Calculus_review.html#find-the-antiderivative-of-the-following-function-2",
    "title": "Lesson 6: Calculus Review",
    "section": "Find the antiderivative of the following function",
    "text": "Find the antiderivative of the following function\n\n\n\n\nExample 2.3\n\n\n\\(f(x) = \\frac1x\\)"
  },
  {
    "objectID": "lessons/06_Calculus_review/06_Calculus_review.html#find-the-antiderivative-of-the-following-function-3",
    "href": "lessons/06_Calculus_review/06_Calculus_review.html#find-the-antiderivative-of-the-following-function-3",
    "title": "Lesson 6: Calculus Review",
    "section": "Find the antiderivative of the following function",
    "text": "Find the antiderivative of the following function\n\n\n\n\nExample 2.4\n\n\n\\(f(x) = x^{3/2}\\)"
  },
  {
    "objectID": "lessons/06_Calculus_review/06_Calculus_review.html#find-the-antiderivative-of-the-following-function-4",
    "href": "lessons/06_Calculus_review/06_Calculus_review.html#find-the-antiderivative-of-the-following-function-4",
    "title": "Lesson 6: Calculus Review",
    "section": "Find the antiderivative of the following function",
    "text": "Find the antiderivative of the following function\n\n\n\n\nExample 2.5\n\n\n\\(f(x) = e^x\\)"
  },
  {
    "objectID": "lessons/06_Calculus_review/06_Calculus_review.html#find-the-antiderivative-of-the-following-function-5",
    "href": "lessons/06_Calculus_review/06_Calculus_review.html#find-the-antiderivative-of-the-following-function-5",
    "title": "Lesson 6: Calculus Review",
    "section": "Find the antiderivative of the following function",
    "text": "Find the antiderivative of the following function\n\n\n\n\nExample 2.6\n\n\n\\(f(x) = e^{-x}\\)"
  },
  {
    "objectID": "lessons/06_Calculus_review/06_Calculus_review.html#find-the-antiderivative-of-the-following-function-6",
    "href": "lessons/06_Calculus_review/06_Calculus_review.html#find-the-antiderivative-of-the-following-function-6",
    "title": "Lesson 6: Calculus Review",
    "section": "Find the antiderivative of the following function",
    "text": "Find the antiderivative of the following function\n\n\n\n\nExample 2.7\n\n\n\\(f(x) = e^{-2x}\\)"
  },
  {
    "objectID": "lessons/06_Calculus_review/06_Calculus_review.html#solve-the-following-integral",
    "href": "lessons/06_Calculus_review/06_Calculus_review.html#solve-the-following-integral",
    "title": "Lesson 6: Calculus Review",
    "section": "Solve the following integral",
    "text": "Solve the following integral\n\n\n\n\nExample 3.1\n\n\n\\(\\displaystyle\\int_0^1 (2x+x^5)dx\\)"
  },
  {
    "objectID": "lessons/06_Calculus_review/06_Calculus_review.html#solve-the-following-integral-1",
    "href": "lessons/06_Calculus_review/06_Calculus_review.html#solve-the-following-integral-1",
    "title": "Lesson 6: Calculus Review",
    "section": "Solve the following integral",
    "text": "Solve the following integral\n\n\n\n\nExample 3.2\n\n\n\\(\\displaystyle\\int_2^3 e^{-x}dx\\)\n\n\n\n\nU-substitution\n\n\n\\(\\displaystyle\\int f\\big(g(x)\\big) g'(x) dx = \\displaystyle\\int f(u) dx\\)"
  },
  {
    "objectID": "lessons/06_Calculus_review/06_Calculus_review.html#solve-the-following-integral-2",
    "href": "lessons/06_Calculus_review/06_Calculus_review.html#solve-the-following-integral-2",
    "title": "Lesson 6: Calculus Review",
    "section": "Solve the following integral",
    "text": "Solve the following integral\n\n\n\n\nExample 3.3\n\n\n\\(\\displaystyle\\int_2^3 x e^{x^2}dx\\)"
  },
  {
    "objectID": "lessons/06_Calculus_review/06_Calculus_review.html#solve-the-following-integral-3",
    "href": "lessons/06_Calculus_review/06_Calculus_review.html#solve-the-following-integral-3",
    "title": "Lesson 6: Calculus Review",
    "section": "Solve the following integral",
    "text": "Solve the following integral\n\n\n\n\nExample 3.4\n\n\n\\(\\displaystyle\\int_0^{\\infty} x e^{-x}dx\\)\n\n\n\n\nIntegrating by Parts\n\n\n\\(\\displaystyle\\int f(x) g'(x) dx = f(x)g(x) -\\)\n\\(\\displaystyle\\int f'(x) g(x) dx\\)\nOR\n\\(\\displaystyle\\int_a^b u dv = uv\\bigg|^b_a - \\displaystyle\\int_a^b vdu\\)"
  },
  {
    "objectID": "lessons/06_Calculus_review/06_Calculus_review.html#solve-the-following-integral-4",
    "href": "lessons/06_Calculus_review/06_Calculus_review.html#solve-the-following-integral-4",
    "title": "Lesson 6: Calculus Review",
    "section": "Solve the following integral",
    "text": "Solve the following integral\n\n\n\n\nExample 3.5\n\n\n\\(\\displaystyle\\int_1^2 x^2 \\ln(x)dx\\)"
  },
  {
    "objectID": "lessons/06_Calculus_review/06_Calculus_review.html#solve-the-following-integral-5",
    "href": "lessons/06_Calculus_review/06_Calculus_review.html#solve-the-following-integral-5",
    "title": "Lesson 6: Calculus Review",
    "section": "Solve the following integral",
    "text": "Solve the following integral\n\n\n\n\nExample 3.6\n\n\n\\(\\displaystyle\\int_1^2 \\ln(x)dx\\)"
  },
  {
    "objectID": "lessons/06_Calculus_review/06_Calculus_review.html#solve-the-following-integral-6",
    "href": "lessons/06_Calculus_review/06_Calculus_review.html#solve-the-following-integral-6",
    "title": "Lesson 6: Calculus Review",
    "section": "Solve the following integral",
    "text": "Solve the following integral\n\n\n\n\nExample 3.7\n\n\n\\(\\displaystyle\\int_1^2 x^2 e^{x}dx\\)"
  },
  {
    "objectID": "lessons/05_Equally_likely_outcomes/05_Equally_likely_outcomes_muddy_points.html",
    "href": "lessons/05_Equally_likely_outcomes/05_Equally_likely_outcomes_muddy_points.html",
    "title": "Muddy Points",
    "section": "",
    "text": "Here’s the slide: \nLet’s think of a different scenario (Scenario 1): Let’s find the probability of drawing a spade and a heart from a standard deck of cards when drawing two cards without replacement.\nThere are 52 cards in a standard deck, with 13 spades and 13 hearts.\n\nLet’s look at the sample space size, \\(|S|\\). We need to enumerate all the possible combinations of two cards. \\[\\binom{52}{2} = \\frac{52 \\times 51}{2} = 1326\\]\nThe more difficult part is the size of our event A, \\(|A|\\). We need to enumerate all the ways to draw 1 spade and 1 heart:\n\nWays to choose 1 spade from 13: \\(\\binom{13}{1} = 13\\)\nWays to choose 1 heart from 13: \\(\\binom{13}{1} = 13\\)\nFor each potential spade we draw, there are 13 potential hearts we could draw. So we can multiply the two together to get the total number of ways to draw a spade and a heart: \\[|A| = 13 \\times 13 = 169\\]\n\n\nThen we could calculate the probability of drawing a spade and a heart as: \\[P(A) = \\frac{|A|}{|S|} = \\frac{13 \\times 13}{1326} = \\frac{169}{1326} = 0.1275\\]\nWe can keep expanding our scenario (Scenario 2): Let’s find the probability of drawing one spade, one heart, one diamond, and one club without replacement.\n\nLet’s look at the sample space size, \\(|S|\\). We need to enumerate all the possible combinations of two cards. \\[\\binom{52}{4} = \\frac{52 \\times 51 \\times 50 \\times 49}{4 \\times 3 \\times 2 \\times 1} = 270725\\]\nNow, size of event A, \\(|A|\\). We need to enumerate all the ways to draw one spade, one heart, one diamond, and one club:\n\nWays to choose 1 spade from 13: \\(\\binom{13}{1} = 13\\)\nWays to choose 1 heart from 13: \\(\\binom{13}{1} = 13\\)\nWays to choose 1 diamond from 13: \\(\\binom{13}{1} = 13\\)\nWays to choose 1 club from 13: \\(\\binom{13}{1} = 13\\)\nFor each potential spade we draw, there are 13 potential hearts we could draw. And from each specific spade and heart combo, there are 13 potential diamonds, etc. So we can multiply all four together to get the total number of ways to draw a spade and a heart: \\[|A| = 13 \\times 13 \\times 13 \\times 13 = 28561\\]\n\n\nThen we could calculate the probability of drawing a spade and a heart as: \\[P(A) = \\frac{|A|}{|S|} = \\frac{13 \\times 13 \\times 13 \\times 13}{270725} = \\frac{28561}{270725} = 0.1055\\]\nIn both the above scenarios, we calculated the size of event A only with the cards we DRAW I just wanted to emphasize that for each suit of cards, we can technically enumerate all the ways to draw a certain number of cards, including 0.\nIf we go back to scenario 1, we could also calculate the size of our event A, \\(|A|\\) as:\n\nWays to choose 1 spade from 13: \\(\\binom{13}{1} = 13\\)\nWays to choose 1 heart from 13: \\(\\binom{13}{1} = 13\\)\nWays to choose 0 diamonds from 13: \\(\\binom{13}{0} = 1\\)\nWays to choose 0 clubs from 13: \\(\\binom{13}{0} = 1\\)\nFor each potential spade we draw, there are 13 potential hearts we could draw. So we can multiply the four together to get the total number of ways to draw a spade and a heart and 0 diamonds and 0 clubs: \\[|A| = 13 \\times 13 \\times 1 \\times 1 = 169\\]"
  },
  {
    "objectID": "lessons/05_Equally_likely_outcomes/05_Equally_likely_outcomes_muddy_points.html#fall-2025",
    "href": "lessons/05_Equally_likely_outcomes/05_Equally_likely_outcomes_muddy_points.html#fall-2025",
    "title": "Muddy Points",
    "section": "",
    "text": "Here’s the slide: \nLet’s think of a different scenario (Scenario 1): Let’s find the probability of drawing a spade and a heart from a standard deck of cards when drawing two cards without replacement.\nThere are 52 cards in a standard deck, with 13 spades and 13 hearts.\n\nLet’s look at the sample space size, \\(|S|\\). We need to enumerate all the possible combinations of two cards. \\[\\binom{52}{2} = \\frac{52 \\times 51}{2} = 1326\\]\nThe more difficult part is the size of our event A, \\(|A|\\). We need to enumerate all the ways to draw 1 spade and 1 heart:\n\nWays to choose 1 spade from 13: \\(\\binom{13}{1} = 13\\)\nWays to choose 1 heart from 13: \\(\\binom{13}{1} = 13\\)\nFor each potential spade we draw, there are 13 potential hearts we could draw. So we can multiply the two together to get the total number of ways to draw a spade and a heart: \\[|A| = 13 \\times 13 = 169\\]\n\n\nThen we could calculate the probability of drawing a spade and a heart as: \\[P(A) = \\frac{|A|}{|S|} = \\frac{13 \\times 13}{1326} = \\frac{169}{1326} = 0.1275\\]\nWe can keep expanding our scenario (Scenario 2): Let’s find the probability of drawing one spade, one heart, one diamond, and one club without replacement.\n\nLet’s look at the sample space size, \\(|S|\\). We need to enumerate all the possible combinations of two cards. \\[\\binom{52}{4} = \\frac{52 \\times 51 \\times 50 \\times 49}{4 \\times 3 \\times 2 \\times 1} = 270725\\]\nNow, size of event A, \\(|A|\\). We need to enumerate all the ways to draw one spade, one heart, one diamond, and one club:\n\nWays to choose 1 spade from 13: \\(\\binom{13}{1} = 13\\)\nWays to choose 1 heart from 13: \\(\\binom{13}{1} = 13\\)\nWays to choose 1 diamond from 13: \\(\\binom{13}{1} = 13\\)\nWays to choose 1 club from 13: \\(\\binom{13}{1} = 13\\)\nFor each potential spade we draw, there are 13 potential hearts we could draw. And from each specific spade and heart combo, there are 13 potential diamonds, etc. So we can multiply all four together to get the total number of ways to draw a spade and a heart: \\[|A| = 13 \\times 13 \\times 13 \\times 13 = 28561\\]\n\n\nThen we could calculate the probability of drawing a spade and a heart as: \\[P(A) = \\frac{|A|}{|S|} = \\frac{13 \\times 13 \\times 13 \\times 13}{270725} = \\frac{28561}{270725} = 0.1055\\]\nIn both the above scenarios, we calculated the size of event A only with the cards we DRAW I just wanted to emphasize that for each suit of cards, we can technically enumerate all the ways to draw a certain number of cards, including 0.\nIf we go back to scenario 1, we could also calculate the size of our event A, \\(|A|\\) as:\n\nWays to choose 1 spade from 13: \\(\\binom{13}{1} = 13\\)\nWays to choose 1 heart from 13: \\(\\binom{13}{1} = 13\\)\nWays to choose 0 diamonds from 13: \\(\\binom{13}{0} = 1\\)\nWays to choose 0 clubs from 13: \\(\\binom{13}{0} = 1\\)\nFor each potential spade we draw, there are 13 potential hearts we could draw. So we can multiply the four together to get the total number of ways to draw a spade and a heart and 0 diamonds and 0 clubs: \\[|A| = 13 \\times 13 \\times 1 \\times 1 = 169\\]"
  },
  {
    "objectID": "lessons/05_Equally_likely_outcomes/05_Equally_likely_outcomes.html",
    "href": "lessons/05_Equally_likely_outcomes/05_Equally_likely_outcomes.html",
    "title": "Lesson 5: Equally Likely Outcomes and Counting",
    "section": "",
    "text": "Define permutations and combinations\nCharacterize difference between sampling with and without replacement\nCharacterize difference between sampling when order matters and when order does not matter\nCalculate the probability of sampling any combination of the following: with or without replacement and order does or does not matter"
  },
  {
    "objectID": "lessons/05_Equally_likely_outcomes/05_Equally_likely_outcomes.html#learning-objectives",
    "href": "lessons/05_Equally_likely_outcomes/05_Equally_likely_outcomes.html#learning-objectives",
    "title": "Lesson 5: Equally Likely Outcomes and Counting",
    "section": "",
    "text": "Define permutations and combinations\nCharacterize difference between sampling with and without replacement\nCharacterize difference between sampling when order matters and when order does not matter\nCalculate the probability of sampling any combination of the following: with or without replacement and order does or does not matter"
  },
  {
    "objectID": "lessons/05_Equally_likely_outcomes/05_Equally_likely_outcomes.html#where-are-we",
    "href": "lessons/05_Equally_likely_outcomes/05_Equally_likely_outcomes.html#where-are-we",
    "title": "Lesson 5: Equally Likely Outcomes and Counting",
    "section": "Where are we?",
    "text": "Where are we?"
  },
  {
    "objectID": "lessons/05_Equally_likely_outcomes/05_Equally_likely_outcomes.html#birthday-example",
    "href": "lessons/05_Equally_likely_outcomes/05_Equally_likely_outcomes.html#birthday-example",
    "title": "Lesson 5: Equally Likely Outcomes and Counting",
    "section": "Birthday example",
    "text": "Birthday example"
  },
  {
    "objectID": "lessons/05_Equally_likely_outcomes/05_Equally_likely_outcomes.html#basic-counting-examples-13",
    "href": "lessons/05_Equally_likely_outcomes/05_Equally_likely_outcomes.html#basic-counting-examples-13",
    "title": "Lesson 5: Equally Likely Outcomes and Counting",
    "section": "Basic Counting Examples (1/3)",
    "text": "Basic Counting Examples (1/3)\n\n\nExample 1\n\n\nSuppose we have 10 (distinguishable) subjects for study.\n\nHow many possible ways are there to order them?\nHow many ways to order them if we can reuse the same subject and\n\nneed 10 total?\nneed 6 total?\n\nHow many ways to order them without replacement and only need 6?\nHow many ways to choose 6 subjects without replacement if the order doesn’t matter?"
  },
  {
    "objectID": "lessons/05_Equally_likely_outcomes/05_Equally_likely_outcomes.html#basic-counting-examples-23",
    "href": "lessons/05_Equally_likely_outcomes/05_Equally_likely_outcomes.html#basic-counting-examples-23",
    "title": "Lesson 5: Equally Likely Outcomes and Counting",
    "section": "Basic Counting Examples (2/3)",
    "text": "Basic Counting Examples (2/3)\nSuppose we have 10 (distinguishable) subjects for study.\n\n\n\n\nExample 1.1\n\n\nHow many possible ways are there to order them?\n\n\n \n\n\nExample 1.2\n\n\nHow many ways to order them if we can reuse the same subject and\n\nneed 10 total?\nneed 6 total?"
  },
  {
    "objectID": "lessons/05_Equally_likely_outcomes/05_Equally_likely_outcomes.html#basic-counting-examples-33",
    "href": "lessons/05_Equally_likely_outcomes/05_Equally_likely_outcomes.html#basic-counting-examples-33",
    "title": "Lesson 5: Equally Likely Outcomes and Counting",
    "section": "Basic Counting Examples (3/3)",
    "text": "Basic Counting Examples (3/3)\nSuppose we have 10 (distinguishable) subjects for study.\n\n\n\n\nExample 1.3\n\n\nHow many ways to order them without replacement and only need 6?\n\n\n \n\n\nExample 1.4\n\n\nHow many ways to choose 6 subjects without replacement if the order doesn’t matter?"
  },
  {
    "objectID": "lessons/05_Equally_likely_outcomes/05_Equally_likely_outcomes.html#permutations-and-combinations-1",
    "href": "lessons/05_Equally_likely_outcomes/05_Equally_likely_outcomes.html#permutations-and-combinations-1",
    "title": "Lesson 5: Equally Likely Outcomes and Counting",
    "section": "Permutations and Combinations",
    "text": "Permutations and Combinations\n\n\nDefinition: Permutations\n\n\nPermutations are the number of ways to arrange in order \\(r\\) distinct objects when there are \\(n\\) total.\n\\[nPr = \\frac{n!}{(n-r)!}\\]\n\n\n\n\nDefinition: Combinations\n\n\nCombinations are the number of ways to choose (order doesn’t matter) \\(r\\) objects from \\(n\\) without replacement.\n\\[nCr = \\textrm{\"n choose r\"} = \\binom{n}{r} = \\frac{n!}{r!(n-r)!}\\]"
  },
  {
    "objectID": "lessons/05_Equally_likely_outcomes/05_Equally_likely_outcomes.html#some-combinations-properties",
    "href": "lessons/05_Equally_likely_outcomes/05_Equally_likely_outcomes.html#some-combinations-properties",
    "title": "Lesson 5: Equally Likely Outcomes and Counting",
    "section": "Some combinations properties",
    "text": "Some combinations properties\n\n\n\n\n\n\n\n\nProperty\n\nProof\n\n\n\n\n\\[\\binom{n}{r} = \\binom{n}{n-r}\\]\n\n\\(\\binom{n}{r} = \\dfrac{n!}{r!(n-r)!}\\) and \\(\\binom{n}{n-r} = \\dfrac{n!}{(n-r)!(n-(n-r))!} = \\dfrac{n!}{(n-r)!r!}\\)\n\n\n\\[\\binom{n}{1} = n\\]\n\n\\[\\binom{n}{1} = \\dfrac{n!}{1!(n-1)!}=\\dfrac{n\\cdot(n-1)\\cdot(n-2)\\cdots1}{1! \\cdot (n-1)\\cdot(n-2)\\cdots1} = \\dfrac{n\\cdot(n-1)!}{1\\cdot (n-1)!} = \\dfrac{n}{1}=n\\]\n\n\n\\[\\binom{n}{0} = 1\\]\n\n\\[\\binom{n}{0} = \\dfrac{n!}{0!(n-0)!}=\\dfrac{n!}{1\\cdot n!} = 1\\]"
  },
  {
    "objectID": "lessons/05_Equally_likely_outcomes/05_Equally_likely_outcomes.html#table-of-different-cases",
    "href": "lessons/05_Equally_likely_outcomes/05_Equally_likely_outcomes.html#table-of-different-cases",
    "title": "Lesson 5: Equally Likely Outcomes and Counting",
    "section": "Table of different cases",
    "text": "Table of different cases\n\n\\(n\\) = total number of objects\n\\(r\\) = number objects needed\n\n\n\n\n\n\n\n\n\nwith replacement\nwithout replacement\n\n\n\n\norder matters\n\\[n^r\\]\n\\[n\\text{P}r = \\frac{n!}{(n-r)!}\\]\n\n\norder doesn’t matter\n\\[ \\binom{n+r-1}{r}\\]\n\\[n\\text{C}r = \\binom{n}{r} = \\frac{n!}{r!(n-r)!}\\]"
  },
  {
    "objectID": "lessons/05_Equally_likely_outcomes/05_Equally_likely_outcomes.html#enumerating-events-and-sample-space",
    "href": "lessons/05_Equally_likely_outcomes/05_Equally_likely_outcomes.html#enumerating-events-and-sample-space",
    "title": "Lesson 5: Equally Likely Outcomes and Counting",
    "section": "Enumerating Events and Sample Space",
    "text": "Enumerating Events and Sample Space\n\nRecall, \\(P(A) = \\dfrac{|A|}{|S|}\\)\n\nWithin combinatorics, we can use the previous equations to help enumerate the event and sample space\nBut \\(A\\) might be a combination of enumerations\n\n\n \n\nFor example in the following example drawing 2 spades when order does not matter, we actually need to enumerate the other cards that are NOT spades. So the event is choosing 2 spades out of 13 AND choosing 0 other cards of 39 cards (13 hearts + 13 clubs + 13 diamonds).\nThus the probability is actually:\n\n\\[ P(\\text{two spades}) = \\dfrac{{13 \\choose 2}{39 \\choose 0}}{{52 \\choose 2}} \\]\n\nNote that \\(13 + 39 = 52\\) and \\(2+ 0 = 2\\). So the numerator’s \\(n\\)’s add up to the denominator’s \\(n\\) and the numerator’s \\(r\\)’s add up to the denominator’s \\(r\\)’s"
  },
  {
    "objectID": "lessons/05_Equally_likely_outcomes/05_Equally_likely_outcomes.html#another-example-order-matters-vs.-not-12",
    "href": "lessons/05_Equally_likely_outcomes/05_Equally_likely_outcomes.html#another-example-order-matters-vs.-not-12",
    "title": "Lesson 5: Equally Likely Outcomes and Counting",
    "section": "Another example: order matters vs. not (1/2)",
    "text": "Another example: order matters vs. not (1/2)\n\n\n\n\nExample 2\n\n\nSuppose we draw 2 cards from a standard deck without replacement. What is the probability that both are spades when\n\norder matters?\norder doesn’t matter?"
  },
  {
    "objectID": "lessons/05_Equally_likely_outcomes/05_Equally_likely_outcomes.html#another-example-order-matters-vs.-not-22",
    "href": "lessons/05_Equally_likely_outcomes/05_Equally_likely_outcomes.html#another-example-order-matters-vs.-not-22",
    "title": "Lesson 5: Equally Likely Outcomes and Counting",
    "section": "Another example: order matters vs. not (2/2)",
    "text": "Another example: order matters vs. not (2/2)\n\n\n\n\nExample 2\n\n\nSuppose we draw 2 cards from a standard deck without replacement. What is the probability that both are spades when\n\norder matters?\norder doesn’t matter?\n\n\n\n\nWe can do a simulation!\n\nset.seed(1234)\nn_sim &lt;- 1000000\ncards = c(rep(\"S\", 13), \n          rep(\"H\", 13), \n          rep(\"C\", 13), \n          rep(\"D\", 13))\ndraws &lt;- replicate(n_sim, \n                   sample(cards, 2, replace = FALSE))\ndraws[, 1:10]\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n[1,] \"C\"  \"H\"  \"D\"  \"S\"  \"C\"  \"S\"  \"C\"  \"H\"  \"H\"  \"D\"  \n[2,] \"H\"  \"C\"  \"D\"  \"S\"  \"H\"  \"C\"  \"H\"  \"S\"  \"H\"  \"H\"  \n\nspades_2 = sum( draws[1, ] == \"S\" & draws[2, ] == \"S\" )\nspades_2 / n_sim\n\n[1] 0.058727"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Week\nDate\nLesson\nTopic\nTB\nKey Info\nSlides HTML\nSlides PDF\nSlides Notes\nExit tix\nRecord-ing\nMuddy Points\n\n\n\n\n1\n01/05\n\nWelcome\n\n\n\n\n\n\n\n\n\n\n\n\n1\nIntroduction to Statistical Inference; Statistics\n6.1, 6.2\n\n\n\n\n\n\n\n\n\n\n01/07\n2\nPoint estimation; Bias, variance, and MSE of estimators\n7.1\n\n\n\n\n\n\n\n\n\n\n01/08\n\nHW 0 due 11pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2\n01/12\n3\nChi-square, t, F distributions\n6.3, 6.4\n\n\n\n\n\n\n\n\n\n\n01/14\n4\nUnbiased estimators, MVUE\n7.1\n\n\n\n\n\n\n\n\n\n\n01/15\n\nHW 1 due 11pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3\n01/19\nNo class\n\n\n\n\n\n\n\n\n\n\n\n\n01/21\n5\nLikelihood, MLE, MME\n7.2\n\n\n\n\n\n\n\n\n\n\n01/22\n\nHW 2 due 11 pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4\n01/26\n6\nSufficient statistics, Fisher Information\n7.3\n\n\n\n\n\n\n\n\n\n\n01/28\n8\nMLE Large sample properties\n7.4\n\n\n\n\n\n\n\n\n\n\n01/29\n\nHW 3 due 11 pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5\n02/02\n9\nInterval estimation: single sample, one-sample t, population proportions\n8.1-8.4\n\n\n\n\n\n\n\n\n\n\n02/04\n10\nBootstrap confidence intervals\n8.5\n\n\n\n\n\n\n\n\n\n\n02/05\n\nHW 4 due 11 pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6\n02/09\n11\nBootstrap confidence intervals; Percentile method\n8.5\n\n\n\n\n\n\n\n\n\n\n02/11\nNo class\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7\n02/16\nNo class\n\n\n\n\n\n\n\n\n\n\n\n\n02/18\n12\nHypotheses and test procedures\n9.1-9.3\n\n\n\n\n\n\n\n\n\n\n02/19\n\nHW 6 due 11 pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n8\n02/23\n13\nType I and Type II errors\n9.2-9.3\n\n\n\n\n\n\n\n\n\n\n02/25\n14\nPower and sample size; P-values\n9.2, 9.4\n\n\n\n\n\n\n\n\n\n\n02/26\n\nHW 7 due 11 pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9\n03/02\n15\nNeyman-Pearson Lemma, Likelihood Ratio Tests\n9.5\n\n\n\n\n\n\n\n\n\n\n03/04\n16\nLarge-sample Z tests; Bootstrap hypothesis testing and bootstrap P-values\n9.6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n03/12\n\nHW 8 due 11 pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n03/19\n\nHW 9 due 11 pm"
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#welcome-to-statistical-inference",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#welcome-to-statistical-inference",
    "title": "BSTA 551: Statistical Inference",
    "section": "Welcome to Statistical Inference!",
    "text": "Welcome to Statistical Inference!\nCourse Focus: How do we learn about populations from samples?\n\nPoint Estimation (Weeks 1-4): What’s our best guess for a parameter?\nConfidence Intervals (Weeks 5-6): What’s a plausible range?\nHypothesis Testing (Weeks 7-9): Can we make decisions from data?\nTwo-Sample Methods (Week 10): Comparing groups"
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#todays-goals",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#todays-goals",
    "title": "BSTA 551: Statistical Inference",
    "section": "Today’s Goals",
    "text": "Today’s Goals\n\nExplain what an estimator is and how it differs from an estimate\nUse R to simulate sampling distributions\nUnderstand how optimization finds “best” values numerically\nDefine bias and calculate it for simple estimators"
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#motivating-example-clinical-trial",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#motivating-example-clinical-trial",
    "title": "BSTA 551: Statistical Inference",
    "section": "Motivating Example: Clinical Trial",
    "text": "Motivating Example: Clinical Trial\nA pharmaceutical company is testing a new blood pressure medication.\nThe Question: What is the true average reduction in systolic blood pressure?\n\nWhat we have: Data from 25 patients in a trial\n\n# Actual blood pressure reductions (mmHg) from 25 patients\nbp_reductions &lt;- c(12, 8, 15, 10, 7, 14, 11, 9, 13, 16,\n                   8, 12, 10, 14, 11, 9, 15, 13, 7, 12,\n                   10, 8, 14, 11, 13)\n\n# What's our estimate of the true mean reduction?\nmean(bp_reductions)\n\n[1] 11.28\n\n\n\n\nBut how reliable is this estimate? Would we get the same answer with different patients?"
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#the-big-picture-population-vs-sample",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#the-big-picture-population-vs-sample",
    "title": "BSTA 551: Statistical Inference",
    "section": "The Big Picture: Population vs Sample",
    "text": "The Big Picture: Population vs Sample\n\n\nKey insight: We use sample data to make inferences about population parameters."
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#review-expected-value",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#review-expected-value",
    "title": "BSTA 551: Statistical Inference",
    "section": "Review: Expected Value",
    "text": "Review: Expected Value\nThe expected value \\(E(X)\\) is the long-run average of a random variable.\n\n\n\n\n\n\nKey Properties We’ll Use Today\n\n\n\n\\(E(c) = c\\) for any constant \\(c\\)\n\\(E(cX) = c \\cdot E(X)\\)\n\\(E(X + Y) = E(X) + E(Y)\\)\n\\(E\\left(\\sum_{i=1}^n X_i\\right) = \\sum_{i=1}^n E(X_i)\\)"
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#worked-example-expected-value-of-sample-mean",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#worked-example-expected-value-of-sample-mean",
    "title": "BSTA 551: Statistical Inference",
    "section": "Worked Example: Expected Value of Sample Mean",
    "text": "Worked Example: Expected Value of Sample Mean\nProblem: If \\(X_1, X_2, \\ldots, X_n\\) are iid observations from a population with mean \\(\\mu\\), what is \\(E(\\bar{X})\\)?\n\nSolution: Let’s work through this step by step.\n\\[E(\\bar{X}) = E\\left(\\frac{1}{n}\\sum_{i=1}^n X_i\\right)\\]\n\n\n\\[= \\frac{1}{n} E\\left(\\sum_{i=1}^n X_i\\right) \\quad \\text{(Property 2: constants come out)}\\]\n\n\n\\[= \\frac{1}{n} \\sum_{i=1}^n E(X_i) \\quad \\text{(Property 4: sum of expectations)}\\]\n\n\n\\[= \\frac{1}{n} \\cdot n\\mu = \\mu \\quad \\text{(Each } E(X_i) = \\mu \\text{)}\\]"
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#your-turn-calculate-expected-value",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#your-turn-calculate-expected-value",
    "title": "BSTA 551: Statistical Inference",
    "section": "Your Turn: Calculate Expected Value",
    "text": "Your Turn: Calculate Expected Value\nExercise: A hospital measures the recovery time (in days) for patients after surgery. Let \\(X_1, X_2, X_3\\) be recovery times for 3 patients. The population mean recovery time is \\(\\mu = 5\\) days.\nQuestions:\n\nWhat is \\(E(X_1)\\)?\nWhat is \\(E(X_1 + X_2 + X_3)\\)?\nWhat is \\(E(\\bar{X})\\) where \\(\\bar{X} = \\frac{X_1 + X_2 + X_3}{3}\\)?\n\n\nAnswers:\n\n\\(E(X_1) = \\mu = 5\\) days\n\\(E(X_1 + X_2 + X_3) = 5 + 5 + 5 = 15\\) days\n\\(E(\\bar{X}) = \\frac{15}{3} = 5\\) days"
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#review-variance",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#review-variance",
    "title": "BSTA 551: Statistical Inference",
    "section": "Review: Variance",
    "text": "Review: Variance\nVariance measures the spread of a distribution: \\(\\text{Var}(X) = E[(X - \\mu)^2]\\)\n\n\n\n\n\n\nKey Properties We’ll Use Today\n\n\n\n\\(\\text{Var}(c) = 0\\) for any constant\n\\(\\text{Var}(cX) = c^2 \\cdot \\text{Var}(X)\\)\nIf \\(X\\) and \\(Y\\) are independent: \\(\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y)\\)"
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#worked-example-variance-of-sample-mean",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#worked-example-variance-of-sample-mean",
    "title": "BSTA 551: Statistical Inference",
    "section": "Worked Example: Variance of Sample Mean",
    "text": "Worked Example: Variance of Sample Mean\nProblem: If \\(X_1, \\ldots, X_n\\) are independent observations with variance \\(\\sigma^2\\), what is \\(\\text{Var}(\\bar{X})\\)?\n\n\\[\\text{Var}(\\bar{X}) = \\text{Var}\\left(\\frac{1}{n}\\sum_{i=1}^n X_i\\right)\\]\n\n\n\\[= \\frac{1}{n^2} \\text{Var}\\left(\\sum_{i=1}^n X_i\\right) \\quad \\text{(Property 2)}\\]\n\n\n\\[= \\frac{1}{n^2} \\sum_{i=1}^n \\text{Var}(X_i) \\quad \\text{(Property 3: independence)}\\]\n\n\n\\[= \\frac{1}{n^2} \\cdot n\\sigma^2 = \\frac{\\sigma^2}{n}\\]\n\n\n\n\n\n\nKey Result\n\n\nThe variance of \\(\\bar{X}\\) decreases as sample size \\(n\\) increases!"
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#simulation-seeing-variance-decrease",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#simulation-seeing-variance-decrease",
    "title": "BSTA 551: Statistical Inference",
    "section": "Simulation: Seeing Variance Decrease",
    "text": "Simulation: Seeing Variance Decrease\n\n# Population parameters\ntrue_mean &lt;- 120  # True mean systolic BP\ntrue_sd &lt;- 15     # Population standard deviation\n\n# Simulate sample means for different sample sizes\nsimulation_data &lt;- tibble(n = c(5, 25, 100)) |&gt; \n  cross_join(tibble(sim = 1:2000)) |&gt; \n  mutate(\n    sample_mean = map2_dbl(n, sim, \\(size, s) {\n      mean(rnorm(size, true_mean, true_sd))\n    })\n  )\n\n# Calculate observed standard deviation for each sample size\nsimulation_data |&gt; \n  group_by(n) |&gt; \n  summarize(\n    observed_sd = sd(sample_mean),\n    theoretical_sd = true_sd / sqrt(first(n))\n  )\n\n# A tibble: 3 × 3\n      n observed_sd theoretical_sd\n  &lt;dbl&gt;       &lt;dbl&gt;          &lt;dbl&gt;\n1     5        6.91           6.71\n2    25        3.01           3   \n3   100        1.50           1.5"
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#visualizing-the-effect-of-sample-size",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#visualizing-the-effect-of-sample-size",
    "title": "BSTA 551: Statistical Inference",
    "section": "Visualizing the Effect of Sample Size",
    "text": "Visualizing the Effect of Sample Size"
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#why-optimization-matters-in-statistics",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#why-optimization-matters-in-statistics",
    "title": "BSTA 551: Statistical Inference",
    "section": "Why Optimization Matters in Statistics",
    "text": "Why Optimization Matters in Statistics\nMany statistical methods require finding the “best” value of a parameter.\nExamples:\n\nMaximum Likelihood: Find the parameter value that makes the observed data most probable\nLeast Squares: Find the parameter value that minimizes prediction errors\nMinimum Variance: Find the estimator with the smallest spread\n\n\nThe Problem: How do we find maximums and minimums without calculus?"
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#optimization-the-graphical-intuition",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#optimization-the-graphical-intuition",
    "title": "BSTA 551: Statistical Inference",
    "section": "Optimization: The Graphical Intuition",
    "text": "Optimization: The Graphical Intuition\n\nThe maximum occurs where the function reaches its peak."
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#concrete-example-finding-the-best-estimate",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#concrete-example-finding-the-best-estimate",
    "title": "BSTA 551: Statistical Inference",
    "section": "Concrete Example: Finding the Best Estimate",
    "text": "Concrete Example: Finding the Best Estimate\nScenario: In a clinical trial, 7 out of 10 patients respond to treatment. What’s the best estimate of the true response rate \\(p\\)?\n\nApproach: Find the value of \\(p\\) that makes observing “7 out of 10” most likely.\nThe probability of observing exactly 7 successes in 10 trials is: \\[P(X = 7) = \\binom{10}{7} p^7 (1-p)^3\\]\n\n\nQuestion: For what value of \\(p\\) is this probability largest?"
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#grid-search-a-simple-numerical-approach",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#grid-search-a-simple-numerical-approach",
    "title": "BSTA 551: Statistical Inference",
    "section": "Grid Search: A Simple Numerical Approach",
    "text": "Grid Search: A Simple Numerical Approach\nIdea: Try many values and see which gives the largest result.\n\n# Try different values of p\ngrid_search &lt;- tibble(p = seq(0.01, 0.99, by = 0.01)) |&gt; \n  mutate(\n    likelihood = dbinom(7, size = 10, prob = p)\n  )\n\n# Find the maximum\ngrid_search |&gt; \n  slice_max(likelihood, n = 1)\n\n# A tibble: 1 × 2\n      p likelihood\n  &lt;dbl&gt;      &lt;dbl&gt;\n1   0.7      0.267\n\n\n\nThe maximum likelihood estimate is \\(\\hat{p} = 0.70 = \\frac{7}{10}\\). This makes intuitive sense!"
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#using-rs-optimizer",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#using-rs-optimizer",
    "title": "BSTA 551: Statistical Inference",
    "section": "Using R’s Optimizer",
    "text": "Using R’s Optimizer\nR has built-in functions to find maximums and minimums more precisely:\n\n# Define the likelihood function\nlikelihood_function &lt;- function(p) {\n  dbinom(7, size = 10, prob = p)\n}\n\n# Use optimize() to find the maximum\n# Note: optimize finds MINIMUM by default, so we negate for maximum\nresult &lt;- optimize(\n  f = function(p) -likelihood_function(p),  # Negative to find max\n  interval = c(0, 1)                         # Search between 0 and 1\n)\n\n# The maximum occurs at:\ncat(\"Maximum likelihood estimate: p =\", result$minimum)\n\nMaximum likelihood estimate: p = 0.6999843"
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#how-numerical-optimization-works",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#how-numerical-optimization-works",
    "title": "BSTA 551: Statistical Inference",
    "section": "How Numerical Optimization Works",
    "text": "How Numerical Optimization Works\n\nKey idea: The algorithm evaluates the function at different points and iteratively narrows in on the maximum."
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#your-turn-numerical-optimization",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#your-turn-numerical-optimization",
    "title": "BSTA 551: Statistical Inference",
    "section": "Your Turn: Numerical Optimization",
    "text": "Your Turn: Numerical Optimization\nExercise: A diagnostic test correctly identifies a disease in 18 out of 25 patients who have it. Find the maximum likelihood estimate for the test’s sensitivity \\(p\\).\n\n# Fill in the blanks:\nlikelihood_fn &lt;- function(p) {\n  dbinom(___, size = ___, prob = p)  # What goes here?\n}\n\nresult &lt;- optimize(\n  f = function(p) -likelihood_fn(p),\n  interval = c(0, 1)\n)\n\nresult$minimum  # This is the MLE\n\n\n\n# Solution:\nlikelihood_fn &lt;- function(p) {\n  dbinom(18, size = 25, prob = p)\n}\n\nresult &lt;- optimize(f = function(p) -likelihood_fn(p), interval = c(0, 1))\ncat(\"MLE of sensitivity:\", result$minimum)  # Should be 18/25 = 0.72\n\nMLE of sensitivity: 0.7200103"
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#when-optimization-gets-harder",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#when-optimization-gets-harder",
    "title": "BSTA 551: Statistical Inference",
    "section": "When Optimization Gets Harder",
    "text": "When Optimization Gets Harder\nSometimes we need to optimize over multiple parameters or complex functions:\n\n# Example: Finding mean and SD that best fit data\npatient_data &lt;- c(120, 135, 128, 142, 131, 125, 138, 129, 133, 127)\n\n# Negative log-likelihood for normal distribution\nneg_log_lik &lt;- function(params) {\n  mu &lt;- params[1]\n  sigma &lt;- params[2]\n  if (sigma &lt;= 0) return(Inf)  # sigma must be positive\n  -sum(dnorm(patient_data, mean = mu, sd = sigma, log = TRUE))\n}\n\n# Use optim() for multiple parameters\nresult &lt;- optim(par = c(130, 10), fn = neg_log_lik)\ncat(\"MLE for mean:\", round(result$par[1], 2), \"\\n\")\n\nMLE for mean: 130.8 \n\ncat(\"MLE for SD:\", round(result$par[2], 2), \"\\n\")\n\nMLE for SD: 6.13 \n\ncat(\"Compare to sample mean:\", round(mean(patient_data), 2))\n\nCompare to sample mean: 130.8"
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#break-15-minutes",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#break-15-minutes",
    "title": "BSTA 551: Statistical Inference",
    "section": "— Break (15 minutes) —",
    "text": "— Break (15 minutes) —"
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#what-is-a-point-estimator",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#what-is-a-point-estimator",
    "title": "BSTA 551: Statistical Inference",
    "section": "What is a Point Estimator?",
    "text": "What is a Point Estimator?\n\n\n\n\n\n\nDefinitions\n\n\n\nA parameter is a fixed (but unknown) characteristic of a population (e.g., \\(\\mu\\), \\(\\sigma\\), \\(p\\))\nAn estimator is a rule/formula for calculating an estimate from sample data\nAn estimate is the actual number you calculate from a specific sample\n\n\n\n\n\nExample:\n\n\n\n\n\n\n\n\nConcept\nSymbol\nExample\n\n\n\n\nParameter\n\\(\\mu\\)\nTrue mean BP reduction\n\n\nEstimator\n\\(\\bar{X} = \\frac{1}{n}\\sum X_i\\)\nThe formula “sample mean”\n\n\nEstimate\n\\(\\bar{x} = 11.2\\)\nThe number from our data"
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#the-sampling-distribution",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#the-sampling-distribution",
    "title": "BSTA 551: Statistical Inference",
    "section": "The Sampling Distribution",
    "text": "The Sampling Distribution\nDifferent samples give different estimates. The sampling distribution describes this variability.\n\n\n\n\n\nSample\nSample Mean (x̄)\n\n\n\n\n1\n9.90\n\n\n2\n10.31\n\n\n3\n10.03\n\n\n4\n10.85\n\n\n5\n9.17\n\n\n6\n9.31\n\n\n\n\n\n\nEach sample gives a different estimate, but they cluster around the true value \\(\\mu = 10\\)."
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#simulation-building-a-sampling-distribution",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#simulation-building-a-sampling-distribution",
    "title": "BSTA 551: Statistical Inference",
    "section": "Simulation: Building a Sampling Distribution",
    "text": "Simulation: Building a Sampling Distribution\n\n# Parameters\ntrue_effect &lt;- 10  # True mean BP reduction (mmHg)\ntrue_sd &lt;- 4       # Standard deviation\nn_patients &lt;- 25   # Patients per trial\nn_trials &lt;- 5000   # Number of simulated trials\n\n# Simulate many clinical trials\nsampling_distribution &lt;- tibble(trial = 1:n_trials) |&gt; \n  mutate(\n    sample_mean = map_dbl(trial, \\(t) {\n      patients &lt;- rnorm(n_patients, true_effect, true_sd)\n      mean(patients)\n    })\n  )\n\n# Visualize\nsampling_distribution |&gt; \n  ggplot(aes(x = sample_mean)) +\n  geom_histogram(bins = 50, fill = \"steelblue\", alpha = 0.7, color = \"white\") +\n  geom_vline(xintercept = true_effect, color = \"red\", linewidth = 1.5) +\n  labs(title = \"Sampling Distribution of the Sample Mean\",\n       subtitle = str_glue(\"True μ = {true_effect}, n = {n_patients}, {n_trials} simulated trials\"),\n       x = \"Sample Mean (estimated BP reduction)\", y = \"Frequency\")"
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#simulation-building-a-sampling-distribution-output",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#simulation-building-a-sampling-distribution-output",
    "title": "BSTA 551: Statistical Inference",
    "section": "Simulation: Building a Sampling Distribution",
    "text": "Simulation: Building a Sampling Distribution"
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#what-makes-a-good-estimator",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#what-makes-a-good-estimator",
    "title": "BSTA 551: Statistical Inference",
    "section": "What Makes a Good Estimator?",
    "text": "What Makes a Good Estimator?\nWe want estimators that are:\n\nAccurate (unbiased): On average, hits the true value\nPrecise (low variance): Estimates are clustered together\nEfficient: Best combination of accuracy and precision"
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#bias-formal-definition",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#bias-formal-definition",
    "title": "BSTA 551: Statistical Inference",
    "section": "Bias: Formal Definition",
    "text": "Bias: Formal Definition\n\n\n\n\n\n\nDefinition\n\n\nThe bias of an estimator \\(\\hat{\\theta}\\) is: \\[\\text{Bias}(\\hat{\\theta}) = E(\\hat{\\theta}) - \\theta\\]\nAn estimator is unbiased if \\(\\text{Bias}(\\hat{\\theta}) = 0\\), i.e., \\(E(\\hat{\\theta}) = \\theta\\).\n\n\n\n\nInterpretation:\n\nBias measures systematic error\nPositive bias = tends to overestimate\nNegative bias = tends to underestimate\nZero bias = correct on average"
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#worked-example-proving-sample-mean-is-unbiased",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#worked-example-proving-sample-mean-is-unbiased",
    "title": "BSTA 551: Statistical Inference",
    "section": "Worked Example: Proving Sample Mean is Unbiased",
    "text": "Worked Example: Proving Sample Mean is Unbiased\nClaim: The sample mean \\(\\bar{X}\\) is an unbiased estimator of \\(\\mu\\).\nProof: We need to show \\(E(\\bar{X}) = \\mu\\).\n\n\\[\\text{Bias}(\\bar{X}) = E(\\bar{X}) - \\mu\\]\n\n\nWe already showed that \\(E(\\bar{X}) = \\mu\\), so:\n\\[\\text{Bias}(\\bar{X}) = \\mu - \\mu = 0 \\checkmark\\]\n\n\nConclusion: The sample mean is unbiased for estimating the population mean."
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#worked-example-sample-proportion",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#worked-example-sample-proportion",
    "title": "BSTA 551: Statistical Inference",
    "section": "Worked Example: Sample Proportion",
    "text": "Worked Example: Sample Proportion\nSetup: In a vaccine trial, \\(X\\) patients out of \\(n\\) develop immunity. The estimator is \\(\\hat{p} = X/n\\).\nClaim: \\(\\hat{p}\\) is unbiased for the true immunity rate \\(p\\).\n\nProof: Since \\(X \\sim \\text{Binomial}(n, p)\\), we know \\(E(X) = np\\).\n\\[E(\\hat{p}) = E\\left(\\frac{X}{n}\\right) = \\frac{1}{n} E(X) = \\frac{1}{n} \\cdot np = p\\]\n\n\n\\[\\text{Bias}(\\hat{p}) = E(\\hat{p}) - p = p - p = 0 \\checkmark\\]"
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#concrete-calculation-bias-of-sample-proportion",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#concrete-calculation-bias-of-sample-proportion",
    "title": "BSTA 551: Statistical Inference",
    "section": "Concrete Calculation: Bias of Sample Proportion",
    "text": "Concrete Calculation: Bias of Sample Proportion\nData: In a study of 80 patients, 52 showed improvement.\n\nn &lt;- 80\nx &lt;- 52\np_hat &lt;- x / n\n\ncat(\"Sample proportion:\", p_hat, \"\\n\")\n\nSample proportion: 0.65 \n\ncat(\"If true p = 0.65, what is the bias of this single estimate?\\n\")\n\nIf true p = 0.65, what is the bias of this single estimate?\n\ncat(\"Observed - True =\", p_hat - 0.65)\n\nObserved - True = 0\n\n\n\nImportant distinction:\n\nA single estimate can be above or below the true value\nBias refers to the average behavior across many samples\nAn unbiased estimator still gives wrong answers for individual samples!"
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#simulation-verifying-unbiasedness",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#simulation-verifying-unbiasedness",
    "title": "BSTA 551: Statistical Inference",
    "section": "Simulation: Verifying Unbiasedness",
    "text": "Simulation: Verifying Unbiasedness\n\n# Verify that sample proportion is unbiased\ntrue_p &lt;- 0.65\nn_patients &lt;- 80\nn_simulations &lt;- 10000\n\nproportion_simulation &lt;- tibble(sim = 1:n_simulations) |&gt; \n  mutate(\n    successes = rbinom(n_simulations, size = n_patients, prob = true_p),\n    p_hat = successes / n_patients\n  )\n\nproportion_simulation |&gt; \n  summarize(\n    true_p = true_p,\n    mean_of_estimates = mean(p_hat),\n    empirical_bias = mean(p_hat) - true_p\n  )\n\n# A tibble: 1 × 3\n  true_p mean_of_estimates empirical_bias\n   &lt;dbl&gt;             &lt;dbl&gt;          &lt;dbl&gt;\n1   0.65             0.650      -0.000183\n\n\nThe bias is essentially zero (just simulation noise)!"
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#a-biased-estimator-the-maximum",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#a-biased-estimator-the-maximum",
    "title": "BSTA 551: Statistical Inference",
    "section": "A Biased Estimator: The Maximum",
    "text": "A Biased Estimator: The Maximum\nProblem: Estimate the upper bound \\(\\theta\\) of a Uniform[0, \\(\\theta\\)] distribution.\nNatural idea: Use the largest observation: \\(\\hat{\\theta} = \\max(X_1, \\ldots, X_n)\\)\n\nThink about it: Can this estimator ever overestimate \\(\\theta\\)?\n\n\nNo! The sample maximum is always ≤ the population maximum.\nThis means the estimator is biased low."
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#calculating-the-bias-mathematically",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#calculating-the-bias-mathematically",
    "title": "BSTA 551: Statistical Inference",
    "section": "Calculating the Bias Mathematically",
    "text": "Calculating the Bias Mathematically\nFor \\(X_1, \\ldots, X_n \\sim \\text{Uniform}[0, \\theta]\\), it can be shown that:\n\\[E(\\max(X_1, \\ldots, X_n)) = \\frac{n}{n+1} \\theta\\]\n\nBias calculation:\n\\[\\text{Bias} = E(\\hat{\\theta}) - \\theta = \\frac{n}{n+1}\\theta - \\theta = -\\frac{\\theta}{n+1}\\]\n\n\nExample: If \\(\\theta = 10\\) and \\(n = 5\\):\n\\[\\text{Bias} = -\\frac{10}{6} = -1.67\\]\nThe estimator underestimates by about 1.67 on average."
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#your-turn-calculate-bias",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#your-turn-calculate-bias",
    "title": "BSTA 551: Statistical Inference",
    "section": "Your Turn: Calculate Bias",
    "text": "Your Turn: Calculate Bias\nExercise: A lab instrument has a maximum detection limit \\(\\theta\\). We take \\(n = 9\\) measurements from Uniform[0, \\(\\theta\\)] and use the maximum as our estimate.\n\nIf \\(\\theta = 100\\), what is \\(E(\\hat{\\theta})\\)?\nWhat is the bias?\nBy what percentage does this estimator underestimate on average?\n\n\nSolution:\n\n\\(E(\\hat{\\theta}) = \\frac{9}{10} \\times 100 = 90\\)\n\\(\\text{Bias} = 90 - 100 = -10\\)\nUnderestimates by \\(\\frac{10}{100} = 10\\%\\)"
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#simulation-visualizing-the-biased-estimator",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#simulation-visualizing-the-biased-estimator",
    "title": "BSTA 551: Statistical Inference",
    "section": "Simulation: Visualizing the Biased Estimator",
    "text": "Simulation: Visualizing the Biased Estimator\n\ntrue_theta &lt;- 100\nn &lt;- 9\nn_sims &lt;- 5000\n\nmax_simulation &lt;- tibble(sim = 1:n_sims) |&gt; \n  mutate(\n    max_estimate = map_dbl(sim, \\(s) max(runif(n, 0, true_theta)))\n  )\n\n# Calculate empirical bias\nmax_simulation |&gt; \n  summarize(\n    theoretical_E = n / (n + 1) * true_theta,\n    empirical_mean = mean(max_estimate),\n    theoretical_bias = -true_theta / (n + 1),\n    empirical_bias = mean(max_estimate) - true_theta\n  )"
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#simulation-visualizing-the-biased-estimator-output",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#simulation-visualizing-the-biased-estimator-output",
    "title": "BSTA 551: Statistical Inference",
    "section": "Simulation: Visualizing the Biased Estimator",
    "text": "Simulation: Visualizing the Biased Estimator\n\n# A tibble: 1 × 4\n  theoretical_E empirical_mean theoretical_bias empirical_bias\n          &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;          &lt;dbl&gt;\n1            90           90.3              -10          -9.70"
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#visualizing-the-biased-estimator",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#visualizing-the-biased-estimator",
    "title": "BSTA 551: Statistical Inference",
    "section": "Visualizing the Biased Estimator",
    "text": "Visualizing the Biased Estimator"
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#correcting-the-bias",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#correcting-the-bias",
    "title": "BSTA 551: Statistical Inference",
    "section": "Correcting the Bias",
    "text": "Correcting the Bias\nIdea: Multiply by a correction factor to “un-bias” the estimator.\nSince \\(E(\\max) = \\frac{n}{n+1}\\theta\\), we can define:\n\\[\\hat{\\theta}_{\\text{unbiased}} = \\frac{n+1}{n} \\cdot \\max(X_1, \\ldots, X_n)\\]\n\nCheck:\n\\[E(\\hat{\\theta}_{\\text{unbiased}}) = \\frac{n+1}{n} \\cdot E(\\max) = \\frac{n+1}{n} \\cdot \\frac{n}{n+1}\\theta = \\theta \\checkmark\\]"
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#your-turn-apply-the-correction",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#your-turn-apply-the-correction",
    "title": "BSTA 551: Statistical Inference",
    "section": "Your Turn: Apply the Correction",
    "text": "Your Turn: Apply the Correction\nExercise: Using the lab instrument example with \\(n = 9\\) and \\(\\theta = 100\\):\n\nIf you observe \\(\\max = 92\\), what is the biased estimate?\nWhat is the unbiased estimate?\n\n\nSolution:\n\nBiased estimate: \\(\\hat{\\theta}_b = 92\\)\nUnbiased estimate: \\(\\hat{\\theta}_u = \\frac{10}{9} \\times 92 = 102.2\\)\n\n\nobserved_max &lt;- 92\nn &lt;- 9\n\nbiased_est &lt;- observed_max\nunbiased_est &lt;- (n + 1) / n * observed_max\n\ncat(\"Biased estimate:\", biased_est, \"\\n\")\n\nBiased estimate: 92 \n\ncat(\"Unbiased estimate:\", round(unbiased_est, 1))\n\nUnbiased estimate: 102.2"
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#day-1-summary",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#day-1-summary",
    "title": "BSTA 551: Statistical Inference",
    "section": "Day 1 Summary",
    "text": "Day 1 Summary\nKey Concepts:\n\nExpected Value: \\(E(\\bar{X}) = \\mu\\) (sample mean is centered at population mean)\nVariance: \\(\\text{Var}(\\bar{X}) = \\sigma^2/n\\) (precision improves with larger \\(n\\))\nOptimization: Finding maximum/minimum values numerically\n\nGrid search: try many values\noptimize(): efficient numerical search\n\nBias: \\(\\text{Bias}(\\hat{\\theta}) = E(\\hat{\\theta}) - \\theta\\)\n\nUnbiased if \\(E(\\hat{\\theta}) = \\theta\\)\nCan sometimes correct biased estimators"
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#day-1-practice-problems",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#day-1-practice-problems",
    "title": "BSTA 551: Statistical Inference",
    "section": "Day 1 Practice Problems",
    "text": "Day 1 Practice Problems\n\nCalculate \\(E(\\bar{X})\\) and \\(\\text{Var}(\\bar{X})\\) for a sample of size \\(n = 16\\) from a population with \\(\\mu = 50\\) and \\(\\sigma = 12\\).\nUse optimize() to find the MLE for \\(p\\) when you observe 23 successes in 40 trials.\nFor a Uniform[0, \\(\\theta\\)] distribution with \\(n = 20\\) observations and \\(\\theta = 50\\), calculate:\n\nThe expected value of the maximum\nThe bias of using the maximum as an estimator\nThe corrected unbiased estimator"
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#review-where-we-left-off",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#review-where-we-left-off",
    "title": "BSTA 551: Statistical Inference",
    "section": "Review: Where We Left Off",
    "text": "Review: Where We Left Off\nKey concepts from Lesson 1:\n\nEstimator vs. estimate\nSampling distribution\nBias: \\(\\text{Bias}(\\hat{\\theta}) = E(\\hat{\\theta}) - \\theta\\)\nUnbiased estimators: \\(E(\\hat{\\theta}) = \\theta\\)\n\n\nToday’s Goals:\n\nStandard error and precision\nMean squared error (MSE)\nThe bias-variance tradeoff\nComparing estimators with simulations"
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#standard-error-measuring-precision",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#standard-error-measuring-precision",
    "title": "BSTA 551: Statistical Inference",
    "section": "Standard Error: Measuring Precision",
    "text": "Standard Error: Measuring Precision\n\n\n\n\n\n\nDefinition\n\n\nThe standard error of an estimator is its standard deviation: \\[SE(\\hat{\\theta}) = \\sqrt{\\text{Var}(\\hat{\\theta})}\\]\n\n\n\n\nKey Standard Errors:\n\n\n\nEstimator\nStandard Error\n\n\n\n\nSample mean \\(\\bar{X}\\)\n\\(\\frac{\\sigma}{\\sqrt{n}}\\)\n\n\nSample proportion \\(\\hat{p}\\)\n\\(\\sqrt{\\frac{p(1-p)}{n}}\\)"
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#worked-example-standard-error-of-sample-mean",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#worked-example-standard-error-of-sample-mean",
    "title": "BSTA 551: Statistical Inference",
    "section": "Worked Example: Standard Error of Sample Mean",
    "text": "Worked Example: Standard Error of Sample Mean\nProblem: In a blood pressure study, the population SD is \\(\\sigma = 15\\) mmHg. Calculate the standard error of \\(\\bar{X}\\) for sample sizes \\(n = 25\\) and \\(n = 100\\).\n\nSolution:\nFor \\(n = 25\\): \\[SE(\\bar{X}) = \\frac{15}{\\sqrt{25}} = \\frac{15}{5} = 3 \\text{ mmHg}\\]\nFor \\(n = 100\\): \\[SE(\\bar{X}) = \\frac{15}{\\sqrt{100}} = \\frac{15}{10} = 1.5 \\text{ mmHg}\\]\n\n\nInterpretation: With 100 patients, our estimate is twice as precise as with 25 patients."
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#your-turn-calculate-standard-error",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#your-turn-calculate-standard-error",
    "title": "BSTA 551: Statistical Inference",
    "section": "Your Turn: Calculate Standard Error",
    "text": "Your Turn: Calculate Standard Error\nExercise: A survey measures patient satisfaction on a 0-100 scale. The population standard deviation is \\(\\sigma = 20\\).\n\nWhat is the SE of \\(\\bar{X}\\) for \\(n = 16\\) patients?\nWhat sample size is needed to achieve \\(SE = 2\\)?\n\n\nSolutions:\n\n\\(SE = \\frac{20}{\\sqrt{16}} = \\frac{20}{4} = 5\\)\nWe need \\(\\frac{20}{\\sqrt{n}} = 2\\), so \\(\\sqrt{n} = 10\\), thus \\(n = 100\\)\n\n\nsigma &lt;- 20\n# Problem 1\nse_n16 &lt;- sigma / sqrt(16)\n# Problem 2\nn_needed &lt;- (sigma / 2)^2\n\ncat(\"SE for n=16:\", se_n16, \"\\nSample size for SE=2:\", n_needed)\n\nSE for n=16: 5 \nSample size for SE=2: 100"
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#the-problem-with-unknown-parameters",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#the-problem-with-unknown-parameters",
    "title": "BSTA 551: Statistical Inference",
    "section": "The Problem with Unknown Parameters",
    "text": "The Problem with Unknown Parameters\nIssue: Standard errors often involve unknown parameters!\n\nSE of \\(\\bar{X}\\) requires knowing \\(\\sigma\\)\nSE of \\(\\hat{p}\\) requires knowing \\(p\\)\n\n\nSolution: Estimated standard error — substitute estimates for unknown parameters\n\\[\\widehat{SE}(\\bar{X}) = \\frac{s}{\\sqrt{n}}\\]\n\\[\\widehat{SE}(\\hat{p}) = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\]"
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#example-estimated-standard-error",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#example-estimated-standard-error",
    "title": "BSTA 551: Statistical Inference",
    "section": "Example: Estimated Standard Error",
    "text": "Example: Estimated Standard Error\n\n# Blood pressure data from 25 patients\nbp_reductions &lt;- c(12, 8, 15, 10, 7, 14, 11, 9, 13, 16,\n                   8, 12, 10, 14, 11, 9, 15, 13, 7, 12,\n                   10, 8, 14, 11, 13)\n\nn &lt;- length(bp_reductions)\nx_bar &lt;- mean(bp_reductions)\ns &lt;- sd(bp_reductions)\n\n# Estimated standard error\nse_estimated &lt;- s / sqrt(n)\n\ntibble(\n  Statistic = c(\"Sample Mean\", \"Sample SD\", \"Sample Size\", \"Estimated SE\"),\n  Value = c(x_bar, round(s, 2), n, round(se_estimated, 2))\n)\n\n# A tibble: 4 × 2\n  Statistic    Value\n  &lt;chr&gt;        &lt;dbl&gt;\n1 Sample Mean  11.3 \n2 Sample SD     2.64\n3 Sample Size  25   \n4 Estimated SE  0.53"
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#mean-squared-error-combining-bias-and-variance",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#mean-squared-error-combining-bias-and-variance",
    "title": "BSTA 551: Statistical Inference",
    "section": "Mean Squared Error: Combining Bias and Variance",
    "text": "Mean Squared Error: Combining Bias and Variance\nWhat if we have to choose between a biased estimator with low variance and an unbiased estimator with high variance?\n\n\n\n\n\n\n\nDefinition: Mean Squared Error\n\n\n\\[\\text{MSE}(\\hat{\\theta}) = E[(\\hat{\\theta} - \\theta)^2]\\]\nKey Formula: \\[\\text{MSE} = \\text{Variance} + \\text{Bias}^2\\]\n\n\n\n\n\nMSE captures total error — both systematic (bias) and random (variance)."
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#proving-the-mse-formula",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#proving-the-mse-formula",
    "title": "BSTA 551: Statistical Inference",
    "section": "Proving the MSE Formula",
    "text": "Proving the MSE Formula\n\\[\\text{MSE}(\\hat{\\theta}) = E[(\\hat{\\theta} - \\theta)^2]\\]\n\nAdd and subtract \\(E(\\hat{\\theta})\\):\n\\[= E[(\\hat{\\theta} - E(\\hat{\\theta}) + E(\\hat{\\theta}) - \\theta)^2]\\]\n\n\nExpand the square:\n\\[= E[(\\hat{\\theta} - E(\\hat{\\theta}))^2] + 2E[(\\hat{\\theta} - E(\\hat{\\theta}))](E(\\hat{\\theta}) - \\theta) + (E(\\hat{\\theta}) - \\theta)^2\\]\n\n\nThe middle term equals zero because \\(E[\\hat{\\theta} - E(\\hat{\\theta})] = 0\\):\n\\[= \\text{Var}(\\hat{\\theta}) + [\\text{Bias}(\\hat{\\theta})]^2\\]"
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#worked-example-computing-mse",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#worked-example-computing-mse",
    "title": "BSTA 551: Statistical Inference",
    "section": "Worked Example: Computing MSE",
    "text": "Worked Example: Computing MSE\nSetup: Estimating \\(\\theta\\) from Uniform[0, \\(\\theta\\)] with \\(n = 9\\) and \\(\\theta = 100\\).\nBiased estimator: \\(\\hat{\\theta}_b = \\max(X_i)\\)\nFrom theory:\n\n\\(E(\\hat{\\theta}_b) = \\frac{9}{10}(100) = 90\\)\n\\(\\text{Var}(\\hat{\\theta}_b) = \\frac{n \\theta^2}{(n+1)^2(n+2)} = \\frac{9 \\times 100^2}{100 \\times 11} = 81.82\\)\n\n\n\\[\\text{Bias} = 90 - 100 = -10\\] \\[\\text{MSE} = 81.82 + (-10)^2 = 81.82 + 100 = 181.82\\]"
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#your-turn-calculate-mse",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#your-turn-calculate-mse",
    "title": "BSTA 551: Statistical Inference",
    "section": "Your Turn: Calculate MSE",
    "text": "Your Turn: Calculate MSE\nExercise: For the unbiased estimator \\(\\hat{\\theta}_u = \\frac{n+1}{n}\\max(X_i)\\) with \\(n = 9\\) and \\(\\theta = 100\\):\n\nWhat is the bias?\nIf \\(\\text{Var}(\\hat{\\theta}_u) = 101.01\\), what is the MSE?\nWhich estimator has lower MSE: biased or unbiased?\n\n\nSolutions:\n\nBias = 0 (it’s unbiased!)\n\\(\\text{MSE} = 101.01 + 0^2 = 101.01\\)\nUnbiased has lower MSE (101.01 &lt; 181.82)\n\nBut this isn’t always the case!"
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#break-15-minutes-1",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#break-15-minutes-1",
    "title": "BSTA 551: Statistical Inference",
    "section": "— Break (15 minutes) —",
    "text": "— Break (15 minutes) —"
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#the-bias-variance-tradeoff",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#the-bias-variance-tradeoff",
    "title": "BSTA 551: Statistical Inference",
    "section": "The Bias-Variance Tradeoff",
    "text": "The Bias-Variance Tradeoff\nSometimes a biased estimator has lower MSE than an unbiased one!\n\nExample: Estimating a proportion \\(p\\) with \\(n = 20\\) observations\nEstimator 1: Standard: \\(\\hat{p}_1 = \\frac{X}{n}\\) (unbiased)\nEstimator 2: “Add-two”: \\(\\hat{p}_2 = \\frac{X + 2}{n + 4}\\) (biased toward 0.5)"
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#comparing-proportion-estimators-theory",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#comparing-proportion-estimators-theory",
    "title": "BSTA 551: Statistical Inference",
    "section": "Comparing Proportion Estimators: Theory",
    "text": "Comparing Proportion Estimators: Theory\nFor \\(\\hat{p}_1 = X/n\\) (standard):\n\nBias = 0\nVariance = \\(\\frac{p(1-p)}{n}\\)\nMSE = \\(\\frac{p(1-p)}{n}\\)\n\n\nFor \\(\\hat{p}_2 = \\frac{X+2}{n+4}\\) (add-two):\n\nBias = \\(\\frac{2 - 4p}{n+4}\\)\nVariance = \\(\\frac{np(1-p)}{(n+4)^2}\\)\nMSE = Variance + Bias²"
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#your-turn-calculate-bias-of-add-two-estimator",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#your-turn-calculate-bias-of-add-two-estimator",
    "title": "BSTA 551: Statistical Inference",
    "section": "Your Turn: Calculate Bias of Add-Two Estimator",
    "text": "Your Turn: Calculate Bias of Add-Two Estimator\nExercise: For \\(n = 20\\) and \\(p = 0.3\\):\n\nCalculate the bias of \\(\\hat{p}_2 = \\frac{X+2}{n+4}\\)\n\nHint: \\(E(X) = np\\) for binomial, so \\(E(\\hat{p}_2) = \\frac{np + 2}{n + 4}\\)\n\nSolution:\n\\[E(\\hat{p}_2) = \\frac{20(0.3) + 2}{24} = \\frac{8}{24} = 0.333\\]\n\\[\\text{Bias} = 0.333 - 0.3 = 0.033\\]\nThe add-two estimator is biased toward 0.5 (and 0.333 is closer to 0.5 than 0.3 is)."
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#simulation-comparing-the-estimators",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#simulation-comparing-the-estimators",
    "title": "BSTA 551: Statistical Inference",
    "section": "Simulation: Comparing the Estimators",
    "text": "Simulation: Comparing the Estimators\n\ntrue_p &lt;- 0.3\nn &lt;- 20\nn_sims &lt;- 10000\n\n# Simulate both estimators\ncomparison_sim &lt;- tibble(sim = 1:n_sims) |&gt; \n  mutate(\n    x = rbinom(n_sims, size = n, prob = true_p),\n    p_hat_standard = x / n,\n    p_hat_addtwo = (x + 2) / (n + 4)\n  )\n\n# Compare MSE\ncomparison_sim |&gt; \n  summarize(\n    `Standard Bias` = mean(p_hat_standard) - true_p,\n    `Add-Two Bias` = mean(p_hat_addtwo) - true_p,\n    `Standard Variance` = var(p_hat_standard),\n    `Add-Two Variance` = var(p_hat_addtwo),\n    `Standard MSE` = mean((p_hat_standard - true_p)^2),\n    `Add-Two MSE` = mean((p_hat_addtwo - true_p)^2)\n  ) |&gt; \n  pivot_longer(everything(), names_to = \"Metric\", values_to = \"Value\") |&gt; \n  mutate(Value = round(Value, 5))\n\n# A tibble: 6 × 2\n  Metric              Value\n  &lt;chr&gt;               &lt;dbl&gt;\n1 Standard Bias     0.00049\n2 Add-Two Bias      0.0337 \n3 Standard Variance 0.0104 \n4 Add-Two Variance  0.00722\n5 Standard MSE      0.0104 \n6 Add-Two MSE       0.00835"
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#visualizing-the-tradeoff",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#visualizing-the-tradeoff",
    "title": "BSTA 551: Statistical Inference",
    "section": "Visualizing the Tradeoff",
    "text": "Visualizing the Tradeoff"
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#mse-comparison-across-different-true-values",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#mse-comparison-across-different-true-values",
    "title": "BSTA 551: Statistical Inference",
    "section": "MSE Comparison Across Different True Values",
    "text": "MSE Comparison Across Different True Values\n\nKey Insight: The “best” estimator depends on the true parameter value!"
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#medical-application-disease-prevalence",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#medical-application-disease-prevalence",
    "title": "BSTA 551: Statistical Inference",
    "section": "Medical Application: Disease Prevalence",
    "text": "Medical Application: Disease Prevalence\nScenario: Estimating prevalence of a rare disease (\\(p \\approx 0.05\\)) vs. a common condition (\\(p \\approx 0.5\\)).\n\n# Compare MSE at different prevalence levels\nn &lt;- 50\n\nmse_at_p &lt;- function(p, n) {\n  mse_std &lt;- p * (1-p) / n\n  bias_add2 &lt;- (2 - 4*p) / (n + 4)\n  var_add2 &lt;- n * p * (1-p) / (n + 4)^2\n  mse_add2 &lt;- var_add2 + bias_add2^2\n  \n  tibble(p = p, MSE_Standard = mse_std, MSE_AddTwo = mse_add2,\n         Better = ifelse(mse_std &lt; mse_add2, \"Standard\", \"Add-Two\"))\n}\n\nbind_rows(\n  mse_at_p(0.05, n),\n  mse_at_p(0.50, n)\n) |&gt; \n  mutate(across(where(is.numeric), \\(x) round(x, 5)))\n\n# A tibble: 2 × 4\n      p MSE_Standard MSE_AddTwo Better  \n  &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;   \n1  0.05      0.00095    0.00193 Standard\n2  0.5       0.005      0.00429 Add-Two"
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#sample-variance-why-n-1",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#sample-variance-why-n-1",
    "title": "BSTA 551: Statistical Inference",
    "section": "Sample Variance: Why n-1?",
    "text": "Sample Variance: Why n-1?\nTwo formulas for sample variance:\n\\[S^2 = \\frac{\\sum(X_i - \\bar{X})^2}{n-1} \\quad \\text{vs.} \\quad \\tilde{S}^2 = \\frac{\\sum(X_i - \\bar{X})^2}{n}\\]\n\nQuestion: Why do we divide by \\(n-1\\) instead of \\(n\\)?\nAnswer: Dividing by \\(n\\) gives a biased estimator!"
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#simulation-comparing-variance-estimators",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#simulation-comparing-variance-estimators",
    "title": "BSTA 551: Statistical Inference",
    "section": "Simulation: Comparing Variance Estimators",
    "text": "Simulation: Comparing Variance Estimators\n\ntrue_variance &lt;- 100  # σ² = 100\nn &lt;- 10\nn_sims &lt;- 10000\n\nvariance_sim &lt;- tibble(sim = 1:n_sims) |&gt; \n  mutate(\n    sample_data = map(sim, \\(s) rnorm(n, 0, sqrt(true_variance))),\n    s2_n_minus_1 = map_dbl(sample_data, var),\n    s2_n = map_dbl(sample_data, \\(x) sum((x - mean(x))^2) / n)\n  )\n\nvariance_sim |&gt; \n  summarize(\n    `True σ²` = true_variance,\n    `E[S² with n-1]` = mean(s2_n_minus_1),\n    `E[S² with n]` = mean(s2_n),\n    `Bias (n-1)` = mean(s2_n_minus_1) - true_variance,\n    `Bias (n)` = mean(s2_n) - true_variance\n  ) |&gt; \n  mutate(across(where(is.numeric), \\(x) round(x, 2)))\n\n# A tibble: 1 × 5\n  `True σ²` `E[S² with n-1]` `E[S² with n]` `Bias (n-1)` `Bias (n)`\n      &lt;dbl&gt;            &lt;dbl&gt;          &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n1       100             100.           90.1         0.15      -9.86"
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#your-turn-comprehensive-example",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#your-turn-comprehensive-example",
    "title": "BSTA 551: Statistical Inference",
    "section": "Your Turn: Comprehensive Example",
    "text": "Your Turn: Comprehensive Example\nExercise: A clinical trial measures cholesterol reduction. Based on \\(n = 36\\) patients:\n\nSample mean: \\(\\bar{x} = 25\\) mg/dL\nSample SD: \\(s = 12\\) mg/dL\n\nCalculate:\n\nThe estimated standard error of \\(\\bar{X}\\)\nIf the true mean reduction is \\(\\mu = 24\\), and we repeated this trial many times, what would be the expected MSE of \\(\\bar{X}\\)?\n\n\nSolutions:\n\n\\(\\widehat{SE} = \\frac{12}{\\sqrt{36}} = 2\\) mg/dL\n\\(\\bar{X}\\) is unbiased, so \\(\\text{MSE} = \\text{Var}(\\bar{X}) = \\frac{\\sigma^2}{n} \\approx \\frac{144}{36} = 4\\)"
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#putting-it-all-together-estimator-summary",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#putting-it-all-together-estimator-summary",
    "title": "BSTA 551: Statistical Inference",
    "section": "Putting It All Together: Estimator Summary",
    "text": "Putting It All Together: Estimator Summary\n\n\n\n\n\n\n\n\nProperty\nFormula\nInterpretation\n\n\n\n\nBias\n\\(E(\\hat{\\theta}) - \\theta\\)\nSystematic error\n\n\nVariance\n\\(E[(\\hat{\\theta} - E(\\hat{\\theta}))^2]\\)\nRandom variability\n\n\nStd Error\n\\(\\sqrt{\\text{Var}(\\hat{\\theta})}\\)\nTypical deviation\n\n\nMSE\n\\(\\text{Var} + \\text{Bias}^2\\)\nTotal error\n\n\n\n\n\n\n\n\n\n\nGuidelines for Choosing Estimators\n\n\n\nUnbiasedness is desirable but not always essential\nLower variance/SE means more precision\nMSE provides a single criterion combining both\nSometimes biased estimators have lower MSE!"
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#week-1-summary",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#week-1-summary",
    "title": "BSTA 551: Statistical Inference",
    "section": "Week 1 Summary",
    "text": "Week 1 Summary\nDay 1:\n\nPopulation parameters vs. sample estimates\nExpected value and variance of \\(\\bar{X}\\)\nNumerical optimization (grid search, optimize())\nBias: definition, calculation, and correction\n\nDay 2:\n\nStandard error and estimated standard error\nMean Squared Error = Variance + Bias²\nBias-variance tradeoff\nNo single “best” estimator for all situations"
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#homework-problems",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#homework-problems",
    "title": "BSTA 551: Statistical Inference",
    "section": "Homework Problems",
    "text": "Homework Problems\n\nBias Calculation: For a sample of size \\(n\\) from Exponential(\\(\\lambda\\)), the MLE is \\(\\hat{\\lambda} = 1/\\bar{X}\\). It can be shown that \\(E(\\hat{\\lambda}) = \\frac{n}{n-1}\\lambda\\). Calculate the bias and propose an unbiased estimator.\nMSE Comparison: Using simulation, compare the MSE of the standard proportion estimator vs. the add-two estimator for \\(n = 10\\) and \\(p = 0.1, 0.3, 0.5\\).\nNumerical Optimization: Use optimize() to find the MLE when you observe \\(x_1 = 2.1, x_2 = 3.5, x_3 = 1.8, x_4 = 2.9\\) from an Exponential(\\(\\lambda\\)) distribution. The likelihood is \\(L(\\lambda) = \\lambda^4 e^{-\\lambda \\sum x_i}\\)."
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#next-week-preview",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#next-week-preview",
    "title": "BSTA 551: Statistical Inference",
    "section": "Next Week Preview",
    "text": "Next Week Preview\nWeek 2: Minimum Variance Unbiased Estimators\n\nAmong all unbiased estimators, which has smallest variance?\nThe Cramér-Rao lower bound\nEfficiency of estimators\nIntroduction to Maximum Likelihood Estimation"
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#references",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#references",
    "title": "BSTA 551: Statistical Inference",
    "section": "References",
    "text": "References\n\nDevore, Berk, and Carlton. Modern Mathematical Statistics with Applications (Springer). Chapter 7.1\nChihara and Hesterberg. Mathematical Statistics with Resampling and R (Wiley). Chapter 6."
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#questions",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#questions",
    "title": "BSTA 551: Statistical Inference",
    "section": "Questions?",
    "text": "Questions?\nThank you!\nOffice hours: [Time/Location]\nCourse website: [URL]"
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#lesson-1summary",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#lesson-1summary",
    "title": "BSTA 551: Statistical Inference",
    "section": "Lesson 1Summary",
    "text": "Lesson 1Summary\nKey Concepts:\n\nExpected Value: \\(E(\\bar{X}) = \\mu\\) (sample mean is centered at population mean)\nVariance: \\(\\text{Var}(\\bar{X}) = \\sigma^2/n\\) (precision improves with larger \\(n\\))\nOptimization: Finding maximum/minimum values numerically\n\nGrid search: try many values\noptimize(): efficient numerical search\n\nBias: \\(\\text{Bias}(\\hat{\\theta}) = E(\\hat{\\theta}) - \\theta\\)\n\nUnbiased if \\(E(\\hat{\\theta}) = \\theta\\)\nCan sometimes correct biased estimators"
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#lesson-1-practice-problems",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#lesson-1-practice-problems",
    "title": "BSTA 551: Statistical Inference",
    "section": "Lesson 1 Practice Problems",
    "text": "Lesson 1 Practice Problems\n\nCalculate \\(E(\\bar{X})\\) and \\(\\text{Var}(\\bar{X})\\) for a sample of size \\(n = 16\\) from a population with \\(\\mu = 50\\) and \\(\\sigma = 12\\).\nUse optimize() to find the MLE for \\(p\\) when you observe 23 successes in 40 trials.\nFor a Uniform[0, \\(\\theta\\)] distribution with \\(n = 20\\) observations and \\(\\theta = 50\\), calculate:\n\nThe expected value of the maximum\nThe bias of using the maximum as an estimator\nThe corrected unbiased estimator"
  },
  {
    "objectID": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#lesson-1-summary",
    "href": "lessons/01_Intro_Point_Estimation/01_Intro_Point_Estimation.html#lesson-1-summary",
    "title": "BSTA 551: Statistical Inference",
    "section": "Lesson 1 Summary",
    "text": "Lesson 1 Summary\nKey Concepts:\n\nExpected Value: \\(E(\\bar{X}) = \\mu\\) (sample mean is centered at population mean)\nVariance: \\(\\text{Var}(\\bar{X}) = \\sigma^2/n\\) (precision improves with larger \\(n\\))\nOptimization: Finding maximum/minimum values numerically\n\nGrid search: try many values\noptimize(): efficient numerical search\n\nBias: \\(\\text{Bias}(\\hat{\\theta}) = E(\\hat{\\theta}) - \\theta\\)\n\nUnbiased if \\(E(\\hat{\\theta}) = \\theta\\)\nCan sometimes correct biased estimators"
  }
]
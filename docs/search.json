[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BSTA 551: Theory of Statistical Inference",
    "section": "",
    "text": "BSTA 551: Theory of Statistical Inference\n\nWinter 2026\n \nThis is course introduces a theoretical foundation of Biostatistics for masters level students focusing on statistical inference. We will study methods of estimation (point and interval), as well as hypothesis testing. Some simulation and boostrap methods for estimation and testing will be included along with more theoretical asympototic and finite sample inference methods.\nPrerequisites: BSTA 550 Probability, familiarity with R.\n \n\n\n\n\n\n\n\nInstructor\n Dr. Jessica Minnier\n (see Sakai)\n minnier@ohsu.edu\n\n\nOffice Hours\nCharles  TBD\nCharles  TBD\nJessica  M/W 12pm-12:30pm after class\n\n\nCourse details\n Mondays, Wednesdays\n Jan 5 - March 18\n 10 AM - 12 PM\n In-person (see Sakai for room)\n\n\nContact\nE-mail or OHSU Teams is the best way to get in contact with me.\n\n\n\n\n\n\n\n\n View the source on GitHub"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Week\nDate\nLesson\nTopic\nTB\nKey Info\nSlides HTML\nSlides PDF\nSlides Notes\nExit tix\nRecord-ing\nMuddy Points\n\n\n\n\n1\n01/05\n\nWelcome\n\n\n\n\n\n\n\n\n\n\n\n\n1\nIntroduction to Statistical Inference; Statistics\n6.1, 6.2\n\n\n\n\n\n\n\n\n\n\n01/07\n2\nPoint estimation; Bias, variance, and MSE of estimators\n7.1\n\n\n\n\n\n\n\n\n\n\n01/08\n\nHW 0 due 11pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2\n01/12\n3\nChi-square, t, F distributions\n6.3, 6.4\n\n\n\n\n\n\n\n\n\n\n01/14\n4\nUnbiased estimators, MVUE\n7.1\n\n\n\n\n\n\n\n\n\n\n01/15\n\nHW 1 due 11pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3\n01/19\nNo class\n\n\n\n\n\n\n\n\n\n\n\n\n01/21\n5\nLikelihood, MLE, MME\n7.2\n\n\n\n\n\n\n\n\n\n\n01/22\n\nHW 2 due 11 pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4\n01/26\n6\nSufficient statistics, Fisher Information\n7.3\n\n\n\n\n\n\n\n\n\n\n01/28\n7\nMLE Large sample properties\n7.4\n\n\n\n\n\n\n\n\n\n\n01/29\n\nHW 3 due 11 pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5\n02/02\n8\nInterval estimation: single sample, one-sample t, population proportions\n8.1-8.4\n\n\n\n\n\n\n\n\n\n\n02/04\n9\nBootstrap confidence intervals\n8.5\n\n\n\n\n\n\n\n\n\n\n02/05\n\nHW 4 due 11 pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6\n02/09\n10\nBootstrap confidence intervals; Percentile method\n8.5\n\n\n\n\n\n\n\n\n\n\n02/11\nNo class\n\n\n\n\n\n\n\n\n\n\n\n\n02/12\n\nHW 5 due 11 pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7\n02/16\nNo class\n\n\n\n\n\n\n\n\n\n\n\n\n02/18\n11\nHypotheses and test procedures\n9.1-9.3\n\n\n\n\n\n\n\n\n\n\n02/19\n\nHW 6 due 11 pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n8\n02/23\n12\nType I and Type II errors\n9.2-9.3\n\n\n\n\n\n\n\n\n\n\n02/25\n13\nPower and sample size; P-values\n9.2, 9.4\n\n\n\n\n\n\n\n\n\n\n02/26\n\nHW 7 due 11 pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9\n03/02\n14\nNeyman-Pearson Lemma, Likelihood Ratio Tests\n9.5\n\n\n\n\n\n\n\n\n\n\n03/04\n15\nLarge-sample Z tests; Bootstrap hypothesis testing and bootstrap P-values; Relationship between testing and interval estimation\n9.6\n\n\n\n\n\n\n\n\n\n\n03/05\n\nHW 8 due 11 pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10\n03/09\n16\nTwo-sample Z and t confidence intervals and tests; Two-sample inference for population proportions\n9.5\n\n\n\n\n\n\n\n\n\n\n03/11\n17\nAnalysis of paired data; Bootstrap and permutation methods for two samples\n9.5\n\n\n\n\n\n\n\n\n\n\n03/12\n\nHW 9 due 11 pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n11\n03/16\n\nFinals week (no class unless catch up needed)\n\n\n\n\n\n\n\n\n\n\n\n03/19\n\nFinal Exam due 11 pm"
  },
  {
    "objectID": "lessons/x09_CDFs/09_CDFs.html",
    "href": "lessons/x09_CDFs/09_CDFs.html",
    "title": "Lesson 9: Cumulative distribution functions (CDFs)",
    "section": "",
    "text": "Understand the definition of cumulative distribution functions (CDFs) for discrete and continuous random variables.\nCompute CDFs for given probability mass functions (pmfs).\nCompute CDFs for given probability density functions (pdfs) and pdfs from CDFs."
  },
  {
    "objectID": "lessons/x09_CDFs/09_CDFs.html#what-is-a-cumulative-distribution-function",
    "href": "lessons/x09_CDFs/09_CDFs.html#what-is-a-cumulative-distribution-function",
    "title": "Lesson 9: Cumulative distribution functions (CDFs)",
    "section": "What is a cumulative distribution function?",
    "text": "What is a cumulative distribution function?\n\n\n\n\nCumulative distribution function (CDF) for discrete random variable\n\n\nThe cumulative distribution function (cdf) of a discrete RV \\(X\\) with pmf \\(p_X(x)\\), is defined for every value \\(x\\) by \\[F_X(x) = \\mathbb{P}(X \\leq x) = \\sum \\limits_{\\{all\\ y:\\ y\\leq x\\}}p_X(y)\\]\n\n\n\n\\(F(x)\\) is increasing or flat (never decreasing)\n\\(\\min\\limits_x F(x) = 0\\)\n\\(\\max\\limits_xF(x)=1\\)\nCDF is a step function\n\n\n\n\nCumulative distribution function (CDF) for continuous random variable\n\n\nThe cumulative distribution function (cdf) of a continuous RV \\(X\\), is the function \\(F_X(x)\\), such that for all real values of \\(x\\), \\[F_X(x)= \\mathbb{P}(X \\leq x) = \\int_{-\\infty}^x f_X(s)ds\\]\n\n\nRemarks: In general, \\(F_X(x)\\) is increasing and\n\n\\(\\lim_{x\\rightarrow -\\infty} F_X(x)= 0\\)\n\\(\\lim_{x\\rightarrow \\infty} F_X(x)= 1\\)\n\\(P(X &gt; a) = 1 - P(X \\leq a) = 1 - F_X(a)\\)\n\\(P(a \\leq X \\leq b) = F_X(b) - F_X(a)\\)"
  },
  {
    "objectID": "lessons/x09_CDFs/09_CDFs.html#how-to-define-cdfs-for-discrete-and-continuous-rvs",
    "href": "lessons/x09_CDFs/09_CDFs.html#how-to-define-cdfs-for-discrete-and-continuous-rvs",
    "title": "Lesson 9: Cumulative distribution functions (CDFs)",
    "section": "How to define CDFs for discrete and continuous RVs?",
    "text": "How to define CDFs for discrete and continuous RVs?\n\n\nDiscrete RV \\(X\\):\n\n\n\n\n\n\n\n\n\n\npmf: \\(p_X(x) = P(X=x)\\)\nCDF: \\(F_X(x) = P(X \\leq x) = \\sum\\limits_{\\{all\\ y:\\ y\\leq x\\}} p_X(y)\\)\n\n\nContinuous RV \\(X\\):\n\n\n\n\n\n\n\n\n\n\ndensity: \\(f_X(x)\\)\nprobability: \\(P(a \\leq X \\leq b) = \\int_a^b f_X(x)dx\\)\nCDF: \\(F_X(x) = P(X \\leq x) = \\int_{-\\infty}^x f_X(s)ds\\)"
  },
  {
    "objectID": "lessons/x09_CDFs/09_CDFs.html#falls-in-older-adults-revisited-15",
    "href": "lessons/x09_CDFs/09_CDFs.html#falls-in-older-adults-revisited-15",
    "title": "Lesson 9: Cumulative distribution functions (CDFs)",
    "section": "Falls in Older Adults Revisited (1/5)",
    "text": "Falls in Older Adults Revisited (1/5)\n\n\n\n\nExample 1: Falls in Older Adults\n\n\nA major public health concern is falls among older adults (age 65+). National data suggests that 25% of older adults will experience at least one fall within a given year. A community health program is tracking a random group of \\(n = 8\\) older adults for one year. Assume the likelihood of falling is independent from person to person.\nLet \\(X\\) be the random variable representing the number of individuals in this group who experience at least one fall.\n\nWrite the CDF of \\(X\\) and make a table of values.\nUse R to calculate the cumulative probability for each possible value of \\(X\\).\nPlot the CDF of \\(X\\).\nSimulate \\(X\\) for 10000 groups and plot the approximated CDF.\n\n\n\n\n\nRecall our pmf: \\[P(X = x) = \\binom{8}{x} 0.25^x 0.75^{8-x},  x= 0, 1, 2, \\dots, 8 \\]"
  },
  {
    "objectID": "lessons/x09_CDFs/09_CDFs.html#falls-in-older-adults-revisited-25",
    "href": "lessons/x09_CDFs/09_CDFs.html#falls-in-older-adults-revisited-25",
    "title": "Lesson 9: Cumulative distribution functions (CDFs)",
    "section": "Falls in Older Adults Revisited (2/5)",
    "text": "Falls in Older Adults Revisited (2/5)\n\n\n\n\nExample 1: Falls in Older Adults\n\n\n\nWrite the CDF of \\(X\\).\n\n\n\n\n\nRecall our pmf: \\[P(X = x) = \\binom{8}{x} 0.25^x 0.75^{8-x},  x= 0, 1, 2, \\dots, 8 \\]\n\\[F_X(x) = P(X \\leq x) = \\sum \\limits_{k=0}^{x} \\binom{8}{k} 0.25^y 0.75^{8-k}\\]"
  },
  {
    "objectID": "lessons/x09_CDFs/09_CDFs.html#falls-in-older-adults-revisited-35",
    "href": "lessons/x09_CDFs/09_CDFs.html#falls-in-older-adults-revisited-35",
    "title": "Lesson 9: Cumulative distribution functions (CDFs)",
    "section": "Falls in Older Adults Revisited (3/5)",
    "text": "Falls in Older Adults Revisited (3/5)\n\n\n\n\nExample 1: Falls in Older Adults\n\n\n\nUse R to calculate the cumulative probability for each possible value of \\(X\\).\n\n\n\n\nn = 8\np = 0.25\n\nfalls_cdf &lt;- tibble(\n  x = 0:n,\n  c_prob = pbinom(x, size = n, prob = p)\n)\n\n\n \n \n\nfalls_cdf\n\n# A tibble: 9 × 2\n      x c_prob\n  &lt;int&gt;  &lt;dbl&gt;\n1     0  0.100\n2     1  0.367\n3     2  0.679\n4     3  0.886\n5     4  0.973\n6     5  0.996\n7     6  1.000\n8     7  1.000\n9     8  1"
  },
  {
    "objectID": "lessons/x09_CDFs/09_CDFs.html#falls-in-older-adults-revisited-45",
    "href": "lessons/x09_CDFs/09_CDFs.html#falls-in-older-adults-revisited-45",
    "title": "Lesson 9: Cumulative distribution functions (CDFs)",
    "section": "Falls in Older Adults Revisited (4/5)",
    "text": "Falls in Older Adults Revisited (4/5)\n\n\nExample 1: Falls in Older Adults\n\n\n\nPlot the CDF of \\(X\\).\n\n\n\n\nggplot(\n  falls_cdf,\n  aes(x = x, y = c_prob)\n       ) +\n  geom_step(\n    size = 1, \n    color = \"black\"\n    ) +\n  labs(\n    x = \"Number of Falls\",\n    y = \"Cumulative Probability\",\n    title = \"CDF of X\"\n    )"
  },
  {
    "objectID": "lessons/x09_CDFs/09_CDFs.html#falls-in-older-adults-revisited-45-1",
    "href": "lessons/x09_CDFs/09_CDFs.html#falls-in-older-adults-revisited-45-1",
    "title": "Lesson 9: Cumulative distribution functions (CDFs)",
    "section": "Falls in Older Adults Revisited (4/5)",
    "text": "Falls in Older Adults Revisited (4/5)\n\n\n\n\nExample 1: Falls in Older Adults\n\n\n\nSimulate \\(X\\) for 10000 groups and plot the approximated CDF.\n\n\n\n\nset.seed(4764)\nreps = 10000\n\nsims = rbinom(n = reps, \n              size = n, \n              prob = p)\n\nsims %&gt;% head(., 14)\n\n [1] 2 1 2 1 3 3 2 2 4 2 3 0 2 0\n\nfalls2 &lt;- tibble(x = 0:n) %&gt;%\n  rowwise() %&gt;%\n  mutate(c_prob = sum(sims &lt;= x) / reps)\n\n\n\nggplot(falls2, aes(x = x, y = c_prob)) +\n  geom_step(size = 1, color = \"black\") +\n  labs(\n    title = \"Approximate CDF of X\",\n    x = \"Number of adults (x)\",\n    y = \"Approximate Cumulative Probability\"\n  )"
  },
  {
    "objectID": "lessons/x09_CDFs/09_CDFs.html#lets-demonstrate-the-cdf-with-an-example",
    "href": "lessons/x09_CDFs/09_CDFs.html#lets-demonstrate-the-cdf-with-an-example",
    "title": "Lesson 9: Cumulative distribution functions (CDFs)",
    "section": "Let’s demonstrate the CDF with an example",
    "text": "Let’s demonstrate the CDF with an example\n\n\n\n\nExample 2\n\n\nLet \\(f_X(x)= 2\\), for \\(2.5 \\leq x \\leq 3\\). Find \\(F_X(x)\\)."
  },
  {
    "objectID": "lessons/x09_CDFs/09_CDFs.html#derivatives-of-the-cdf",
    "href": "lessons/x09_CDFs/09_CDFs.html#derivatives-of-the-cdf",
    "title": "Lesson 9: Cumulative distribution functions (CDFs)",
    "section": "Derivatives of the CDF",
    "text": "Derivatives of the CDF\n\n\nTheorem 1\n\n\nIf \\(X\\) is a continuous random variable with pdf \\(f_X(x)\\) and cdf \\(F_X(x)\\), then for all real values of \\(x\\) at which \\(F'_X(x)\\) exists, \\[\\frac{d}{dx} F_X(x)= F'_X(x) = f_X(x)\\]"
  },
  {
    "objectID": "lessons/x09_CDFs/09_CDFs.html#finding-the-pdf-from-a-cdf",
    "href": "lessons/x09_CDFs/09_CDFs.html#finding-the-pdf-from-a-cdf",
    "title": "Lesson 9: Cumulative distribution functions (CDFs)",
    "section": "Finding the PDF from a CDF",
    "text": "Finding the PDF from a CDF\n\n\n\n\nExample 3\n\n\nLet \\(X\\) be a RV with cdf \\[F_X(x)= \\left\\{\n        \\begin{array}{ll}\n            0 & \\quad x &lt; 2.5 \\\\\n            2x-5 & \\quad 2.5 \\leq x \\leq 3 \\\\\n            1 & \\quad x &gt; 3\n        \\end{array}\n    \\right.\\] Find the pdf \\(f_X(x)\\)."
  },
  {
    "objectID": "lessons/x09_CDFs/09_CDFs.html#lets-go-through-another-example-17",
    "href": "lessons/x09_CDFs/09_CDFs.html#lets-go-through-another-example-17",
    "title": "Lesson 9: Cumulative distribution functions (CDFs)",
    "section": "Let’s go through another example (1/7)",
    "text": "Let’s go through another example (1/7)\n\n\n\n\nExample 4\n\n\nLet \\(X\\) be a RV with pdf \\(f_X(x)= 2e^{-2x}\\), for \\(x&gt;0\\).\n\nShow \\(f_X(x)\\) is a pdf.\nFind \\(\\mathbb{P}(1 \\leq X \\leq 3)\\).\nFind \\(F_X(x)\\).\nGiven \\(F_X(x)\\), find \\(f_X(x)\\).\nFind \\(\\mathbb{P}(X \\geq 1 | X \\leq 3)\\).\nFind the median of the distribution of \\(X\\)."
  },
  {
    "objectID": "lessons/x09_CDFs/09_CDFs.html#lets-go-through-another-example-27",
    "href": "lessons/x09_CDFs/09_CDFs.html#lets-go-through-another-example-27",
    "title": "Lesson 9: Cumulative distribution functions (CDFs)",
    "section": "Let’s go through another example (2/7)",
    "text": "Let’s go through another example (2/7)\n\n\n\n\nExample 4.1\n\n\nLet \\(X\\) be a RV with pdf \\(f_X(x)= 2e^{-2x}\\), for \\(x&gt;0\\).\n\nShow \\(f_X(x)\\) is a pdf."
  },
  {
    "objectID": "lessons/x09_CDFs/09_CDFs.html#lets-go-through-another-example-37",
    "href": "lessons/x09_CDFs/09_CDFs.html#lets-go-through-another-example-37",
    "title": "Lesson 9: Cumulative distribution functions (CDFs)",
    "section": "Let’s go through another example (3/7)",
    "text": "Let’s go through another example (3/7)\n\n\n\n\n\n\nDo this problem at home for extra practice.\n\n\n\n\n\n\n\nExample 4.2\n\n\nLet \\(X\\) be a RV with pdf \\(f_X(x)= 2e^{-2x}\\), for \\(x&gt;0\\).\n\nFind \\(\\mathbb{P}(1 \\leq X \\leq 3)\\)."
  },
  {
    "objectID": "lessons/x09_CDFs/09_CDFs.html#lets-go-through-another-example-47",
    "href": "lessons/x09_CDFs/09_CDFs.html#lets-go-through-another-example-47",
    "title": "Lesson 9: Cumulative distribution functions (CDFs)",
    "section": "Let’s go through another example (4/7)",
    "text": "Let’s go through another example (4/7)\n\n\n\n\nExample 4.3\n\n\nLet \\(X\\) be a RV with pdf \\(f_X(x)= 2e^{-2x}\\), for \\(x&gt;0\\).\n\nFind \\(F_X(x)\\)."
  },
  {
    "objectID": "lessons/x09_CDFs/09_CDFs.html#lets-go-through-another-example-57",
    "href": "lessons/x09_CDFs/09_CDFs.html#lets-go-through-another-example-57",
    "title": "Lesson 9: Cumulative distribution functions (CDFs)",
    "section": "Let’s go through another example (5/7)",
    "text": "Let’s go through another example (5/7)\n\n\n\n\n\n\nDo this problem at home for extra practice.\n\n\n\n\n\n\n\nExample 4.4\n\n\nLet \\(X\\) be a RV with pdf \\(f_X(x)= 2e^{-2x}\\), for \\(x&gt;0\\).\n\nGiven \\(F_X(x)\\), find \\(f_X(x)\\)."
  },
  {
    "objectID": "lessons/x09_CDFs/09_CDFs.html#lets-go-through-another-example-67",
    "href": "lessons/x09_CDFs/09_CDFs.html#lets-go-through-another-example-67",
    "title": "Lesson 9: Cumulative distribution functions (CDFs)",
    "section": "Let’s go through another example (6/7)",
    "text": "Let’s go through another example (6/7)\n\n\n\n\nExample 4.5\n\n\nLet \\(X\\) be a RV with pdf \\(f_X(x)= 2e^{-2x}\\), for \\(x&gt;0\\).\n\nFind \\(\\mathbb{P}(X \\geq 1 | X \\leq 3)\\)."
  },
  {
    "objectID": "lessons/x09_CDFs/09_CDFs.html#lets-go-through-another-example-77",
    "href": "lessons/x09_CDFs/09_CDFs.html#lets-go-through-another-example-77",
    "title": "Lesson 9: Cumulative distribution functions (CDFs)",
    "section": "Let’s go through another example (7/7)",
    "text": "Let’s go through another example (7/7)\n\n\n\n\nExample 4.6\n\n\nLet \\(X\\) be a RV with pdf \\(f_X(x)= 2e^{-2x}\\), for \\(x&gt;0\\).\n\nFind the median of the distribution of \\(X\\)."
  },
  {
    "objectID": "lessons/x12_Independence_Conditioning/09_Joint_distributions.html",
    "href": "lessons/x12_Independence_Conditioning/09_Joint_distributions.html",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "",
    "text": "Calculate probabilities for a pair of discrete random variables\nCalculate a joint, marginal, and conditional probability mass function (pmf)\nCalculate a joint, marginal, and conditional cumulative distribution function (CDF)"
  },
  {
    "objectID": "lessons/x12_Independence_Conditioning/09_Joint_distributions.html#learning-objectives",
    "href": "lessons/x12_Independence_Conditioning/09_Joint_distributions.html#learning-objectives",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "",
    "text": "Calculate probabilities for a pair of discrete random variables\nCalculate a joint, marginal, and conditional probability mass function (pmf)\nCalculate a joint, marginal, and conditional cumulative distribution function (CDF)"
  },
  {
    "objectID": "lessons/x12_Independence_Conditioning/09_Joint_distributions.html#where-are-we",
    "href": "lessons/x12_Independence_Conditioning/09_Joint_distributions.html#where-are-we",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "Where are we?",
    "text": "Where are we?"
  },
  {
    "objectID": "lessons/x12_Independence_Conditioning/09_Joint_distributions.html#what-is-a-joint-pmf",
    "href": "lessons/x12_Independence_Conditioning/09_Joint_distributions.html#what-is-a-joint-pmf",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "What is a joint pmf?",
    "text": "What is a joint pmf?\n\n\nDefinition: joint pmf\n\n\nThe joint pmf of a pair of discrete r.v.’s \\(X\\) and \\(Y\\) is \\[p_{X,Y}(x,y) = \\mathbb{P}(X=x\\ and\\ Y=y) = \\mathbb{P}(X=x, Y=y)\\]"
  },
  {
    "objectID": "lessons/x12_Independence_Conditioning/09_Joint_distributions.html#this-chapters-main-example",
    "href": "lessons/x12_Independence_Conditioning/09_Joint_distributions.html#this-chapters-main-example",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "This chapter’s main example",
    "text": "This chapter’s main example\n\n\n\n\nExample 1\n\n\nLet \\(X\\) and \\(Y\\) be two random draws from a box containing balls labelled 1, 2, and 3 without replacement.\n\nFind \\(p_{X,Y}(x,y)\\).\nFind \\(\\mathbb{P}(X+Y=3).\\)\nFind \\(\\mathbb{P}(Y = 1).\\)\nFind \\(\\mathbb{P}(Y \\leq 2).\\)\nFind the joint CDF \\(F_{X,Y}(x,y)\\) for the joint pmf \\(p_{X,Y}(x,y)\\)\nFind the marginal CDFs \\(F_{X}(x)\\) and \\(F_{Y}(y)\\)\nFind \\(p_{X|Y}(x|y)\\).\nAre \\(X\\) and \\(Y\\) independent? Why or why not?"
  },
  {
    "objectID": "lessons/x12_Independence_Conditioning/09_Joint_distributions.html#joint-pmf",
    "href": "lessons/x12_Independence_Conditioning/09_Joint_distributions.html#joint-pmf",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "Joint pmf",
    "text": "Joint pmf\n\n\n\n\nExample 1\n\n\nLet \\(X\\) and \\(Y\\) be two random draws from a box containing balls labelled 1, 2, and 3 without replacement.\n\nFind \\(p_{X,Y}(x,y)\\).\nFind \\(\\mathbb{P}(X+Y=3).\\)"
  },
  {
    "objectID": "lessons/x12_Independence_Conditioning/09_Joint_distributions.html#marginal-pmfs",
    "href": "lessons/x12_Independence_Conditioning/09_Joint_distributions.html#marginal-pmfs",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "Marginal pmf’s",
    "text": "Marginal pmf’s\n\n\n\n\nExample 1\n\n\nLet \\(X\\) and \\(Y\\) be two random draws from a box containing balls labelled 1, 2, and 3 without replacement.\n\nFind \\(\\mathbb{P}(Y = 1).\\)\nFind \\(\\mathbb{P}(Y \\leq 2).\\)"
  },
  {
    "objectID": "lessons/x12_Independence_Conditioning/09_Joint_distributions.html#remarks-on-the-joint-pmf",
    "href": "lessons/x12_Independence_Conditioning/09_Joint_distributions.html#remarks-on-the-joint-pmf",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "Remarks on the joint pmf",
    "text": "Remarks on the joint pmf\nSome properties of joint pmf’s:\n\nA joint pmf \\(p_{X,Y}(x,y)\\) must satisfy the following properties:\n\n\\(p_{X,Y}(x,y)\\geq 0\\) for all \\(x, y\\).\n\\(\\sum \\limits_{\\{all\\ x\\}} \\sum \\limits_{\\{all\\ y\\}} p_{X,Y}(x,y)=1\\).\n\nMarginal pmf’s:\n\n\\(p_X(x) = \\sum \\limits_{\\{all\\ y\\}} p_{X,Y}(x,y)\\)\n\\(p_Y(y) = \\sum \\limits_{\\{all\\ x\\}} p_{X,Y}(x,y)\\)"
  },
  {
    "objectID": "lessons/x12_Independence_Conditioning/09_Joint_distributions.html#what-is-a-joint-cdf",
    "href": "lessons/x12_Independence_Conditioning/09_Joint_distributions.html#what-is-a-joint-cdf",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "What is a joint CDF?",
    "text": "What is a joint CDF?\n\n\nDefinition: joint CDF\n\n\nThe joint CDF of a pair of discrete r.v.’s \\(X\\) and \\(Y\\) is \\[F_{X,Y}(x,y) = \\mathbb{P}(X \\leq x\\ and\\ Y \\leq y) = \\mathbb{P}(X \\leq x, Y \\leq y)\\]"
  },
  {
    "objectID": "lessons/x12_Independence_Conditioning/09_Joint_distributions.html#joint-cdfs",
    "href": "lessons/x12_Independence_Conditioning/09_Joint_distributions.html#joint-cdfs",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "Joint CDFs",
    "text": "Joint CDFs\n\n\n\n\nExample 1\n\n\nLet \\(X\\) and \\(Y\\) be two random draws from a box containing balls labelled 1, 2, and 3 without replacement.\n\nFind the joint CDF \\(F_{X,Y}(x,y)\\) for the joint pmf \\(p_{X,Y}(x,y)\\)"
  },
  {
    "objectID": "lessons/x12_Independence_Conditioning/09_Joint_distributions.html#marginal-cdfs",
    "href": "lessons/x12_Independence_Conditioning/09_Joint_distributions.html#marginal-cdfs",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "Marginal CDFs",
    "text": "Marginal CDFs\n\n\n\n\nExample 1\n\n\nLet \\(X\\) and \\(Y\\) be two random draws from a box containing balls labelled 1, 2, and 3 without replacement.\n\nFind the marginal CDFs \\(F_{X}(x)\\) and \\(F_{Y}(y)\\)"
  },
  {
    "objectID": "lessons/x12_Independence_Conditioning/09_Joint_distributions.html#remarks-on-the-joint-and-marginal-cdf",
    "href": "lessons/x12_Independence_Conditioning/09_Joint_distributions.html#remarks-on-the-joint-and-marginal-cdf",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "Remarks on the joint and marginal CDF",
    "text": "Remarks on the joint and marginal CDF\n\n\\(F_X(x)\\): right most columns of the CDf table (where the \\(Y\\) values are largest)\n\\(F_Y(y)\\): bottom row of the table (where X values are largest)\n\\(F_X(x)=\\lim\\limits_{y\\rightarrow\\infty}F_{X, Y}(x,y)\\)\n\\(F_Y(y)=\\lim\\limits_{x\\rightarrow\\infty}F_{X, Y}(x,y)\\)"
  },
  {
    "objectID": "lessons/x12_Independence_Conditioning/09_Joint_distributions.html#independence-and-conditioning",
    "href": "lessons/x12_Independence_Conditioning/09_Joint_distributions.html#independence-and-conditioning",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "Independence and Conditioning",
    "text": "Independence and Conditioning\nRecall that for events \\(A\\) and \\(B\\),\n\n\\(\\mathbb{P}(A|B) = \\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(B)}\\)\n\\(A\\) and \\(B\\) are independent if and only if\n\n\\(\\mathbb{P}(A|B) = \\mathbb{P}(A)\\)\n\\(\\mathbb{P}(A \\cap B) = \\mathbb{P}(A)\\cdot\\mathbb{P}(B)\\)\n\n\nIndependence and conditioning are defined similarly for r.v.’s, since \\[p_X(x) = \\mathbb{P}(X=x)\\ \\mathrm{and}\\ \\ p_{X,Y}(x,y) = \\mathbb{P}(X = x ,Y = y).\\]"
  },
  {
    "objectID": "lessons/x12_Independence_Conditioning/09_Joint_distributions.html#what-is-the-conditional-pmf",
    "href": "lessons/x12_Independence_Conditioning/09_Joint_distributions.html#what-is-the-conditional-pmf",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "What is the conditional pmf?",
    "text": "What is the conditional pmf?\n\n\nDefinition: conditional pmf\n\n\nThe conditional pmf of a pair of discrete r.v.’s \\(X\\) and \\(Y\\) is defined as \\[p_{X|Y}(x|y) = \\mathbb{P}(X = x |Y = y) = \\frac{\\mathbb{P}(X = x\\ and\\ Y = y)}{\\mathbb{P}(Y = y)}\n=\\frac{p_{X,Y}(x,y) }{p_{Y}(y) }\\] if \\(p_{Y}(y) &gt; 0\\)."
  },
  {
    "objectID": "lessons/x12_Independence_Conditioning/09_Joint_distributions.html#remarks-on-the-conditional-pmf",
    "href": "lessons/x12_Independence_Conditioning/09_Joint_distributions.html#remarks-on-the-conditional-pmf",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "Remarks on the conditional pmf",
    "text": "Remarks on the conditional pmf\nThe following properties follow from the conditional pmf definition:\n\nIf \\(X \\perp Y\\) (independent)\n\n\\(p_{X|Y}(x|y) = p_X(x)\\) for all \\(x\\) and \\(y\\)\n\\(p_{X,Y}(x,y) = p_X(x)p_Y(y)\\) for all \\(x\\) and \\(y\\)\nWhich also implies (\\(\\Rightarrow\\)): \\(F_{X,Y}(x,y) = F_X(x)F_Y(y)\\) for all \\(x\\) and \\(y\\)\n\nIf \\(X_1, X_2, …, X_n\\) are independent\n\n\\[p_{X_1, X_2, …, X_n}(x_1, x_2, …, x_n) = P(X_1=x_1, X_2=x_2, …, X_n=x_n)=\\prod\\limits_{i=1}^np_{X_i}(x_i)\\]\n\\[F_{X_1, X_2, …, X_n}(x_1, x_2, …, x_n) = P(X_1\\leq x_1, X_2\\leq x_2, …, X_n\\leq x_n)=\\prod\\limits_{i=1}^nP(X_i \\leq x_i) = \\prod\\limits_{i=1}^nF_{X_i}(x_i)\\]"
  },
  {
    "objectID": "lessons/x12_Independence_Conditioning/09_Joint_distributions.html#conditional-pmfs",
    "href": "lessons/x12_Independence_Conditioning/09_Joint_distributions.html#conditional-pmfs",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "Conditional pmf’s",
    "text": "Conditional pmf’s\n\n\n\n\nExample 1\n\n\nLet \\(X\\) and \\(Y\\) be two random draws from a box containing balls labelled 1, 2, and 3 without replacement.\n\nFind \\(p_{X|Y}(x|y)\\).\nAre \\(X\\) and \\(Y\\) independent? Why or why not?\n\n\n\n\n\nRemark:\n\nTo show that \\(X\\) and \\(Y\\) are not independent, we just need to find one counter example\nHowever, to show that they are independent, we need to verify this for all possible pairs of \\(x\\) and \\(y\\)"
  },
  {
    "objectID": "lessons/x12_Independence_Conditioning/09_Joint_distributions.html#hypothetical-4-sided-die",
    "href": "lessons/x12_Independence_Conditioning/09_Joint_distributions.html#hypothetical-4-sided-die",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "Hypothetical 4-sided die",
    "text": "Hypothetical 4-sided die\n\n\nExample 3\n\n\n\nSuppose you have a 4-sided die, and you roll the 4-sided die until the first 4 appears.\nLet \\(X\\) be the number of rolls required until (and including) the first 4.\nAfter the first 4, you keep rolling it again until you roll a 3.\nLet \\(Y\\) be the number of rolls, after the first 4, required until (and including) the 3.\n\n\nFind \\(p_{X,Y}(x,y)\\).\nUsing \\(p_{X,Y}(x,y)\\), find \\(p_{Y}(y)\\).\nFind \\(p_{X}(x)\\).\nAre \\(X\\) and \\(Y\\) are independent? Why or why not?\nFind \\(F_{X,Y}(x,y)\\)."
  },
  {
    "objectID": "lessons/x12_Independence_Conditioning/12_Independence_conditioning_muddy_points.html",
    "href": "lessons/x12_Independence_Conditioning/12_Independence_conditioning_muddy_points.html",
    "title": "Muddy Points",
    "section": "",
    "text": "If we start at a joint probability \\(f_{X,Y}(x,y)\\)…. we can look at a few probabilities:\n\nJoint probability: \\(P(a \\leq X \\leq b, c \\leq Y \\leq d)\\)\n\\[P(a \\leq X \\leq b, c \\leq Y \\leq d) = \\displaystyle\\int_{x=a}^{x=b}\\displaystyle\\int_{y=c}^{y=d} f_{X,Y}(x,y) dydx\\]\nMarginal probability: \\(P(a \\leq X \\leq b)\\)\n\\[P(a \\leq X \\leq b) = \\displaystyle\\int_{x=a}^{x=b} f_{X}(x) dx\\]\nOR\n\\[P(a \\leq X \\leq b) = \\displaystyle\\int_{x=a}^{x=b}\\displaystyle\\int_{y=-\\inf}^{y=\\inf} f_{X,Y}(x,y) dydx\\]\nConditional probability: \\(P(a \\leq X \\leq b | Y = c)\\)\n\\[P(a \\leq X \\leq b | Y=c) = \\displaystyle\\int_{x=a}^{x=b} f_{X|Y}(x|y=c) dx\\]\nYou cannot calculate \\(P(a \\leq X \\leq b | Y = c)\\) by \\(\\dfrac{P(a \\leq X \\leq b, Y=c)}{P(Y = c)}\\) because \\(P(Y = c)\\) is 0. Instead, we need to find \\(f_{X|Y}(x|y=c)\\) by \\(\\dfrac{f_{X,Y}(x,y=c)}{f_{Y}(y=c)}\\) and THEN integrate over X."
  },
  {
    "objectID": "lessons/x12_Independence_Conditioning/12_Independence_conditioning_muddy_points.html#fall-2023",
    "href": "lessons/x12_Independence_Conditioning/12_Independence_conditioning_muddy_points.html#fall-2023",
    "title": "Muddy Points",
    "section": "",
    "text": "If we start at a joint probability \\(f_{X,Y}(x,y)\\)…. we can look at a few probabilities:\n\nJoint probability: \\(P(a \\leq X \\leq b, c \\leq Y \\leq d)\\)\n\\[P(a \\leq X \\leq b, c \\leq Y \\leq d) = \\displaystyle\\int_{x=a}^{x=b}\\displaystyle\\int_{y=c}^{y=d} f_{X,Y}(x,y) dydx\\]\nMarginal probability: \\(P(a \\leq X \\leq b)\\)\n\\[P(a \\leq X \\leq b) = \\displaystyle\\int_{x=a}^{x=b} f_{X}(x) dx\\]\nOR\n\\[P(a \\leq X \\leq b) = \\displaystyle\\int_{x=a}^{x=b}\\displaystyle\\int_{y=-\\inf}^{y=\\inf} f_{X,Y}(x,y) dydx\\]\nConditional probability: \\(P(a \\leq X \\leq b | Y = c)\\)\n\\[P(a \\leq X \\leq b | Y=c) = \\displaystyle\\int_{x=a}^{x=b} f_{X|Y}(x|y=c) dx\\]\nYou cannot calculate \\(P(a \\leq X \\leq b | Y = c)\\) by \\(\\dfrac{P(a \\leq X \\leq b, Y=c)}{P(Y = c)}\\) because \\(P(Y = c)\\) is 0. Instead, we need to find \\(f_{X|Y}(x|y=c)\\) by \\(\\dfrac{f_{X,Y}(x,y=c)}{f_{Y}(y=c)}\\) and THEN integrate over X."
  },
  {
    "objectID": "lessons/x05_Equally_likely_outcomes/05_Equally_likely_outcomes.html",
    "href": "lessons/x05_Equally_likely_outcomes/05_Equally_likely_outcomes.html",
    "title": "Lesson 5: Equally Likely Outcomes and Counting",
    "section": "",
    "text": "Define permutations and combinations\nCharacterize difference between sampling with and without replacement\nCharacterize difference between sampling when order matters and when order does not matter\nCalculate the probability of sampling any combination of the following: with or without replacement and order does or does not matter"
  },
  {
    "objectID": "lessons/x05_Equally_likely_outcomes/05_Equally_likely_outcomes.html#learning-objectives",
    "href": "lessons/x05_Equally_likely_outcomes/05_Equally_likely_outcomes.html#learning-objectives",
    "title": "Lesson 5: Equally Likely Outcomes and Counting",
    "section": "",
    "text": "Define permutations and combinations\nCharacterize difference between sampling with and without replacement\nCharacterize difference between sampling when order matters and when order does not matter\nCalculate the probability of sampling any combination of the following: with or without replacement and order does or does not matter"
  },
  {
    "objectID": "lessons/x05_Equally_likely_outcomes/05_Equally_likely_outcomes.html#where-are-we",
    "href": "lessons/x05_Equally_likely_outcomes/05_Equally_likely_outcomes.html#where-are-we",
    "title": "Lesson 5: Equally Likely Outcomes and Counting",
    "section": "Where are we?",
    "text": "Where are we?"
  },
  {
    "objectID": "lessons/x05_Equally_likely_outcomes/05_Equally_likely_outcomes.html#birthday-example",
    "href": "lessons/x05_Equally_likely_outcomes/05_Equally_likely_outcomes.html#birthday-example",
    "title": "Lesson 5: Equally Likely Outcomes and Counting",
    "section": "Birthday example",
    "text": "Birthday example"
  },
  {
    "objectID": "lessons/x05_Equally_likely_outcomes/05_Equally_likely_outcomes.html#basic-counting-examples-13",
    "href": "lessons/x05_Equally_likely_outcomes/05_Equally_likely_outcomes.html#basic-counting-examples-13",
    "title": "Lesson 5: Equally Likely Outcomes and Counting",
    "section": "Basic Counting Examples (1/3)",
    "text": "Basic Counting Examples (1/3)\n\n\nExample 1\n\n\nSuppose we have 10 (distinguishable) subjects for study.\n\nHow many possible ways are there to order them?\nHow many ways to order them if we can reuse the same subject and\n\nneed 10 total?\nneed 6 total?\n\nHow many ways to order them without replacement and only need 6?\nHow many ways to choose 6 subjects without replacement if the order doesn’t matter?"
  },
  {
    "objectID": "lessons/x05_Equally_likely_outcomes/05_Equally_likely_outcomes.html#basic-counting-examples-23",
    "href": "lessons/x05_Equally_likely_outcomes/05_Equally_likely_outcomes.html#basic-counting-examples-23",
    "title": "Lesson 5: Equally Likely Outcomes and Counting",
    "section": "Basic Counting Examples (2/3)",
    "text": "Basic Counting Examples (2/3)\nSuppose we have 10 (distinguishable) subjects for study.\n\n\n\n\nExample 1.1\n\n\nHow many possible ways are there to order them?\n\n\n \n\n\nExample 1.2\n\n\nHow many ways to order them if we can reuse the same subject and\n\nneed 10 total?\nneed 6 total?"
  },
  {
    "objectID": "lessons/x05_Equally_likely_outcomes/05_Equally_likely_outcomes.html#basic-counting-examples-33",
    "href": "lessons/x05_Equally_likely_outcomes/05_Equally_likely_outcomes.html#basic-counting-examples-33",
    "title": "Lesson 5: Equally Likely Outcomes and Counting",
    "section": "Basic Counting Examples (3/3)",
    "text": "Basic Counting Examples (3/3)\nSuppose we have 10 (distinguishable) subjects for study.\n\n\n\n\nExample 1.3\n\n\nHow many ways to order them without replacement and only need 6?\n\n\n \n\n\nExample 1.4\n\n\nHow many ways to choose 6 subjects without replacement if the order doesn’t matter?"
  },
  {
    "objectID": "lessons/x05_Equally_likely_outcomes/05_Equally_likely_outcomes.html#permutations-and-combinations-1",
    "href": "lessons/x05_Equally_likely_outcomes/05_Equally_likely_outcomes.html#permutations-and-combinations-1",
    "title": "Lesson 5: Equally Likely Outcomes and Counting",
    "section": "Permutations and Combinations",
    "text": "Permutations and Combinations\n\n\nDefinition: Permutations\n\n\nPermutations are the number of ways to arrange in order \\(r\\) distinct objects when there are \\(n\\) total.\n\\[nPr = \\frac{n!}{(n-r)!}\\]\n\n\n\n\nDefinition: Combinations\n\n\nCombinations are the number of ways to choose (order doesn’t matter) \\(r\\) objects from \\(n\\) without replacement.\n\\[nCr = \\textrm{\"n choose r\"} = \\binom{n}{r} = \\frac{n!}{r!(n-r)!}\\]"
  },
  {
    "objectID": "lessons/x05_Equally_likely_outcomes/05_Equally_likely_outcomes.html#some-combinations-properties",
    "href": "lessons/x05_Equally_likely_outcomes/05_Equally_likely_outcomes.html#some-combinations-properties",
    "title": "Lesson 5: Equally Likely Outcomes and Counting",
    "section": "Some combinations properties",
    "text": "Some combinations properties\n\n\n\n\n\n\n\n\nProperty\n\nProof\n\n\n\n\n\\[\\binom{n}{r} = \\binom{n}{n-r}\\]\n\n\\(\\binom{n}{r} = \\dfrac{n!}{r!(n-r)!}\\) and \\(\\binom{n}{n-r} = \\dfrac{n!}{(n-r)!(n-(n-r))!} = \\dfrac{n!}{(n-r)!r!}\\)\n\n\n\\[\\binom{n}{1} = n\\]\n\n\\[\\binom{n}{1} = \\dfrac{n!}{1!(n-1)!}=\\dfrac{n\\cdot(n-1)\\cdot(n-2)\\cdots1}{1! \\cdot (n-1)\\cdot(n-2)\\cdots1} = \\dfrac{n\\cdot(n-1)!}{1\\cdot (n-1)!} = \\dfrac{n}{1}=n\\]\n\n\n\\[\\binom{n}{0} = 1\\]\n\n\\[\\binom{n}{0} = \\dfrac{n!}{0!(n-0)!}=\\dfrac{n!}{1\\cdot n!} = 1\\]"
  },
  {
    "objectID": "lessons/x05_Equally_likely_outcomes/05_Equally_likely_outcomes.html#table-of-different-cases",
    "href": "lessons/x05_Equally_likely_outcomes/05_Equally_likely_outcomes.html#table-of-different-cases",
    "title": "Lesson 5: Equally Likely Outcomes and Counting",
    "section": "Table of different cases",
    "text": "Table of different cases\n\n\\(n\\) = total number of objects\n\\(r\\) = number objects needed\n\n\n\n\n\n\n\n\n\nwith replacement\nwithout replacement\n\n\n\n\norder matters\n\\[n^r\\]\n\\[n\\text{P}r = \\frac{n!}{(n-r)!}\\]\n\n\norder doesn’t matter\n\\[ \\binom{n+r-1}{r}\\]\n\\[n\\text{C}r = \\binom{n}{r} = \\frac{n!}{r!(n-r)!}\\]"
  },
  {
    "objectID": "lessons/x05_Equally_likely_outcomes/05_Equally_likely_outcomes.html#enumerating-events-and-sample-space",
    "href": "lessons/x05_Equally_likely_outcomes/05_Equally_likely_outcomes.html#enumerating-events-and-sample-space",
    "title": "Lesson 5: Equally Likely Outcomes and Counting",
    "section": "Enumerating Events and Sample Space",
    "text": "Enumerating Events and Sample Space\n\nRecall, \\(P(A) = \\dfrac{|A|}{|S|}\\)\n\nWithin combinatorics, we can use the previous equations to help enumerate the event and sample space\nBut \\(A\\) might be a combination of enumerations\n\n\n \n\nFor example in the following example drawing 2 spades when order does not matter, we actually need to enumerate the other cards that are NOT spades. So the event is choosing 2 spades out of 13 AND choosing 0 other cards of 39 cards (13 hearts + 13 clubs + 13 diamonds).\nThus the probability is actually:\n\n\\[ P(\\text{two spades}) = \\dfrac{{13 \\choose 2}{39 \\choose 0}}{{52 \\choose 2}} \\]\n\nNote that \\(13 + 39 = 52\\) and \\(2+ 0 = 2\\). So the numerator’s \\(n\\)’s add up to the denominator’s \\(n\\) and the numerator’s \\(r\\)’s add up to the denominator’s \\(r\\)’s"
  },
  {
    "objectID": "lessons/x05_Equally_likely_outcomes/05_Equally_likely_outcomes.html#another-example-order-matters-vs.-not-12",
    "href": "lessons/x05_Equally_likely_outcomes/05_Equally_likely_outcomes.html#another-example-order-matters-vs.-not-12",
    "title": "Lesson 5: Equally Likely Outcomes and Counting",
    "section": "Another example: order matters vs. not (1/2)",
    "text": "Another example: order matters vs. not (1/2)\n\n\n\n\nExample 2\n\n\nSuppose we draw 2 cards from a standard deck without replacement. What is the probability that both are spades when\n\norder matters?\norder doesn’t matter?"
  },
  {
    "objectID": "lessons/x05_Equally_likely_outcomes/05_Equally_likely_outcomes.html#another-example-order-matters-vs.-not-22",
    "href": "lessons/x05_Equally_likely_outcomes/05_Equally_likely_outcomes.html#another-example-order-matters-vs.-not-22",
    "title": "Lesson 5: Equally Likely Outcomes and Counting",
    "section": "Another example: order matters vs. not (2/2)",
    "text": "Another example: order matters vs. not (2/2)\n\n\n\n\nExample 2\n\n\nSuppose we draw 2 cards from a standard deck without replacement. What is the probability that both are spades when\n\norder matters?\norder doesn’t matter?\n\n\n\n\nWe can do a simulation!\n\nset.seed(1234)\nn_sim &lt;- 1000000\ncards = c(rep(\"S\", 13), \n          rep(\"H\", 13), \n          rep(\"C\", 13), \n          rep(\"D\", 13))\ndraws &lt;- replicate(n_sim, \n                   sample(cards, 2, replace = FALSE))\ndraws[, 1:10]\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n[1,] \"C\"  \"H\"  \"D\"  \"S\"  \"C\"  \"S\"  \"C\"  \"H\"  \"H\"  \"D\"  \n[2,] \"H\"  \"C\"  \"D\"  \"S\"  \"H\"  \"C\"  \"H\"  \"S\"  \"H\"  \"H\"  \n\nspades_2 = sum( draws[1, ] == \"S\" & draws[2, ] == \"S\" )\nspades_2 / n_sim\n\n[1] 0.058727"
  },
  {
    "objectID": "lessons/x05_Equally_likely_outcomes/05_Equally_likely_outcomes_muddy_points.html",
    "href": "lessons/x05_Equally_likely_outcomes/05_Equally_likely_outcomes_muddy_points.html",
    "title": "Muddy Points",
    "section": "",
    "text": "Here’s the slide: \nLet’s think of a different scenario (Scenario 1): Let’s find the probability of drawing a spade and a heart from a standard deck of cards when drawing two cards without replacement.\nThere are 52 cards in a standard deck, with 13 spades and 13 hearts.\n\nLet’s look at the sample space size, \\(|S|\\). We need to enumerate all the possible combinations of two cards. \\[\\binom{52}{2} = \\frac{52 \\times 51}{2} = 1326\\]\nThe more difficult part is the size of our event A, \\(|A|\\). We need to enumerate all the ways to draw 1 spade and 1 heart:\n\nWays to choose 1 spade from 13: \\(\\binom{13}{1} = 13\\)\nWays to choose 1 heart from 13: \\(\\binom{13}{1} = 13\\)\nFor each potential spade we draw, there are 13 potential hearts we could draw. So we can multiply the two together to get the total number of ways to draw a spade and a heart: \\[|A| = 13 \\times 13 = 169\\]\n\n\nThen we could calculate the probability of drawing a spade and a heart as: \\[P(A) = \\frac{|A|}{|S|} = \\frac{13 \\times 13}{1326} = \\frac{169}{1326} = 0.1275\\]\nWe can keep expanding our scenario (Scenario 2): Let’s find the probability of drawing one spade, one heart, one diamond, and one club without replacement.\n\nLet’s look at the sample space size, \\(|S|\\). We need to enumerate all the possible combinations of two cards. \\[\\binom{52}{4} = \\frac{52 \\times 51 \\times 50 \\times 49}{4 \\times 3 \\times 2 \\times 1} = 270725\\]\nNow, size of event A, \\(|A|\\). We need to enumerate all the ways to draw one spade, one heart, one diamond, and one club:\n\nWays to choose 1 spade from 13: \\(\\binom{13}{1} = 13\\)\nWays to choose 1 heart from 13: \\(\\binom{13}{1} = 13\\)\nWays to choose 1 diamond from 13: \\(\\binom{13}{1} = 13\\)\nWays to choose 1 club from 13: \\(\\binom{13}{1} = 13\\)\nFor each potential spade we draw, there are 13 potential hearts we could draw. And from each specific spade and heart combo, there are 13 potential diamonds, etc. So we can multiply all four together to get the total number of ways to draw a spade and a heart: \\[|A| = 13 \\times 13 \\times 13 \\times 13 = 28561\\]\n\n\nThen we could calculate the probability of drawing a spade and a heart as: \\[P(A) = \\frac{|A|}{|S|} = \\frac{13 \\times 13 \\times 13 \\times 13}{270725} = \\frac{28561}{270725} = 0.1055\\]\nIn both the above scenarios, we calculated the size of event A only with the cards we DRAW I just wanted to emphasize that for each suit of cards, we can technically enumerate all the ways to draw a certain number of cards, including 0.\nIf we go back to scenario 1, we could also calculate the size of our event A, \\(|A|\\) as:\n\nWays to choose 1 spade from 13: \\(\\binom{13}{1} = 13\\)\nWays to choose 1 heart from 13: \\(\\binom{13}{1} = 13\\)\nWays to choose 0 diamonds from 13: \\(\\binom{13}{0} = 1\\)\nWays to choose 0 clubs from 13: \\(\\binom{13}{0} = 1\\)\nFor each potential spade we draw, there are 13 potential hearts we could draw. So we can multiply the four together to get the total number of ways to draw a spade and a heart and 0 diamonds and 0 clubs: \\[|A| = 13 \\times 13 \\times 1 \\times 1 = 169\\]"
  },
  {
    "objectID": "lessons/x05_Equally_likely_outcomes/05_Equally_likely_outcomes_muddy_points.html#fall-2025",
    "href": "lessons/x05_Equally_likely_outcomes/05_Equally_likely_outcomes_muddy_points.html#fall-2025",
    "title": "Muddy Points",
    "section": "",
    "text": "Here’s the slide: \nLet’s think of a different scenario (Scenario 1): Let’s find the probability of drawing a spade and a heart from a standard deck of cards when drawing two cards without replacement.\nThere are 52 cards in a standard deck, with 13 spades and 13 hearts.\n\nLet’s look at the sample space size, \\(|S|\\). We need to enumerate all the possible combinations of two cards. \\[\\binom{52}{2} = \\frac{52 \\times 51}{2} = 1326\\]\nThe more difficult part is the size of our event A, \\(|A|\\). We need to enumerate all the ways to draw 1 spade and 1 heart:\n\nWays to choose 1 spade from 13: \\(\\binom{13}{1} = 13\\)\nWays to choose 1 heart from 13: \\(\\binom{13}{1} = 13\\)\nFor each potential spade we draw, there are 13 potential hearts we could draw. So we can multiply the two together to get the total number of ways to draw a spade and a heart: \\[|A| = 13 \\times 13 = 169\\]\n\n\nThen we could calculate the probability of drawing a spade and a heart as: \\[P(A) = \\frac{|A|}{|S|} = \\frac{13 \\times 13}{1326} = \\frac{169}{1326} = 0.1275\\]\nWe can keep expanding our scenario (Scenario 2): Let’s find the probability of drawing one spade, one heart, one diamond, and one club without replacement.\n\nLet’s look at the sample space size, \\(|S|\\). We need to enumerate all the possible combinations of two cards. \\[\\binom{52}{4} = \\frac{52 \\times 51 \\times 50 \\times 49}{4 \\times 3 \\times 2 \\times 1} = 270725\\]\nNow, size of event A, \\(|A|\\). We need to enumerate all the ways to draw one spade, one heart, one diamond, and one club:\n\nWays to choose 1 spade from 13: \\(\\binom{13}{1} = 13\\)\nWays to choose 1 heart from 13: \\(\\binom{13}{1} = 13\\)\nWays to choose 1 diamond from 13: \\(\\binom{13}{1} = 13\\)\nWays to choose 1 club from 13: \\(\\binom{13}{1} = 13\\)\nFor each potential spade we draw, there are 13 potential hearts we could draw. And from each specific spade and heart combo, there are 13 potential diamonds, etc. So we can multiply all four together to get the total number of ways to draw a spade and a heart: \\[|A| = 13 \\times 13 \\times 13 \\times 13 = 28561\\]\n\n\nThen we could calculate the probability of drawing a spade and a heart as: \\[P(A) = \\frac{|A|}{|S|} = \\frac{13 \\times 13 \\times 13 \\times 13}{270725} = \\frac{28561}{270725} = 0.1055\\]\nIn both the above scenarios, we calculated the size of event A only with the cards we DRAW I just wanted to emphasize that for each suit of cards, we can technically enumerate all the ways to draw a certain number of cards, including 0.\nIf we go back to scenario 1, we could also calculate the size of our event A, \\(|A|\\) as:\n\nWays to choose 1 spade from 13: \\(\\binom{13}{1} = 13\\)\nWays to choose 1 heart from 13: \\(\\binom{13}{1} = 13\\)\nWays to choose 0 diamonds from 13: \\(\\binom{13}{0} = 1\\)\nWays to choose 0 clubs from 13: \\(\\binom{13}{0} = 1\\)\nFor each potential spade we draw, there are 13 potential hearts we could draw. So we can multiply the four together to get the total number of ways to draw a spade and a heart and 0 diamonds and 0 clubs: \\[|A| = 13 \\times 13 \\times 1 \\times 1 = 169\\]"
  },
  {
    "objectID": "lessons/x03_Lang_prob/03_Lang_prob_muddy_points.html",
    "href": "lessons/x03_Lang_prob/03_Lang_prob_muddy_points.html",
    "title": "Muddy Points",
    "section": "",
    "text": "See below for questions on De Morgan’s laws and propositions from last year."
  },
  {
    "objectID": "lessons/x03_Lang_prob/03_Lang_prob_muddy_points.html#fall-2025",
    "href": "lessons/x03_Lang_prob/03_Lang_prob_muddy_points.html#fall-2025",
    "title": "Muddy Points",
    "section": "",
    "text": "See below for questions on De Morgan’s laws and propositions from last year."
  },
  {
    "objectID": "lessons/x03_Lang_prob/03_Lang_prob_muddy_points.html#fall-2024",
    "href": "lessons/x03_Lang_prob/03_Lang_prob_muddy_points.html#fall-2024",
    "title": "Muddy Points",
    "section": "Fall 2024",
    "text": "Fall 2024\n\n3. Confusion on De Morgan’s law and the high blood pressure example\nThis is in reference to the Lesson 3 notes on “BP example variation (3/3)” slide. I explained the event that at least one subject does not have high blood pressure using a venn diagram. In this venn diagram, I assumed \\(n=4\\), and I wanted to show that the union of complements is equal to the complement of unions: \\(\\bigcup\\limits_{i=1}^{n}H_i^C = \\Big(\\bigcap\\limits_{i=1}^{n}H_i\\Big)^C\\), which is De Morgan’s 2nd Law.\n\n Now we can look at \\(\\bigcup\\limits_{i=1}^{4}H_i^C\\). We first need to define \\(H_i^c\\)\n\n\n\n\n\n4. Proofs of propositions\nFurther explanations of the propositions can be found in the textbook from pages 24-27. For many of the explanations in class, I was working to produce a union of disjoint events, so that the probability could easily be calculated. Proposition 3 and 4 were specifically mentioned, so I will include some writing notes on them here:\n\nProposition 3\nIf \\(A \\subseteq B\\), then \\(\\mathbb{P}(A) \\leq \\mathbb{P}(B)\\)\nIn this proposition, I want to define event \\(B\\) as a union of disjoint events so that I can show \\(P(B)\\) is the sum of \\(P(A)\\) and some greater-than-or-equal-to 0 probability event. If the following is my venn diagram of A and B:\n\n\n\n\n\nThen I can define B as the union of disjoint events: \nIf we then take the probability of each side of the equation \\(B = A \\cup (A^c \\cap B)\\), we get \\[P(B) = P\\big(A \\cup (A^c \\cap B)\\big)\\]\nSince events \\(A\\) and \\(A^c \\cap B\\) are disjoint, the probability of their union is just: \\[P(A) + P(A^c \\cap B)\\]\nThus, our equation is now \\[P(B) = P(A) + P(A^c \\cap B)\\]\nFrom Axiom 1, we know for event \\(A^c \\cap B\\), \\(P(A^c \\cap B) \\geq 0\\).\nSo the probability of event B is the sum of the probability of event A and an event that is \\(\\geq\\) 0. This means \\(P(B) \\geq P(A)\\).\n\n\nProposition 4\n\\(\\mathbb{P}(A \\cup B) = \\mathbb{P}(A) + \\mathbb{P}(B) - \\mathbb{P}(A \\cap B)\\)\n\n\nFrom the pictures above, we can see some similar disjoint events.\nIf we look back at \\(A \\cup B\\), we can start manipulating the right side of the equation:\n\n \n\n\n\n5. Example at end of Chapter 2 slides (Venn Diagram)\nI will post this in the previous week’s Muddy Points as well. Please follow this link for my work through of the example. And here is the PDF with my work.\nSub-question: why don’t we just multiply the probability of A and B to get the intersection? This is a specific property of probability when A and B are independent. Only when A and B are independent can we conclude that \\(P(A \\cap B) = P(A)P(B)\\).\n\n\n6. Partition of events\nWe’ve been working with event partitions throughout Chapter 2, but we have not formally identified them. Partitions are advantageous to define for two reasons:\n\nThe partitions may be easier to calculate. We can then use the partitions to reconstruct other probabilities that may be more difficult to calculate\nPartitions have nice properties as a consequence of being disjoint. For example, the probability of the union of partitions is the sum of the probabilities across each partition: \\[P\\bigg(\\bigcup_{i=1}^n A_i\\bigg) = P(A_1)P(A_2)P(A_3) \\cdot \\cdot \\cdot P(A_n)\\]"
  },
  {
    "objectID": "lessons/02_Point_Estimation/02_Point_Estimation.html",
    "href": "lessons/02_Point_Estimation/02_Point_Estimation.html",
    "title": "BSTA 551: Statistical Inference",
    "section": "",
    "text": "Key concepts from Lesson 1:\n\nPopulation vs. sample; parameters vs. statistics\nSampling distributions\n\\(E(\\bar{X}) = \\mu\\) and \\(\\text{Var}(\\bar{X}) = \\sigma^2/n\\)\nNumerical optimization with optimize()\n\n. . .\nToday’s Goals:\n\nDefine point estimators formally\nUnderstand bias and how to calculate it\nLearn about standard error and precision\nIntroduce Mean Squared Error (MSE)\nExplore the bias-variance tradeoff"
  },
  {
    "objectID": "lessons/02_Point_Estimation/02_Point_Estimation.html#review-where-we-left-off",
    "href": "lessons/02_Point_Estimation/02_Point_Estimation.html#review-where-we-left-off",
    "title": "BSTA 551: Statistical Inference",
    "section": "Review: Where We Left Off",
    "text": "Review: Where We Left Off\nKey concepts from Lesson 1:\n\nPopulation vs. sample; parameters vs. statistics\nSampling distributions\n\\(E(\\bar{X}) = \\mu\\) and \\(\\text{Var}(\\bar{X}) = \\sigma^2/n\\)\nNumerical optimization with optimize()\n\n\nToday’s Goals:\n\nDefine point estimators formally\nUnderstand bias and how to calculate it\nLearn about standard error and precision\nIntroduce Mean Squared Error (MSE)\nExplore the bias-variance tradeoff"
  },
  {
    "objectID": "lessons/02_Point_Estimation/02_Point_Estimation.html#what-is-a-point-estimator-devore-7.1",
    "href": "lessons/02_Point_Estimation/02_Point_Estimation.html#what-is-a-point-estimator-devore-7.1",
    "title": "BSTA 551: Statistical Inference",
    "section": "What is a Point Estimator? (Devore 7.1)",
    "text": "What is a Point Estimator? (Devore 7.1)\n\n\n\n\n\n\nDefinitions\n\n\n\nA parameter is a fixed (but unknown) characteristic of a population (e.g., \\(\\mu\\), \\(\\sigma\\), \\(p\\))\nAn estimator is a rule/formula for calculating an estimate from sample data\nAn estimate is the actual number you calculate from a specific sample\n\n\n\n\n\nExample:\n\n\n\n\n\n\n\n\nConcept\nSymbol\nExample\n\n\n\n\nParameter\n\\(\\mu\\)\nTrue mean BP reduction\n\n\nEstimator\n\\(\\bar{X} = \\frac{1}{n}\\sum X_i\\)\nThe formula “sample mean”\n\n\nEstimate\n\\(\\bar{x} = 11.2\\)\nThe number from our data"
  },
  {
    "objectID": "lessons/02_Point_Estimation/02_Point_Estimation.html#key-distinction-estimator-vs.-estimate",
    "href": "lessons/02_Point_Estimation/02_Point_Estimation.html#key-distinction-estimator-vs.-estimate",
    "title": "BSTA 551: Statistical Inference",
    "section": "Key Distinction: Estimator vs. Estimate",
    "text": "Key Distinction: Estimator vs. Estimate\nEstimator: A random variable (before data is collected)\n\n\\(\\bar{X}\\) is a function of random variables \\(X_1, \\ldots, X_n\\)\nHas a sampling distribution\nCan calculate \\(E(\\bar{X})\\), \\(\\text{Var}(\\bar{X})\\)\n\n\nEstimate: A specific number (after data is collected)\n\n\\(\\bar{x} = 11.2\\) is a specific value\nJust one realization from the sampling distribution\n\n\n\n\n\n\n\n\n\nTip\n\n\nThink of it like this: the estimator is the recipe; the estimate is what you get when you cook it with specific ingredients."
  },
  {
    "objectID": "lessons/02_Point_Estimation/02_Point_Estimation.html#the-sampling-distribution-revisited",
    "href": "lessons/02_Point_Estimation/02_Point_Estimation.html#the-sampling-distribution-revisited",
    "title": "BSTA 551: Statistical Inference",
    "section": "The Sampling Distribution (Revisited)",
    "text": "The Sampling Distribution (Revisited)\nDifferent samples give different estimates. The sampling distribution describes this variability.\n\n\n\n\n\nSample\nSample Mean (x̄)\n\n\n\n\n1\n9.90\n\n\n2\n10.31\n\n\n3\n10.03\n\n\n4\n10.85\n\n\n5\n9.17\n\n\n6\n9.31\n\n\n\n\n\n\nEach sample gives a different estimate, but they cluster around the true value \\(\\mu = 10\\)."
  },
  {
    "objectID": "lessons/02_Point_Estimation/02_Point_Estimation.html#what-makes-a-good-estimator",
    "href": "lessons/02_Point_Estimation/02_Point_Estimation.html#what-makes-a-good-estimator",
    "title": "BSTA 551: Statistical Inference",
    "section": "What Makes a Good Estimator?",
    "text": "What Makes a Good Estimator?\nWe want estimators that are:\n\nAccurate (unbiased): On average, hits the true value\nPrecise (low variance): Estimates are clustered together\nEfficient: Best combination of accuracy and precision"
  },
  {
    "objectID": "lessons/02_Point_Estimation/02_Point_Estimation.html#bias-formal-definition",
    "href": "lessons/02_Point_Estimation/02_Point_Estimation.html#bias-formal-definition",
    "title": "BSTA 551: Statistical Inference",
    "section": "Bias: Formal Definition",
    "text": "Bias: Formal Definition\n\n\n\n\n\n\nDefinition\n\n\nThe bias of an estimator \\(\\hat{\\theta}\\) is: \\[\\text{Bias}(\\hat{\\theta}) = E(\\hat{\\theta}) - \\theta\\]\nAn estimator is unbiased if \\(\\text{Bias}(\\hat{\\theta}) = 0\\), i.e., \\(E(\\hat{\\theta}) = \\theta\\).\n\n\n\n\nInterpretation:\n\nBias measures systematic error\nPositive bias = tends to overestimate\nNegative bias = tends to underestimate\nZero bias = correct on average"
  },
  {
    "objectID": "lessons/02_Point_Estimation/02_Point_Estimation.html#worked-example-proving-sample-mean-is-unbiased",
    "href": "lessons/02_Point_Estimation/02_Point_Estimation.html#worked-example-proving-sample-mean-is-unbiased",
    "title": "BSTA 551: Statistical Inference",
    "section": "Worked Example: Proving Sample Mean is Unbiased",
    "text": "Worked Example: Proving Sample Mean is Unbiased\nClaim: The sample mean \\(\\bar{X}\\) is an unbiased estimator of \\(\\mu\\).\nProof: We need to show \\(E(\\bar{X}) = \\mu\\).\n\n\\[\\text{Bias}(\\bar{X}) = E(\\bar{X}) - \\mu\\]\n\n\nWe already showed that \\(E(\\bar{X}) = \\mu\\), so:\n\\[\\text{Bias}(\\bar{X}) = \\mu - \\mu = 0 \\checkmark\\]\n\n\nConclusion: The sample mean is unbiased for estimating the population mean."
  },
  {
    "objectID": "lessons/02_Point_Estimation/02_Point_Estimation.html#worked-example-sample-proportion",
    "href": "lessons/02_Point_Estimation/02_Point_Estimation.html#worked-example-sample-proportion",
    "title": "BSTA 551: Statistical Inference",
    "section": "Worked Example: Sample Proportion",
    "text": "Worked Example: Sample Proportion\nSetup: In a vaccine trial, \\(X\\) patients out of \\(n\\) develop immunity. The estimator is \\(\\hat{p} = X/n\\).\nClaim: \\(\\hat{p}\\) is unbiased for the true immunity rate \\(p\\).\n\nProof: Since \\(X \\sim \\text{Binomial}(n, p)\\), we know \\(E(X) = np\\).\n\\[E(\\hat{p}) = E\\left(\\frac{X}{n}\\right) = \\frac{1}{n} E(X) = \\frac{1}{n} \\cdot np = p\\]\n\n\n\\[\\text{Bias}(\\hat{p}) = E(\\hat{p}) - p = p - p = 0 \\checkmark\\]"
  },
  {
    "objectID": "lessons/02_Point_Estimation/02_Point_Estimation.html#concrete-calculation-bias-of-sample-proportion",
    "href": "lessons/02_Point_Estimation/02_Point_Estimation.html#concrete-calculation-bias-of-sample-proportion",
    "title": "BSTA 551: Statistical Inference",
    "section": "Concrete Calculation: Bias of Sample Proportion",
    "text": "Concrete Calculation: Bias of Sample Proportion\nData: In a study of 80 patients, 52 showed improvement.\n\nn &lt;- 80\nx &lt;- 52\np_hat &lt;- x / n\n\ncat(\"Sample proportion:\", p_hat, \"\\n\")\n\nSample proportion: 0.65 \n\ncat(\"If true p = 0.65, what is the bias of this single estimate?\\n\")\n\nIf true p = 0.65, what is the bias of this single estimate?\n\ncat(\"Observed - True =\", p_hat - 0.65)\n\nObserved - True = 0\n\n\n\nImportant distinction:\n\nA single estimate can be above or below the true value\nBias refers to the average behavior across many samples\nAn unbiased estimator still gives wrong answers for individual samples!"
  },
  {
    "objectID": "lessons/02_Point_Estimation/02_Point_Estimation.html#simulation-verifying-unbiasedness",
    "href": "lessons/02_Point_Estimation/02_Point_Estimation.html#simulation-verifying-unbiasedness",
    "title": "BSTA 551: Statistical Inference",
    "section": "Simulation: Verifying Unbiasedness",
    "text": "Simulation: Verifying Unbiasedness\n\n# Verify that sample proportion is unbiased\ntrue_p &lt;- 0.65\nn_patients &lt;- 80\nn_simulations &lt;- 10000\n\nproportion_simulation &lt;- tibble(sim = 1:n_simulations) |&gt; \n  mutate(\n    successes = rbinom(n_simulations, size = n_patients, prob = true_p),\n    p_hat = successes / n_patients\n  )\n\nproportion_simulation |&gt; \n  summarize(\n    true_p = true_p,\n    mean_of_estimates = mean(p_hat),\n    empirical_bias = mean(p_hat) - true_p\n  )\n\n# A tibble: 1 × 3\n  true_p mean_of_estimates empirical_bias\n   &lt;dbl&gt;             &lt;dbl&gt;          &lt;dbl&gt;\n1   0.65             0.650      -0.000183\n\n\nThe bias is essentially zero (just simulation noise)!"
  },
  {
    "objectID": "lessons/02_Point_Estimation/02_Point_Estimation.html#a-biased-estimator-the-maximum",
    "href": "lessons/02_Point_Estimation/02_Point_Estimation.html#a-biased-estimator-the-maximum",
    "title": "BSTA 551: Statistical Inference",
    "section": "A Biased Estimator: The Maximum",
    "text": "A Biased Estimator: The Maximum\nProblem: Estimate the upper bound \\(\\theta\\) of a Uniform[0, \\(\\theta\\)] distribution.\nNatural idea: Use the largest observation: \\(\\hat{\\theta} = \\max(X_1, \\ldots, X_n)\\)\n\nThink about it: Can this estimator ever overestimate \\(\\theta\\)?\n\n\nNo! The sample maximum is always ≤ the population maximum.\nThis means the estimator is biased low."
  },
  {
    "objectID": "lessons/02_Point_Estimation/02_Point_Estimation.html#calculating-the-bias-mathematically",
    "href": "lessons/02_Point_Estimation/02_Point_Estimation.html#calculating-the-bias-mathematically",
    "title": "BSTA 551: Statistical Inference",
    "section": "Calculating the Bias Mathematically",
    "text": "Calculating the Bias Mathematically\nFor \\(X_1, \\ldots, X_n \\sim \\text{Uniform}[0, \\theta]\\), it can be shown that:\n\\[E(\\max(X_1, \\ldots, X_n)) = \\frac{n}{n+1} \\theta\\]\n\nBias calculation:\n\\[\\text{Bias} = E(\\hat{\\theta}) - \\theta = \\frac{n}{n+1}\\theta - \\theta = -\\frac{\\theta}{n+1}\\]\n\n\nExample: If \\(\\theta = 10\\) and \\(n = 5\\):\n\\[\\text{Bias} = -\\frac{10}{6} = -1.67\\]\nThe estimator underestimates by about 1.67 on average."
  },
  {
    "objectID": "lessons/02_Point_Estimation/02_Point_Estimation.html#your-turn-calculate-bias",
    "href": "lessons/02_Point_Estimation/02_Point_Estimation.html#your-turn-calculate-bias",
    "title": "BSTA 551: Statistical Inference",
    "section": "Your Turn: Calculate Bias",
    "text": "Your Turn: Calculate Bias\nExercise: A lab instrument has a maximum detection limit \\(\\theta\\). We take \\(n = 9\\) measurements from Uniform[0, \\(\\theta\\)] and use the maximum as our estimate.\n\nIf \\(\\theta = 100\\), what is \\(E(\\hat{\\theta})\\)?\nWhat is the bias?\nBy what percentage does this estimator underestimate on average?\n\n\nSolution:\n\n\\(E(\\hat{\\theta}) = \\frac{9}{10} \\times 100 = 90\\)\n\\(\\text{Bias} = 90 - 100 = -10\\)\nUnderestimates by \\(\\frac{10}{100} = 10\\%\\)"
  },
  {
    "objectID": "lessons/02_Point_Estimation/02_Point_Estimation.html#simulation-visualizing-the-biased-estimator",
    "href": "lessons/02_Point_Estimation/02_Point_Estimation.html#simulation-visualizing-the-biased-estimator",
    "title": "BSTA 551: Statistical Inference",
    "section": "Simulation: Visualizing the Biased Estimator",
    "text": "Simulation: Visualizing the Biased Estimator\n\ntrue_theta &lt;- 100\nn &lt;- 9\nn_sims &lt;- 5000\n\nmax_simulation &lt;- tibble(sim = 1:n_sims) |&gt; \n  mutate(\n    max_estimate = map_dbl(sim, \\(s) max(runif(n, 0, true_theta)))\n  )\n\n# Calculate empirical bias\nmax_simulation |&gt; \n  summarize(\n    theoretical_E = n / (n + 1) * true_theta,\n    empirical_mean = mean(max_estimate),\n    theoretical_bias = -true_theta / (n + 1),\n    empirical_bias = mean(max_estimate) - true_theta\n  )"
  },
  {
    "objectID": "lessons/02_Point_Estimation/02_Point_Estimation.html#visualizing-the-biased-estimator",
    "href": "lessons/02_Point_Estimation/02_Point_Estimation.html#visualizing-the-biased-estimator",
    "title": "BSTA 551: Statistical Inference",
    "section": "Visualizing the Biased Estimator",
    "text": "Visualizing the Biased Estimator"
  },
  {
    "objectID": "lessons/02_Point_Estimation/02_Point_Estimation.html#correcting-the-bias",
    "href": "lessons/02_Point_Estimation/02_Point_Estimation.html#correcting-the-bias",
    "title": "BSTA 551: Statistical Inference",
    "section": "Correcting the Bias",
    "text": "Correcting the Bias\nIdea: Multiply by a correction factor to “un-bias” the estimator.\nSince \\(E(\\max) = \\frac{n}{n+1}\\theta\\), we can define:\n\\[\\hat{\\theta}_{\\text{unbiased}} = \\frac{n+1}{n} \\cdot \\max(X_1, \\ldots, X_n)\\]\n\nCheck:\n\\[E(\\hat{\\theta}_{\\text{unbiased}}) = \\frac{n+1}{n} \\cdot E(\\max) = \\frac{n+1}{n} \\cdot \\frac{n}{n+1}\\theta = \\theta \\checkmark\\]"
  },
  {
    "objectID": "lessons/02_Point_Estimation/02_Point_Estimation.html#your-turn-apply-the-correction",
    "href": "lessons/02_Point_Estimation/02_Point_Estimation.html#your-turn-apply-the-correction",
    "title": "BSTA 551: Statistical Inference",
    "section": "Your Turn: Apply the Correction",
    "text": "Your Turn: Apply the Correction\nExercise: Using the lab instrument example with \\(n = 9\\) and \\(\\theta = 100\\):\n\nIf you observe \\(\\max = 92\\), what is the biased estimate?\nWhat is the unbiased estimate?\n\n\nSolution:\n\nBiased estimate: \\(\\hat{\\theta}_b = 92\\)\nUnbiased estimate: \\(\\hat{\\theta}_u = \\frac{10}{9} \\times 92 = 102.2\\)\n\n\nobserved_max &lt;- 92\nn &lt;- 9\n\nbiased_est &lt;- observed_max\nunbiased_est &lt;- (n + 1) / n * observed_max\n\ncat(\"Biased estimate:\", biased_est, \"\\n\")\n\nBiased estimate: 92 \n\ncat(\"Unbiased estimate:\", round(unbiased_est, 1))\n\nUnbiased estimate: 102.2"
  },
  {
    "objectID": "lessons/02_Point_Estimation/02_Point_Estimation.html#standard-error-definition",
    "href": "lessons/02_Point_Estimation/02_Point_Estimation.html#standard-error-definition",
    "title": "BSTA 551: Statistical Inference",
    "section": "Standard Error: Definition",
    "text": "Standard Error: Definition\n\n\n\n\n\n\nDefinition\n\n\nThe standard error of an estimator is its standard deviation: \\[SE(\\hat{\\theta}) = \\sqrt{\\text{Var}(\\hat{\\theta})}\\]\n\n\n\n\nKey Standard Errors:\n\n\n\nEstimator\nStandard Error\n\n\n\n\nSample mean \\(\\bar{X}\\)\n\\(\\frac{\\sigma}{\\sqrt{n}}\\)\n\n\nSample proportion \\(\\hat{p}\\)\n\\(\\sqrt{\\frac{p(1-p)}{n}}\\)"
  },
  {
    "objectID": "lessons/02_Point_Estimation/02_Point_Estimation.html#worked-example-standard-error-of-sample-mean",
    "href": "lessons/02_Point_Estimation/02_Point_Estimation.html#worked-example-standard-error-of-sample-mean",
    "title": "BSTA 551: Statistical Inference",
    "section": "Worked Example: Standard Error of Sample Mean",
    "text": "Worked Example: Standard Error of Sample Mean\nProblem: In a blood pressure study, the population SD is \\(\\sigma = 15\\) mmHg. Calculate the standard error of \\(\\bar{X}\\) for sample sizes \\(n = 25\\) and \\(n = 100\\).\n\nSolution:\nFor \\(n = 25\\): \\[SE(\\bar{X}) = \\frac{15}{\\sqrt{25}} = \\frac{15}{5} = 3 \\text{ mmHg}\\]\nFor \\(n = 100\\): \\[SE(\\bar{X}) = \\frac{15}{\\sqrt{100}} = \\frac{15}{10} = 1.5 \\text{ mmHg}\\]\n\n\nInterpretation: With 100 patients, our estimate is twice as precise as with 25 patients."
  },
  {
    "objectID": "lessons/02_Point_Estimation/02_Point_Estimation.html#your-turn-calculate-standard-error",
    "href": "lessons/02_Point_Estimation/02_Point_Estimation.html#your-turn-calculate-standard-error",
    "title": "BSTA 551: Statistical Inference",
    "section": "Your Turn: Calculate Standard Error",
    "text": "Your Turn: Calculate Standard Error\nExercise: A survey measures patient satisfaction on a 0-100 scale. The population standard deviation is \\(\\sigma = 20\\).\n\nWhat is the SE of \\(\\bar{X}\\) for \\(n = 16\\) patients?\nWhat sample size is needed to achieve \\(SE = 2\\)?\n\n\nSolutions:\n\n\\(SE = \\frac{20}{\\sqrt{16}} = \\frac{20}{4} = 5\\)\nWe need \\(\\frac{20}{\\sqrt{n}} = 2\\), so \\(\\sqrt{n} = 10\\), thus \\(n = 100\\)\n\n\nsigma &lt;- 20\n# Problem 1\nse_n16 &lt;- sigma / sqrt(16)\n# Problem 2\nn_needed &lt;- (sigma / 2)^2\n\ncat(\"SE for n=16:\", se_n16, \"\\nSample size for SE=2:\", n_needed)\n\nSE for n=16: 5 \nSample size for SE=2: 100"
  },
  {
    "objectID": "lessons/02_Point_Estimation/02_Point_Estimation.html#the-problem-with-unknown-parameters",
    "href": "lessons/02_Point_Estimation/02_Point_Estimation.html#the-problem-with-unknown-parameters",
    "title": "BSTA 551: Statistical Inference",
    "section": "The Problem with Unknown Parameters",
    "text": "The Problem with Unknown Parameters\nIssue: Standard errors often involve unknown parameters!\n\nSE of \\(\\bar{X}\\) requires knowing \\(\\sigma\\)\nSE of \\(\\hat{p}\\) requires knowing \\(p\\)\n\n\nSolution: Estimated standard error — substitute estimates for unknown parameters\n\\[\\widehat{SE}(\\bar{X}) = \\frac{s}{\\sqrt{n}}\\]\n\\[\\widehat{SE}(\\hat{p}) = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\]"
  },
  {
    "objectID": "lessons/02_Point_Estimation/02_Point_Estimation.html#example-estimated-standard-error",
    "href": "lessons/02_Point_Estimation/02_Point_Estimation.html#example-estimated-standard-error",
    "title": "BSTA 551: Statistical Inference",
    "section": "Example: Estimated Standard Error",
    "text": "Example: Estimated Standard Error\n\n# Blood pressure data from 25 patients\nbp_reductions &lt;- c(12, 8, 15, 10, 7, 14, 11, 9, 13, 16,\n                   8, 12, 10, 14, 11, 9, 15, 13, 7, 12,\n                   10, 8, 14, 11, 13)\n\nn &lt;- length(bp_reductions)\nx_bar &lt;- mean(bp_reductions)\ns &lt;- sd(bp_reductions)\n\n# Estimated standard error\nse_estimated &lt;- s / sqrt(n)\n\ntibble(\n  Statistic = c(\"Sample Mean\", \"Sample SD\", \"Sample Size\", \"Estimated SE\"),\n  Value = c(x_bar, round(s, 2), n, round(se_estimated, 2))\n)\n\n# A tibble: 4 × 2\n  Statistic    Value\n  &lt;chr&gt;        &lt;dbl&gt;\n1 Sample Mean  11.3 \n2 Sample SD     2.64\n3 Sample Size  25   \n4 Estimated SE  0.53"
  },
  {
    "objectID": "lessons/02_Point_Estimation/02_Point_Estimation.html#mean-squared-error-combining-bias-and-variance",
    "href": "lessons/02_Point_Estimation/02_Point_Estimation.html#mean-squared-error-combining-bias-and-variance",
    "title": "BSTA 551: Statistical Inference",
    "section": "Mean Squared Error: Combining Bias and Variance",
    "text": "Mean Squared Error: Combining Bias and Variance\nWhat if we have to choose between a biased estimator with low variance and an unbiased estimator with high variance?\n\n\n\n\n\n\n\nDefinition: Mean Squared Error\n\n\n\\[\\text{MSE}(\\hat{\\theta}) = E[(\\hat{\\theta} - \\theta)^2]\\]\nKey Formula: \\[\\text{MSE} = \\text{Variance} + \\text{Bias}^2\\]\n\n\n\n\n\nMSE captures total error — both systematic (bias) and random (variance)."
  },
  {
    "objectID": "lessons/02_Point_Estimation/02_Point_Estimation.html#proving-the-mse-formula",
    "href": "lessons/02_Point_Estimation/02_Point_Estimation.html#proving-the-mse-formula",
    "title": "BSTA 551: Statistical Inference",
    "section": "Proving the MSE Formula",
    "text": "Proving the MSE Formula\n\\[\\text{MSE}(\\hat{\\theta}) = E[(\\hat{\\theta} - \\theta)^2]\\]\n\nAdd and subtract \\(E(\\hat{\\theta})\\):\n\\[= E[(\\hat{\\theta} - E(\\hat{\\theta}) + E(\\hat{\\theta}) - \\theta)^2]\\]\n\n\nExpand the square:\n\\[= E[(\\hat{\\theta} - E(\\hat{\\theta}))^2] + 2E[(\\hat{\\theta} - E(\\hat{\\theta}))](E(\\hat{\\theta}) - \\theta) + (E(\\hat{\\theta}) - \\theta)^2\\]\n\n\nThe middle term equals zero because \\(E[\\hat{\\theta} - E(\\hat{\\theta})] = 0\\):\n\\[= \\text{Var}(\\hat{\\theta}) + [\\text{Bias}(\\hat{\\theta})]^2\\]"
  },
  {
    "objectID": "lessons/02_Point_Estimation/02_Point_Estimation.html#worked-example-computing-mse",
    "href": "lessons/02_Point_Estimation/02_Point_Estimation.html#worked-example-computing-mse",
    "title": "BSTA 551: Statistical Inference",
    "section": "Worked Example: Computing MSE",
    "text": "Worked Example: Computing MSE\nSetup: Estimating \\(\\theta\\) from Uniform[0, \\(\\theta\\)] with \\(n = 9\\) and \\(\\theta = 100\\).\nBiased estimator: \\(\\hat{\\theta}_b = \\max(X_i)\\)\nFrom theory:\n\n\\(E(\\hat{\\theta}_b) = \\frac{9}{10}(100) = 90\\)\n\\(\\text{Var}(\\hat{\\theta}_b) = \\frac{n \\theta^2}{(n+1)^2(n+2)} = \\frac{9 \\times 100^2}{100 \\times 11} = 81.82\\)\n\n\n\\[\\text{Bias} = 90 - 100 = -10\\] \\[\\text{MSE} = 81.82 + (-10)^2 = 81.82 + 100 = 181.82\\]"
  },
  {
    "objectID": "lessons/02_Point_Estimation/02_Point_Estimation.html#your-turn-calculate-mse",
    "href": "lessons/02_Point_Estimation/02_Point_Estimation.html#your-turn-calculate-mse",
    "title": "BSTA 551: Statistical Inference",
    "section": "Your Turn: Calculate MSE",
    "text": "Your Turn: Calculate MSE\nExercise: For the unbiased estimator \\(\\hat{\\theta}_u = \\frac{n+1}{n}\\max(X_i)\\) with \\(n = 9\\) and \\(\\theta = 100\\):\n\nWhat is the bias?\nIf \\(\\text{Var}(\\hat{\\theta}_u) = 101.01\\), what is the MSE?\nWhich estimator has lower MSE: biased or unbiased?\n\n\nSolutions:\n\nBias = 0 (it’s unbiased!)\n\\(\\text{MSE} = 101.01 + 0^2 = 101.01\\)\nUnbiased has lower MSE (101.01 &lt; 181.82)\n\nBut this isn’t always the case!"
  },
  {
    "objectID": "lessons/02_Point_Estimation/02_Point_Estimation.html#the-bias-variance-tradeoff-1",
    "href": "lessons/02_Point_Estimation/02_Point_Estimation.html#the-bias-variance-tradeoff-1",
    "title": "BSTA 551: Statistical Inference",
    "section": "The Bias-Variance Tradeoff",
    "text": "The Bias-Variance Tradeoff\nSometimes a biased estimator has lower MSE than an unbiased one!\n\nExample: Estimating a proportion \\(p\\) with \\(n = 20\\) observations\nEstimator 1: Standard: \\(\\hat{p}_1 = \\frac{X}{n}\\) (unbiased)\nEstimator 2: “Add-two”: \\(\\hat{p}_2 = \\frac{X + 2}{n + 4}\\) (biased toward 0.5)"
  },
  {
    "objectID": "lessons/02_Point_Estimation/02_Point_Estimation.html#comparing-proportion-estimators-theory",
    "href": "lessons/02_Point_Estimation/02_Point_Estimation.html#comparing-proportion-estimators-theory",
    "title": "BSTA 551: Statistical Inference",
    "section": "Comparing Proportion Estimators: Theory",
    "text": "Comparing Proportion Estimators: Theory\nFor \\(\\hat{p}_1 = X/n\\) (standard):\n\nBias = 0\nVariance = \\(\\frac{p(1-p)}{n}\\)\nMSE = \\(\\frac{p(1-p)}{n}\\)\n\n\nFor \\(\\hat{p}_2 = \\frac{X+2}{n+4}\\) (add-two):\n\nBias = \\(\\frac{2 - 4p}{n+4}\\)\nVariance = \\(\\frac{np(1-p)}{(n+4)^2}\\)\nMSE = Variance + Bias²"
  },
  {
    "objectID": "lessons/02_Point_Estimation/02_Point_Estimation.html#your-turn-calculate-bias-of-add-two-estimator",
    "href": "lessons/02_Point_Estimation/02_Point_Estimation.html#your-turn-calculate-bias-of-add-two-estimator",
    "title": "BSTA 551: Statistical Inference",
    "section": "Your Turn: Calculate Bias of Add-Two Estimator",
    "text": "Your Turn: Calculate Bias of Add-Two Estimator\nExercise: For \\(n = 20\\) and \\(p = 0.3\\):\n\nCalculate the bias of \\(\\hat{p}_2 = \\frac{X+2}{n+4}\\)\n\nHint: \\(E(X) = np\\) for binomial, so \\(E(\\hat{p}_2) = \\frac{np + 2}{n + 4}\\)\n\nSolution:\n\\[E(\\hat{p}_2) = \\frac{20(0.3) + 2}{24} = \\frac{8}{24} = 0.333\\]\n\\[\\text{Bias} = 0.333 - 0.3 = 0.033\\]\nThe add-two estimator is biased toward 0.5 (and 0.333 is closer to 0.5 than 0.3 is)."
  },
  {
    "objectID": "lessons/02_Point_Estimation/02_Point_Estimation.html#simulation-comparing-the-estimators",
    "href": "lessons/02_Point_Estimation/02_Point_Estimation.html#simulation-comparing-the-estimators",
    "title": "BSTA 551: Statistical Inference",
    "section": "Simulation: Comparing the Estimators",
    "text": "Simulation: Comparing the Estimators\n\ntrue_p &lt;- 0.3\nn &lt;- 20\nn_sims &lt;- 10000\n\n# Simulate both estimators\ncomparison_sim &lt;- tibble(sim = 1:n_sims) |&gt; \n  mutate(\n    x = rbinom(n_sims, size = n, prob = true_p),\n    p_hat_standard = x / n,\n    p_hat_addtwo = (x + 2) / (n + 4)\n  )\n\n# Compare MSE\ncomparison_sim |&gt; \n  summarize(\n    `Standard Bias` = mean(p_hat_standard) - true_p,\n    `Add-Two Bias` = mean(p_hat_addtwo) - true_p,\n    `Standard Variance` = var(p_hat_standard),\n    `Add-Two Variance` = var(p_hat_addtwo),\n    `Standard MSE` = mean((p_hat_standard - true_p)^2),\n    `Add-Two MSE` = mean((p_hat_addtwo - true_p)^2)\n  ) |&gt; \n  pivot_longer(everything(), names_to = \"Metric\", values_to = \"Value\") |&gt; \n  mutate(Value = round(Value, 5))\n\n# A tibble: 6 × 2\n  Metric              Value\n  &lt;chr&gt;               &lt;dbl&gt;\n1 Standard Bias     0.00049\n2 Add-Two Bias      0.0337 \n3 Standard Variance 0.0104 \n4 Add-Two Variance  0.00722\n5 Standard MSE      0.0104 \n6 Add-Two MSE       0.00835"
  },
  {
    "objectID": "lessons/02_Point_Estimation/02_Point_Estimation.html#visualizing-the-tradeoff",
    "href": "lessons/02_Point_Estimation/02_Point_Estimation.html#visualizing-the-tradeoff",
    "title": "BSTA 551: Statistical Inference",
    "section": "Visualizing the Tradeoff",
    "text": "Visualizing the Tradeoff"
  },
  {
    "objectID": "lessons/02_Point_Estimation/02_Point_Estimation.html#mse-comparison-across-different-true-values",
    "href": "lessons/02_Point_Estimation/02_Point_Estimation.html#mse-comparison-across-different-true-values",
    "title": "BSTA 551: Statistical Inference",
    "section": "MSE Comparison Across Different True Values",
    "text": "MSE Comparison Across Different True Values\n\nKey Insight: The “best” estimator depends on the true parameter value!"
  },
  {
    "objectID": "lessons/02_Point_Estimation/02_Point_Estimation.html#medical-application-disease-prevalence",
    "href": "lessons/02_Point_Estimation/02_Point_Estimation.html#medical-application-disease-prevalence",
    "title": "BSTA 551: Statistical Inference",
    "section": "Medical Application: Disease Prevalence",
    "text": "Medical Application: Disease Prevalence\nScenario: Estimating prevalence of a rare disease (\\(p \\approx 0.05\\)) vs. a common condition (\\(p \\approx 0.5\\)).\n\n# Compare MSE at different prevalence levels\nn &lt;- 50\n\nmse_at_p &lt;- function(p, n) {\n  mse_std &lt;- p * (1-p) / n\n  bias_add2 &lt;- (2 - 4*p) / (n + 4)\n  var_add2 &lt;- n * p * (1-p) / (n + 4)^2\n  mse_add2 &lt;- var_add2 + bias_add2^2\n  \n  tibble(p = p, MSE_Standard = mse_std, MSE_AddTwo = mse_add2,\n         Better = ifelse(mse_std &lt; mse_add2, \"Standard\", \"Add-Two\"))\n}\n\nbind_rows(\n  mse_at_p(0.05, n),\n  mse_at_p(0.50, n)\n) |&gt; \n  mutate(across(where(is.numeric), \\(x) round(x, 5)))\n\n# A tibble: 2 × 4\n      p MSE_Standard MSE_AddTwo Better  \n  &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;   \n1  0.05      0.00095    0.00193 Standard\n2  0.5       0.005      0.00429 Add-Two"
  },
  {
    "objectID": "lessons/02_Point_Estimation/02_Point_Estimation.html#sample-variance-why-n-1",
    "href": "lessons/02_Point_Estimation/02_Point_Estimation.html#sample-variance-why-n-1",
    "title": "BSTA 551: Statistical Inference",
    "section": "Sample Variance: Why n-1?",
    "text": "Sample Variance: Why n-1?\nTwo formulas for sample variance:\n\\[S^2 = \\frac{\\sum(X_i - \\bar{X})^2}{n-1} \\quad \\text{vs.} \\quad \\tilde{S}^2 = \\frac{\\sum(X_i - \\bar{X})^2}{n}\\]\n\nQuestion: Why do we divide by \\(n-1\\) instead of \\(n\\)?\nAnswer: Dividing by \\(n\\) gives a biased estimator!"
  },
  {
    "objectID": "lessons/02_Point_Estimation/02_Point_Estimation.html#simulation-comparing-variance-estimators",
    "href": "lessons/02_Point_Estimation/02_Point_Estimation.html#simulation-comparing-variance-estimators",
    "title": "BSTA 551: Statistical Inference",
    "section": "Simulation: Comparing Variance Estimators",
    "text": "Simulation: Comparing Variance Estimators\n\ntrue_variance &lt;- 100  # σ² = 100\nn &lt;- 10\nn_sims &lt;- 10000\n\nvariance_sim &lt;- tibble(sim = 1:n_sims) |&gt; \n  mutate(\n    sample_data = map(sim, \\(s) rnorm(n, 0, sqrt(true_variance))),\n    s2_n_minus_1 = map_dbl(sample_data, var),\n    s2_n = map_dbl(sample_data, \\(x) sum((x - mean(x))^2) / n)\n  )\n\nvariance_sim |&gt; \n  summarize(\n    `True σ²` = true_variance,\n    `E[S² with n-1]` = mean(s2_n_minus_1),\n    `E[S² with n]` = mean(s2_n),\n    `Bias (n-1)` = mean(s2_n_minus_1) - true_variance,\n    `Bias (n)` = mean(s2_n) - true_variance\n  ) |&gt; \n  mutate(across(where(is.numeric), \\(x) round(x, 2)))\n\n# A tibble: 1 × 5\n  `True σ²` `E[S² with n-1]` `E[S² with n]` `Bias (n-1)` `Bias (n)`\n      &lt;dbl&gt;            &lt;dbl&gt;          &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n1       100             100.           90.1         0.15      -9.86"
  },
  {
    "objectID": "lessons/02_Point_Estimation/02_Point_Estimation.html#your-turn-comprehensive-example",
    "href": "lessons/02_Point_Estimation/02_Point_Estimation.html#your-turn-comprehensive-example",
    "title": "BSTA 551: Statistical Inference",
    "section": "Your Turn: Comprehensive Example",
    "text": "Your Turn: Comprehensive Example\nExercise: A clinical trial measures cholesterol reduction. Based on \\(n = 36\\) patients:\n\nSample mean: \\(\\bar{x} = 25\\) mg/dL\nSample SD: \\(s = 12\\) mg/dL\n\nCalculate:\n\nThe estimated standard error of \\(\\bar{X}\\)\nIf the true mean reduction is \\(\\mu = 24\\), and we repeated this trial many times, what would be the expected MSE of \\(\\bar{X}\\)?\n\n\nSolutions:\n\n\\(\\widehat{SE} = \\frac{12}{\\sqrt{36}} = 2\\) mg/dL\n\\(\\bar{X}\\) is unbiased, so \\(\\text{MSE} = \\text{Var}(\\bar{X}) = \\frac{\\sigma^2}{n} \\approx \\frac{144}{36} = 4\\)"
  },
  {
    "objectID": "lessons/02_Point_Estimation/02_Point_Estimation.html#putting-it-all-together-estimator-summary",
    "href": "lessons/02_Point_Estimation/02_Point_Estimation.html#putting-it-all-together-estimator-summary",
    "title": "BSTA 551: Statistical Inference",
    "section": "Putting It All Together: Estimator Summary",
    "text": "Putting It All Together: Estimator Summary\n\n\n\n\n\n\n\n\nProperty\nFormula\nInterpretation\n\n\n\n\nBias\n\\(E(\\hat{\\theta}) - \\theta\\)\nSystematic error\n\n\nVariance\n\\(E[(\\hat{\\theta} - E(\\hat{\\theta}))^2]\\)\nRandom variability\n\n\nStd Error\n\\(\\sqrt{\\text{Var}(\\hat{\\theta})}\\)\nTypical deviation\n\n\nMSE\n\\(\\text{Var} + \\text{Bias}^2\\)\nTotal error\n\n\n\n\n\n\n\n\n\n\nGuidelines for Choosing Estimators\n\n\n\nUnbiasedness is desirable but not always essential\nLower variance/SE means more precision\nMSE provides a single criterion combining both\nSometimes biased estimators have lower MSE!"
  },
  {
    "objectID": "lessons/02_Point_Estimation/02_Point_Estimation.html#lesson-2-summary",
    "href": "lessons/02_Point_Estimation/02_Point_Estimation.html#lesson-2-summary",
    "title": "BSTA 551: Statistical Inference",
    "section": "Lesson 2 Summary",
    "text": "Lesson 2 Summary\nKey Concepts:\n\nPoint Estimators: Rules for calculating estimates from data\n\nEstimator = random variable; estimate = specific value\n\nBias: \\(\\text{Bias}(\\hat{\\theta}) = E(\\hat{\\theta}) - \\theta\\)\n\nUnbiased if \\(E(\\hat{\\theta}) = \\theta\\)\nCan sometimes correct biased estimators\n\nStandard Error: \\(SE(\\hat{\\theta}) = \\sqrt{\\text{Var}(\\hat{\\theta})}\\)\n\nMeasures precision of the estimator\n\nMean Squared Error: \\(\\text{MSE} = \\text{Var} + \\text{Bias}^2\\)\n\nCaptures total estimation error\nBias-variance tradeoff: sometimes biased is better!"
  },
  {
    "objectID": "lessons/02_Point_Estimation/02_Point_Estimation.html#lesson-2-practice-problems",
    "href": "lessons/02_Point_Estimation/02_Point_Estimation.html#lesson-2-practice-problems",
    "title": "BSTA 551: Statistical Inference",
    "section": "Lesson 2 Practice Problems",
    "text": "Lesson 2 Practice Problems\n\nFor a Uniform[0, \\(\\theta\\)] distribution with \\(n = 20\\) observations and \\(\\theta = 50\\), calculate:\n\nThe expected value of the maximum\nThe bias of using the maximum as an estimator\nThe corrected unbiased estimator\n\nUsing simulation, compare the MSE of the standard proportion estimator vs. the add-two estimator for \\(n = 10\\) and \\(p = 0.1, 0.3, 0.5\\).\nFor a sample of size \\(n\\) from Exponential(\\(\\lambda\\)), the MLE is \\(\\hat{\\lambda} = 1/\\bar{X}\\). It can be shown that \\(E(\\hat{\\lambda}) = \\frac{n}{n-1}\\lambda\\). Calculate the bias and propose an unbiased estimator."
  },
  {
    "objectID": "lessons/02_Point_Estimation/02_Point_Estimation.html#next-week-preview",
    "href": "lessons/02_Point_Estimation/02_Point_Estimation.html#next-week-preview",
    "title": "BSTA 551: Statistical Inference",
    "section": "Next Week Preview",
    "text": "Next Week Preview\nWeek 2: Minimum Variance Unbiased Estimators\n\nAmong all unbiased estimators, which has smallest variance?\nThe Cramér-Rao lower bound\nEfficiency of estimators\nIntroduction to Maximum Likelihood Estimation"
  },
  {
    "objectID": "lessons/02_Point_Estimation/02_Point_Estimation.html#references",
    "href": "lessons/02_Point_Estimation/02_Point_Estimation.html#references",
    "title": "BSTA 551: Statistical Inference",
    "section": "References",
    "text": "References\n\nDevore, Berk, and Carlton. Modern Mathematical Statistics with Applications (Springer). Chapter 7.1\nChihara and Hesterberg. Mathematical Statistics with Resampling and R (Wiley). Chapter 6."
  },
  {
    "objectID": "lessons/02_Point_Estimation/02_Point_Estimation.html#questions",
    "href": "lessons/02_Point_Estimation/02_Point_Estimation.html#questions",
    "title": "BSTA 551: Statistical Inference",
    "section": "Questions?",
    "text": "Questions?\nThank you!"
  },
  {
    "objectID": "lessons/x11_Joint_distributions/11_Transformations.html",
    "href": "lessons/x11_Joint_distributions/11_Transformations.html",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "",
    "text": "Calculate probabilities for a pair of discrete random variables\nCalculate a joint, marginal, and conditional probability mass function (pmf)\nCalculate a joint, marginal, and conditional cumulative distribution function (CDF)"
  },
  {
    "objectID": "lessons/x11_Joint_distributions/11_Transformations.html#learning-objectives",
    "href": "lessons/x11_Joint_distributions/11_Transformations.html#learning-objectives",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "",
    "text": "Calculate probabilities for a pair of discrete random variables\nCalculate a joint, marginal, and conditional probability mass function (pmf)\nCalculate a joint, marginal, and conditional cumulative distribution function (CDF)"
  },
  {
    "objectID": "lessons/x11_Joint_distributions/11_Transformations.html#where-are-we",
    "href": "lessons/x11_Joint_distributions/11_Transformations.html#where-are-we",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "Where are we?",
    "text": "Where are we?"
  },
  {
    "objectID": "lessons/x11_Joint_distributions/11_Transformations.html#what-is-a-joint-pmf",
    "href": "lessons/x11_Joint_distributions/11_Transformations.html#what-is-a-joint-pmf",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "What is a joint pmf?",
    "text": "What is a joint pmf?\n\n\nDefinition: joint pmf\n\n\nThe joint pmf of a pair of discrete r.v.’s \\(X\\) and \\(Y\\) is \\[p_{X,Y}(x,y) = \\mathbb{P}(X=x\\ and\\ Y=y) = \\mathbb{P}(X=x, Y=y)\\]"
  },
  {
    "objectID": "lessons/x11_Joint_distributions/11_Transformations.html#this-chapters-main-example",
    "href": "lessons/x11_Joint_distributions/11_Transformations.html#this-chapters-main-example",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "This chapter’s main example",
    "text": "This chapter’s main example\n\n\n\n\nExample 1\n\n\nLet \\(X\\) and \\(Y\\) be two random draws from a box containing balls labelled 1, 2, and 3 without replacement.\n\nFind \\(p_{X,Y}(x,y)\\).\nFind \\(\\mathbb{P}(X+Y=3).\\)\nFind \\(\\mathbb{P}(Y = 1).\\)\nFind \\(\\mathbb{P}(Y \\leq 2).\\)\nFind the joint CDF \\(F_{X,Y}(x,y)\\) for the joint pmf \\(p_{X,Y}(x,y)\\)\nFind the marginal CDFs \\(F_{X}(x)\\) and \\(F_{Y}(y)\\)\nFind \\(p_{X|Y}(x|y)\\).\nAre \\(X\\) and \\(Y\\) independent? Why or why not?"
  },
  {
    "objectID": "lessons/x11_Joint_distributions/11_Transformations.html#joint-pmf",
    "href": "lessons/x11_Joint_distributions/11_Transformations.html#joint-pmf",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "Joint pmf",
    "text": "Joint pmf\n\n\n\n\nExample 1\n\n\nLet \\(X\\) and \\(Y\\) be two random draws from a box containing balls labelled 1, 2, and 3 without replacement.\n\nFind \\(p_{X,Y}(x,y)\\).\nFind \\(\\mathbb{P}(X+Y=3).\\)"
  },
  {
    "objectID": "lessons/x11_Joint_distributions/11_Transformations.html#marginal-pmfs",
    "href": "lessons/x11_Joint_distributions/11_Transformations.html#marginal-pmfs",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "Marginal pmf’s",
    "text": "Marginal pmf’s\n\n\n\n\nExample 1\n\n\nLet \\(X\\) and \\(Y\\) be two random draws from a box containing balls labelled 1, 2, and 3 without replacement.\n\nFind \\(\\mathbb{P}(Y = 1).\\)\nFind \\(\\mathbb{P}(Y \\leq 2).\\)"
  },
  {
    "objectID": "lessons/x11_Joint_distributions/11_Transformations.html#remarks-on-the-joint-pmf",
    "href": "lessons/x11_Joint_distributions/11_Transformations.html#remarks-on-the-joint-pmf",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "Remarks on the joint pmf",
    "text": "Remarks on the joint pmf\nSome properties of joint pmf’s:\n\nA joint pmf \\(p_{X,Y}(x,y)\\) must satisfy the following properties:\n\n\\(p_{X,Y}(x,y)\\geq 0\\) for all \\(x, y\\).\n\\(\\sum \\limits_{\\{all\\ x\\}} \\sum \\limits_{\\{all\\ y\\}} p_{X,Y}(x,y)=1\\).\n\nMarginal pmf’s:\n\n\\(p_X(x) = \\sum \\limits_{\\{all\\ y\\}} p_{X,Y}(x,y)\\)\n\\(p_Y(y) = \\sum \\limits_{\\{all\\ x\\}} p_{X,Y}(x,y)\\)"
  },
  {
    "objectID": "lessons/x11_Joint_distributions/11_Transformations.html#what-is-a-joint-cdf",
    "href": "lessons/x11_Joint_distributions/11_Transformations.html#what-is-a-joint-cdf",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "What is a joint CDF?",
    "text": "What is a joint CDF?\n\n\nDefinition: joint CDF\n\n\nThe joint CDF of a pair of discrete r.v.’s \\(X\\) and \\(Y\\) is \\[F_{X,Y}(x,y) = \\mathbb{P}(X \\leq x\\ and\\ Y \\leq y) = \\mathbb{P}(X \\leq x, Y \\leq y)\\]"
  },
  {
    "objectID": "lessons/x11_Joint_distributions/11_Transformations.html#joint-cdfs",
    "href": "lessons/x11_Joint_distributions/11_Transformations.html#joint-cdfs",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "Joint CDFs",
    "text": "Joint CDFs\n\n\n\n\nExample 1\n\n\nLet \\(X\\) and \\(Y\\) be two random draws from a box containing balls labelled 1, 2, and 3 without replacement.\n\nFind the joint CDF \\(F_{X,Y}(x,y)\\) for the joint pmf \\(p_{X,Y}(x,y)\\)"
  },
  {
    "objectID": "lessons/x11_Joint_distributions/11_Transformations.html#marginal-cdfs",
    "href": "lessons/x11_Joint_distributions/11_Transformations.html#marginal-cdfs",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "Marginal CDFs",
    "text": "Marginal CDFs\n\n\n\n\nExample 1\n\n\nLet \\(X\\) and \\(Y\\) be two random draws from a box containing balls labelled 1, 2, and 3 without replacement.\n\nFind the marginal CDFs \\(F_{X}(x)\\) and \\(F_{Y}(y)\\)"
  },
  {
    "objectID": "lessons/x11_Joint_distributions/11_Transformations.html#remarks-on-the-joint-and-marginal-cdf",
    "href": "lessons/x11_Joint_distributions/11_Transformations.html#remarks-on-the-joint-and-marginal-cdf",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "Remarks on the joint and marginal CDF",
    "text": "Remarks on the joint and marginal CDF\n\n\\(F_X(x)\\): right most columns of the CDf table (where the \\(Y\\) values are largest)\n\\(F_Y(y)\\): bottom row of the table (where X values are largest)\n\\(F_X(x)=\\lim\\limits_{y\\rightarrow\\infty}F_{X, Y}(x,y)\\)\n\\(F_Y(y)=\\lim\\limits_{x\\rightarrow\\infty}F_{X, Y}(x,y)\\)"
  },
  {
    "objectID": "lessons/x11_Joint_distributions/11_Transformations.html#independence-and-conditioning",
    "href": "lessons/x11_Joint_distributions/11_Transformations.html#independence-and-conditioning",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "Independence and Conditioning",
    "text": "Independence and Conditioning\nRecall that for events \\(A\\) and \\(B\\),\n\n\\(\\mathbb{P}(A|B) = \\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(B)}\\)\n\\(A\\) and \\(B\\) are independent if and only if\n\n\\(\\mathbb{P}(A|B) = \\mathbb{P}(A)\\)\n\\(\\mathbb{P}(A \\cap B) = \\mathbb{P}(A)\\cdot\\mathbb{P}(B)\\)\n\n\nIndependence and conditioning are defined similarly for r.v.’s, since \\[p_X(x) = \\mathbb{P}(X=x)\\ \\mathrm{and}\\ \\ p_{X,Y}(x,y) = \\mathbb{P}(X = x ,Y = y).\\]"
  },
  {
    "objectID": "lessons/x11_Joint_distributions/11_Transformations.html#what-is-the-conditional-pmf",
    "href": "lessons/x11_Joint_distributions/11_Transformations.html#what-is-the-conditional-pmf",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "What is the conditional pmf?",
    "text": "What is the conditional pmf?\n\n\nDefinition: conditional pmf\n\n\nThe conditional pmf of a pair of discrete r.v.’s \\(X\\) and \\(Y\\) is defined as \\[p_{X|Y}(x|y) = \\mathbb{P}(X = x |Y = y) = \\frac{\\mathbb{P}(X = x\\ and\\ Y = y)}{\\mathbb{P}(Y = y)}\n=\\frac{p_{X,Y}(x,y) }{p_{Y}(y) }\\] if \\(p_{Y}(y) &gt; 0\\)."
  },
  {
    "objectID": "lessons/x11_Joint_distributions/11_Transformations.html#remarks-on-the-conditional-pmf",
    "href": "lessons/x11_Joint_distributions/11_Transformations.html#remarks-on-the-conditional-pmf",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "Remarks on the conditional pmf",
    "text": "Remarks on the conditional pmf\nThe following properties follow from the conditional pmf definition:\n\nIf \\(X \\perp Y\\) (independent)\n\n\\(p_{X|Y}(x|y) = p_X(x)\\) for all \\(x\\) and \\(y\\)\n\\(p_{X,Y}(x,y) = p_X(x)p_Y(y)\\) for all \\(x\\) and \\(y\\)\nWhich also implies (\\(\\Rightarrow\\)): \\(F_{X,Y}(x,y) = F_X(x)F_Y(y)\\) for all \\(x\\) and \\(y\\)\n\nIf \\(X_1, X_2, …, X_n\\) are independent\n\n\\[p_{X_1, X_2, …, X_n}(x_1, x_2, …, x_n) = P(X_1=x_1, X_2=x_2, …, X_n=x_n)=\\prod\\limits_{i=1}^np_{X_i}(x_i)\\]\n\\[F_{X_1, X_2, …, X_n}(x_1, x_2, …, x_n) = P(X_1\\leq x_1, X_2\\leq x_2, …, X_n\\leq x_n)=\\prod\\limits_{i=1}^nP(X_i \\leq x_i) = \\prod\\limits_{i=1}^nF_{X_i}(x_i)\\]"
  },
  {
    "objectID": "lessons/x11_Joint_distributions/11_Transformations.html#conditional-pmfs",
    "href": "lessons/x11_Joint_distributions/11_Transformations.html#conditional-pmfs",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "Conditional pmf’s",
    "text": "Conditional pmf’s\n\n\n\n\nExample 1\n\n\nLet \\(X\\) and \\(Y\\) be two random draws from a box containing balls labelled 1, 2, and 3 without replacement.\n\nFind \\(p_{X|Y}(x|y)\\).\nAre \\(X\\) and \\(Y\\) independent? Why or why not?\n\n\n\n\n\nRemark:\n\nTo show that \\(X\\) and \\(Y\\) are not independent, we just need to find one counter example\nHowever, to show that they are independent, we need to verify this for all possible pairs of \\(x\\) and \\(y\\)"
  },
  {
    "objectID": "lessons/x11_Joint_distributions/11_Transformations.html#hypothetical-4-sided-die",
    "href": "lessons/x11_Joint_distributions/11_Transformations.html#hypothetical-4-sided-die",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "Hypothetical 4-sided die",
    "text": "Hypothetical 4-sided die\n\n\nExample 3\n\n\n\nSuppose you have a 4-sided die, and you roll the 4-sided die until the first 4 appears.\nLet \\(X\\) be the number of rolls required until (and including) the first 4.\nAfter the first 4, you keep rolling it again until you roll a 3.\nLet \\(Y\\) be the number of rolls, after the first 4, required until (and including) the 3.\n\n\nFind \\(p_{X,Y}(x,y)\\).\nUsing \\(p_{X,Y}(x,y)\\), find \\(p_{Y}(y)\\).\nFind \\(p_{X}(x)\\).\nAre \\(X\\) and \\(Y\\) are independent? Why or why not?\nFind \\(F_{X,Y}(x,y)\\)."
  },
  {
    "objectID": "lessons/x11_Joint_distributions/11_Joint_distributions_muddy_points.html",
    "href": "lessons/x11_Joint_distributions/11_Joint_distributions_muddy_points.html",
    "title": "Muddy Points",
    "section": "",
    "text": "Yes! \\(M\\) is just a transformation of \\(X\\) and \\(Y\\)! Think of creating a new random variable that is dependent on \\(X\\) and \\(Y\\). We have created a single random variable from two. Then we want to see for the random variable, M, what does its pdf look like?\n\n\n\nYes! If we are going from a joint pdf to a marginal, we can to integrate over one of the random variables. If we want a marginal of X, then we integrate over Y, and that will be a single integral.\nIf we are working with a joint pdf, then we need to have two integrals if we have two random variables.\n\n\n\nI feel that. In general, a transformation (say \\(M\\)) of \\(X\\) and \\(Y\\) will be in the 3rd dimension (3D plot). The joint pdf of \\(X\\) and \\(Y\\) will be in the 3rd dimension (3D plot). But once we have \\(M\\) and its CDF/pdf, those are in 2D plot of \\(M\\) vs. \\(f_M(m)\\)."
  },
  {
    "objectID": "lessons/x11_Joint_distributions/11_Joint_distributions_muddy_points.html#fall-2025",
    "href": "lessons/x11_Joint_distributions/11_Joint_distributions_muddy_points.html#fall-2025",
    "title": "Muddy Points",
    "section": "",
    "text": "Yes! \\(M\\) is just a transformation of \\(X\\) and \\(Y\\)! Think of creating a new random variable that is dependent on \\(X\\) and \\(Y\\). We have created a single random variable from two. Then we want to see for the random variable, M, what does its pdf look like?\n\n\n\nYes! If we are going from a joint pdf to a marginal, we can to integrate over one of the random variables. If we want a marginal of X, then we integrate over Y, and that will be a single integral.\nIf we are working with a joint pdf, then we need to have two integrals if we have two random variables.\n\n\n\nI feel that. In general, a transformation (say \\(M\\)) of \\(X\\) and \\(Y\\) will be in the 3rd dimension (3D plot). The joint pdf of \\(X\\) and \\(Y\\) will be in the 3rd dimension (3D plot). But once we have \\(M\\) and its CDF/pdf, those are in 2D plot of \\(M\\) vs. \\(f_M(m)\\)."
  },
  {
    "objectID": "lessons/x11_Joint_distributions/11_Joint_distributions_muddy_points.html#fall-2024",
    "href": "lessons/x11_Joint_distributions/11_Joint_distributions_muddy_points.html#fall-2024",
    "title": "Muddy Points",
    "section": "Fall 2024",
    "text": "Fall 2024\n\n1. What are we actually finding by solving the double integral. In the first example, we found the probability was 1/16 after integrating but what does 1/16 mean in relation to the random variables X and Y?\nIt means that the volume contained by \\(0\\leq X \\leq 1\\), \\(0\\leq Y \\leq 1/2\\), and their joint pdf is 1/16 of the total volume contained by \\(0\\leq X \\leq 2\\), \\(0\\leq Y \\leq 1\\), and their joint pdf. The probability for a joint pdf is now a measure of the proportion of the volume.\nThis is not be confused with a probability from marginal pdf or pdf from one RV. The probability for marginal/single RV pdfs is the proportion of the area under the pdf for a specific range of values.\n\n\n2. Here’s a 3D plot of one of our joint pdf’s\n\\[\nf_{X,Y}(x,y) = 5e^{-x-3y} \\text{ for } 0 \\leq y \\leq x/2\n\\]\n\nlibrary(plotly)\n\nx = seq(0, 5, 0.1)\ny = seq(0, max(x)/2, 0.1/2)\nfn = expand.grid(x=x,y=y)\nfn$z = ifelse(fn$y&lt;fn$x/2, 5*exp( (-1)*fn$x - 3*fn$y), NA)\n\nz = matrix(fn$z, ncol = 51, nrow = 51, byrow = T)\n\nfig &lt;- plot_ly(x = x, y=y, z=z) %&gt;% add_surface()\n\nfig"
  },
  {
    "objectID": "lessons/x11_Joint_distributions/11_Joint_distributions_muddy_points.html#fall-2023",
    "href": "lessons/x11_Joint_distributions/11_Joint_distributions_muddy_points.html#fall-2023",
    "title": "Muddy Points",
    "section": "Fall 2023",
    "text": "Fall 2023\n\n1. How do we set the bounds on a double integral?\nWhen the domain of the RVs are not dependent on each other, then we use the bounds as is. In example 2 of Chapter 26 notes, we have the joint pdf \\(f_{X,Y}(x,y)= 18 x^2 y^5\\), for\\(0 \\leq x \\leq 1, \\ 0 \\leq y \\leq 1\\). If we wanted to calculate something like \\(E(X)\\), then we could use the bounds as they are. Below is the domain for \\(x\\) and \\(y\\):\n\n\n\n\n\nHere is the integral for the expected value where we integrate over the whole domain of \\(x\\) and \\(y\\):\n\\[ E(X) = \\displaystyle\\int_0^1 \\displaystyle\\int_0^1 x (18 x^2 y^5 )dy dx \\]If we want to find the probability \\(P(0.25 \\leq X \\leq 0.5, 0.5 \\leq Y \\leq 0.75)\\), then we can look at the specific values of the probability:\n\n\n\n\n\nNote the blue lines above indicate how we integrate over \\(y\\) first from 0.5 to 0.75 and the green lines indicate how integrate over \\(x\\) first from 0.25 to 0.5. It seems like we’ve integrated over an area that isn’t within our specified probability. However, the integrated area is ONLY the overlap of the \\(x\\) and \\(y\\) bounds for the probability.\n\\[P(0.25 \\leq X \\leq 0.5, 0.5 \\leq Y \\leq 0.75) = \\displaystyle\\int_{0.25}^{0.5} \\displaystyle\\int_{0.5}^{0.75} 18 x^2 y^5 dy dx\\]\nLet’s use the same pdf, but now the domain of the two RVs is dependent on one another. We have the joint pdf \\(f_{X,Y}(x,y)= 18 x^2 y^5\\), for\\(0 \\leq x \\leq y\\leq1\\)\nIf we wanted to calculate something like \\(E(X)\\), then we need to account for fact that \\(x\\) must be less than of equal to \\(y\\). We can look back at the domain for this:\n\n\n\n\n\nNote the blue lines above still indicate how we integrate over \\(y\\) first from \\(x\\) to 1, and the green lines indicate how integrate over \\(x\\) first from 0 to 1. Once again, it seems like we’ve integrated over an area that isn’t within the domain. However, the integrated area is ONLY the overlap of the \\(x\\) and \\(y\\) bounds. Thus, once we’ve restricted \\(y\\) to the area between \\(x\\) and 1, we no longer need to restrict \\(x\\) to the are of 0 to \\(y\\).\n\\[ E(X) = \\displaystyle\\int_0^1 \\displaystyle\\int_x^1 x (18 x^2 y^5 )dy dx \\] If we want to find the probability \\(P(0.25 \\leq X \\leq 0.5, 0.5 \\leq Y \\leq 0.75)\\), then we should look back at our domain. For now, we are focusing on the orange area:\n\n\n\n\n\nBecause the orange area is totally within our domain, we can leave our integral our bounds as the exact values we specified:\n\\[P(0.25 \\leq X \\leq 0.5, 0.5 \\leq Y \\leq 0.75) = \\displaystyle\\int_{0.25}^{0.5} \\displaystyle\\int_{0.5}^{0.75} 18 x^2 y^5 dy dx\\]\nHowever, if we want the probability \\(P(0.5 \\leq X \\leq 0.75, 0.5 \\leq Y \\leq 0.75)\\), we would focus on the pink area above. We would limit one of our integrals to the \\(y=x\\) equation:\n\\[P(0.5 \\leq X \\leq 0.75, 0.5 \\leq Y \\leq 0.75) = \\displaystyle\\int_{0.5}^{0.75} \\displaystyle\\int_{x}^{0.75} 18 x^2 y^5 dy dx\\]\nOR\n\\[P(0.5 \\leq X \\leq 0.75, 0.5 \\leq Y \\leq 0.75) = \\displaystyle\\int_{0.5}^{0.75} \\displaystyle\\int_{0.5}^{y} 18 x^2 y^5 dx dy\\]\nThe key to these probabilities is that the bounds with the other variable is on the inside integral! Otherwise we end up with a answer that includes a RV."
  },
  {
    "objectID": "lessons/x11_Joint_distributions/11_Joint_distributions_review_key_info.html",
    "href": "lessons/x11_Joint_distributions/11_Joint_distributions_review_key_info.html",
    "title": "Key Info",
    "section": "",
    "text": "Midterm feedback!\n\nDid you fill out the final survey to record your name?\nDislikes/suggestions\n\nTextbook\nWant more R code\npdf lectures posted earlier\nSome in-class work\nSlower on slides + more examples:\n\nI can try to post more completed examples, but ultimately both of these are limited by the content we need to cover and the total lecture time in quarter\n\n\nLikes/helping learn\n\nPace of class\nStress-free environment\nAnnotated notes + rewatching lectures\nVisualizations\nHomework based on completion\nActively working on problems together\nUsing AI for help\n\n\nWe will be taking a pause before expected values\n\nFinish conditional pdf problem\nReview joint distributions\n\nHW 4: looked pretty good!\nCome to office hours!\n\nDon’t spend hours on a problem! Charles and I can help you!"
  },
  {
    "objectID": "lessons/x11_Joint_distributions/11_Joint_distributions_review_key_info.html#announcements",
    "href": "lessons/x11_Joint_distributions/11_Joint_distributions_review_key_info.html#announcements",
    "title": "Key Info",
    "section": "",
    "text": "Midterm feedback!\n\nDid you fill out the final survey to record your name?\nDislikes/suggestions\n\nTextbook\nWant more R code\npdf lectures posted earlier\nSome in-class work\nSlower on slides + more examples:\n\nI can try to post more completed examples, but ultimately both of these are limited by the content we need to cover and the total lecture time in quarter\n\n\nLikes/helping learn\n\nPace of class\nStress-free environment\nAnnotated notes + rewatching lectures\nVisualizations\nHomework based on completion\nActively working on problems together\nUsing AI for help\n\n\nWe will be taking a pause before expected values\n\nFinish conditional pdf problem\nReview joint distributions\n\nHW 4: looked pretty good!\nCome to office hours!\n\nDon’t spend hours on a problem! Charles and I can help you!"
  },
  {
    "objectID": "lessons/x11_Joint_distributions/11_Joint_distributions_review_key_info.html#key-dates",
    "href": "lessons/x11_Joint_distributions/11_Joint_distributions_review_key_info.html#key-dates",
    "title": "Key Info",
    "section": "Key Dates",
    "text": "Key Dates\n\nHW 05 due this Sunday!"
  },
  {
    "objectID": "lessons/x11_Joint_distributions/Old_notes/25_Joint_densities_key_info.html",
    "href": "lessons/x11_Joint_distributions/Old_notes/25_Joint_densities_key_info.html",
    "title": "Key Info",
    "section": "",
    "text": "Grading HW 6 assignment today!\n\nFeel free to turn in solutions later than 11/24 since I took so long to grade\n\nI made quite a few changes to the homeworks\n\nHW 10 is gone\nHW 9 is optional\nInstead of a HW assignment due over late November break\n\nHW 8 will include material from Week 8 and 9 and be due 12/5\n\n\nI have quite a bit of grading to catch up on!\nRealizing I never gave you mid-quarter feedback forms…\n\nLet’s chat about this and our assessment breakdown in the coming weeks\n\nAnything else?"
  },
  {
    "objectID": "lessons/x11_Joint_distributions/Old_notes/25_Joint_densities_key_info.html#announcements",
    "href": "lessons/x11_Joint_distributions/Old_notes/25_Joint_densities_key_info.html#announcements",
    "title": "Key Info",
    "section": "",
    "text": "Grading HW 6 assignment today!\n\nFeel free to turn in solutions later than 11/24 since I took so long to grade\n\nI made quite a few changes to the homeworks\n\nHW 10 is gone\nHW 9 is optional\nInstead of a HW assignment due over late November break\n\nHW 8 will include material from Week 8 and 9 and be due 12/5\n\n\nI have quite a bit of grading to catch up on!\nRealizing I never gave you mid-quarter feedback forms…\n\nLet’s chat about this and our assessment breakdown in the coming weeks\n\nAnything else?"
  },
  {
    "objectID": "lessons/x11_Joint_distributions/Old_notes/25_Joint_densities_key_info.html#key-dates",
    "href": "lessons/x11_Joint_distributions/Old_notes/25_Joint_densities_key_info.html#key-dates",
    "title": "Key Info",
    "section": "Key Dates",
    "text": "Key Dates\n\nThursday: HW 7 due\nSunday: HW 6 solutions due"
  },
  {
    "objectID": "lessons/01_Intro_Inference/01_Intro_Inference.html",
    "href": "lessons/01_Intro_Inference/01_Intro_Inference.html",
    "title": "BSTA 551: Statistical Inference",
    "section": "",
    "text": "Course Focus: How do we learn about populations from samples?\n\n\nPoint Estimation (Weeks 1-4): What’s our best guess for a parameter?\nConfidence Intervals (Weeks 5-6): What’s a plausible range?\nHypothesis Testing (Weeks 7-9): Can we make decisions from data?\nTwo-Sample Methods (Week 10): Comparing groups\n\n\n\n\n\n\n\nUnderstand the fundamental problem of statistical inference\nDistinguish between populations and samples, parameters and statistics\nUse R to simulate sampling distributions\nReview key properties of expected value and variance\n\n\n\n\n\nA pharmaceutical company is testing a new blood pressure medication.\nThe Question: What is the true average reduction in systolic blood pressure?\n. . .\nWhat we have: Data from 25 patients in a trial\n\n# Actual blood pressure reductions (mmHg) from 25 patients\nbp_reductions &lt;- c(12, 8, 15, 10, 7, 14, 11, 9, 13, 16,\n                   8, 12, 10, 14, 11, 9, 15, 13, 7, 12,\n                   10, 8, 14, 11, 13)\n\n# What's our estimate of the true mean reduction?\nmean(bp_reductions)\n\n[1] 11.28\n\n\n. . .\nThis “estimate” is the observed value of the (random variable) sample mean statistic: \\(\\bar{X} = \\sum_{i=1}^n X_i\\)\n. . .\nBut how reliable is this estimate? Would we get the same answer with different patients?"
  },
  {
    "objectID": "lessons/01_Intro_Inference/01_Intro_Inference.html#welcome-to-statistical-inference",
    "href": "lessons/01_Intro_Inference/01_Intro_Inference.html#welcome-to-statistical-inference",
    "title": "BSTA 551: Statistical Inference",
    "section": "Welcome to Statistical Inference!",
    "text": "Welcome to Statistical Inference!\nCourse Focus: How do we learn about populations from samples?\n\nPoint Estimation (Weeks 1-4): What’s our best guess for a parameter?\nConfidence Intervals (Weeks 5-6): What’s a plausible range?\nHypothesis Testing (Weeks 7-9): Can we make decisions from data?\nTwo-Sample Methods (Week 10): Comparing groups"
  },
  {
    "objectID": "lessons/01_Intro_Inference/01_Intro_Inference.html#todays-goals",
    "href": "lessons/01_Intro_Inference/01_Intro_Inference.html#todays-goals",
    "title": "BSTA 551: Statistical Inference",
    "section": "Today’s Goals",
    "text": "Today’s Goals\n\nUnderstand the fundamental problem of statistical inference\nDistinguish between populations and samples, parameters and statistics\nUse R to simulate sampling distributions\nReview key properties of expected value and variance"
  },
  {
    "objectID": "lessons/01_Intro_Inference/01_Intro_Inference.html#motivating-example-clinical-trial",
    "href": "lessons/01_Intro_Inference/01_Intro_Inference.html#motivating-example-clinical-trial",
    "title": "BSTA 551: Statistical Inference",
    "section": "Motivating Example: Clinical Trial",
    "text": "Motivating Example: Clinical Trial\nA pharmaceutical company is testing a new blood pressure medication.\nThe Question: What is the true average reduction in systolic blood pressure?\n\nWhat we have: Data from 25 patients in a trial\n\n# Actual blood pressure reductions (mmHg) from 25 patients\nbp_reductions &lt;- c(12, 8, 15, 10, 7, 14, 11, 9, 13, 16,\n                   8, 12, 10, 14, 11, 9, 15, 13, 7, 12,\n                   10, 8, 14, 11, 13)\n\n# What's our estimate of the true mean reduction?\nmean(bp_reductions)\n\n[1] 11.28\n\n\n\n\nThis “estimate” is the observed value of the (random variable) sample mean statistic: \\(\\bar{X} = \\sum_{i=1}^n X_i\\)\n\n\nBut how reliable is this estimate? Would we get the same answer with different patients?"
  },
  {
    "objectID": "lessons/01_Intro_Inference/01_Intro_Inference.html#the-big-picture-population-vs-sample",
    "href": "lessons/01_Intro_Inference/01_Intro_Inference.html#the-big-picture-population-vs-sample",
    "title": "BSTA 551: Statistical Inference",
    "section": "The Big Picture: Population vs Sample",
    "text": "The Big Picture: Population vs Sample\n\n\nKey insight: We use sample data to make inferences about population parameters."
  },
  {
    "objectID": "lessons/01_Intro_Inference/01_Intro_Inference.html#key-terminology-devore-6.1",
    "href": "lessons/01_Intro_Inference/01_Intro_Inference.html#key-terminology-devore-6.1",
    "title": "BSTA 551: Statistical Inference",
    "section": "Key Terminology (Devore 6.1)",
    "text": "Key Terminology (Devore 6.1)\n\n\n\n\n\n\nDefinitions\n\n\n\nPopulation: The entire collection of individuals or measurements of interest\nSample: A subset of the population that we actually observe\nParameter: A numerical characteristic of the population (e.g., \\(\\mu\\), \\(\\sigma\\), \\(p\\))\nStatistic: A numerical characteristic computed from sample data (e.g., \\(\\bar{X}\\), \\(S\\), \\(\\hat{p}\\))\n\n\n\n\n\nThe Central Challenge: Parameters are fixed but unknown; statistics are known but random."
  },
  {
    "objectID": "lessons/01_Intro_Inference/01_Intro_Inference.html#parameters-vs.-statistics",
    "href": "lessons/01_Intro_Inference/01_Intro_Inference.html#parameters-vs.-statistics",
    "title": "BSTA 551: Statistical Inference",
    "section": "Parameters vs. Statistics",
    "text": "Parameters vs. Statistics\n\n\n\n\n\n\n\n\nConcept\nPopulation (Parameter)\nSample (Statistic)\n\n\n\n\nMean\n\\(\\mu\\)\n\\(\\bar{X} = \\frac{1}{n}\\sum X_i\\)\n\n\nVariance\n\\(\\sigma^2\\)\n\\(S^2 = \\frac{1}{n-1}\\sum(X_i - \\bar{X})^2\\)\n\n\nProportion\n\\(p\\)\n\\(\\hat{p} = X/n\\)\n\n\nMaximum\n\\(\\theta\\) (upper bound)\n\\(\\max(X_1, \\ldots, X_n)\\)\n\n\n\n\nKey Point: Parameters use Greek letters; statistics use Roman letters or “hats.”"
  },
  {
    "objectID": "lessons/01_Intro_Inference/01_Intro_Inference.html#the-sampling-process-devore-6.2",
    "href": "lessons/01_Intro_Inference/01_Intro_Inference.html#the-sampling-process-devore-6.2",
    "title": "BSTA 551: Statistical Inference",
    "section": "The Sampling Process (Devore 6.2)",
    "text": "The Sampling Process (Devore 6.2)\nRandom Sampling Assumptions:\n\nEach observation \\(X_i\\) is a random variable\nThe \\(X_i\\) are independent of each other\nEach \\(X_i\\) has the same distribution (identically distributed)\n\n\nTogether: \\(X_1, X_2, \\ldots, X_n\\) are i.i.d. (independent and identically distributed)\n\n\n\n\n\n\n\n\nWhy This Matters\n\n\nThe i.i.d. assumption allows us to derive the properties of statistics mathematically."
  },
  {
    "objectID": "lessons/01_Intro_Inference/01_Intro_Inference.html#review-expected-value",
    "href": "lessons/01_Intro_Inference/01_Intro_Inference.html#review-expected-value",
    "title": "BSTA 551: Statistical Inference",
    "section": "Review: Expected Value",
    "text": "Review: Expected Value\nThe expected value \\(E(X)\\) is the long-run average of a random variable.\n\n\n\n\n\n\nKey Properties We’ll Use Today\n\n\n\n\\(E(c) = c\\) for any constant \\(c\\)\n\\(E(cX) = c \\cdot E(X)\\)\n\\(E(X + Y) = E(X) + E(Y)\\)\n\\(E\\left(\\sum_{i=1}^n X_i\\right) = \\sum_{i=1}^n E(X_i)\\)"
  },
  {
    "objectID": "lessons/01_Intro_Inference/01_Intro_Inference.html#worked-example-expected-value-of-sample-mean",
    "href": "lessons/01_Intro_Inference/01_Intro_Inference.html#worked-example-expected-value-of-sample-mean",
    "title": "BSTA 551: Statistical Inference",
    "section": "Worked Example: Expected Value of Sample Mean",
    "text": "Worked Example: Expected Value of Sample Mean\nProblem: If \\(X_1, X_2, \\ldots, X_n\\) are iid observations from a population with mean \\(\\mu\\), what is \\(E(\\bar{X})\\)?\n\nSolution: Let’s work through this step by step.\n\\[E(\\bar{X}) = E\\left(\\frac{1}{n}\\sum_{i=1}^n X_i\\right)\\]\n\n\n\\[= \\frac{1}{n} E\\left(\\sum_{i=1}^n X_i\\right) \\quad \\text{(Property 2: constants come out)}\\]\n\n\n\\[= \\frac{1}{n} \\sum_{i=1}^n E(X_i) \\quad \\text{(Property 4: sum of expectations)}\\]\n\n\n\\[= \\frac{1}{n} \\cdot n\\mu = \\mu \\quad \\text{(Each } E(X_i) = \\mu \\text{)}\\]"
  },
  {
    "objectID": "lessons/01_Intro_Inference/01_Intro_Inference.html#your-turn-calculate-expected-value",
    "href": "lessons/01_Intro_Inference/01_Intro_Inference.html#your-turn-calculate-expected-value",
    "title": "BSTA 551: Statistical Inference",
    "section": "Your Turn: Calculate Expected Value",
    "text": "Your Turn: Calculate Expected Value\nExercise: A hospital measures the recovery time (in days) for patients after surgery. Let \\(X_1, X_2, X_3\\) be recovery times for 3 patients. The population mean recovery time is \\(\\mu = 5\\) days.\nQuestions:\n\nWhat is \\(E(X_1)\\)?\nWhat is \\(E(X_1 + X_2 + X_3)\\)?\nWhat is \\(E(\\bar{X})\\) where \\(\\bar{X} = \\frac{X_1 + X_2 + X_3}{3}\\)?\n\n\nAnswers:\n\n\\(E(X_1) = \\mu = 5\\) days\n\\(E(X_1 + X_2 + X_3) = 5 + 5 + 5 = 15\\) days\n\\(E(\\bar{X}) = \\frac{15}{3} = 5\\) days"
  },
  {
    "objectID": "lessons/01_Intro_Inference/01_Intro_Inference.html#review-variance",
    "href": "lessons/01_Intro_Inference/01_Intro_Inference.html#review-variance",
    "title": "BSTA 551: Statistical Inference",
    "section": "Review: Variance",
    "text": "Review: Variance\nVariance measures the spread of a distribution: \\(\\text{Var}(X) = E[(X - \\mu)^2]\\)\n\n\n\n\n\n\nKey Properties We’ll Use Today\n\n\n\n\\(\\text{Var}(c) = 0\\) for any constant\n\\(\\text{Var}(cX) = c^2 \\cdot \\text{Var}(X)\\)\nIf \\(X\\) and \\(Y\\) are independent: \\(\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y)\\)"
  },
  {
    "objectID": "lessons/01_Intro_Inference/01_Intro_Inference.html#worked-example-variance-of-sample-mean",
    "href": "lessons/01_Intro_Inference/01_Intro_Inference.html#worked-example-variance-of-sample-mean",
    "title": "BSTA 551: Statistical Inference",
    "section": "Worked Example: Variance of Sample Mean",
    "text": "Worked Example: Variance of Sample Mean\nProblem: If \\(X_1, \\ldots, X_n\\) are independent observations with variance \\(\\sigma^2\\), what is \\(\\text{Var}(\\bar{X})\\)?\n\n\\[\\text{Var}(\\bar{X}) = \\text{Var}\\left(\\frac{1}{n}\\sum_{i=1}^n X_i\\right)\\]\n\n\n\\[= \\frac{1}{n^2} \\text{Var}\\left(\\sum_{i=1}^n X_i\\right) \\quad \\text{(Property 2)}\\]\n\n\n\\[= \\frac{1}{n^2} \\sum_{i=1}^n \\text{Var}(X_i) \\quad \\text{(Property 3: independence)}\\]\n\n\n\\[= \\frac{1}{n^2} \\cdot n\\sigma^2 = \\frac{\\sigma^2}{n}\\]\n\n\n\n\n\n\nKey Result\n\n\nThe variance of \\(\\bar{X}\\) decreases as sample size \\(n\\) increases!"
  },
  {
    "objectID": "lessons/01_Intro_Inference/01_Intro_Inference.html#what-is-a-sampling-distribution",
    "href": "lessons/01_Intro_Inference/01_Intro_Inference.html#what-is-a-sampling-distribution",
    "title": "BSTA 551: Statistical Inference",
    "section": "What is a Sampling Distribution?",
    "text": "What is a Sampling Distribution?\nDifferent samples give different values of a statistic. The sampling distribution describes this variability.\n\n\n\n\n\nSample\nSample Mean (x̄)\n\n\n\n\n1\n9.90\n\n\n2\n10.31\n\n\n3\n10.03\n\n\n4\n10.85\n\n\n5\n9.17\n\n\n6\n9.31\n\n\n\n\n\n\nEach sample gives a different estimate, but they cluster around the true value \\(\\mu = 10\\)."
  },
  {
    "objectID": "lessons/01_Intro_Inference/01_Intro_Inference.html#simulation-building-a-sampling-distribution",
    "href": "lessons/01_Intro_Inference/01_Intro_Inference.html#simulation-building-a-sampling-distribution",
    "title": "BSTA 551: Statistical Inference",
    "section": "Simulation: Building a Sampling Distribution",
    "text": "Simulation: Building a Sampling Distribution\n\n# Parameters\ntrue_effect &lt;- 10  # True mean BP reduction (mmHg)\ntrue_sd &lt;- 4       # Standard deviation\nn_patients &lt;- 25   # Patients per trial\nn_trials &lt;- 5000   # Number of simulated trials\n\n# Simulate many clinical trials\nsampling_distribution &lt;- tibble(trial = 1:n_trials) |&gt; \n  mutate(\n    sample_mean = map_dbl(trial, \\(t) {\n      patients &lt;- rnorm(n_patients, true_effect, true_sd)\n      mean(patients)\n    })\n  )\n\n# Visualize\nsampling_distribution |&gt; \n  ggplot(aes(x = sample_mean)) +\n  geom_histogram(bins = 50, fill = \"steelblue\", alpha = 0.7, color = \"white\") +\n  geom_vline(xintercept = true_effect, color = \"red\", linewidth = 1.5) +\n  labs(title = \"Sampling Distribution of the Sample Mean\",\n       subtitle = str_glue(\"True μ = {true_effect}, n = {n_patients}, {n_trials} simulated trials\"),\n       x = \"Sample Mean (estimated BP reduction)\", y = \"Frequency\")"
  },
  {
    "objectID": "lessons/01_Intro_Inference/01_Intro_Inference.html#simulation-seeing-variance-decrease",
    "href": "lessons/01_Intro_Inference/01_Intro_Inference.html#simulation-seeing-variance-decrease",
    "title": "BSTA 551: Statistical Inference",
    "section": "Simulation: Seeing Variance Decrease",
    "text": "Simulation: Seeing Variance Decrease\n\n# Population parameters\ntrue_mean &lt;- 120  # True mean systolic BP\ntrue_sd &lt;- 15     # Population standard deviation\n\n# Simulate sample means for different sample sizes\nsimulation_data &lt;- tibble(n = c(5, 25, 100)) |&gt; \n  cross_join(tibble(sim = 1:2000)) |&gt; \n  mutate(\n    sample_mean = map2_dbl(n, sim, \\(size, s) {\n      mean(rnorm(size, true_mean, true_sd))\n    })\n  )\n\n# Calculate observed standard deviation for each sample size\nsimulation_data |&gt; \n  group_by(n) |&gt; \n  summarize(\n    observed_sd = sd(sample_mean),\n    theoretical_sd = true_sd / sqrt(first(n))\n  )\n\n# A tibble: 3 × 3\n      n observed_sd theoretical_sd\n  &lt;dbl&gt;       &lt;dbl&gt;          &lt;dbl&gt;\n1     5        6.64           6.71\n2    25        3.00           3   \n3   100        1.50           1.5"
  },
  {
    "objectID": "lessons/01_Intro_Inference/01_Intro_Inference.html#visualizing-the-effect-of-sample-size",
    "href": "lessons/01_Intro_Inference/01_Intro_Inference.html#visualizing-the-effect-of-sample-size",
    "title": "BSTA 551: Statistical Inference",
    "section": "Visualizing the Effect of Sample Size",
    "text": "Visualizing the Effect of Sample Size"
  },
  {
    "objectID": "lessons/01_Intro_Inference/01_Intro_Inference.html#why-optimization-matters-in-statistics",
    "href": "lessons/01_Intro_Inference/01_Intro_Inference.html#why-optimization-matters-in-statistics",
    "title": "BSTA 551: Statistical Inference",
    "section": "Why Optimization Matters in Statistics",
    "text": "Why Optimization Matters in Statistics\nMany statistical methods require finding the “best” value of a parameter.\nExamples:\n\nMaximum Likelihood: Find the parameter value that makes the observed data most probable\nLeast Squares: Find the parameter value that minimizes prediction errors\nMinimum Variance: Find the estimator with the smallest spread\n\n\nThe Problem: How do we find maximums and minimums numerically?"
  },
  {
    "objectID": "lessons/01_Intro_Inference/01_Intro_Inference.html#optimization-the-graphical-intuition",
    "href": "lessons/01_Intro_Inference/01_Intro_Inference.html#optimization-the-graphical-intuition",
    "title": "BSTA 551: Statistical Inference",
    "section": "Optimization: The Graphical Intuition",
    "text": "Optimization: The Graphical Intuition\n\nThe maximum occurs where the function reaches its peak."
  },
  {
    "objectID": "lessons/01_Intro_Inference/01_Intro_Inference.html#concrete-example-finding-the-best-estimate",
    "href": "lessons/01_Intro_Inference/01_Intro_Inference.html#concrete-example-finding-the-best-estimate",
    "title": "BSTA 551: Statistical Inference",
    "section": "Concrete Example: Finding the Best Estimate",
    "text": "Concrete Example: Finding the Best Estimate\nScenario: In a clinical trial, 7 out of 10 patients respond to treatment. What’s the best estimate of the true response rate \\(p\\)?\n\nApproach: Find the value of \\(p\\) that makes observing “7 out of 10” most likely.\nThe probability of observing exactly 7 successes in 10 trials is: \\[P(X = 7) = \\binom{10}{7} p^7 (1-p)^3\\]\n\n\nQuestion: For what value of \\(p\\) is this probability largest?"
  },
  {
    "objectID": "lessons/01_Intro_Inference/01_Intro_Inference.html#grid-search-a-simple-numerical-approach",
    "href": "lessons/01_Intro_Inference/01_Intro_Inference.html#grid-search-a-simple-numerical-approach",
    "title": "BSTA 551: Statistical Inference",
    "section": "Grid Search: A Simple Numerical Approach",
    "text": "Grid Search: A Simple Numerical Approach\nIdea: Try many values and see which gives the largest result.\n\n# Try different values of p\ngrid_search &lt;- tibble(p = seq(0.01, 0.99, by = 0.01)) |&gt; \n  mutate(\n    likelihood = dbinom(7, size = 10, prob = p)\n  )\n\n# Find the maximum\ngrid_search |&gt; \n  slice_max(likelihood, n = 1)\n\n# A tibble: 1 × 2\n      p likelihood\n  &lt;dbl&gt;      &lt;dbl&gt;\n1   0.7      0.267\n\n\n\nThe maximum likelihood estimate is \\(\\hat{p} = 0.70 = \\frac{7}{10}\\). This makes intuitive sense!"
  },
  {
    "objectID": "lessons/01_Intro_Inference/01_Intro_Inference.html#using-rs-optimizer",
    "href": "lessons/01_Intro_Inference/01_Intro_Inference.html#using-rs-optimizer",
    "title": "BSTA 551: Statistical Inference",
    "section": "Using R’s Optimizer",
    "text": "Using R’s Optimizer\nR has built-in functions to find maximums and minimums more precisely:\n\n# Define the likelihood function\nlikelihood_function &lt;- function(p) {\n  dbinom(7, size = 10, prob = p)\n}\n\n# Use optimize() to find the maximum\n# Note: optimize finds MINIMUM by default, so we negate for maximum\nresult &lt;- optimize(\n  f = function(p) -likelihood_function(p),  # Negative to find max\n  interval = c(0, 1)                         # Search between 0 and 1\n)\n\n# The maximum occurs at:\ncat(\"Maximum likelihood estimate: p =\", result$minimum)\n\nMaximum likelihood estimate: p = 0.6999843"
  },
  {
    "objectID": "lessons/01_Intro_Inference/01_Intro_Inference.html#how-numerical-optimization-works",
    "href": "lessons/01_Intro_Inference/01_Intro_Inference.html#how-numerical-optimization-works",
    "title": "BSTA 551: Statistical Inference",
    "section": "How Numerical Optimization Works",
    "text": "How Numerical Optimization Works\n\nKey idea: The algorithm evaluates the function at different points and iteratively narrows in on the maximum."
  },
  {
    "objectID": "lessons/01_Intro_Inference/01_Intro_Inference.html#your-turn-numerical-optimization",
    "href": "lessons/01_Intro_Inference/01_Intro_Inference.html#your-turn-numerical-optimization",
    "title": "BSTA 551: Statistical Inference",
    "section": "Your Turn: Numerical Optimization",
    "text": "Your Turn: Numerical Optimization\nExercise: A diagnostic test correctly identifies a disease in 18 out of 25 patients who have it. Find the maximum likelihood estimate for the test’s sensitivity \\(p\\).\n\n# Fill in the blanks:\nlikelihood_fn &lt;- function(p) {\n  dbinom(___, size = ___, prob = p)  # What goes here?\n}\n\nresult &lt;- optimize(\n  f = function(p) -likelihood_fn(p),\n  interval = c(0, 1)\n)\n\nresult$minimum  # This is the MLE\n\n\n\n# Solution:\nlikelihood_fn &lt;- function(p) {\n  dbinom(18, size = 25, prob = p)\n}\n\nresult &lt;- optimize(f = function(p) -likelihood_fn(p), interval = c(0, 1))\ncat(\"MLE of sensitivity:\", result$minimum)  # Should be 18/25 = 0.72\n\nMLE of sensitivity: 0.7200103"
  },
  {
    "objectID": "lessons/01_Intro_Inference/01_Intro_Inference.html#when-optimization-gets-harder",
    "href": "lessons/01_Intro_Inference/01_Intro_Inference.html#when-optimization-gets-harder",
    "title": "BSTA 551: Statistical Inference",
    "section": "When Optimization Gets Harder",
    "text": "When Optimization Gets Harder\nSometimes we need to optimize over multiple parameters or complex functions:\n\n# Example: Finding mean and SD that best fit data\npatient_data &lt;- c(120, 135, 128, 142, 131, 125, 138, 129, 133, 127)\n\n# Negative log-likelihood for normal distribution\nneg_log_lik &lt;- function(params) {\n  mu &lt;- params[1]\n  sigma &lt;- params[2]\n  if (sigma &lt;= 0) return(Inf)  # sigma must be positive\n  -sum(dnorm(patient_data, mean = mu, sd = sigma, log = TRUE))\n}\n\n# Use optim() for multiple parameters\nresult &lt;- optim(par = c(130, 10), fn = neg_log_lik)\ncat(\"MLE for mean:\", round(result$par[1], 2), \"\\n\")\n\nMLE for mean: 130.8 \n\ncat(\"MLE for SD:\", round(result$par[2], 2), \"\\n\")\n\nMLE for SD: 6.13 \n\ncat(\"Compare to sample mean:\", round(mean(patient_data), 2))\n\nCompare to sample mean: 130.8"
  },
  {
    "objectID": "lessons/01_Intro_Inference/01_Intro_Inference.html#lesson-1-summary",
    "href": "lessons/01_Intro_Inference/01_Intro_Inference.html#lesson-1-summary",
    "title": "BSTA 551: Statistical Inference",
    "section": "Lesson 1 Summary",
    "text": "Lesson 1 Summary\nKey Concepts:\n\nStatistical Inference: Using sample data to learn about population parameters\nParameters vs. Statistics:\n\nParameters (\\(\\mu\\), \\(\\sigma\\), \\(p\\)): Fixed but unknown population values\nStatistics (\\(\\bar{X}\\), \\(S\\), \\(\\hat{p}\\)): Calculated from sample data\n\nSampling Distribution: The distribution of a statistic across many samples\n\n\\(E(\\bar{X}) = \\mu\\) (centered at population mean)\n\\(\\text{Var}(\\bar{X}) = \\sigma^2/n\\) (precision improves with larger \\(n\\))\n\nNumerical Optimization: Finding maximum/minimum values\n\nGrid search: try many values\noptimize(): efficient numerical search"
  },
  {
    "objectID": "lessons/01_Intro_Inference/01_Intro_Inference.html#lesson-1-practice-problems",
    "href": "lessons/01_Intro_Inference/01_Intro_Inference.html#lesson-1-practice-problems",
    "title": "BSTA 551: Statistical Inference",
    "section": "Lesson 1 Practice Problems",
    "text": "Lesson 1 Practice Problems\n\nCalculate \\(E(\\bar{X})\\) and \\(\\text{Var}(\\bar{X})\\) for a sample of size \\(n = 16\\) from a population with \\(\\mu = 50\\) and \\(\\sigma = 12\\).\nUse optimize() to find the MLE for \\(p\\) when you observe 23 successes in 40 trials.\nA quality control engineer samples 5 items from a production line. If the population mean weight is 100g with SD = 5g, what is the expected value and variance of the sample mean?\nSimulate the sampling distribution of the sample median for \\(n = 30\\) observations from a Normal(100, 15) distribution. Compare to the sampling distribution of the sample mean."
  },
  {
    "objectID": "lessons/01_Intro_Inference/01_Intro_Inference.html#next-lesson-preview",
    "href": "lessons/01_Intro_Inference/01_Intro_Inference.html#next-lesson-preview",
    "title": "BSTA 551: Statistical Inference",
    "section": "Next Lesson Preview",
    "text": "Next Lesson Preview\nLesson 2: Point Estimation; Bias, Variance, and MSE\n\nWhat is a point estimator?\nBias: systematic error in estimation\nStandard error: precision of estimators\nMean Squared Error: combining bias and variance\nThe bias-variance tradeoff"
  },
  {
    "objectID": "lessons/01_Intro_Inference/01_Intro_Inference.html#references",
    "href": "lessons/01_Intro_Inference/01_Intro_Inference.html#references",
    "title": "BSTA 551: Statistical Inference",
    "section": "References",
    "text": "References\n\nDevore, Berk, and Carlton. Modern Mathematical Statistics with Applications (Springer). Chapters 6.1, 6.2\nChihara and Hesterberg. Mathematical Statistics with Resampling and R (Wiley). Chapter 6."
  },
  {
    "objectID": "lessons/01_Intro_Inference/01_Intro_Inference.html#questions",
    "href": "lessons/01_Intro_Inference/01_Intro_Inference.html#questions",
    "title": "BSTA 551: Statistical Inference",
    "section": "Questions?",
    "text": "Questions?\nThank you!"
  },
  {
    "objectID": "lessons/x08_pdfs/24_01_Continuous_rv_muddy_points.html",
    "href": "lessons/x08_pdfs/24_01_Continuous_rv_muddy_points.html",
    "title": "Muddy Points",
    "section": "",
    "text": "Muddy points from Fall 2023:\n\n1. Why we are allowed to “split” the operator d/dx into two pieces as if it were a fraction when it’s an operator\nHere is a pretty helpful StackExchange post talking about this!\n\n\n2. How to know what to use as \\(u\\) and \\(dv\\) for integration by parts\nI have two approaches to identifying \\(u\\) and \\(dv\\):\n\nTry to find a \\(u\\) that will eventually differentiate into a constant. This usually works unless you’re left with a \\(dv\\) that is hard to integrate.\n\nFor example, \\(u=x^6\\). the first derivative, \\(u'=6x^5\\), which may lead us to do another integration by parts, but eventually, at the 6th derivative, we get 720. This means, in our integration by parts, we eventually get an integral that only has \\(x\\) in the function once.\n\nIn this example, \\(x^6\\) would result in many integration by parts. I feel like we don’t typically see that many in our work.\n\nIn Example 3.5 in Calculus Review, setting \\(u=x^2\\) is a bad idea because we don’t really know how to integrate \\(dv=ln(x)\\) into \\(v\\).\n\nTry to find a \\(u\\) where \\(du\\) is the reciprocal of \\(v\\) or \\(du\\) cancels with \\(v\\).\n\nLook again at Example 3.5 in the notes! Hint: What is the derivative of \\(ln(x)\\)? And what does \\(\\dfrac{1}{x}x^3\\) equal?\n\n\n\n\n3. Looking for more practice in calculus?\nI just stumbled upon this website! Just a bunch of calculus practice problems! Might be some help."
  },
  {
    "objectID": "lessons/x08_pdfs/08_pdfs_muddy_points.html",
    "href": "lessons/x08_pdfs/08_pdfs_muddy_points.html",
    "title": "Muddy Points",
    "section": "",
    "text": "I think you’re on the right track… \\(P(X)\\), \\(P(X=x)\\), and \\(p_X(x)\\) represent probabilities.\nFor discrete random variables, we can use \\(P(X=x)\\) or \\(p_X(x)\\) to represent the probability that the random variable \\(X\\) takes on the value \\(x\\). And then we can discuss the probability mass function (pmf) as the probability distribution as well!\nFor continuous random variables, we use \\(f_X(x)\\) to represent the probability density function (pdf) of \\(X\\) at the value \\(x\\), BUT it is not equal to the actual probability! Aka \\(f_X(x)\\) does not represent a probability, but the area under the curve of \\(f_X(x)\\) over an interval gives us the probability."
  },
  {
    "objectID": "lessons/x08_pdfs/08_pdfs_muddy_points.html#fall-2025",
    "href": "lessons/x08_pdfs/08_pdfs_muddy_points.html#fall-2025",
    "title": "Muddy Points",
    "section": "",
    "text": "I think you’re on the right track… \\(P(X)\\), \\(P(X=x)\\), and \\(p_X(x)\\) represent probabilities.\nFor discrete random variables, we can use \\(P(X=x)\\) or \\(p_X(x)\\) to represent the probability that the random variable \\(X\\) takes on the value \\(x\\). And then we can discuss the probability mass function (pmf) as the probability distribution as well!\nFor continuous random variables, we use \\(f_X(x)\\) to represent the probability density function (pdf) of \\(X\\) at the value \\(x\\), BUT it is not equal to the actual probability! Aka \\(f_X(x)\\) does not represent a probability, but the area under the curve of \\(f_X(x)\\) over an interval gives us the probability."
  },
  {
    "objectID": "lessons/x08_pdfs/08_pdfs_muddy_points.html#fall-2023",
    "href": "lessons/x08_pdfs/08_pdfs_muddy_points.html#fall-2023",
    "title": "Muddy Points",
    "section": "Fall 2023",
    "text": "Fall 2023\n\n1. How do pdf, CDF, and probability interact with each other?\nLet’s say we have a pdf, \\(f_X(x) = \\dfrac{1}{9}x^2\\) for \\(0 \\leq x \\leq 3\\). This is just a function. The pdf is not used on its own to report any probability. We must integrate over the pdf to find a probability.\n\nlibrary(\"ggplot2\")\neq = function(x){(1/9)*x^2}\nggplot(data.frame(x=c(1, 50)), aes(x=x)) + \n  stat_function(fun=eq) +\n  xlab(\"x\") + ylab(\"pdf\") +\n  xlim(0,3)\n\n\n\n\n\n\n\n\nThe total area under the pdf is 1. This makes our pdf valid.\n\neq = function(x){(1/9)*x^2}\nggplot(data.frame(x=c(1, 50)), aes(x=x)) + \n  stat_function(fun=eq) +\n  xlab(\"x\") + ylab(\"pdf\") +\n  xlim(0,3) +\n  stat_function(fun=eq, \n                xlim = c(0, 3),\n                geom = \"area\", \n                aes(fill = \"red\")) +\n  theme(legend.position = \"none\") +\n  annotate(\"text\", x = 0.5, y = 0.7, label = \"AUC = 1\", color = \"black\")\n\n\n\n\n\n\n\n\nIf we only look at a proportion of the area under the pdf, then we start constructing our probabilities. For example, we can look at probability that we have a value between 0 and 1.5.\n\nggplot(data.frame(x=c(1, 50)), aes(x=x)) + \n  stat_function(fun=eq) +\n  xlab(\"x\") + ylab(\"pdf\") +\n  xlim(0,3) +\n  stat_function(fun=eq, \n                xlim = c(0, 1.5),\n                geom = \"area\", \n                aes(fill = \"blue\")) +\n  theme(legend.position = \"none\") +\n  annotate(\"text\", x = 0.5, y = 0.7, label = \"AUC = 0.125\", color = \"black\")\n\n\n\n\n\n\n\n\nInstead of calculating the EXACT probability for each value between 0 and 3, we can find the CDF of the pdf.\nThe CDF is: \\[\nF_X(x) = \\left\\{\n        \\begin{array}{ll}\n            0 & \\quad x&lt;3 \\quad \\\\\n            \\dfrac{1}{27}x^3 & \\quad 0 \\leq x \\leq 3\\quad \\\\\n            1 & \\quad x&gt;3 \\quad\n        \\end{array}\n    \\right.\n\\]\n\ncdf = function(x){(1/27)*x^3}\nggplot(data.frame(x=c(1, 50)), aes(x=x)) + \n  stat_function(fun=cdf) +\n  xlab(\"x\") + ylab(\"CDF\") +\n  xlim(0,3) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nWhen \\(x=1.5\\), we can calculate the probability using the CDF. Remember that \\(F_X(x) = P(X \\leq x)\\). So we can say \\(P(X \\leq 1.5) = F_X(1.5) = \\dfrac{1}{27}(1.5)^3\\), which equals 0.125.\n\ncdf = function(x){(1/27)*x^3}\nggplot(data.frame(x=c(1, 50)), aes(x=x)) + \n  stat_function(fun=cdf) +\n  xlab(\"x\") + ylab(\"CDF\") +\n  xlim(0,3) +\n  theme(legend.position = \"none\") +\n  geom_point(aes(x=1.5, y=.125), colour=\"blue\", size=3) +\n  annotate(\"text\", x = 0.5, y = 0.7, label = \"CDF = 0.125\", color = \"black\")\n\nWarning in geom_point(aes(x = 1.5, y = 0.125), colour = \"blue\", size = 3): All aesthetics have length 1, but the data has 2 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\nWe can also calculate the probability with an integral: \\(P(X \\leq 1.5) = \\displaystyle\\int_0^{1.5} \\dfrac{1}{9}x^2 dx\\).\nWe can also find the probability that X is between two numbers. \\(P(1\\leq X \\leq 1.5) = F_X(1.5) - F_X(1)\\) or \\(P(1\\leq X \\leq 1.5) = \\displaystyle\\int_1^{1.5} \\dfrac{1}{9}x^2 dx\\)."
  },
  {
    "objectID": "lessons/x04_Rules_of_prob/04_Rules_of_prob_key_info.html",
    "href": "lessons/x04_Rules_of_prob/04_Rules_of_prob_key_info.html",
    "title": "Key Info",
    "section": "",
    "text": "No office hours from me this week - I have a standing appointment\n\nBut we will start next week!\n\nDrop deadline with 100% refund: this Friday, 10/10\nNew AI Video Tutorials! If anyone is looking for course materials on the responsible use of generative AI tools such as ChatGPT, I wanted to highlight that the OHSU library has released a new set of AI video tutorials!\n\nCreated by librarian David Carson, this series of short videos (3-4 minutes each) is based on the Kickstart Your Writing with AI workshop taught with writing specialist Zoe Speidel. Each video is standalone and can be viewed in any order, so faculty can share and reuse whichever are most relevant. The topics include effective prompting, AI and publishing, and how to cite or disclose AI. The videos are designed to be flexible resources and are available to embed directly into Sakai. This series is available on the Library’s Generative AI student guide."
  },
  {
    "objectID": "lessons/x04_Rules_of_prob/04_Rules_of_prob_key_info.html#announcements",
    "href": "lessons/x04_Rules_of_prob/04_Rules_of_prob_key_info.html#announcements",
    "title": "Key Info",
    "section": "",
    "text": "No office hours from me this week - I have a standing appointment\n\nBut we will start next week!\n\nDrop deadline with 100% refund: this Friday, 10/10\nNew AI Video Tutorials! If anyone is looking for course materials on the responsible use of generative AI tools such as ChatGPT, I wanted to highlight that the OHSU library has released a new set of AI video tutorials!\n\nCreated by librarian David Carson, this series of short videos (3-4 minutes each) is based on the Kickstart Your Writing with AI workshop taught with writing specialist Zoe Speidel. Each video is standalone and can be viewed in any order, so faculty can share and reuse whichever are most relevant. The topics include effective prompting, AI and publishing, and how to cite or disclose AI. The videos are designed to be flexible resources and are available to embed directly into Sakai. This series is available on the Library’s Generative AI student guide."
  },
  {
    "objectID": "lessons/x04_Rules_of_prob/04_Rules_of_prob_key_info.html#key-dates",
    "href": "lessons/x04_Rules_of_prob/04_Rules_of_prob_key_info.html#key-dates",
    "title": "Key Info",
    "section": "Key Dates",
    "text": "Key Dates\n\nHW 1 Assignment due this Sunday at 11pm"
  },
  {
    "objectID": "lessons/x04_Rules_of_prob/05_Bayes_Theorem.html",
    "href": "lessons/x04_Rules_of_prob/05_Bayes_Theorem.html",
    "title": "Chapter 5: Bayes’ Theorem",
    "section": "",
    "text": "Calculate conditional probability of an event using Bayes’ Theorem\nUtilize additional probability rules in probability calculations, specifically the Higher Order Multiplication Rule and the Law of Total Probabilities"
  },
  {
    "objectID": "lessons/x04_Rules_of_prob/05_Bayes_Theorem.html#learning-objectives",
    "href": "lessons/x04_Rules_of_prob/05_Bayes_Theorem.html#learning-objectives",
    "title": "Chapter 5: Bayes’ Theorem",
    "section": "",
    "text": "Calculate conditional probability of an event using Bayes’ Theorem\nUtilize additional probability rules in probability calculations, specifically the Higher Order Multiplication Rule and the Law of Total Probabilities"
  },
  {
    "objectID": "lessons/x04_Rules_of_prob/05_Bayes_Theorem.html#where-are-we",
    "href": "lessons/x04_Rules_of_prob/05_Bayes_Theorem.html#where-are-we",
    "title": "Chapter 5: Bayes’ Theorem",
    "section": "Where are we?",
    "text": "Where are we?"
  },
  {
    "objectID": "lessons/x04_Rules_of_prob/05_Bayes_Theorem.html#introduction",
    "href": "lessons/x04_Rules_of_prob/05_Bayes_Theorem.html#introduction",
    "title": "Chapter 5: Bayes’ Theorem",
    "section": "Introduction",
    "text": "Introduction\n\nSo we learned about conditional probabilities\n\nWe learned how the occurrence of event A affects event B (B conditional on A)\n\nCan we figure out information on how the occurrence of event B affects event A?\nWe can use the conditional probability (\\(\\mathbb{P}(A|B)\\)) to get information on the flipped conditional probability (\\(\\mathbb{P}(B|A)\\))"
  },
  {
    "objectID": "lessons/x04_Rules_of_prob/05_Bayes_Theorem.html#bayes-rule-for-two-events",
    "href": "lessons/x04_Rules_of_prob/05_Bayes_Theorem.html#bayes-rule-for-two-events",
    "title": "Chapter 5: Bayes’ Theorem",
    "section": "Bayes’ Rule for two events",
    "text": "Bayes’ Rule for two events\n\n\n\n\nTheorem: Bayes’ Rule (for two events)\n\n\nFor any two events \\(A\\) and \\(B\\) with nonzero probabilties,\n\\[\\mathbb{P}(A| B) =\n\\frac{\\mathbb{P}(A) \\cdot \\mathbb{P}(B|A)}\n{\\mathbb{P}(B)}\\]"
  },
  {
    "objectID": "lessons/x04_Rules_of_prob/05_Bayes_Theorem.html#calculating-probability-with-higher-order-multiplication-rule",
    "href": "lessons/x04_Rules_of_prob/05_Bayes_Theorem.html#calculating-probability-with-higher-order-multiplication-rule",
    "title": "Chapter 5: Bayes’ Theorem",
    "section": "Calculating probability with Higher Order Multiplication Rule",
    "text": "Calculating probability with Higher Order Multiplication Rule\n\n\n\n\nExample 1\n\n\nSuppose we draw 5 cards from a standard shuffled deck of 52 cards. What is the probability of a flush, that is all the cards are of the same suit (including straight flushes)?\n\n\n\n\n\nHigher Order Multiplication Rule\n\n\n\\[\\mathbb{P}(A_1\\cap A_2 \\cap  \\ldots \\cap A_n)=\\mathbb{P}(A_1)\\cdot\\mathbb{P}(A_2|A_1) \\cdot \\\\\n\\mathbb{P}(A_3|A_1A_2)\\ldots \\cdot\\mathbb{P}(A_n|A_1A_2\\ldots A_{n-1})\\]"
  },
  {
    "objectID": "lessons/x04_Rules_of_prob/05_Bayes_Theorem.html#calculating-probability-with-law-of-total-probability",
    "href": "lessons/x04_Rules_of_prob/05_Bayes_Theorem.html#calculating-probability-with-law-of-total-probability",
    "title": "Chapter 5: Bayes’ Theorem",
    "section": "Calculating probability with Law of Total Probability",
    "text": "Calculating probability with Law of Total Probability\n\n\n\n\nExample 2\n\n\nSuppose 1% of people assigned female at birth (AFAB) and 5% of people assigned male at birth (AMAB) are color-blind. Assume person born is equally likely AFAB or AMAB (not including intersex). What is the probability that a person chosen at random is color-blind?\n\n\n\n\n\nLaw of Total Probability for 2 Events\n\n\nFor events \\(A\\) and \\(B\\),\n\\[%\\left(\n\\begin{array}{ccl}\n\\mathbb{P}(B)&=&\\mathbb{P}(B \\cap A) + \\mathbb{P}(B \\cap A^C)\\\\\n           &=& \\mathbb{P}(B|A) \\cdot \\mathbb{P}(A)+ \\mathbb{P}(B | A^C)\\cdot \\mathbb{P}(A^C)\n\\end{array}\n%\\right)\\]"
  },
  {
    "objectID": "lessons/x04_Rules_of_prob/05_Bayes_Theorem.html#general-law-of-total-proability",
    "href": "lessons/x04_Rules_of_prob/05_Bayes_Theorem.html#general-law-of-total-proability",
    "title": "Chapter 5: Bayes’ Theorem",
    "section": "General Law of Total Proability",
    "text": "General Law of Total Proability\n\n\nLaw of Total Probability (general)\n\n\nIf \\(\\{A_i\\}_{i=1}^{n} = \\{A_1, A_2, \\ldots, A_n\\}\\) form a partition of the sample space, then for event \\(B\\),\n\\[%\\left(\n\\begin{array}{ccl}\n\\mathbb{P}(B)&=& \\sum_{i=1}^{n} \\mathbb{P}(B \\cap A_i)\\\\\n           &=& \\sum_{i=1}^{n} \\mathbb{P}(B|A_i) \\cdot \\mathbb{P}(A_i)\n\\end{array}\n%\\right)\\]"
  },
  {
    "objectID": "lessons/x04_Rules_of_prob/05_Bayes_Theorem.html#calculating-probability-with-generalized-law-of-total-probability",
    "href": "lessons/x04_Rules_of_prob/05_Bayes_Theorem.html#calculating-probability-with-generalized-law-of-total-probability",
    "title": "Chapter 5: Bayes’ Theorem",
    "section": "Calculating probability with generalized Law of Total Probability",
    "text": "Calculating probability with generalized Law of Total Probability"
  },
  {
    "objectID": "lessons/x04_Rules_of_prob/05_Bayes_Theorem.html#calculating-probability-with-generalized-law-of-total-probability-1",
    "href": "lessons/x04_Rules_of_prob/05_Bayes_Theorem.html#calculating-probability-with-generalized-law-of-total-probability-1",
    "title": "Chapter 5: Bayes’ Theorem",
    "section": "Calculating probability with generalized Law of Total Probability",
    "text": "Calculating probability with generalized Law of Total Probability\n\n\n\n\nExample 3\n\n\nIndividuals are diagnosed with a particular type of cancer that can take on three different disease forms,* \\(D_1\\), \\(D_2\\), and \\(D_3\\). It is known that amongst people diagnosed with this particular type of cancer,\n\n20% of people will eventually be diagnosed with form \\(D_1\\),\n30% with form \\(D_2\\), and\n50% with form \\(D_3\\).\n\nThe probability of requiring chemotherapy (\\(C\\)) differs among the three forms of disease:\n\n80% with \\(D_1\\),\n30% with \\(D_2\\), and\n10% with \\(D_3\\).\n\nBased solely on the preliminary test of being diagnosed with the cancer, what is the probability of requiring chemotherapy (the event C)?\n\n\n\nSkipping in class! Let me know if you would like me to post solutions to this if you work through it!"
  },
  {
    "objectID": "lessons/x04_Rules_of_prob/05_Bayes_Theorem.html#lets-revisit-the-color-blind-example",
    "href": "lessons/x04_Rules_of_prob/05_Bayes_Theorem.html#lets-revisit-the-color-blind-example",
    "title": "Chapter 5: Bayes’ Theorem",
    "section": "Let’s revisit the color-blind example",
    "text": "Let’s revisit the color-blind example\n\n\n\n\nExample 4\n\n\nRecall the color-blind example (Example 2), where\n\na person is AMAB with probability 0.5,\nAMAB people are color-blind with probability 0.05, and\nall people are color-blind with probability 0.03.\n\nAssuming people are AMAB or AFAB, find the probability that a color-blind person is AMAB."
  },
  {
    "objectID": "lessons/x04_Rules_of_prob/05_Bayes_Theorem.html#calculate-probability-with-both-rules",
    "href": "lessons/x04_Rules_of_prob/05_Bayes_Theorem.html#calculate-probability-with-both-rules",
    "title": "Chapter 5: Bayes’ Theorem",
    "section": "Calculate probability with both rules",
    "text": "Calculate probability with both rules\n\n\n\n\nExample 5\n\n\nSuppose\n\n1% of people who are AFAB aged 40-50 years have breast cancer,\nan AFAB person with breast cancer has a 90% chance of a positive test from a mammogram, and\nan AFAB person has a 10% chance of a false-positive result from a mammogram.\n\nWhat is the probability that an AFAB person has breast cancer given that they just had a positive test?"
  },
  {
    "objectID": "lessons/x04_Rules_of_prob/05_Bayes_Theorem.html#bayes-rule",
    "href": "lessons/x04_Rules_of_prob/05_Bayes_Theorem.html#bayes-rule",
    "title": "Chapter 5: Bayes’ Theorem",
    "section": "Bayes’ Rule",
    "text": "Bayes’ Rule\n\n\nTheorem: Bayes’ Rule\n\n\nIf \\(\\{A_i\\}_{i=1}^{n}\\) form a partition of the sample space \\(S\\), with \\(\\mathbb{P}(A_i)&gt;0\\) for \\(i=1\\ldots n\\) and \\(\\mathbb{P}(B)&gt;0\\), then\n\\[\\mathbb{P}(A_j | B) =\n\\frac{\\mathbb{P}(B|A_j) \\cdot \\mathbb{P}(A_j)}\n{\\sum_{i=1}^{n} \\mathbb{P}(B|A_i) \\cdot \\mathbb{P}(A_i)}\\]"
  },
  {
    "objectID": "lessons/x04_Rules_of_prob/04_Rules_of_prob_muddy_points.html",
    "href": "lessons/x04_Rules_of_prob/04_Rules_of_prob_muddy_points.html",
    "title": "Muddy Points",
    "section": "",
    "text": "When trying to figure out what theorem/rule/property to use, the most helpful thing is to (1) write the probability statement for your answer and (2) write the information you know in probability statements. Then you want to examine all the rules/theorems/properties we learned to see which can help you connect the information you know to the probability statement for the question."
  },
  {
    "objectID": "lessons/x04_Rules_of_prob/04_Rules_of_prob_muddy_points.html#fall-2025",
    "href": "lessons/x04_Rules_of_prob/04_Rules_of_prob_muddy_points.html#fall-2025",
    "title": "Muddy Points",
    "section": "",
    "text": "When trying to figure out what theorem/rule/property to use, the most helpful thing is to (1) write the probability statement for your answer and (2) write the information you know in probability statements. Then you want to examine all the rules/theorems/properties we learned to see which can help you connect the information you know to the probability statement for the question."
  },
  {
    "objectID": "lessons/x04_Rules_of_prob/04_Rules_of_prob_muddy_points.html#fall-2024",
    "href": "lessons/x04_Rules_of_prob/04_Rules_of_prob_muddy_points.html#fall-2024",
    "title": "Muddy Points",
    "section": "Fall 2024",
    "text": "Fall 2024\n\n1. How do I know if two events are independent?\nMy main piece of advice for independence is to rely on the math to show it, not logic! Don’t go into a problem thinking “Logically, these two events are independent.” If the problem does not say “Assume independence,” then we need to show it mathematically.\nSame goes with the definition of independence. When we say “knowing the outcome of one provides no information about the outcome of the other,” we once again need to show this mathematically, not just using logic. We translate the previous statement to \\[P(A) = P(A|B)\\] And we need to show this mathematically!\n\n\n2. Disjoint vs. Independent Events\nHere is a pretty good video breaking down disjoint (mutually exclusive) events and independent events. It includes examples as well."
  },
  {
    "objectID": "lessons/x13_Expected_Values/13_Expected_Values.html",
    "href": "lessons/x13_Expected_Values/13_Expected_Values.html",
    "title": "Lesson 13: Expected Values",
    "section": "",
    "text": "Define the expected value for discrete and continuous RVs\nCalculate the expected value (mean) of a single discrete RV\nCalculate the expected value (mean) of a single continuous RV\nCalculate expected value for the sum of discrete or continuous RVs\nCalculate expected value for joint densities (continuous RVs)"
  },
  {
    "objectID": "lessons/x13_Expected_Values/13_Expected_Values.html#our-good-and-fair-friend-the-6-sided-die",
    "href": "lessons/x13_Expected_Values/13_Expected_Values.html#our-good-and-fair-friend-the-6-sided-die",
    "title": "Lesson 13: Expected Values",
    "section": "Our good and fair friend, the 6-sided die",
    "text": "Our good and fair friend, the 6-sided die\n\n\n\n\nExample 1\n\n\nSuppose you roll a fair 6-sided die. What value do you expect to get?"
  },
  {
    "objectID": "lessons/x13_Expected_Values/13_Expected_Values.html#what-is-an-expected-value",
    "href": "lessons/x13_Expected_Values/13_Expected_Values.html#what-is-an-expected-value",
    "title": "Lesson 13: Expected Values",
    "section": "What is an expected value?",
    "text": "What is an expected value?\n\n\n\n\nDefinition: Expected value of discrete RV\n\n\nThe expected value of a discrete RV \\(X\\) that takes on values \\(x_1, x_2, \\ldots, x_n\\) is \\[\\mathbb{E}[X] = \\sum_{i=1}^n x_ip_X(x_i)\\] where \\(n\\) can be \\(\\infty\\)\n\n\n\n\n\nDefinition: Expected value of continuous RV\n\n\nThe expected value of a continuous RV \\(X\\) is \\[\\mathbb{E}[X] = \\displaystyle\\int_{-\\infty}^\\infty xf_X(x) dx\\] where we adjust the integrand based on the bounds of \\(X\\)\n\n\n\n\n\nExpected values are not necessarily an actual outcome\n\nIn previous example, we cannot roll a 3.5\nIt could be that our expected value is not in the sample space (\\(E(X) \\notin S\\))"
  },
  {
    "objectID": "lessons/x13_Expected_Values/13_Expected_Values.html#our-good-and-not-so-fair-friend-the-6-sided-die",
    "href": "lessons/x13_Expected_Values/13_Expected_Values.html#our-good-and-not-so-fair-friend-the-6-sided-die",
    "title": "Lesson 13: Expected Values",
    "section": "Our good and not-so-fair friend, the 6-sided die",
    "text": "Our good and not-so-fair friend, the 6-sided die\n\n\n\n\nExample 2\n\n\nSuppose the die is 6-sided, but not fair. And the probabilities of each side is distributed as:\n\n\n\n\\(x\\)\n\\(p_X(x)\\)\n\n\n\n\n1\n0.10\n\n\n2\n0.05\n\n\n3\n0.02\n\n\n4\n0.30\n\n\n5\n0.50\n\n\n6\n0.03\n\n\n\nWhat value do you expect to get on a roll?"
  },
  {
    "objectID": "lessons/x13_Expected_Values/13_Expected_Values.html#expected-value-of-a-bernoulli-distribution",
    "href": "lessons/x13_Expected_Values/13_Expected_Values.html#expected-value-of-a-bernoulli-distribution",
    "title": "Lesson 13: Expected Values",
    "section": "Expected value of a Bernoulli distribution",
    "text": "Expected value of a Bernoulli distribution\n\n\n\n\nExample 3\n\n\nSuppose \\[X = \\left\\{\n        \\begin{array}{ll}\n            1 & \\quad \\mathrm{with\\ probability}\\ p \\quad\\mathrm{(success)}\\\\\n            0 & \\quad \\mathrm{with\\ probability}\\ 1-p \\quad\\mathrm{(failure)}\n        \\end{array}\n    \\right.\\] Find the expected value of \\(X\\)."
  },
  {
    "objectID": "lessons/x13_Expected_Values/13_Expected_Values.html#lets-slightly-change-our-random-variable",
    "href": "lessons/x13_Expected_Values/13_Expected_Values.html#lets-slightly-change-our-random-variable",
    "title": "Lesson 13: Expected Values",
    "section": "Let’s slightly change our random variable",
    "text": "Let’s slightly change our random variable\n\n\n\n\nExample 4\n\n\nSuppose \\[X = \\left\\{\n        \\begin{array}{ll}\n            1 & \\quad \\mathrm{with\\ probability}\\ p \\\\\n            -1 & \\quad \\mathrm{with\\ probability}\\ 1-p\n        \\end{array}\n    \\right.\\] Find the expected value of \\(X\\)."
  },
  {
    "objectID": "lessons/x13_Expected_Values/13_Expected_Values.html#bullseye",
    "href": "lessons/x13_Expected_Values/13_Expected_Values.html#bullseye",
    "title": "Lesson 13: Expected Values",
    "section": "Bullseye! 🎯",
    "text": "Bullseye! 🎯\n\n\n\n\nExample 5\n\n\nSuppose I throw darts at a dartboard until I hit the bullseye, and that my probability of hitting the bullseye is \\(p\\). Suppose further that all of my throws are independent, and that the probability of a bullseye never changes, no matter how many times I throw a dart. How many times should I expect to have to throw the dart until I hit the bullseye?"
  },
  {
    "objectID": "lessons/x13_Expected_Values/13_Expected_Values.html#ghost",
    "href": "lessons/x13_Expected_Values/13_Expected_Values.html#ghost",
    "title": "Lesson 13: Expected Values",
    "section": "Ghost! 👻",
    "text": "Ghost! 👻\n\n\n\n\nExample 6\n\n\nA ghost is trick-or-treating. It comes to a house where it is known that there are 30 candies in the bag and only one is a watermelon Jolly Rancher, which is the ghost’s favorite. The ghost takes pieces of candy without replacement until it gets the watermelon Jolly Rancher. How many pieces of candy do we expect the ghost to take?\n\n\nCan we model this with a distribution?"
  },
  {
    "objectID": "lessons/x13_Expected_Values/13_Expected_Values.html#some-remarks-on-last-two-examples",
    "href": "lessons/x13_Expected_Values/13_Expected_Values.html#some-remarks-on-last-two-examples",
    "title": "Lesson 13: Expected Values",
    "section": "Some remarks on last two examples",
    "text": "Some remarks on last two examples\nBoth examples are repeated random processes. They are fundamentally different though:\n\nThe bullseye example is “with replacement” since the probability of success remains constant.\nThe ghost trick-or-treating example is without replacement, and thus the probability of success changes with each trial."
  },
  {
    "objectID": "lessons/x13_Expected_Values/13_Expected_Values.html#expected-value-of-the-uniform-distribution",
    "href": "lessons/x13_Expected_Values/13_Expected_Values.html#expected-value-of-the-uniform-distribution",
    "title": "Lesson 13: Expected Values",
    "section": "Expected Value of the Uniform Distribution",
    "text": "Expected Value of the Uniform Distribution\n\n\n\n\nExample 5\n\n\nLet \\(f_X(x)= \\frac{1}{b-a}\\), for \\(a \\leq x \\leq b\\). Find \\(\\mathbb{E}[X]\\)."
  },
  {
    "objectID": "lessons/x13_Expected_Values/13_Expected_Values.html#expected-value-of-the-exponential-distribution",
    "href": "lessons/x13_Expected_Values/13_Expected_Values.html#expected-value-of-the-exponential-distribution",
    "title": "Lesson 13: Expected Values",
    "section": "Expected Value of the Exponential Distribution",
    "text": "Expected Value of the Exponential Distribution\n\n\n\n\nExample 6\n\n\nLet \\(f_X(x)= \\lambda e^{-\\lambda x}\\), for \\(x &gt; 0\\) and \\(\\lambda&gt; 0\\). Find \\(\\mathbb{E}[X]\\).\n\n\n\n\nIntegrating by Parts\n\n\n\\(\\displaystyle\\int_a^b u dv = uv\\bigg|^b_a - \\displaystyle\\int_a^b vdu\\)"
  },
  {
    "objectID": "lessons/x13_Expected_Values/13_Expected_Values.html#revisiting-our-two-card-draw",
    "href": "lessons/x13_Expected_Values/13_Expected_Values.html#revisiting-our-two-card-draw",
    "title": "Lesson 13: Expected Values",
    "section": "Revisiting our two card draw",
    "text": "Revisiting our two card draw\n\n\n\n\nExample 7\n\n\nSuppose you draw 2 cards from a standard deck of cards with replacement. Let \\(X\\) be the number of hearts you draw. Find \\(\\mathbb{E}[X]\\).\n\n\n\n\nRecall Binomial RV with \\(n=2\\):\n\\[p_X(x) = {2 \\choose x}p^x(1-p)^{2-x} \\text{  for } x = 0, 1, 2\\]"
  },
  {
    "objectID": "lessons/x13_Expected_Values/13_Expected_Values.html#what-if-we-draw-a-lot-of-cards",
    "href": "lessons/x13_Expected_Values/13_Expected_Values.html#what-if-we-draw-a-lot-of-cards",
    "title": "Lesson 13: Expected Values",
    "section": "What if we draw A LOT of cards?",
    "text": "What if we draw A LOT of cards?\n\n\n\n\nExample 8\n\n\nWhat is the expected number of hearts in Example 1 if you draw 200 cards?\n\n\n\n\nRecall Binomial RV with \\(n=200\\):\n\\[p_X(x) = {200 \\choose x}p^x(1-p)^{200-x}\\] \\[\\text{  for } x = 0, 1, 2, ..., 200\\]"
  },
  {
    "objectID": "lessons/x13_Expected_Values/13_Expected_Values.html#sums-of-random-variables",
    "href": "lessons/x13_Expected_Values/13_Expected_Values.html#sums-of-random-variables",
    "title": "Lesson 13: Expected Values",
    "section": "Sums of Random Variables",
    "text": "Sums of Random Variables\n\n\nTheorem: Sum of random variables\n\n\nFor RVs (discrete or continuous) \\(X_i\\) and constants \\(a_i\\), \\(i=1,2,\\dots, n\\), \\[\\mathbb{E}\\Bigg[\\sum_{i=1}^n a_iX_i\\Bigg] = \\sum_{i=1}^n a_i\\mathbb{E}[X_i] .\\] Remark: The theorem holds for infinitely RV’s \\(X_i\\) as well.\n\n\n\nFor two RVs, \\(X\\) and \\(Y\\):\n\nWe can say \\(E[X+Y] = E[X] + E[Y]\\)\n… and constant numbers \\(a\\) and \\(b\\), we can also say \\(E[aX+bY] = aE[X] + bE[Y]\\)\nWe can also also say \\(E[X-Y] = E[X] - E[Y]\\), since \\(b=-1\\)"
  },
  {
    "objectID": "lessons/x13_Expected_Values/13_Expected_Values.html#corollaries-from-theorem",
    "href": "lessons/x13_Expected_Values/13_Expected_Values.html#corollaries-from-theorem",
    "title": "Lesson 13: Expected Values",
    "section": "Corollaries from Theorem",
    "text": "Corollaries from Theorem\n\n\n\n\nFunction with two constants\n\n\nFor a RV \\(X\\), and constants \\(a\\) and \\(b\\), \\[\\mathbb{E}[aX+b] = a\\mathbb{E}[X] + b.\\]\n\n\n\n\n\nExpected value of sum of identically distributed RVs\n\n\nIf \\(X_i\\), \\(i=1,2,\\dots, n\\), are identically distributed RV’s, then \\[\\mathbb{E}\\bigg[\\sum_{i=1}^n X_i\\bigg] = n\\mathbb{E}[X_1] .\\]"
  },
  {
    "objectID": "lessons/x13_Expected_Values/13_Expected_Values.html#cost-of-hotel-rooms",
    "href": "lessons/x13_Expected_Values/13_Expected_Values.html#cost-of-hotel-rooms",
    "title": "Lesson 13: Expected Values",
    "section": "Cost of hotel rooms",
    "text": "Cost of hotel rooms\n\n\n\n\nExample 8\n\n\nA tour group is planning a visit to the city of Minneapolis and needs to book 30 hotel rooms. The average price of a room is $200. In addition, there is a 10% tourism tax for each room. What is the expected cost for the 30 hotel rooms?"
  },
  {
    "objectID": "lessons/x13_Expected_Values/13_Expected_Values.html#expected-value-of-one-rv-from-joint-pdf",
    "href": "lessons/x13_Expected_Values/13_Expected_Values.html#expected-value-of-one-rv-from-joint-pdf",
    "title": "Lesson 13: Expected Values",
    "section": "Expected value of one RV from joint pdf",
    "text": "Expected value of one RV from joint pdf\nIf you have a joint distribution \\(f_{X,Y}(x,y)\\) and want to calculate \\(\\mathbb{E}[X]\\), you have two options:\n\nFind \\(f_X(x)\\) and use it to calculate \\(\\mathbb{E}[X]\\).\nCalculate \\(\\mathbb{E}[X]\\) using the joint density: \\[\\mathbb{E}[X] = \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}x f_{X,Y}(x,y)dydx.\\]\n\nYou can do the same for \\(\\mathbb{E}[Y]\\)!"
  },
  {
    "objectID": "lessons/x13_Expected_Values/13_Expected_Values.html#option-1-find-marginal-first",
    "href": "lessons/x13_Expected_Values/13_Expected_Values.html#option-1-find-marginal-first",
    "title": "Lesson 13: Expected Values",
    "section": "Option 1: Find marginal first",
    "text": "Option 1: Find marginal first\n\n\n\n\nExample 9\n\n\nLet \\(f_{X,Y}(x,y)= 2e^{-(x+y)}\\), for \\(0 \\leq x \\leq y\\). Find \\(\\mathbb{E}[X]\\).\n\n\n\nDo this one at home by finding \\(f_X(x)\\) then \\(\\mathbb{E}[X] = \\displaystyle\\int_{-\\infty}^\\infty xf_X(x) dx\\). See if you get the same result as next page’s answer!"
  },
  {
    "objectID": "lessons/x13_Expected_Values/13_Expected_Values.html#option-2-expected-value-from-a-joint-distribution",
    "href": "lessons/x13_Expected_Values/13_Expected_Values.html#option-2-expected-value-from-a-joint-distribution",
    "title": "Lesson 13: Expected Values",
    "section": "Option 2: Expected value from a joint distribution",
    "text": "Option 2: Expected value from a joint distribution\n\n\n\n\nExample 9\n\n\nLet \\(f_{X,Y}(x,y)= 2e^{-(x+y)}\\), for \\(0 \\leq x \\leq y\\). Find \\(\\mathbb{E}[X]\\)."
  },
  {
    "objectID": "lessons/x13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/28_29_Joint_expected_value_Variance/28_29_Joint_expected_value_Variance_muddy_points.html",
    "href": "lessons/x13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/28_29_Joint_expected_value_Variance/28_29_Joint_expected_value_Variance_muddy_points.html",
    "title": "Muddy Points",
    "section": "",
    "text": "Muddy points from Fall 2023:\n\n2. How do we set the bounds on a double integral?\nWhen the domain of the RVs are not dependent on each other, then we use the bounds as is. In example 2 of Chapter 26 notes, we have the joint pdf \\(f_{X,Y}(x,y)= 18 x^2 y^5\\), for\\(0 \\leq x \\leq 1, \\ 0 \\leq y \\leq 1\\). If we wanted to calculate something like \\(E(X)\\), then we could use the bounds as they are. Below is the domain for \\(x\\) and \\(y\\):\n\n\n\n\n\nHere is the integral for the expected value where we integrate over the whole domain of \\(x\\) and \\(y\\):\n\\[ E(X) = \\displaystyle\\int_0^1 \\displaystyle\\int_0^1 x (18 x^2 y^5 )dy dx \\]If we want to find the probability \\(P(0.25 \\leq X \\leq 0.5, 0.5 \\leq Y \\leq 0.75)\\), then we can look at the specific values of the probability:\n\n\n\n\n\nNote the blue lines above indicate how we integrate over \\(y\\) first from 0.5 to 0.75 and the green lines indicate how integrate over \\(x\\) first from 0.25 to 0.5. It seems like we’ve integrated over an area that isn’t within our specified probability. However, the integrated area is ONLY the overlap of the \\(x\\) and \\(y\\) bounds for the probability.\n\\[P(0.25 \\leq X \\leq 0.5, 0.5 \\leq Y \\leq 0.75) = \\displaystyle\\int_{0.25}^{0.5} \\displaystyle\\int_{0.5}^{0.75} 18 x^2 y^5 dy dx\\]\nLet’s use the same pdf, but now the domain of the two RVs is dependent on one another. We have the joint pdf \\(f_{X,Y}(x,y)= 18 x^2 y^5\\), for\\(0 \\leq x \\leq y\\leq1\\)\nIf we wanted to calculate something like \\(E(X)\\), then we need to account for fact that \\(x\\) must be less than of equal to \\(y\\). We can look back at the domain for this:\n\n\n\n\n\nNote the blue lines above still indicate how we integrate over \\(y\\) first from \\(x\\) to 1, and the green lines indicate how integrate over \\(x\\) first from 0 to 1. Once again, it seems like we’ve integrated over an area that isn’t within the domain. However, the integrated area is ONLY the overlap of the \\(x\\) and \\(y\\) bounds. Thus, once we’ve restricted \\(y\\) to the area between \\(x\\) and 1, we no longer need to restrict \\(x\\) to the are of 0 to \\(y\\).\n\\[ E(X) = \\displaystyle\\int_0^1 \\displaystyle\\int_x^1 x (18 x^2 y^5 )dy dx \\] If we want to find the probability \\(P(0.25 \\leq X \\leq 0.5, 0.5 \\leq Y \\leq 0.75)\\), then we should look back at our domain. For now, we are focusing on the orange area:\n\n\n\n\n\nBecause the orange area is totally within our domain, we can leave our integral our bounds as the exact values we specified:\n\\[P(0.25 \\leq X \\leq 0.5, 0.5 \\leq Y \\leq 0.75) = \\displaystyle\\int_{0.25}^{0.5} \\displaystyle\\int_{0.5}^{0.75} 18 x^2 y^5 dy dx\\]\nHowever, if we want the probability \\(P(0.5 \\leq X \\leq 0.75, 0.5 \\leq Y \\leq 0.75)\\), we would focus on the pink area above. We would limit one of our integrals to the \\(y=x\\) equation:\n\\[P(0.5 \\leq X \\leq 0.75, 0.5 \\leq Y \\leq 0.75) = \\displaystyle\\int_{0.5}^{0.75} \\displaystyle\\int_{x}^{0.75} 18 x^2 y^5 dy dx\\]\nOR\n\\[P(0.5 \\leq X \\leq 0.75, 0.5 \\leq Y \\leq 0.75) = \\displaystyle\\int_{0.5}^{0.75} \\displaystyle\\int_{0.5}^{y} 18 x^2 y^5 dx dy\\]\nThe key to these probabilities is that the bounds with the other variable is on the inside integral! Otherwise we end up with a answer that includes a RV."
  },
  {
    "objectID": "lessons/x13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/11_Expected_Values_of_Sums_of_rvs.html",
    "href": "lessons/x13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/11_Expected_Values_of_Sums_of_rvs.html",
    "title": "Chapter 11: Expected Values of Sums of Discrete RVs",
    "section": "",
    "text": "Calculate the mean (expected value) of sums of discrete random variables"
  },
  {
    "objectID": "lessons/x13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/11_Expected_Values_of_Sums_of_rvs.html#learning-objectives",
    "href": "lessons/x13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/11_Expected_Values_of_Sums_of_rvs.html#learning-objectives",
    "title": "Chapter 11: Expected Values of Sums of Discrete RVs",
    "section": "",
    "text": "Calculate the mean (expected value) of sums of discrete random variables"
  },
  {
    "objectID": "lessons/x13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/11_Expected_Values_of_Sums_of_rvs.html#where-are-we",
    "href": "lessons/x13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/11_Expected_Values_of_Sums_of_rvs.html#where-are-we",
    "title": "Chapter 11: Expected Values of Sums of Discrete RVs",
    "section": "Where are we?",
    "text": "Where are we?"
  },
  {
    "objectID": "lessons/x13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/11_Expected_Values_of_Sums_of_rvs.html#revisiting-our-two-card-draw",
    "href": "lessons/x13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/11_Expected_Values_of_Sums_of_rvs.html#revisiting-our-two-card-draw",
    "title": "Chapter 11: Expected Values of Sums of Discrete RVs",
    "section": "Revisiting our two card draw",
    "text": "Revisiting our two card draw\n\n\n\n\nExample 1\n\n\nSuppose you draw 2 cards from a standard deck of cards with replacement. Let \\(X\\) be the number of hearts you draw. Find \\(\\mathbb{E}[X]\\).\n\n\n\n\nRecall Binomial RV with \\(n=2\\):\n\\[p_X(x) = {2 \\choose x}p^x(1-p)^{2-x} \\text{  for } x = 0, 1, 2\\]"
  },
  {
    "objectID": "lessons/x13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/11_Expected_Values_of_Sums_of_rvs.html#what-if-we-draw-a-lot-of-cards",
    "href": "lessons/x13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/11_Expected_Values_of_Sums_of_rvs.html#what-if-we-draw-a-lot-of-cards",
    "title": "Chapter 11: Expected Values of Sums of Discrete RVs",
    "section": "What if we draw A LOT of cards?",
    "text": "What if we draw A LOT of cards?\n\n\n\n\nExample 2\n\n\nWhat is the expected number of hearts in Example 1 if you draw 200 cards?\n\n\n\n\nRecall Binomial RV with \\(n=200\\):\n\\[p_X(x) = {200 \\choose x}p^x(1-p)^{200-x}\\] \\[\\text{  for } x = 0, 1, 2, ..., 200\\]"
  },
  {
    "objectID": "lessons/x13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/11_Expected_Values_of_Sums_of_rvs.html#sum-of-discrete-rvs",
    "href": "lessons/x13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/11_Expected_Values_of_Sums_of_rvs.html#sum-of-discrete-rvs",
    "title": "Chapter 11: Expected Values of Sums of Discrete RVs",
    "section": "Sum of discrete RVs",
    "text": "Sum of discrete RVs\n\n\nTheorem 11.1: Sum of discrete RVs\n\n\nFor discrete r.v.’s \\(X_i\\) and constants \\(a_i\\), \\(i=1,2,\\dots, n\\), \\[\\mathbb{E}\\Bigg[\\sum_{i=1}^n a_iX_i\\Bigg] = \\sum_{i=1}^n a_i\\mathbb{E}[X_i] .\\] Remark: The theorem holds for infinitely r.v.’s \\(X_i\\) as well.\n\n\n\nFor two RVs, \\(X\\) and \\(Y\\):\n\nWe can say \\(E[X+Y] = E[X] + E[Y]\\)\n… and constant numbers \\(a\\) and \\(b\\), we can also say \\(E[aX+bY] = aE[X] + bE[Y]\\)\nWe can also also say \\(E[X-Y] = E[X] - E[Y]\\), since \\(b=-1\\)"
  },
  {
    "objectID": "lessons/x13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/11_Expected_Values_of_Sums_of_rvs.html#corollaries-from-thm-11.1",
    "href": "lessons/x13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/11_Expected_Values_of_Sums_of_rvs.html#corollaries-from-thm-11.1",
    "title": "Chapter 11: Expected Values of Sums of Discrete RVs",
    "section": "Corollaries from Thm 11.1",
    "text": "Corollaries from Thm 11.1\n\n\n\n\nCorollary 11.1.1\n\n\nFor a discrete r.v. \\(X\\), and constants \\(a\\) and \\(b\\), \\[\\mathbb{E}[aX+b] = a\\mathbb{E}[X] + b.\\]\n\n\n\n\n\nCorollary 11.1.2\n\n\nIf \\(X_i\\), \\(i=1,2,\\dots, n\\), are identically distributed r.v.’s, then \\[\\mathbb{E}\\bigg[\\sum_{i=1}^n X_i\\bigg] = n\\mathbb{E}[X_1] .\\]"
  },
  {
    "objectID": "lessons/x13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/11_Expected_Values_of_Sums_of_rvs.html#cost-of-hotel-rooms",
    "href": "lessons/x13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/11_Expected_Values_of_Sums_of_rvs.html#cost-of-hotel-rooms",
    "title": "Chapter 11: Expected Values of Sums of Discrete RVs",
    "section": "Cost of hotel rooms",
    "text": "Cost of hotel rooms\n\n\n\n\nExample 4\n\n\nA tour group is planning a visit to the city of Minneapolis and needs to book 30 hotel rooms. The average price of a room is $200. In addition, there is a 10% tourism tax for each room. What is the expected cost for the 30 hotel rooms?"
  },
  {
    "objectID": "lessons/x13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/36_Sums_of_Independent_Normal_rv/36_Sums_of_Independent_Normal_rv.html",
    "href": "lessons/x13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/36_Sums_of_Independent_Normal_rv/36_Sums_of_Independent_Normal_rv.html",
    "title": "Chapter 36: Sums of Independent Normal RVs",
    "section": "",
    "text": "Calculate probability of a sample mean using a Normally distributed population"
  },
  {
    "objectID": "lessons/x13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/36_Sums_of_Independent_Normal_rv/36_Sums_of_Independent_Normal_rv.html#learning-objectives",
    "href": "lessons/x13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/36_Sums_of_Independent_Normal_rv/36_Sums_of_Independent_Normal_rv.html#learning-objectives",
    "title": "Chapter 36: Sums of Independent Normal RVs",
    "section": "",
    "text": "Calculate probability of a sample mean using a Normally distributed population"
  },
  {
    "objectID": "lessons/x13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/36_Sums_of_Independent_Normal_rv/36_Sums_of_Independent_Normal_rv.html#sum-of-normal-rvs",
    "href": "lessons/x13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/36_Sums_of_Independent_Normal_rv/36_Sums_of_Independent_Normal_rv.html#sum-of-normal-rvs",
    "title": "Chapter 36: Sums of Independent Normal RVs",
    "section": "Sum of Normal RVs",
    "text": "Sum of Normal RVs\n\n\nTheorem 1\n\n\nLet \\(X\\sim N(\\mu, \\sigma^2)\\), and let \\(Y=aX+b\\), where \\(a\\) and \\(b\\) are constants. Then \\[Y \\sim N(a\\mu+b, a^2\\sigma^2)\\]\n\n\n\n\nTheorem 2\n\n\nLet \\(X_i \\sim N(\\mu_i, \\sigma_i^2)\\) be independent normal rv’s, for \\(i=1,2,\\ldots,n\\). Then \\[\\sum_{i=1}^n X_i \\sim N\\Bigg(\\sum_{i=1}^n \\mu_i , \\sum_{i=1}^n \\sigma^2_i\\Bigg)\\]"
  },
  {
    "objectID": "lessons/x13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/36_Sums_of_Independent_Normal_rv/36_Sums_of_Independent_Normal_rv.html#special-cases",
    "href": "lessons/x13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/36_Sums_of_Independent_Normal_rv/36_Sums_of_Independent_Normal_rv.html#special-cases",
    "title": "Chapter 36: Sums of Independent Normal RVs",
    "section": "Special Cases",
    "text": "Special Cases\n\nLet \\(X_i \\sim N(\\mu, \\sigma^2)\\) be iid normal rv’s, for \\(i=1,2,\\ldots,n\\). Then \\[\\sum_{i=1}^n X_i \\sim N\\big(n\\mu, n \\sigma^2\\big)\\]\nLet \\(X_i \\sim N(\\mu, \\sigma^2)\\) be iid normal rv’s, for \\(i=1,2,\\ldots,n\\). Then \\[\\bar{X}=\\frac{\\sum_{i=1}^n X_i}{n} \\sim N\\big(\\mu, \\sigma^2 / n\\big)\\]\nLet \\(X\\sim N(\\mu_X,\\sigma_X^2)\\), and \\(Y\\sim N(\\mu_Y,\\sigma_Y^2)\\). Then \\[X-Y \\sim N\\big(\\mu_X - \\mu_Y, \\sigma^2_X + \\sigma^2_Y \\big)\\]"
  },
  {
    "objectID": "lessons/x13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/36_Sums_of_Independent_Normal_rv/36_Sums_of_Independent_Normal_rv.html#detecting-and-solving-sums-of-normal-rvs-from-a-word-problem",
    "href": "lessons/x13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/36_Sums_of_Independent_Normal_rv/36_Sums_of_Independent_Normal_rv.html#detecting-and-solving-sums-of-normal-rvs-from-a-word-problem",
    "title": "Chapter 36: Sums of Independent Normal RVs",
    "section": "Detecting and solving sums of Normal RVs from a word problem",
    "text": "Detecting and solving sums of Normal RVs from a word problem\n\n\n\n\nExample 1\n\n\nGlaucoma is an eye disease that is manifested by high intraocular pressure (IOP). The distribution of IOP in the general population is approximately normal with mean 16 mmHg and standard deviation 3 mmHg.\n\nSuppose a patient has 40 IOP readings. What is the probability that their average reading is greater than 20.32 mmHg, assuming their eyes are healthy?\nRepeat the previous question for a patient with 10 IOP readings."
  },
  {
    "objectID": "lessons/x01_Probability/01_Probability_muddy_points_F24.html",
    "href": "lessons/x01_Probability/01_Probability_muddy_points_F24.html",
    "title": "Muddy Points",
    "section": "",
    "text": "The muddy points from this year were a subset of the ones from last year, so I just decided to copy those below!"
  },
  {
    "objectID": "lessons/x01_Probability/01_Probability_muddy_points_F24.html#proofs-of-propositions",
    "href": "lessons/x01_Probability/01_Probability_muddy_points_F24.html#proofs-of-propositions",
    "title": "Muddy Points",
    "section": "1. Proofs of propositions",
    "text": "1. Proofs of propositions\nFurther explanations of the propositions can be found in the textbook from pages 24-27. For many of the explanations in class, I was working to produce a union of disjoint events, so that the probability could easily be calculated. Proposition 3 and 4 were specifically mentioned, so I will include some writing notes on them here:\n\nProposition 3\nIf \\(A \\subseteq B\\), then \\(\\mathbb{P}(A) \\leq \\mathbb{P}(B)\\)\nIn this proposition, I want to define event \\(B\\) as a union of disjoint events so that I can show \\(P(B)\\) is the sum of \\(P(A)\\) and some greater-than-or-equal-to 0 probability event. If the following is my venn diagram of A and B:\n\n\n\n\n\nThen I can define B as the union of disjoint events: \nIf we then take the probability of each side of the equation \\(B = A \\cup (A^c \\cap B)\\), we get \\[P(B) = P\\big(A \\cup (A^c \\cap B)\\big)\\]\nSince events \\(A\\) and \\(A^c \\cap B\\) are disjoint, the probability of their union is just: \\[P(A) + P(A^c \\cap B)\\]\nThus, our equation is now \\[P(B) = P(A) + P(A^c \\cap B)\\]\nFrom Axiom 1, we know for event \\(A^c \\cap B\\), \\(P(A^c \\cap B) \\geq 0\\).\nSo the probability of event B is the sum of the probability of event A and an event that is \\(\\geq\\) 0. This means \\(P(B) \\geq P(A)\\).\n\n\nProposition 4\n\\(\\mathbb{P}(A \\cup B) = \\mathbb{P}(A) + \\mathbb{P}(B) - \\mathbb{P}(A \\cap B)\\)\n\n\nFrom the pictures above, we can see some similar disjoint events.\nIf we look back at \\(A \\cup B\\), we can start manipulating the right side of the equation:"
  },
  {
    "objectID": "lessons/x01_Probability/01_Probability_muddy_points_F24.html#example-at-end-of-chapter-2-slides-venn-diagram",
    "href": "lessons/x01_Probability/01_Probability_muddy_points_F24.html#example-at-end-of-chapter-2-slides-venn-diagram",
    "title": "Muddy Points",
    "section": "2. Example at end of Chapter 2 slides (Venn Diagram)",
    "text": "2. Example at end of Chapter 2 slides (Venn Diagram)\nI will post this in the previous week’s Muddy Points as well. Please follow this link for my work through of the example. And here is the PDF with my work.\nSub-question: why don’t we just multiply the probability of A and B to get the intersection? This is a specific property of probability when A and B are independent. Only when A and B are independent can we conclude that \\(P(A \\cap B) = P(A)P(B)\\)."
  },
  {
    "objectID": "lessons/x01_Probability/01_Probability_muddy_points_F24.html#partition-of-events",
    "href": "lessons/x01_Probability/01_Probability_muddy_points_F24.html#partition-of-events",
    "title": "Muddy Points",
    "section": "3. Partition of events",
    "text": "3. Partition of events\nWe’ve been working with event partitions throughout Chapter 2, but we have not formally identified them. Partitions are advantageous to define for two reasons:\n\nThe partitions may be easier to calculate. We can then use the partitions to reconstruct other probabilities that may be more difficult to calculate\nPartitions have nice properties as a consequence of being disjoint. For example, the probability of the union of partitions is the sum of the probabilities across each partition: \\[P\\bigg(\\bigcup_{i=1}^n A_i\\bigg) = P(A_1)P(A_2)P(A_3) \\cdot \\cdot \\cdot P(A_n)\\]"
  },
  {
    "objectID": "lessons/x01_Probability/01_Probability.html",
    "href": "lessons/x01_Probability/01_Probability.html",
    "title": "Lesson 1: Introduction to Probability",
    "section": "",
    "text": "Use simulations (e.g., coin flips in R) to illustrate randomness in equally likely outcomes.\nIdentify sample spaces and events for basic experiments, and define probability calculations.\n\n\n\n\nBig part of this lesson is motivating why we care about probability and simulations, then we’ll dive into some basics\n\n\n\n\n\n\n\n\n\n\nWe hear the word “probability” pretty often\n\nCommon in news reports, advertisements, sports, medicine, etc.\n\n\n\nResearchers say the probability of living past 110 is on the rise\nCNBC, 2021\n\n\nWe may hear “probability” or similar words like “chance,” “likelihood,” or “odds”\n\n\nScientists fine-tune odds of asteroid Bennu hitting Earth through 2300 with NASA probe’s help\nSpace.com, 2021\n\n\n\n\n\nThe following few slides use some undefined words to define new words that in turn define the previously undefined words\nIt’s confusing!\nWe’re going off assumption that we all have some daily understanding of probability, so stop me if you are confused!"
  },
  {
    "objectID": "lessons/x01_Probability/01_Probability.html#where-are-we",
    "href": "lessons/x01_Probability/01_Probability.html#where-are-we",
    "title": "Lesson 1: Introduction to Probability",
    "section": "",
    "text": "Big part of this lesson is motivating why we care about probability and simulations, then we’ll dive into some basics"
  },
  {
    "objectID": "lessons/x01_Probability/01_Probability.html#what-is-probability-12",
    "href": "lessons/x01_Probability/01_Probability.html#what-is-probability-12",
    "title": "Lesson 1: Introduction to Probability",
    "section": "",
    "text": "We hear the word “probability” pretty often\n\nCommon in news reports, advertisements, sports, medicine, etc.\n\n\n\nResearchers say the probability of living past 110 is on the rise\nCNBC, 2021\n\n\nWe may hear “probability” or similar words like “chance,” “likelihood,” or “odds”\n\n\nScientists fine-tune odds of asteroid Bennu hitting Earth through 2300 with NASA probe’s help\nSpace.com, 2021"
  },
  {
    "objectID": "lessons/x01_Probability/01_Probability.html#before-we-take-one-more-step",
    "href": "lessons/x01_Probability/01_Probability.html#before-we-take-one-more-step",
    "title": "Lesson 1: Introduction to Probability",
    "section": "",
    "text": "The following few slides use some undefined words to define new words that in turn define the previously undefined words\nIt’s confusing!\nWe’re going off assumption that we all have some daily understanding of probability, so stop me if you are confused!"
  },
  {
    "objectID": "lessons/x01_Probability/01_Probability.html#probability-and-randomness",
    "href": "lessons/x01_Probability/01_Probability.html#probability-and-randomness",
    "title": "Lesson 1: Introduction to Probability",
    "section": "Probability and randomness",
    "text": "Probability and randomness\n\nProbability requires randomness\nSomething is random if there are many potential outcomes, but there is uncertainty which outcome will occur\n\n \n\nOutcomes can be “equally likely,” meaning each outcome has the same probability of happening\n\nBut random does NOT necessarily mean equally likely\n\nWe often use physical randomness to demonstrate equally likely outcomes\n\nThink: flipping coins, rolling a dice, drawing cards\n\n\n \n\nThe occurrence of outcomes can be uncertain, but there is an underlying distribution of the probability of outcomes\n\nThere is a distribution of outcomes over large number of (hypothetical repetitions)"
  },
  {
    "objectID": "lessons/x01_Probability/01_Probability.html#a-single-coin-flip-then-100-coin-flips",
    "href": "lessons/x01_Probability/01_Probability.html#a-single-coin-flip-then-100-coin-flips",
    "title": "Lesson 1: Introduction to Probability",
    "section": "A single coin flip then 100 coin flips",
    "text": "A single coin flip then 100 coin flips\nSeeing Theory, Chapter 1: Basic Probability, Chance Events"
  },
  {
    "objectID": "lessons/x01_Probability/01_Probability.html#how-do-we-simulate-this-in-r",
    "href": "lessons/x01_Probability/01_Probability.html#how-do-we-simulate-this-in-r",
    "title": "Lesson 1: Introduction to Probability",
    "section": "How do we simulate this in R?",
    "text": "How do we simulate this in R?\n\nWe know that heads and tails are equally likely for a single flip\n\n\nset.seed(13)\ncoin = c(\"heads\", \"tails\")\nsample(coin, 1)\n\n[1] \"tails\"\n\n\n\nWhen we only flip the coin once, we only only get one outcome (heads or tails)\n\nWe cannot see any distribution of the probability of outcomes"
  },
  {
    "objectID": "lessons/x01_Probability/01_Probability.html#what-if-i-sample-10-coin-flips",
    "href": "lessons/x01_Probability/01_Probability.html#what-if-i-sample-10-coin-flips",
    "title": "Lesson 1: Introduction to Probability",
    "section": "What if I sample 10 coin flips?",
    "text": "What if I sample 10 coin flips?\n\nHow many tails do we get? Are we getting closer to a distribution of heads and tails that we expect?\n\n\n\n\nResults and running proportion of H for 10 flips of a fair coin.\n\n\nFlip\nResult\nRunning count of H\nRunning proportion of H\n\n\n\n\n1\nH\n1\n1.000\n\n\n2\nT\n1\n0.500\n\n\n3\nH\n2\n0.667\n\n\n4\nT\n2\n0.500\n\n\n5\nH\n3\n0.600\n\n\n6\nT\n3\n0.500\n\n\n7\nT\n3\n0.429\n\n\n8\nT\n3\n0.375\n\n\n9\nT\n3\n0.333\n\n\n10\nT\n3\n0.300"
  },
  {
    "objectID": "lessons/x01_Probability/01_Probability.html#what-if-i-sample-100-coin-flips",
    "href": "lessons/x01_Probability/01_Probability.html#what-if-i-sample-100-coin-flips",
    "title": "Lesson 1: Introduction to Probability",
    "section": "What if I sample 100 coin flips?",
    "text": "What if I sample 100 coin flips?"
  },
  {
    "objectID": "lessons/x01_Probability/01_Probability.html#can-we-simulate-more",
    "href": "lessons/x01_Probability/01_Probability.html#can-we-simulate-more",
    "title": "Lesson 1: Introduction to Probability",
    "section": "Can we simulate more??",
    "text": "Can we simulate more??\n\nI can start to count the number of tails in the flips\n\n\nsum( sample(coin, 100, replace = T) == \"heads\" )\n\n[1] 42\n\n\n\nAnd I can see the proportion of tails in the flips\n\n\nsum( sample(coin, 100, replace = T) == \"heads\" ) / 100\n\n[1] 0.42\n\n\n\nI can do this with more flips\n\n\nsum( sample(coin, 1000, replace = T) == \"heads\" ) / 1000\n\n[1] 0.531\n\nsum( sample(coin, 10000, replace = T) == \"heads\" ) / 10000\n\n[1] 0.4956\n\nsum( sample(coin, 100000, replace = T) == \"heads\" ) / 100000\n\n[1] 0.50033"
  },
  {
    "objectID": "lessons/x01_Probability/01_Probability.html#whats-the-point",
    "href": "lessons/x01_Probability/01_Probability.html#whats-the-point",
    "title": "Lesson 1: Introduction to Probability",
    "section": "What’s the point?",
    "text": "What’s the point?\n\nWe know the probability of a heads is 0.5!\n\n \n\nWhy do we need to simulate 100s or 1000s of coin flips?\n\nWith enough repetitions, we can use simulations to approximate the probability of an event\n\n\n \n\nOkay, but why is that helpful? (next slide!)"
  },
  {
    "objectID": "lessons/x01_Probability/01_Probability.html#why-are-simulations-important-bigger-picture",
    "href": "lessons/x01_Probability/01_Probability.html#why-are-simulations-important-bigger-picture",
    "title": "Lesson 1: Introduction to Probability",
    "section": "Why are simulations important? (bigger picture)",
    "text": "Why are simulations important? (bigger picture)\nIn the previous example, coding a simulation seemed more educational than necessary\n \n\nSimulations can help us (or be necessary) to solve a problem when calculations are complex\n\nWe can often calculate probabilities mathematically, but we will eventually get to complex calculations\n\nSimulations are a great way to check your work!\nSimulation based reasoning is helpful in statistics\n\nYou’ll see this in confidence intervals in Biostatistics courses\n\nSimulations allow you to change assumptions easily and see how they affect your results\nIt is often how statisticians “run” experiments on their methods of hypotheses"
  },
  {
    "objectID": "lessons/x01_Probability/01_Probability.html#outcomes-events-sample-spaces",
    "href": "lessons/x01_Probability/01_Probability.html#outcomes-events-sample-spaces",
    "title": "Lesson 1: Introduction to Probability",
    "section": "Outcomes, events, sample spaces",
    "text": "Outcomes, events, sample spaces\n\n\nDefinition: Outcome\n\n\nThe possible results in a random phenomenon.\n\n\n\n\nDefinition: Sample Space\n\n\nThe sample space \\(S\\) is the set of all outcomes\n\n\n\n\nDefinition: Event\n\n\nAn event is a collection of some outcomes. An event can include multiple outcomes or no outcomes (a subset of the sample space).\n\n\nWhen thinking about events, think about outcomes that you might be asking the probability of. For example, what is the probability that you get a heads or a tails in one flip? (Answer: 1)"
  },
  {
    "objectID": "lessons/x01_Probability/01_Probability.html#coin-toss-example-1-coin-13",
    "href": "lessons/x01_Probability/01_Probability.html#coin-toss-example-1-coin-13",
    "title": "Lesson 1: Introduction to Probability",
    "section": "Coin Toss Example: 1 coin (1/3)",
    "text": "Coin Toss Example: 1 coin (1/3)\n\n\n\n\nSingle coin toss\n\n\nSuppose you toss one coin.\n\nWhat are the possible outcomes?\n \nWhat is the sample space?\n \nWhat are the possible events?"
  },
  {
    "objectID": "lessons/x01_Probability/01_Probability.html#coin-toss-example-1-coin-23",
    "href": "lessons/x01_Probability/01_Probability.html#coin-toss-example-1-coin-23",
    "title": "Lesson 1: Introduction to Probability",
    "section": "Coin Toss Example: 1 coin (2/3)",
    "text": "Coin Toss Example: 1 coin (2/3)\nSuppose you toss one coin.\n\nWhat are the possible outcomes?\n\nHeads (\\(H\\))\nTails (\\(T\\))\n\n\n \n\n\nNote\n\n\nWhen something happens at random, such as a coin toss, there are several possible outcomes, and exactly one of the outcomes will occur."
  },
  {
    "objectID": "lessons/x01_Probability/01_Probability.html#coin-toss-example-1-coin-33",
    "href": "lessons/x01_Probability/01_Probability.html#coin-toss-example-1-coin-33",
    "title": "Lesson 1: Introduction to Probability",
    "section": "Coin Toss Example: 1 coin (3/3)",
    "text": "Coin Toss Example: 1 coin (3/3)\n\n\n\nWhat is the sample space?\n\n\\(S =\\)\n\n\n \n \n\nWhat are the possible events?\n\n\n\n\n\n\n \n\n\n\n\nNote #1\n\n\nWe use curly brackets (\\(\\{\\}\\)) to denote a set (collecting a list of outcomes or values)\n\n\n\n\nNote #2\n\n\nThe total number of possible events is \\[2^{|S|}\\] where \\(|S|\\) is the total number of outcomes in the sample space. Also, possible events are not necessarily something that can actually occur (i.e. getting a heads and a tails on a single coin flip)"
  },
  {
    "objectID": "lessons/x01_Probability/01_Probability.html#coin-toss-example-2-coins",
    "href": "lessons/x01_Probability/01_Probability.html#coin-toss-example-2-coins",
    "title": "Lesson 1: Introduction to Probability",
    "section": "Coin Toss Example: 2 coins",
    "text": "Coin Toss Example: 2 coins\nSuppose you toss two coins.\n\nWhat is the sample space? Assume the coins are distinguishable\n\n\\(S =\\)\n\n\n \n\nWhat are some possible events?\n\n\\(A =\\) exactly one \\(H =\\)\n\\(B =\\) at least one \\(H =\\)"
  },
  {
    "objectID": "lessons/x01_Probability/01_Probability.html#more-info-on-events-and-sample-spaces",
    "href": "lessons/x01_Probability/01_Probability.html#more-info-on-events-and-sample-spaces",
    "title": "Lesson 1: Introduction to Probability",
    "section": "More info on events and sample spaces",
    "text": "More info on events and sample spaces\n\nWe usually use capital letters from the beginning of the alphabet to denote events. However, other letters might be chosen to be more descriptive.\n\nExamples: \\(A, B, C, A_1, A_2\\)\n\nWe can also define a new event as a combination of other events (next lesson)\n\nExamples: \\(A \\cup B\\) (union), \\(A \\cap B\\) (intersection), \\(A^C\\) (complement)\n\n\n \n\nWe use the notation \\(|S|\\) to denote the size of the sample space.\n\n \n\nThe total number of possible events is \\(2^{|S|}\\), which is the total number of possible subsets of \\(S\\).\n\n \n\nThe empty set, denoted by \\(\\emptyset\\), is the set containing no outcomes."
  },
  {
    "objectID": "lessons/x01_Probability/01_Probability.html#example-keep-sampling-until",
    "href": "lessons/x01_Probability/01_Probability.html#example-keep-sampling-until",
    "title": "Lesson 1: Introduction to Probability",
    "section": "Example: Keep sampling until…",
    "text": "Example: Keep sampling until…\nSuppose you keep sampling people until you have someone with high blood pressure (BP)\n \nWhat is the sample space?\n\nLet \\(H =\\) denote someone with high BP.\nLet \\(H^C =\\) denote someone with not high blood pressure, such as low or regular BP.\n\n \n\nThen, \\(S =\\)"
  },
  {
    "objectID": "lessons/x01_Probability/01_Probability.html#lets-define-probability-with-events-and-spaces",
    "href": "lessons/x01_Probability/01_Probability.html#lets-define-probability-with-events-and-spaces",
    "title": "Lesson 1: Introduction to Probability",
    "section": "Let’s define probability with events and spaces",
    "text": "Let’s define probability with events and spaces\nIf \\(S\\) is a finite sample space, with equally likely outcomes, then\n\\[\\mathbb{P}(A) = \\frac{|A|}{|S|}\\]\nIn human speak:\n\nFor equally likely outcomes, the probability that a certain event occurs is: the number of outcomes within the event of interest (\\(|A|\\)) divided by the total number of possible outcomes (\\(|S|\\))\n\n\\[\\mathbb{P}(A) = \\frac{\\text{total number of outcomes in event A}}{\\text{total number of outcomes in sample space}}\\]\n\nThus, it is important to be able to count the outcomes within an event"
  },
  {
    "objectID": "lessons/x01_Probability/01_Probability.html#a-probability-is-a-function",
    "href": "lessons/x01_Probability/01_Probability.html#a-probability-is-a-function",
    "title": "Lesson 1: Introduction to Probability",
    "section": "A probability is a function…",
    "text": "A probability is a function…\n\n\\(\\mathbb{P}(A)\\) is a function with\n\nInput: event \\(A\\) from the sample space \\(S\\), (\\(A \\subseteq S\\))\n\n\\(A \\subseteq S\\) means “A contained within S” or “A is a subset of S”\n\nOutput: a number between 0 and 1 (inclusive)\n\n\n \n\nThe probability function maps an event (input) to value between 0 and 1 (output)\n\nWhen we speak of the probability function, we often call the values between 0 and 1 “probabilities”\n\nExample: “The probability of drawing a heart is 0.25” for \\(P(\\text{heart}) = 0.25\\)\n\n\n\n \n\nThe probability function needs to follow some specific rules (called axioms)!"
  },
  {
    "objectID": "lessons/x01_Probability/01_Probability.html#coin-toss-example-revisited-2-coins",
    "href": "lessons/x01_Probability/01_Probability.html#coin-toss-example-revisited-2-coins",
    "title": "Lesson 1: Introduction to Probability",
    "section": "Coin Toss Example Revisited: 2 coins",
    "text": "Coin Toss Example Revisited: 2 coins\n\nFrom our sample space and events:\n\nSample space: \\(S = \\{HH, HT, TH, TT\\}\\)\nEvent A: \\(A = \\text{exactly one H} = \\{HT, TH\\}\\)\nEvent B: \\(B = \\text{at least one H} = \\{HT, TH, HH\\}\\)\n\n\n \n\nCalculate \\(P(A)\\) and \\(P(B)\\)"
  },
  {
    "objectID": "lessons/x10_Transformations/10_Transformations.html",
    "href": "lessons/x10_Transformations/10_Transformations.html",
    "title": "Lesson 10: Transformations",
    "section": "",
    "text": "Find the pdf of a linear rescaling of a random variable\nFind the pdf of a nonlinear transformation of a random variable using the CDF method\n\n\n\n\nOften make transformations of RVs\nA function of a random variable is a random variable\n\nIf \\(X\\) is a random variable and \\(g\\) is a function then \\(Y=g(X)\\) is a random variable\nSince \\(g(X)\\) is a random variable it has a distribution\n\nDistribution of \\(g(X)\\) will have a different shape than the distribution of \\(X\\)\nTwo types:\n\nLinear rescalings: \\(g(u) = a + bu\\)\nNonlinear transformations: e.g. \\(g(u) = u^2\\), \\(g(u) = \\log(u)\\)"
  },
  {
    "objectID": "lessons/x10_Transformations/10_Transformations.html#cdf-method",
    "href": "lessons/x10_Transformations/10_Transformations.html#cdf-method",
    "title": "Lesson 10: Transformations",
    "section": "",
    "text": "Often make transformations of RVs\nA function of a random variable is a random variable\n\nIf \\(X\\) is a random variable and \\(g\\) is a function then \\(Y=g(X)\\) is a random variable\nSince \\(g(X)\\) is a random variable it has a distribution\n\nDistribution of \\(g(X)\\) will have a different shape than the distribution of \\(X\\)\nTwo types:\n\nLinear rescalings: \\(g(u) = a + bu\\)\nNonlinear transformations: e.g. \\(g(u) = u^2\\), \\(g(u) = \\log(u)\\)"
  },
  {
    "objectID": "lessons/x10_Transformations/10_Transformations.html#linear-rescaling",
    "href": "lessons/x10_Transformations/10_Transformations.html#linear-rescaling",
    "title": "Lesson 10: Transformations",
    "section": "Linear rescaling",
    "text": "Linear rescaling\n\n\nDefinition: Linear Rescaling\n\n\nA linear rescaling is a transformation of the form \\(g(u) = a + bu\\), where \\(a\\) and \\(b\\) are constants\n\n\n\nThus, if we have a random variable, \\(X\\), then a linear rescaling of \\(X\\) could be \\(M = g(X) = a + bX\\)\nFor example, converting temperature from Celsius to Fahrenheit using \\(g(u) = 32 + 1.8u\\) is a linear rescaling."
  },
  {
    "objectID": "lessons/x10_Transformations/10_Transformations.html#sim-linear-rescaling",
    "href": "lessons/x10_Transformations/10_Transformations.html#sim-linear-rescaling",
    "title": "Lesson 10: Transformations",
    "section": "Example of linear rescaling (4/4)",
    "text": "Example of linear rescaling (4/4)\n\n\nExample 1: Linear rescaling of \\(U\\)\n\n\nLet \\(U\\) be a random variable with \\(f_U(u)= \\dfrac{4}{15}u^3\\) for \\(1\\leq u \\leq 2\\). Define \\(V=1-U\\)\n\nFind the pdf of \\(V\\).\nDoes \\(V\\) have the same distribution as \\(U\\)?"
  },
  {
    "objectID": "lessons/x10_Transformations/10_Transformations.html#summary-of-linear-rescaling",
    "href": "lessons/x10_Transformations/10_Transformations.html#summary-of-linear-rescaling",
    "title": "Lesson 10: Transformations",
    "section": "Summary of linear rescaling",
    "text": "Summary of linear rescaling\n\nA linear rescaling of a random variable does not change the basic shape of its distribution, just the range of possible values.\n\nIt can flip it, widen it, condense it, and/or shift it\n\nRemember, do NOT confuse a random variable with its distribution\n\nThe random variable is the numerical quantity being measured\nThe distribution is the long run pattern of variation of many observed values of the random variable"
  },
  {
    "objectID": "lessons/x10_Transformations/10_Transformations.html#nonlinear-transformations",
    "href": "lessons/x10_Transformations/10_Transformations.html#nonlinear-transformations",
    "title": "Lesson 10: Transformations",
    "section": "Nonlinear transformations",
    "text": "Nonlinear transformations\n\nWhat happens when we make a nonlinear transformation, like a logarithmic or square root transformation?\nNonlinear transformations do not necessarily preserve the distribution shape\nExamples of nonlinear transformations:\n\n\\(g(u) = u^2\\)\n\\(g(u) = \\sqrt{u}\\)\n\\(g(u) = \\log(u)\\)\n\\(g(u) = e^u\\)\n\\(g(u) = \\dfrac{1}{u}\\)"
  },
  {
    "objectID": "lessons/x10_Transformations/10_Transformations.html#finding-the-pdf-of-a-transformation",
    "href": "lessons/x10_Transformations/10_Transformations.html#finding-the-pdf-of-a-transformation",
    "title": "Lesson 10: Transformations",
    "section": "Finding the pdf of a transformation",
    "text": "Finding the pdf of a transformation\n\nLet \\(M\\) be a transformation of \\(X\\): \\(M = g(X)\\)\nWhen we have a transformation of \\(X\\), \\(M\\), we need to follow the CDF method to find the pdf of \\(M\\)\n\nWe follow CDF method:\n\nStart with the pdf for \\(X\\)\n\naka \\(f_{X}(x)\\)\n\nTranslate the domain of \\(X\\) to \\(M\\): find the possible values of \\(M\\)\nFind the CDF of \\(M\\)\n\naka \\(F_M(m) = P(M \\leq m) = P(g(X) \\leq m)\\)\nWill require manipulating \\(g(X) \\leq m\\) in terms of \\(X\\) (aka \\(X\\) alone on the left side)\n\nTake the derivative of the CDF of \\(M\\) with respect to \\(m\\) to find the pdf of \\(M\\)\n\naka \\(f_M(m) = \\dfrac{d}{dm}F_M(m)\\)"
  },
  {
    "objectID": "lessons/x10_Transformations/10_Transformations.html#example-of-nonlinear-transformation-14",
    "href": "lessons/x10_Transformations/10_Transformations.html#example-of-nonlinear-transformation-14",
    "title": "Lesson 10: Transformations",
    "section": "Example of nonlinear transformation (1/4)",
    "text": "Example of nonlinear transformation (1/4)\n\n\nExample 2: Nonlinear transformation of \\(U\\)\n\n\nLet \\(U\\) be a random variable with \\(f_U(u)= \\dfrac{4}{15}u^3\\) for \\(1\\leq u \\leq 2\\). Define \\(V=\\log(U)\\)\n\nWhat are the possible values of \\(V\\)?\nFind the CDF of \\(V\\)\nFind the pdf of \\(V\\)"
  },
  {
    "objectID": "lessons/x10_Transformations/10_Transformations.html#example-of-nonlinear-transformation-24",
    "href": "lessons/x10_Transformations/10_Transformations.html#example-of-nonlinear-transformation-24",
    "title": "Lesson 10: Transformations",
    "section": "Example of nonlinear transformation (2/4)",
    "text": "Example of nonlinear transformation (2/4)\n\n\nExample 2: Nonlinear transformation of \\(U\\)\n\n\nLet \\(U\\) be a random variable with \\(f_U(u)= \\dfrac{4}{15}u^3\\) for \\(1\\leq u \\leq 2\\). Define \\(V=\\log(U)\\)\n\nWhat are the possible values of \\(V\\)?\n\n\n\n\n\n\ndomain_u_v = tibble(\n  u = seq(1, 2, 0.01), \n  v = log(u)\n)\n\nggplot(domain_u_v, aes(x = u, y = v)) +\n  geom_line() +\n  labs(\n    title = \"Transformation: V = log(U)\",\n    x = \"u\",\n    y = \"log(u)\"\n  )"
  },
  {
    "objectID": "lessons/x10_Transformations/10_Transformations.html#example-of-nonlinear-transformation-34",
    "href": "lessons/x10_Transformations/10_Transformations.html#example-of-nonlinear-transformation-34",
    "title": "Lesson 10: Transformations",
    "section": "Example of nonlinear transformation (3/4)",
    "text": "Example of nonlinear transformation (3/4)\n\n\nExample 2: Nonlinear transformation of \\(U\\)\n\n\nLet \\(U\\) be a random variable with \\(f_U(u)= \\dfrac{4}{15}u^3\\) for \\(1\\leq u \\leq 2\\). Define \\(V=\\log(U)\\)\n\nFind the CDF of \\(V\\)"
  },
  {
    "objectID": "lessons/x10_Transformations/10_Transformations.html#example-of-nonlinear-transformation-44",
    "href": "lessons/x10_Transformations/10_Transformations.html#example-of-nonlinear-transformation-44",
    "title": "Lesson 10: Transformations",
    "section": "Example of nonlinear transformation (4/4)",
    "text": "Example of nonlinear transformation (4/4)\n\n\nExample 2: Nonlinear transformation of \\(U\\)\n\n\nLet \\(U\\) be a random variable with \\(f_U(u)= \\dfrac{4}{15}u^3\\) for \\(1\\leq u \\leq 2\\). Define \\(V=\\log(U)\\)\n\nFind the pdf of \\(V\\)"
  },
  {
    "objectID": "lessons/x10_Transformations/10_Transformations.html#example-of-nonlinear-transformation-domain-14",
    "href": "lessons/x10_Transformations/10_Transformations.html#example-of-nonlinear-transformation-domain-14",
    "title": "Lesson 10: Transformations",
    "section": "Example of nonlinear transformation: domain (1/4)",
    "text": "Example of nonlinear transformation: domain (1/4)\n\n\nExample 3: Nonlinear transformation of \\(X\\)\n\n\nLet \\(X\\) be a random variable with \\(f_X(x)= \\dfrac{1}{2}\\) for \\(-1\\leq x \\leq 1\\). Define \\(Y=X^2\\)\n\nWhat are the possible values of \\(Y\\)?\nFind the CDF of \\(Y\\)\nFind the pdf of \\(Y\\)"
  },
  {
    "objectID": "lessons/x10_Transformations/10_Transformations.html#example-of-nonlinear-transformation-domain-24",
    "href": "lessons/x10_Transformations/10_Transformations.html#example-of-nonlinear-transformation-domain-24",
    "title": "Lesson 10: Transformations",
    "section": "Example of nonlinear transformation: domain (2/4)",
    "text": "Example of nonlinear transformation: domain (2/4)\n\n\nExample 3: Nonlinear transformation of \\(X\\)\n\n\nLet \\(X\\) be a random variable with \\(f_X(x)= \\dfrac{1}{2}\\) for \\(-1\\leq x \\leq 1\\). Define \\(Y=X^2\\)\n\nWhat are the possible values of \\(Y\\)?\n\n\n\n\n\n\ndomain_x_y = tibble(\n  x = seq(-1.5, 1.5, 0.01), \n  y = x^2\n)\n\nggplot(domain_x_y, aes(x = x, y = y)) +\n  geom_line() +\n  labs(\n    title = \"Transformation: Y = X^2\",\n    x = \"x\",\n    y = \"x^2\"\n  )"
  },
  {
    "objectID": "lessons/x10_Transformations/10_Transformations.html#example-of-nonlinear-transformation-domain-34",
    "href": "lessons/x10_Transformations/10_Transformations.html#example-of-nonlinear-transformation-domain-34",
    "title": "Lesson 10: Transformations",
    "section": "Example of nonlinear transformation: domain (3/4)",
    "text": "Example of nonlinear transformation: domain (3/4)\n\n\nExample 3: Nonlinear transformation of \\(X\\)\n\n\nLet \\(X\\) be a random variable with \\(f_X(x)= \\dfrac{1}{2}\\) for \\(-1\\leq x \\leq 1\\). Define \\(Y=X^2\\)\n\nFind the CDF of \\(Y\\)"
  },
  {
    "objectID": "lessons/x10_Transformations/10_Transformations.html#example-of-nonlinear-transformation-domain-44",
    "href": "lessons/x10_Transformations/10_Transformations.html#example-of-nonlinear-transformation-domain-44",
    "title": "Lesson 10: Transformations",
    "section": "Example of nonlinear transformation: domain (4/4)",
    "text": "Example of nonlinear transformation: domain (4/4)\n\n\nExample 3: Nonlinear transformation of \\(X\\)\n\n\nLet \\(X\\) be a random variable with \\(f_X(x)= \\dfrac{1}{2}\\) for \\(-1\\leq x \\leq 1\\). Define \\(Y=X^2\\)\n\nFind the pdf of \\(Y\\)"
  },
  {
    "objectID": "lessons/x10_Transformations/10_Transformations.html#summary-of-nonlinear-transformations",
    "href": "lessons/x10_Transformations/10_Transformations.html#summary-of-nonlinear-transformations",
    "title": "Lesson 10: Transformations",
    "section": "Summary of nonlinear transformations",
    "text": "Summary of nonlinear transformations\n\nNonlinear transformations can change the shape of a distribution\nAlways use the CDF method to find the pdf of a nonlinear transformation of a random variable\nRemember to carefully determine the possible values of the transformed random variable"
  },
  {
    "objectID": "lessons/x10_Transformations/Old_notes/25_Joint_densities_muddy_points.html",
    "href": "lessons/x10_Transformations/Old_notes/25_Joint_densities_muddy_points.html",
    "title": "Muddy Points",
    "section": "",
    "text": "Muddy points from Fall 2023:\n\n1. How do pdf, CDF, and probability interact with each other?\nLet’s say we have a pdf, \\(f_X(x) = \\dfrac{1}{9}x^2\\) for \\(0 \\leq x \\leq 3\\). This is just a function. The pdf is not used on its own to report any probability. We must integrate over the pdf to find a probability.\n\nlibrary(\"ggplot2\")\neq = function(x){(1/9)*x^2}\nggplot(data.frame(x=c(1, 50)), aes(x=x)) + \n  stat_function(fun=eq) +\n  xlab(\"x\") + ylab(\"pdf\") +\n  xlim(0,3)\n\n\n\n\n\n\n\n\nThe total area under the pdf is 1. This makes our pdf valid.\n\neq = function(x){(1/9)*x^2}\nggplot(data.frame(x=c(1, 50)), aes(x=x)) + \n  stat_function(fun=eq) +\n  xlab(\"x\") + ylab(\"pdf\") +\n  xlim(0,3) +\n  stat_function(fun=eq, \n                xlim = c(0, 3),\n                geom = \"area\", \n                aes(fill = \"red\")) +\n  theme(legend.position = \"none\") +\n  annotate(\"text\", x = 0.5, y = 0.7, label = \"AUC = 1\", color = \"black\")\n\n\n\n\n\n\n\n\nIf we only look at a proportion of the area under the pdf, then we start constructing our probabilities. For example, we can look at probability that we have a value between 0 and 1.5.\n\nggplot(data.frame(x=c(1, 50)), aes(x=x)) + \n  stat_function(fun=eq) +\n  xlab(\"x\") + ylab(\"pdf\") +\n  xlim(0,3) +\n  stat_function(fun=eq, \n                xlim = c(0, 1.5),\n                geom = \"area\", \n                aes(fill = \"blue\")) +\n  theme(legend.position = \"none\") +\n  annotate(\"text\", x = 0.5, y = 0.7, label = \"AUC = 0.125\", color = \"black\")\n\n\n\n\n\n\n\n\nInstead of calculating the EXACT probability for each value between 0 and 3, we can find the CDF of the pdf.\nThe CDF is: \\[\nF_X(x) = \\left\\{\n        \\begin{array}{ll}\n            0 & \\quad x&lt;3 \\quad \\\\\n            \\dfrac{1}{27}x^3 & \\quad 0 \\leq x \\leq 3\\quad \\\\\n            1 & \\quad x&gt;3 \\quad\n        \\end{array}\n    \\right.\n\\]\n\ncdf = function(x){(1/27)*x^3}\nggplot(data.frame(x=c(1, 50)), aes(x=x)) + \n  stat_function(fun=cdf) +\n  xlab(\"x\") + ylab(\"CDF\") +\n  xlim(0,3) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nWhen \\(x=1.5\\), we can calculate the probability using the CDF. Remember that \\(F_X(x) = P(X \\leq x)\\). So we can say \\(P(X \\leq 1.5) = F_X(1.5) = \\dfrac{1}{27}(1.5)^3\\), which equals 0.125.\n\ncdf = function(x){(1/27)*x^3}\nggplot(data.frame(x=c(1, 50)), aes(x=x)) + \n  stat_function(fun=cdf) +\n  xlab(\"x\") + ylab(\"CDF\") +\n  xlim(0,3) +\n  theme(legend.position = \"none\") +\n  geom_point(aes(x=1.5, y=.125), colour=\"blue\", size=3) +\n  annotate(\"text\", x = 0.5, y = 0.7, label = \"CDF = 0.125\", color = \"black\")\n\nWarning in geom_point(aes(x = 1.5, y = 0.125), colour = \"blue\", size = 3): All aesthetics have length 1, but the data has 2 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\nWe can also calculate the probability with an integral: \\(P(X \\leq 1.5) = \\displaystyle\\int_0^{1.5} \\dfrac{1}{9}x^2 dx\\).\nWe can also find the probability that X is between two numbers. \\(P(1\\leq X \\leq 1.5) = F_X(1.5) - F_X(1)\\) or \\(P(1\\leq X \\leq 1.5) = \\displaystyle\\int_1^{1.5} \\dfrac{1}{9}x^2 dx\\).\n\n\n2. Joint vs marginal vs conditional: How are we calculating the probability?\nIf we start at a joint probability \\(f_{X,Y}(x,y)\\)…. we can look at a few probabilities:\n\nJoint probability: \\(P(a \\leq X \\leq b, c \\leq Y \\leq d)\\)\n\\[P(a \\leq X \\leq b, c \\leq Y \\leq d) = \\displaystyle\\int_{x=a}^{x=b}\\displaystyle\\int_{y=c}^{y=d} f_{X,Y}(x,y) dydx\\]\nMarginal probability: \\(P(a \\leq X \\leq b)\\)\n\\[P(a \\leq X \\leq b) = \\displaystyle\\int_{x=a}^{x=b} f_{X}(x) dx\\]\nOR\n\\[P(a \\leq X \\leq b) = \\displaystyle\\int_{x=a}^{x=b}\\displaystyle\\int_{y=-\\inf}^{y=\\inf} f_{X,Y}(x,y) dydx\\]\nConditional probability: \\(P(a \\leq X \\leq b | Y = c)\\)\n\\[P(a \\leq X \\leq b | Y=c) = \\displaystyle\\int_{x=a}^{x=b} f_{X|Y}(x|y=c) dx\\]\nYou cannot calculate \\(P(a \\leq X \\leq b | Y = c)\\) by \\(\\dfrac{P(a \\leq X \\leq b, Y=c)}{P(Y = c)}\\) because \\(P(Y = c)\\) is 0. Instead, we need to find \\(f_{X|Y}(x|y=c)\\) by \\(\\dfrac{f_{X,Y}(x,y=c)}{f_{Y}(y=c)}\\) and THEN integrate over X.\n\n\n\n3. What are we actually finding by solving the double integral. In the first example, we found the probability was 1/16 after integrating but what does 1/16 mean in relation to the random variables X and Y?\nIt means that the volume contained by \\(0\\leq X \\leq 1\\), \\(0\\leq Y \\leq 1/2\\), and their joint pdf is 1/16 of the total volume contained by \\(0\\leq X \\leq 2\\), \\(0\\leq Y \\leq 1\\), and their joint pdf. The probability for a joint pdf is now a measure of the proportion of the volume.\nThis is not be confused with a probability from marginal pdf or pdf from one RV. The probability for marginal/single RV pdfs is the proportion of the area under the pdf for a specific range of values.\n\n\n4. Here’s a 3D plot of one of our joint pdf’s\n\\[\nf_{X,Y}(x,y) = 5e^{-x-3y} \\text{ for } 0 \\leq y \\leq x/2\n\\]\n\nlibrary(plotly)\n\nx = seq(0, 5, 0.1)\ny = seq(0, max(x)/2, 0.1/2)\nfn = expand.grid(x=x,y=y)\nfn$z = ifelse(fn$y&lt;fn$x/2, 5*exp( (-1)*fn$x - 3*fn$y), NA)\n\nz = matrix(fn$z, ncol = 51, nrow = 51, byrow = T)\n\nfig &lt;- plot_ly(x = x, y=y, z=z) %&gt;% add_surface()\n\nfig"
  },
  {
    "objectID": "lessons/x10_Transformations/Old_notes/25_Joint_densities.html",
    "href": "lessons/x10_Transformations/Old_notes/25_Joint_densities.html",
    "title": "Chapter 25: Joint densities",
    "section": "",
    "text": "Solve double integrals in our mini lesson!\nCalculate probabilities for a pair of continuous random variables\nCalculate a joint and marginal probability density function (pdf)\nCalculate a joint and marginal cumulative distribution function (CDF) from a pdf"
  },
  {
    "objectID": "lessons/x10_Transformations/Old_notes/25_Joint_densities.html#learning-objectives",
    "href": "lessons/x10_Transformations/Old_notes/25_Joint_densities.html#learning-objectives",
    "title": "Chapter 25: Joint densities",
    "section": "",
    "text": "Solve double integrals in our mini lesson!\nCalculate probabilities for a pair of continuous random variables\nCalculate a joint and marginal probability density function (pdf)\nCalculate a joint and marginal cumulative distribution function (CDF) from a pdf"
  },
  {
    "objectID": "lessons/x10_Transformations/Old_notes/25_Joint_densities.html#double-integrals-mini-lesson-13",
    "href": "lessons/x10_Transformations/Old_notes/25_Joint_densities.html#double-integrals-mini-lesson-13",
    "title": "Chapter 25: Joint densities",
    "section": "Double Integrals Mini Lesson (1/3)",
    "text": "Double Integrals Mini Lesson (1/3)\n\n\n\n\n\n\nDo this problem at home for extra practice. I’ll add the solution to the annotated notes!\n\n\n\n\n\n\n\nMini Lesson Example 1\n\n\nSolve the following integral: \\(\\displaystyle\\int_{2}^{3}\\displaystyle\\int_{0}^{1} xy dydx\\)"
  },
  {
    "objectID": "lessons/x10_Transformations/Old_notes/25_Joint_densities.html#double-integrals-mini-lesson-23",
    "href": "lessons/x10_Transformations/Old_notes/25_Joint_densities.html#double-integrals-mini-lesson-23",
    "title": "Chapter 25: Joint densities",
    "section": "Double Integrals Mini Lesson (2/3)",
    "text": "Double Integrals Mini Lesson (2/3)\n\n\n\n\n\n\nDo this problem at home for extra practice. I’ll add the solution to the annotated notes!\n\n\n\n\n\n\n\nMini Lesson Example 2\n\n\nSolve the following integral: \\(\\displaystyle\\int_{2}^{3}\\displaystyle\\int_{0}^{1} (x+y) dydx\\)"
  },
  {
    "objectID": "lessons/x10_Transformations/Old_notes/25_Joint_densities.html#double-integrals-mini-lesson-33",
    "href": "lessons/x10_Transformations/Old_notes/25_Joint_densities.html#double-integrals-mini-lesson-33",
    "title": "Chapter 25: Joint densities",
    "section": "Double Integrals Mini Lesson (3/3)",
    "text": "Double Integrals Mini Lesson (3/3)\n\n\n\n\n\n\nDo this problem at home for extra practice. I’ll add the solution to the annotated notes!\n\n\n\n\n\n\n\nMini Lesson Example 3\n\n\nSolve the following integral: \\(\\displaystyle\\int_{2}^{3}\\displaystyle\\int_{0}^{1} e^{x+y} dydx\\)"
  },
  {
    "objectID": "lessons/x10_Transformations/Old_notes/25_Joint_densities.html#how-to-define-the-joint-pdf-for-continuous-rvs",
    "href": "lessons/x10_Transformations/Old_notes/25_Joint_densities.html#how-to-define-the-joint-pdf-for-continuous-rvs",
    "title": "Chapter 25: Joint densities",
    "section": "How to define the joint pdf for continuous RVs?",
    "text": "How to define the joint pdf for continuous RVs?\n\n\nFor a single continuous RV \\(X\\) is a function \\(f_X(x)\\), such that for all real values \\(a,b\\) with \\(a \\leq b\\), \\[\\mathbb{P}(a \\leq X \\leq b) = \\int_a^b f_X(x)dx\\]\n\nFor two continuous RVs (\\(X\\) and \\(Y\\)), we can define the joint pdf, \\(f_{X,Y}(x,y)\\), such that for all real values \\(a,b, c, d\\) with \\(a \\leq b\\) and \\(c \\leq d\\), \\[\\mathbb{P}(a \\leq X \\leq b, c \\leq Y \\leq d) = \\int_a^b \\int_c^d f_{X,Y}(x,y)dydx\\]"
  },
  {
    "objectID": "lessons/x10_Transformations/Old_notes/25_Joint_densities.html#important-properties-of-the-joint-pdf",
    "href": "lessons/x10_Transformations/Old_notes/25_Joint_densities.html#important-properties-of-the-joint-pdf",
    "title": "Chapter 25: Joint densities",
    "section": "Important properties of the joint pdf",
    "text": "Important properties of the joint pdf\n\nNote that \\(f_{X,Y}(x,y)\\neq \\mathbb{P}(X=x, Y=y)\\)!!!\nIn order for \\(f_{X,Y}(x,y)\\) to be a pdf, it needs to satisfy the properties\n\n\\(f_{X,Y}(x,y)\\geq 0\\) for all \\(x,y\\)\n\\(\\displaystyle\\int_{-\\infty}^{\\infty}\\displaystyle\\int_{-\\infty}^{\\infty} f_{X,Y}(x,y)dxdy=1\\)"
  },
  {
    "objectID": "lessons/x10_Transformations/Old_notes/25_Joint_densities.html#what-is-the-joint-cumulative-distribution-function",
    "href": "lessons/x10_Transformations/Old_notes/25_Joint_densities.html#what-is-the-joint-cumulative-distribution-function",
    "title": "Chapter 25: Joint densities",
    "section": "What is the joint cumulative distribution function?",
    "text": "What is the joint cumulative distribution function?\n\n\nDefinition: Joint cumulative distribution function (Joint CDF)\n\n\nThe joint cumulative distribution function (cdf) of continuous random variables \\(X\\) and \\(Y\\), is the function \\(F_{X,Y}(x,y)\\), such that for all real values of \\(x\\) and \\(y\\), \\[F_{X,Y}(x,y)= \\mathbb{P}(X \\leq x, Y \\leq y) = \\int_{-\\infty}^x\\int_{-\\infty}^y f_{X,Y}(s,t)dtds\\]\n\n\nRemarks:\n\nThe definition above for \\(F_{X,Y}(x,y)\\) is a function of \\(x\\) and \\(y\\).\nThe joint cdf at the point \\((a,b)\\), is \\[F_{X,Y}(a,b) = \\mathbb{P}(X \\leq a, Y \\leq b) = \\int_{-\\infty}^a\\int_{-\\infty}^b f_{X,Y}(s,t)dtds\\]"
  },
  {
    "objectID": "lessons/x10_Transformations/Old_notes/25_Joint_densities.html#what-are-the-marginal-pdfs",
    "href": "lessons/x10_Transformations/Old_notes/25_Joint_densities.html#what-are-the-marginal-pdfs",
    "title": "Chapter 25: Joint densities",
    "section": "What are the marginal pdf’s?",
    "text": "What are the marginal pdf’s?\n\n\nDefinition: Marginal pdf’s\n\n\nSuppose \\(X\\) and \\(Y\\) are continuous r.v.’s, with joint pdf \\(f_{X,Y}(x,y)\\). Then the marginal probability density functions are \\[\\begin{aligned}\nf_X(x)&=& \\int_{-\\infty}^{\\infty} f_{X,Y}(x,y)dy\\\\\nf_Y(y)&=& \\int_{-\\infty}^{\\infty} f_{X,Y}(x,y)dx\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/x10_Transformations/Old_notes/25_Joint_densities.html#common-steps-for-solving-problems",
    "href": "lessons/x10_Transformations/Old_notes/25_Joint_densities.html#common-steps-for-solving-problems",
    "title": "Chapter 25: Joint densities",
    "section": "Common steps for solving problems",
    "text": "Common steps for solving problems\n\nSet up the domain of the pdf with a picture\nTranslate to needed integrands\n\nFor probability: shade in the area of interest, then translate\nFor expected value: translate domain\n\nSet up integral: \\(dxdy\\) or \\(dydx\\)?\nSolve integral!"
  },
  {
    "objectID": "lessons/x10_Transformations/Old_notes/25_Joint_densities.html#example-of-joint-pdf",
    "href": "lessons/x10_Transformations/Old_notes/25_Joint_densities.html#example-of-joint-pdf",
    "title": "Chapter 25: Joint densities",
    "section": "Example of joint pdf",
    "text": "Example of joint pdf\n\n\n\n\nExample 1.1\n\n\nLet \\(f_{X,Y}(x,y)= \\frac32 y^2\\), for \\(0 \\leq x \\leq 2, \\ 0 \\leq y \\leq 1\\).\n\nFind \\(\\mathbb{P}(0 \\leq X \\leq 1, 0 \\leq Y \\leq \\frac12)\\)."
  },
  {
    "objectID": "lessons/x10_Transformations/Old_notes/25_Joint_densities.html#example-of-joint-pdf-1",
    "href": "lessons/x10_Transformations/Old_notes/25_Joint_densities.html#example-of-joint-pdf-1",
    "title": "Chapter 25: Joint densities",
    "section": "Example of joint pdf",
    "text": "Example of joint pdf\n\n\n\n\nExample 1.2\n\n\nLet \\(f_{X,Y}(x,y)= \\frac32 y^2\\), for \\(0 \\leq x \\leq 2, \\ 0 \\leq y \\leq 1\\).\n\nFind \\(f_X(x)\\) and \\(f_Y(y)\\)."
  },
  {
    "objectID": "lessons/x10_Transformations/Old_notes/25_Joint_densities.html#example-of-a-more-complicated-joint-pdf",
    "href": "lessons/x10_Transformations/Old_notes/25_Joint_densities.html#example-of-a-more-complicated-joint-pdf",
    "title": "Chapter 25: Joint densities",
    "section": "Example of a more complicated joint pdf",
    "text": "Example of a more complicated joint pdf\n\n\n\n\n\n\nDo this problem at home for extra practice. I’ll add the solution to the annotated notes!\n\n\n\n\n\n\n\nExample 2.1\n\n\nLet \\(f_{X,Y}(x,y)= 2 e^{-(x+y)}\\), for \\(0 \\leq x \\leq y\\).\n\nFind \\(f_X(x)\\) and \\(f_Y(y)\\)."
  },
  {
    "objectID": "lessons/x10_Transformations/Old_notes/25_Joint_densities.html#example-of-a-more-complicated-joint-pdf-1",
    "href": "lessons/x10_Transformations/Old_notes/25_Joint_densities.html#example-of-a-more-complicated-joint-pdf-1",
    "title": "Chapter 25: Joint densities",
    "section": "Example of a more complicated joint pdf",
    "text": "Example of a more complicated joint pdf\n\n\n\n\n\n\nDo this problem at home for extra practice. I’ll add the solution to the annotated notes!\n\n\n\n\n\n\n\nExample 2.2\n\n\nLet \\(f_{X,Y}(x,y)= 2 e^{-(x+y)}\\), for \\(0 \\leq x \\leq y\\).\n\nFind \\(\\mathbb{P}(Y &lt; 3)\\)."
  },
  {
    "objectID": "lessons/x10_Transformations/Old_notes/25_Joint_densities.html#lets-complicate-this-even-more",
    "href": "lessons/x10_Transformations/Old_notes/25_Joint_densities.html#lets-complicate-this-even-more",
    "title": "Chapter 25: Joint densities",
    "section": "Let’s complicate this even more!",
    "text": "Let’s complicate this even more!\n\n\n\n\nExample 3.1\n\n\nLet \\(X\\) and \\(Y\\) have constant density on the square \\(0 \\leq X \\leq 4, 0 \\leq Y \\leq 4\\).\n\nFind \\(\\mathbb{P}(|X-Y| &lt; 2)\\)"
  },
  {
    "objectID": "lessons/x10_Transformations/Old_notes/25_Joint_densities.html#finding-the-pdf-of-a-transformation",
    "href": "lessons/x10_Transformations/Old_notes/25_Joint_densities.html#finding-the-pdf-of-a-transformation",
    "title": "Chapter 25: Joint densities",
    "section": "Finding the pdf of a transformation",
    "text": "Finding the pdf of a transformation\n\nLet \\(M\\) be a transformation of \\(X\\) and \\(Y\\)\nWhen we have a transformation of \\(X\\) and \\(Y\\), \\(M\\), we need to follow a specific process to find the pdf of \\(M\\)\n\nWe follow this process:\n\nStart with the joint pdf for \\(X\\) and \\(Y\\)\n\naka \\(f_{X,Y}(x, y)\\)\n\nTranslate the domain of \\(X\\) and \\(Y\\) to \\(M\\)\nFind the CDF of \\(M\\)\n\naka \\(F_M(m)\\) or \\(P(M \\leq m)\\)\n\nTake the derivative of the CDF of \\(M\\) to find the pdf of \\(M\\)\n\naka \\(f_M(m) = \\dfrac{d}{dm}F_M(m)\\)"
  },
  {
    "objectID": "lessons/x10_Transformations/Old_notes/25_Joint_densities.html#lets-complicate-this-even-more-1",
    "href": "lessons/x10_Transformations/Old_notes/25_Joint_densities.html#lets-complicate-this-even-more-1",
    "title": "Chapter 25: Joint densities",
    "section": "Let’s complicate this even more!",
    "text": "Let’s complicate this even more!\n\n\n\n\nExample 3.2\n\n\nLet \\(X\\) and \\(Y\\) have constant density on the square \\(0 \\leq X \\leq 4, 0 \\leq Y \\leq 4\\).\n\nLet \\(M = \\max(X,Y)\\). Find the pdf for \\(M\\), that is \\(f_M(m)\\)."
  },
  {
    "objectID": "lessons/x10_Transformations/Old_notes/25_Joint_densities.html#lets-complicate-this-even-more-2",
    "href": "lessons/x10_Transformations/Old_notes/25_Joint_densities.html#lets-complicate-this-even-more-2",
    "title": "Chapter 25: Joint densities",
    "section": "Let’s complicate this even more!",
    "text": "Let’s complicate this even more!\n\n\n\n\n\n\nDo this problem at home for extra practice. I’ll add the solution to the annotated notes!\n\n\n\n\n\n\n\nExample 3.3\n\n\nLet \\(X\\) and \\(Y\\) have constant density on the square \\(0 \\leq X \\leq 4, 0 \\leq Y \\leq 4\\).\n\nLet \\(Z = \\min(X,Y)\\). Find the pdf for \\(Z\\), that is \\(f_Z(z)\\)."
  },
  {
    "objectID": "lessons/x10_Transformations/Old_notes/25_Joint_densities.html#lets-complicate-this-even-further",
    "href": "lessons/x10_Transformations/Old_notes/25_Joint_densities.html#lets-complicate-this-even-further",
    "title": "Chapter 25: Joint densities",
    "section": "Let’s complicate this even further!",
    "text": "Let’s complicate this even further!\n\n\n\n\n\n\nDo this problem at home for extra practice. I’ll add the solution to the annotated notes!\n\n\n\n\n\n\n\nExample 4\n\n\nLet \\(X\\) and \\(Y\\) have joint density \\(f_{X,Y}(x,y)= \\frac85(x+y)\\) in the region \\(0 &lt; x &lt; 1,\\ \\frac12 &lt; y &lt;1\\). Find the pdf of the r.v. \\(Z\\), where \\(Z=XY\\)."
  },
  {
    "objectID": "lessons/03_Test_Distributions/03_Test_Distributions.html",
    "href": "lessons/03_Test_Distributions/03_Test_Distributions.html",
    "title": "BSTA 551: Statistical Inference",
    "section": "",
    "text": "Key concepts from Lessons 1-2:\n\nEstimator vs. estimate; parameters vs. statistics\nBias: \\(\\text{Bias}(\\hat{\\theta}) = E(\\hat{\\theta}) - \\theta\\)\nVariance and Standard Error: \\(SE(\\hat{\\theta}) = \\sqrt{\\text{Var}(\\hat{\\theta})}\\)\nMean Squared Error: \\(\\text{MSE} = \\text{Var} + \\text{Bias}^2\\)\n\n. . .\nToday’s Goals:\n\n\nUnderstand the chi-squared distribution and its connection to normal samples\nLearn the t distribution and when it arises\nUnderstand the F distribution as a ratio of chi-squared variables\nUse R to work with these distributions\n\n\n\n\n\nA pharmaceutical company is studying individual variation in drug metabolism.\nQuestion: How do we characterize the variability in patient responses, not just the average?\n\n# Liver enzyme levels (U/L) from 15 patients\nenzyme_levels &lt;- c(42, 38, 51, 45, 40, 55, 48, 37, 44, 52, \n                   46, 43, 49, 41, 47)\n\ntibble(\n  Statistic = c(\"Sample Mean\", \"Sample SD\", \"Sample Variance\"),\n  Value = c(mean(enzyme_levels), sd(enzyme_levels), var(enzyme_levels))\n) |&gt; mutate(Value = round(Value, 2))\n\n# A tibble: 3 × 2\n  Statistic       Value\n  &lt;chr&gt;           &lt;dbl&gt;\n1 Sample Mean     45.2 \n2 Sample SD        5.23\n3 Sample Variance 27.3 \n\n\n. . .\nKey insight: To make inferences about variability, we need to understand the sampling distribution of the variance."
  },
  {
    "objectID": "lessons/03_Test_Distributions/03_Test_Distributions.html#review-where-we-left-off",
    "href": "lessons/03_Test_Distributions/03_Test_Distributions.html#review-where-we-left-off",
    "title": "BSTA 551: Statistical Inference",
    "section": "",
    "text": "Key concepts from Lessons 1-2:\n\nEstimator vs. estimate; parameters vs. statistics\nBias: \\(\\text{Bias}(\\hat{\\theta}) = E(\\hat{\\theta}) - \\theta\\)\nVariance and Standard Error: \\(SE(\\hat{\\theta}) = \\sqrt{\\text{Var}(\\hat{\\theta})}\\)\nMean Squared Error: \\(\\text{MSE} = \\text{Var} + \\text{Bias}^2\\)\n\n. . .\nToday’s Goals:\n\n\nUnderstand the chi-squared distribution and its connection to normal samples\nLearn the t distribution and when it arises\nUnderstand the F distribution as a ratio of chi-squared variables\nUse R to work with these distributions"
  },
  {
    "objectID": "lessons/03_Test_Distributions/03_Test_Distributions.html#motivating-example-variability-in-drug-response",
    "href": "lessons/03_Test_Distributions/03_Test_Distributions.html#motivating-example-variability-in-drug-response",
    "title": "BSTA 551: Statistical Inference",
    "section": "",
    "text": "A pharmaceutical company is studying individual variation in drug metabolism.\nQuestion: How do we characterize the variability in patient responses, not just the average?\n\n# Liver enzyme levels (U/L) from 15 patients\nenzyme_levels &lt;- c(42, 38, 51, 45, 40, 55, 48, 37, 44, 52, \n                   46, 43, 49, 41, 47)\n\ntibble(\n  Statistic = c(\"Sample Mean\", \"Sample SD\", \"Sample Variance\"),\n  Value = c(mean(enzyme_levels), sd(enzyme_levels), var(enzyme_levels))\n) |&gt; mutate(Value = round(Value, 2))\n\n# A tibble: 3 × 2\n  Statistic       Value\n  &lt;chr&gt;           &lt;dbl&gt;\n1 Sample Mean     45.2 \n2 Sample SD        5.23\n3 Sample Variance 27.3 \n\n\n. . .\nKey insight: To make inferences about variability, we need to understand the sampling distribution of the variance."
  },
  {
    "objectID": "lessons/03_Test_Distributions/03_Test_Distributions.html#the-chi-squared-distribution-definition-devore-6.3",
    "href": "lessons/03_Test_Distributions/03_Test_Distributions.html#the-chi-squared-distribution-definition-devore-6.3",
    "title": "BSTA 551: Statistical Inference",
    "section": "The Chi-Squared Distribution: Definition (Devore 6.3)",
    "text": "The Chi-Squared Distribution: Definition (Devore 6.3)\n\n\n\n\n\n\nDefinition\n\n\n\nFor a positive integer \\(\\nu\\), let \\(Z_1, \\ldots, Z_\\nu\\) be independent standard normal random variables. The chi-squared distribution with \\(\\nu\\) degrees of freedom is the distribution of:\n\\[\\chi^2_\\nu = Z_1^2 + Z_2^2 + \\cdots + Z_\\nu^2\\]\n\n\n. . .\nKey Properties:\n\n\n\nProperty\nValue\n\n\n\n\nMean\n\\(E(\\chi^2_\\nu) = \\nu\\)\n\n\nVariance\n\\(\\text{Var}(\\chi^2_\\nu) = 2\\nu\\)\n\n\nShape\nRight-skewed, becomes more symmetric as \\(\\nu\\) increases"
  },
  {
    "objectID": "lessons/03_Test_Distributions/03_Test_Distributions.html#visualizing-the-chi-squared-distribution",
    "href": "lessons/03_Test_Distributions/03_Test_Distributions.html#visualizing-the-chi-squared-distribution",
    "title": "BSTA 551: Statistical Inference",
    "section": "Visualizing the Chi-Squared Distribution",
    "text": "Visualizing the Chi-Squared Distribution\n\ntibble(x = seq(0.01, 30, length.out = 500)) |&gt;\n  crossing(df = c(1, 2, 5, 10)) |&gt;\n  mutate(\n    density = dchisq(x, df),\n    df = factor(df, labels = paste(c(1, 2, 5, 10), \"df\"))\n  ) |&gt;\n  ggplot(aes(x = x, y = density, color = df)) +\n  geom_line(linewidth = 1.2) +\n  labs(\n    title = \"Chi-Squared Distribution for Various Degrees of Freedom\",\n    x = \"x\", y = \"Density\", color = \"Degrees of\\nFreedom\"\n  ) +\n  scale_color_brewer(palette = \"Set1\") +\n  theme(legend.position = c(0.8, 0.7))"
  },
  {
    "objectID": "lessons/03_Test_Distributions/03_Test_Distributions.html#chi-squared-and-the-gamma-distribution",
    "href": "lessons/03_Test_Distributions/03_Test_Distributions.html#chi-squared-and-the-gamma-distribution",
    "title": "BSTA 551: Statistical Inference",
    "section": "Chi-Squared and the Gamma Distribution",
    "text": "Chi-Squared and the Gamma Distribution\n\n\n\n\n\n\nConnection to Gamma\n\n\n\nThe chi-squared distribution with \\(\\nu\\) df is equivalent to a Gamma distribution with:\n\nShape parameter \\(\\alpha = \\nu/2\\)\nScale parameter \\(\\beta = 2\\)\n\nThis means \\(\\chi^2_\\nu = \\text{Gamma}(\\nu/2, 2)\\).\n\n\n. . .\nWhy this matters:\n\nWe can use gamma distribution properties\nChi-squared variables are additive: if \\(X_1 \\sim \\chi^2_{\\nu_1}\\) and \\(X_2 \\sim \\chi^2_{\\nu_2}\\) are independent, then \\(X_1 + X_2 \\sim \\chi^2_{\\nu_1 + \\nu_2}\\)"
  },
  {
    "objectID": "lessons/03_Test_Distributions/03_Test_Distributions.html#medical-example-led-lamp-lifecycle-devore-example-6.12",
    "href": "lessons/03_Test_Distributions/03_Test_Distributions.html#medical-example-led-lamp-lifecycle-devore-example-6.12",
    "title": "BSTA 551: Statistical Inference",
    "section": "Medical Example: LED Lamp Lifecycle (Devore Example 6.12)",
    "text": "Medical Example: LED Lamp Lifecycle (Devore Example 6.12)\nFrom Devore, Berk & Carlton: The lifecycle (in thousands of hours) of certain LED medical lamps follows a \\(\\chi^2_8\\) distribution.\n\n# Parameters for χ²₈ distribution\ndf_lamps &lt;- 8\nmean_life &lt;- df_lamps  # E(X) = ν\nsd_life &lt;- sqrt(2 * df_lamps)  # SD(X) = √(2ν)\n\ncat(\"Mean lifecycle:\", mean_life, \"thousand hours\\n\")\n\nMean lifecycle: 8 thousand hours\n\ncat(\"SD of lifecycle:\", round(sd_life, 2), \"thousand hours\\n\")\n\nSD of lifecycle: 4 thousand hours\n\n# Probability lamp lasts between 6 and 10 thousand hours\nprob_range &lt;- pchisq(10, df_lamps) - pchisq(6, df_lamps)\ncat(\"P(6 ≤ X ≤ 10):\", round(prob_range, 3))\n\nP(6 ≤ X ≤ 10): 0.382"
  },
  {
    "objectID": "lessons/03_Test_Distributions/03_Test_Distributions.html#your-turn-chi-squared-calculations",
    "href": "lessons/03_Test_Distributions/03_Test_Distributions.html#your-turn-chi-squared-calculations",
    "title": "BSTA 551: Statistical Inference",
    "section": "Your Turn: Chi-Squared Calculations",
    "text": "Your Turn: Chi-Squared Calculations\nExercise: For the LED lamp example (\\(\\chi^2_8\\) distribution):\n\nWhat is the probability a lamp lasts more than 15,000 hours?\nFind the 90th percentile of the lifecycle distribution.\nWhat is the “middle 95%” range of lifecycles?\n\n. . .\nSolutions:\n\n# 1. P(X &gt; 15)\nprob_over_15 &lt;- 1 - pchisq(15, 8)\ncat(\"P(X &gt; 15):\", round(prob_over_15, 4), \"\\n\")\n\nP(X &gt; 15): 0.0591 \n\n# 2. 90th percentile\np90 &lt;- qchisq(0.90, 8)\ncat(\"90th percentile:\", round(p90, 2), \"thousand hours\\n\")\n\n90th percentile: 13.36 thousand hours\n\n# 3. Middle 95%\nlower &lt;- qchisq(0.025, 8)\nupper &lt;- qchisq(0.975, 8)\ncat(\"Middle 95%: [\", round(lower, 2), \",\", round(upper, 2), \"]\")\n\nMiddle 95%: [ 2.18 , 17.53 ]"
  },
  {
    "objectID": "lessons/03_Test_Distributions/03_Test_Distributions.html#the-critical-connection-sample-variance-devore-6.4",
    "href": "lessons/03_Test_Distributions/03_Test_Distributions.html#the-critical-connection-sample-variance-devore-6.4",
    "title": "BSTA 551: Statistical Inference",
    "section": "The Critical Connection: Sample Variance (Devore 6.4)",
    "text": "The Critical Connection: Sample Variance (Devore 6.4)\n\n\n\n\n\n\nFundamental Theorem\n\n\n\nIf \\(X_1, X_2, \\ldots, X_n\\) is a random sample from a normal distribution \\(N(\\mu, \\sigma^2)\\), then:\n\\[\\frac{(n-1)S^2}{\\sigma^2} \\sim \\chi^2_{n-1}\\]\nwhere \\(S^2 = \\frac{1}{n-1}\\sum_{i=1}^n (X_i - \\bar{X})^2\\) is the sample variance.\n\n\n. . .\nThis is crucial because:\n\nIt tells us exactly how sample variances behave\nThe \\((n-1)\\) degrees of freedom arise because we “use up” one df estimating \\(\\mu\\) with \\(\\bar{X}\\)\nThis forms the basis for inference about population variances"
  },
  {
    "objectID": "lessons/03_Test_Distributions/03_Test_Distributions.html#simulation-verifying-the-chi-squared-result",
    "href": "lessons/03_Test_Distributions/03_Test_Distributions.html#simulation-verifying-the-chi-squared-result",
    "title": "BSTA 551: Statistical Inference",
    "section": "Simulation: Verifying the Chi-Squared Result",
    "text": "Simulation: Verifying the Chi-Squared Result\n\nn &lt;- 10\nsigma_sq &lt;- 100  # True variance\nn_sims &lt;- 10000\n\n# Simulate the transformation (n-1)S²/σ²\nchi_sq_sim &lt;- tibble(sim = 1:n_sims) |&gt;\n  mutate(\n    sample_data = map(sim, ~rnorm(n, mean = 50, sd = sqrt(sigma_sq))),\n    s_squared = map_dbl(sample_data, var),\n    chi_sq_stat = (n - 1) * s_squared / sigma_sq\n  )\n\n# Compare to theoretical χ²(n-1) distribution\nchi_sq_sim |&gt;\n  ggplot(aes(x = chi_sq_stat)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 50, \n                 fill = \"steelblue\", alpha = 0.7, color = \"white\") +\n  stat_function(fun = dchisq, args = list(df = n - 1), \n                color = \"red\", linewidth = 1.5) +\n  labs(title = \"Simulated (n-1)S²/σ² vs. Theoretical χ²(n-1)\",\n       subtitle = paste(\"n =\", n, \", σ² =\", sigma_sq, \",\", n_sims, \"simulations\"),\n       x = \"Value\", y = \"Density\") +\n  annotate(\"text\", x = 20, y = 0.08, label = \"Red: χ²(9) density\", \n           color = \"red\", size = 5)"
  },
  {
    "objectID": "lessons/03_Test_Distributions/03_Test_Distributions.html#the-t-distribution-definition-devore-6.3",
    "href": "lessons/03_Test_Distributions/03_Test_Distributions.html#the-t-distribution-definition-devore-6.3",
    "title": "BSTA 551: Statistical Inference",
    "section": "The t Distribution: Definition (Devore 6.3)",
    "text": "The t Distribution: Definition (Devore 6.3)\n\n\n\n\n\n\nDefinition\n\n\n\nLet \\(Z\\) be a standard normal random variable and \\(Y\\) be a \\(\\chi^2_\\nu\\) random variable, independent of \\(Z\\). Then the t distribution with \\(\\nu\\) degrees of freedom is the distribution of:\n\\[T = \\frac{Z}{\\sqrt{Y/\\nu}}\\]\n\n\n. . .\nKey Properties:\n\n\n\nProperty\nValue\n\n\n\n\nMean\n\\(E(T) = 0\\) (for \\(\\nu &gt; 1\\))\n\n\nVariance\n\\(\\text{Var}(T) = \\frac{\\nu}{\\nu - 2}\\) (for \\(\\nu &gt; 2\\))\n\n\nShape\nSymmetric, bell-shaped, heavier tails than normal"
  },
  {
    "objectID": "lessons/03_Test_Distributions/03_Test_Distributions.html#comparing-t-and-normal-distributions",
    "href": "lessons/03_Test_Distributions/03_Test_Distributions.html#comparing-t-and-normal-distributions",
    "title": "BSTA 551: Statistical Inference",
    "section": "Comparing t and Normal Distributions",
    "text": "Comparing t and Normal Distributions\n\ntibble(x = seq(-4, 4, length.out = 500)) |&gt;\n  crossing(df = c(1, 2, 5, 30)) |&gt;\n  mutate(\n    t_density = dt(x, df),\n    df = factor(df, labels = paste(c(1, 2, 5, 30), \"df\"))\n  ) |&gt;\n  ggplot(aes(x = x, y = t_density, color = df)) +\n  geom_line(linewidth = 1.2) +\n  stat_function(fun = dnorm, color = \"black\", linewidth = 1.5, linetype = \"dashed\") +\n  labs(\n    title = \"t Distribution Compared to Standard Normal\",\n    subtitle = \"Dashed black line: Standard Normal (Z)\",\n    x = \"x\", y = \"Density\", color = \"t Distribution\"\n  ) +\n  scale_color_brewer(palette = \"Set1\") +\n  annotate(\"text\", x = 2.5, y = 0.35, label = \"As df → ∞, t → Z\", size = 5)"
  },
  {
    "objectID": "lessons/03_Test_Distributions/03_Test_Distributions.html#gossets-theorem-the-practical-t-distribution-devore-6.4",
    "href": "lessons/03_Test_Distributions/03_Test_Distributions.html#gossets-theorem-the-practical-t-distribution-devore-6.4",
    "title": "BSTA 551: Statistical Inference",
    "section": "Gosset’s Theorem: The Practical t Distribution (Devore 6.4)",
    "text": "Gosset’s Theorem: The Practical t Distribution (Devore 6.4)\n\n\n\n\n\n\nGosset’s Theorem (1908)\n\n\n\nIf \\(X_1, X_2, \\ldots, X_n\\) is a random sample from a normal distribution \\(N(\\mu, \\sigma)\\), then:\n\\[T = \\frac{\\bar{X} - \\mu}{S/\\sqrt{n}} \\sim t_{n-1}\\]\nThis is the one-sample t statistic.\n\n\n. . .\nWhy this matters:\n\nIn practice, we rarely know \\(\\sigma\\)\nReplacing \\(\\sigma\\) with \\(S\\) introduces additional uncertainty\nThe t distribution accounts for this extra variability"
  },
  {
    "objectID": "lessons/03_Test_Distributions/03_Test_Distributions.html#comparing-z-and-t-statistics",
    "href": "lessons/03_Test_Distributions/03_Test_Distributions.html#comparing-z-and-t-statistics",
    "title": "BSTA 551: Statistical Inference",
    "section": "Comparing Z and T Statistics",
    "text": "Comparing Z and T Statistics\nTwo related quantities for inference about \\(\\mu\\):\n\\[Z = \\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}} \\quad \\text{vs.} \\quad T = \\frac{\\bar{X} - \\mu}{S/\\sqrt{n}}\\]\n. . .\n\n\n\nWhen to use\nDistribution\nRequires knowing\n\n\n\n\nZ statistic\nStandard Normal\nPopulation \\(\\sigma\\)\n\n\nT statistic\n\\(t_{n-1}\\)\nOnly sample data\n\n\n\n. . .\nKey insight: The t distribution has heavier tails because \\(S\\) varies from sample to sample."
  },
  {
    "objectID": "lessons/03_Test_Distributions/03_Test_Distributions.html#medical-example-blood-pressure-reduction",
    "href": "lessons/03_Test_Distributions/03_Test_Distributions.html#medical-example-blood-pressure-reduction",
    "title": "BSTA 551: Statistical Inference",
    "section": "Medical Example: Blood Pressure Reduction",
    "text": "Medical Example: Blood Pressure Reduction\nA clinical trial measures systolic blood pressure reduction (mmHg) in 12 patients taking a new medication.\n\n# Blood pressure reductions from 12 patients\nbp_reduction &lt;- c(8.2, 12.5, 9.1, 7.8, 11.3, 10.4, \n                  9.7, 13.1, 8.9, 10.8, 12.0, 9.5)\n\nn &lt;- length(bp_reduction)\nx_bar &lt;- mean(bp_reduction)\ns &lt;- sd(bp_reduction)\n\n# T statistic (testing if true mean reduction is μ₀ = 8)\nmu_0 &lt;- 8\nt_stat &lt;- (x_bar - mu_0) / (s / sqrt(n))\n\ncat(\"Sample mean:\", round(x_bar, 2), \"mmHg\\n\")\n\nSample mean: 10.28 mmHg\n\ncat(\"Sample SD:\", round(s, 2), \"mmHg\\n\")\n\nSample SD: 1.7 mmHg\n\ncat(\"T statistic (vs μ₀ = 8):\", round(t_stat, 3), \"\\n\")\n\nT statistic (vs μ₀ = 8): 4.629 \n\ncat(\"P-value:\", round(2 * pt(-abs(t_stat), df = n - 1), 4))\n\nP-value: 7e-04"
  },
  {
    "objectID": "lessons/03_Test_Distributions/03_Test_Distributions.html#your-turn-t-distribution-application",
    "href": "lessons/03_Test_Distributions/03_Test_Distributions.html#your-turn-t-distribution-application",
    "title": "BSTA 551: Statistical Inference",
    "section": "Your Turn: t Distribution Application",
    "text": "Your Turn: t Distribution Application\nExercise: A hospital measures recovery times (days) for 8 post-surgery patients:\n\nrecovery_times &lt;- c(4.2, 5.8, 3.9, 5.1, 4.7, 6.2, 4.4, 5.5)\n\n\nCalculate the sample mean and sample standard deviation\nCompute the t statistic for testing whether the true mean is 5 days\nFind the critical value \\(t_{0.025, 7}\\) (for a two-sided 95% test)\n\n. . .\nSolutions:\n\nn &lt;- length(recovery_times)\nx_bar &lt;- mean(recovery_times)\ns &lt;- sd(recovery_times)\nt_stat &lt;- (x_bar - 5) / (s / sqrt(n))\nt_crit &lt;- qt(0.975, df = n - 1)\n\ncat(\"Mean:\", round(x_bar, 3), \"SD:\", round(s, 3), \"\\n\")\n\nMean: 4.975 SD: 0.814 \n\ncat(\"T statistic:\", round(t_stat, 3), \"\\n\")\n\nT statistic: -0.087 \n\ncat(\"Critical value t₀.₀₂₅,₇:\", round(t_crit, 3))\n\nCritical value t₀.₀₂₅,₇: 2.365"
  },
  {
    "objectID": "lessons/03_Test_Distributions/03_Test_Distributions.html#the-f-distribution-definition-devore-6.3",
    "href": "lessons/03_Test_Distributions/03_Test_Distributions.html#the-f-distribution-definition-devore-6.3",
    "title": "BSTA 551: Statistical Inference",
    "section": "The F Distribution: Definition (Devore 6.3)",
    "text": "The F Distribution: Definition (Devore 6.3)\n\n\n\n\n\n\nDefinition\n\n\n\nLet \\(Y_1 \\sim \\chi^2_{\\nu_1}\\) and \\(Y_2 \\sim \\chi^2_{\\nu_2}\\) be independent chi-squared random variables. The F distribution with \\(\\nu_1\\) numerator df and \\(\\nu_2\\) denominator df is:\n\\[F = \\frac{Y_1/\\nu_1}{Y_2/\\nu_2} \\sim F_{\\nu_1, \\nu_2}\\]\n\n\n. . .\nKey Properties:\n\n\\(E(F) = \\frac{\\nu_2}{\\nu_2 - 2}\\) (for \\(\\nu_2 &gt; 2\\))\nRight-skewed, positive values only\nUsed for comparing variances and in ANOVA"
  },
  {
    "objectID": "lessons/03_Test_Distributions/03_Test_Distributions.html#visualizing-the-f-distribution",
    "href": "lessons/03_Test_Distributions/03_Test_Distributions.html#visualizing-the-f-distribution",
    "title": "BSTA 551: Statistical Inference",
    "section": "Visualizing the F Distribution",
    "text": "Visualizing the F Distribution\n\nexpand_grid(\n  df1 = c(1, 5, 10),\n  df2 = c(5, 20)\n) |&gt;\n  mutate(df_label = paste0(\"(\", df1, \", \", df2, \")\")) |&gt;\n  crossing(x = seq(0.01, 5, length.out = 300)) |&gt;\n  mutate(density = df(x, df1, df2)) |&gt;\n  ggplot(aes(x = x, y = density, color = df_label)) +\n  geom_line(linewidth = 1.2) +\n  labs(\n    title = \"F Distribution for Various Degrees of Freedom\",\n    subtitle = \"Label format: (numerator df, denominator df)\",\n    x = \"x\", y = \"Density\", color = \"Degrees of\\nFreedom\"\n  ) +\n  scale_color_brewer(palette = \"Set2\") +\n  coord_cartesian(ylim = c(0, 1))"
  },
  {
    "objectID": "lessons/03_Test_Distributions/03_Test_Distributions.html#the-f-statistic-for-comparing-variances",
    "href": "lessons/03_Test_Distributions/03_Test_Distributions.html#the-f-statistic-for-comparing-variances",
    "title": "BSTA 551: Statistical Inference",
    "section": "The F Statistic for Comparing Variances",
    "text": "The F Statistic for Comparing Variances\n\n\n\n\n\n\nComparing Two Population Variances\n\n\n\nIf we have independent samples from two normal populations:\n\nSample 1: \\(n_1\\) observations, sample variance \\(S_1^2\\)\nSample 2: \\(n_2\\) observations, sample variance \\(S_2^2\\)\n\nThen under \\(H_0: \\sigma_1^2 = \\sigma_2^2\\):\n\\[F = \\frac{S_1^2}{S_2^2} \\sim F_{n_1-1, n_2-1}\\]\n\n\n. . .\nThis will be essential for ANOVA and comparing treatment groups."
  },
  {
    "objectID": "lessons/03_Test_Distributions/03_Test_Distributions.html#medical-example-treatment-variability-comparison",
    "href": "lessons/03_Test_Distributions/03_Test_Distributions.html#medical-example-treatment-variability-comparison",
    "title": "BSTA 551: Statistical Inference",
    "section": "Medical Example: Treatment Variability Comparison",
    "text": "Medical Example: Treatment Variability Comparison\nTwo pain medications are being compared. Is the variability in pain relief different?\n\n# Pain relief scores (higher = more relief)\ndrug_A &lt;- c(6.2, 7.1, 5.8, 6.9, 7.3, 6.5, 7.0, 6.4, 6.8, 7.2)\ndrug_B &lt;- c(5.5, 8.2, 4.9, 7.8, 6.1, 8.5, 5.2, 7.4, 6.3, 8.0)\n\n# Compare variances\nvar_A &lt;- var(drug_A)\nvar_B &lt;- var(drug_B)\nf_stat &lt;- var_B / var_A  # Put larger variance in numerator\n\ncat(\"Var(Drug A):\", round(var_A, 3), \"\\n\")\n\nVar(Drug A): 0.233 \n\ncat(\"Var(Drug B):\", round(var_B, 3), \"\\n\")\n\nVar(Drug B): 1.805 \n\ncat(\"F statistic:\", round(f_stat, 3), \"\\n\")\n\nF statistic: 7.752 \n\ncat(\"P-value (two-sided):\", round(2 * pf(f_stat, 9, 9, lower.tail = FALSE), 4))\n\nP-value (two-sided): 0.0054\n\n\n. . .\nDrug B shows significantly more variable responses!"
  },
  {
    "objectID": "lessons/03_Test_Distributions/03_Test_Distributions.html#relationships-between-distributions",
    "href": "lessons/03_Test_Distributions/03_Test_Distributions.html#relationships-between-distributions",
    "title": "BSTA 551: Statistical Inference",
    "section": "Relationships Between Distributions",
    "text": "Relationships Between Distributions"
  },
  {
    "objectID": "lessons/03_Test_Distributions/03_Test_Distributions.html#r-functions-for-these-distributions",
    "href": "lessons/03_Test_Distributions/03_Test_Distributions.html#r-functions-for-these-distributions",
    "title": "BSTA 551: Statistical Inference",
    "section": "R Functions for These Distributions",
    "text": "R Functions for These Distributions\n\n# Chi-squared distribution\npchisq(10, df = 5)       # P(χ²₅ ≤ 10)\n\n[1] 0.9247648\n\nqchisq(0.95, df = 5)     # 95th percentile\n\n[1] 11.0705\n\nrchisq(5, df = 5)        # 5 random values\n\n[1] 2.913437 9.932238 2.024977 3.125219 2.908850\n\n# t distribution  \npt(2, df = 10)           # P(t₁₀ ≤ 2)\n\n[1] 0.963306\n\nqt(0.975, df = 10)       # 97.5th percentile (for 95% CI)\n\n[1] 2.228139\n\nrt(5, df = 10)           # 5 random values\n\n[1] -0.9367386  0.8773822  1.2727263 -0.4356913  1.3334783\n\n# F distribution\npf(3, df1 = 5, df2 = 20) # P(F₅,₂₀ ≤ 3)\n\n[1] 0.9647987\n\nqf(0.95, df1 = 5, df2 = 20)  # 95th percentile\n\n[1] 2.71089\n\nrf(5, df1 = 5, df2 = 20)     # 5 random values\n\n[1] 0.9798762 0.9294727 0.5288312 0.2722935 1.1200395"
  },
  {
    "objectID": "lessons/03_Test_Distributions/03_Test_Distributions.html#lesson-3-summary",
    "href": "lessons/03_Test_Distributions/03_Test_Distributions.html#lesson-3-summary",
    "title": "BSTA 551: Statistical Inference",
    "section": "Lesson 3 Summary",
    "text": "Lesson 3 Summary\nThe Chi-Squared Distribution:\n\nSum of squared standard normals: \\(\\chi^2_\\nu = \\sum Z_i^2\\)\nMean = \\(\\nu\\), Variance = \\(2\\nu\\)\n\\((n-1)S^2/\\sigma^2 \\sim \\chi^2_{n-1}\\) for normal samples\n\nThe t Distribution:\n\nRatio: \\(T = Z/\\sqrt{\\chi^2_\\nu/\\nu}\\)\nArises when we estimate \\(\\sigma\\) with \\(S\\)\n\\((\\bar{X} - \\mu)/(S/\\sqrt{n}) \\sim t_{n-1}\\)\n\nThe F Distribution:\n\nRatio of two scaled chi-squared: \\(F = (Y_1/\\nu_1)/(Y_2/\\nu_2)\\)\nUsed for comparing variances\nKey relationship: \\(t_\\nu^2 = F_{1,\\nu}\\)"
  },
  {
    "objectID": "lessons/03_Test_Distributions/03_Test_Distributions.html#lesson-3-practice-problems",
    "href": "lessons/03_Test_Distributions/03_Test_Distributions.html#lesson-3-practice-problems",
    "title": "BSTA 551: Statistical Inference",
    "section": "Lesson 3 Practice Problems",
    "text": "Lesson 3 Practice Problems\n\nA sample of 16 observations from a normal distribution has sample variance \\(s^2 = 25\\). If \\(\\sigma^2 = 20\\), what is \\(P\\left(\\frac{(n-1)S^2}{\\sigma^2} \\leq 18.75\\right)\\)?\nFind the critical values \\(t_{0.05, 15}\\) and \\(t_{0.025, 15}\\).\nTwo independent samples from normal populations have \\(s_1^2 = 45\\) (n=10) and \\(s_2^2 = 20\\) (n=8). Calculate the F statistic for testing equal variances.\nVerify that \\(t_{0.05,10}^2 = F_{0.10, 1, 10}\\) using R."
  },
  {
    "objectID": "lessons/03_Test_Distributions/03_Test_Distributions.html#next-lesson-preview",
    "href": "lessons/03_Test_Distributions/03_Test_Distributions.html#next-lesson-preview",
    "title": "BSTA 551: Statistical Inference",
    "section": "Next Lesson Preview",
    "text": "Next Lesson Preview\nLesson 4: Normal Sample Statistics and MVUE\n\nKey results for statistics from normal samples\nIndependence of \\(\\bar{X}\\) and \\(S^2\\)\nConfidence intervals for variance\nMinimum variance unbiased estimators (MVUE)\nConsistency of estimators"
  },
  {
    "objectID": "lessons/03_Test_Distributions/03_Test_Distributions.html#references",
    "href": "lessons/03_Test_Distributions/03_Test_Distributions.html#references",
    "title": "BSTA 551: Statistical Inference",
    "section": "References",
    "text": "References\n\n\nDevore, Berk, and Carlton. Modern Mathematical Statistics with Applications (Springer). Chapters 6.3, 6.4\nChihara and Hesterberg. Mathematical Statistics with Resampling and R (Wiley). Appendix B."
  },
  {
    "objectID": "lessons/03_Test_Distributions/03_Test_Distributions.html#questions",
    "href": "lessons/03_Test_Distributions/03_Test_Distributions.html#questions",
    "title": "BSTA 551: Statistical Inference",
    "section": "Questions?",
    "text": "Questions?\nThank you!"
  },
  {
    "objectID": "lessons/00_Intro/00_Intro.html",
    "href": "lessons/00_Intro/00_Intro.html",
    "title": "Welcome to BSTA 551!",
    "section": "",
    "text": "Call me “Jessica,” “Dr. M,” “Professor Minnier [MIN-ee-ay],” or any combo!\nAssociate Professor of Biostatistics\n \nThis is a “newish” class to me and the program\nTaught 552 Math/Stats II (RIP) for 9 years; Intro to R course developed & taught 3 years\nAbout me\n\nSPH, Knight Cancer Institute\nResearch: risk prediction, ’omics, large cohort studies, oncology trials"
  },
  {
    "objectID": "lessons/00_Intro/00_Intro.html#jessica-minnier-sheher",
    "href": "lessons/00_Intro/00_Intro.html#jessica-minnier-sheher",
    "title": "Welcome to BSTA 551!",
    "section": "Jessica Minnier (she/her)",
    "text": "Jessica Minnier (she/her)\n\n\n\nCall me “Jessica,” “Dr. M,” “Professor Minnier [MIN-ee-ay],” or any combo!\nAssociate Professor of Biostatistics\n \nThis is a “newish” class to me and the program\nTaught 552 Math/Stats II (RIP) for 9 years; Intro to R course developed & taught 3 years\nAbout me\n\nSPH, Knight Cancer Institute\nResearch: risk prediction, ’omics, large cohort studies, oncology trials"
  },
  {
    "objectID": "lessons/00_Intro/00_Intro.html#some-important-tasks",
    "href": "lessons/00_Intro/00_Intro.html#some-important-tasks",
    "title": "Welcome to BSTA 551!",
    "section": "Some important tasks",
    "text": "Some important tasks\n\nStar the class website: https://jessicaminnier.com/BSTA551_W26/\n\n \n\nComplete Homework 0 by this Thursday at 11pm!\n\nPoll for office hours set up\n\n\n \n\nHighly suggest that you make an appointment with a learning specialist through Student Academic Success Center!"
  },
  {
    "objectID": "lessons/00_Intro/00_Intro.html#lets-visit-the-website-homepage",
    "href": "lessons/00_Intro/00_Intro.html#lets-visit-the-website-homepage",
    "title": "Welcome to BSTA 551!",
    "section": "Let’s visit the website: Homepage",
    "text": "Let’s visit the website: Homepage"
  },
  {
    "objectID": "lessons/00_Intro/00_Intro.html#lets-visit-the-website-syllabus",
    "href": "lessons/00_Intro/00_Intro.html#lets-visit-the-website-syllabus",
    "title": "Welcome to BSTA 551!",
    "section": "Let’s visit the website: Syllabus",
    "text": "Let’s visit the website: Syllabus"
  },
  {
    "objectID": "lessons/00_Intro/00_Intro.html#lets-visit-the-website-schedule-12",
    "href": "lessons/00_Intro/00_Intro.html#lets-visit-the-website-schedule-12",
    "title": "Welcome to BSTA 551!",
    "section": "Let’s visit the website: Schedule (1/2)",
    "text": "Let’s visit the website: Schedule (1/2)\n\nWeeks, class info, homeworks, labs"
  },
  {
    "objectID": "lessons/00_Intro/00_Intro.html#lets-visit-the-website-schedule-22",
    "href": "lessons/00_Intro/00_Intro.html#lets-visit-the-website-schedule-22",
    "title": "Welcome to BSTA 551!",
    "section": "Let’s visit the website: Schedule (2/2)",
    "text": "Let’s visit the website: Schedule (2/2)\n\n\n\n\nKey Info\nI will post announcements and other important class related info here. For example, if I change a due date or discuss a common mistake in homework, I will put it here.\n\n\n\nSlides HTML\nThese are the basic slides that will open in your browser.\n\n\n\nSlides PDF\nThese are the slides in pdf form for easy note taking. I’m not always the best at posting these before class, so make sure you know how to save your own copy of pdf slides!\n\n\n\nSlides Notes\nThese are the annotated slides in pdf form. In class, I add my own notes to slides. After class, I will post them here.\n\n\n)\nExit tix\nThese are links to that day’s exit ticket.\n\n\n\nRecording\nI record our classes. This will be a link to the OneDrive folder containing this recording.\n\n\n\nMuddy Points\nYou will have a chance to ask questions about class in your exit tickets. If I notice a trend in confusion, I will add explanations to these “Muddy Points”"
  },
  {
    "objectID": "lessons/00_Intro/00_Intro.html#lets-visit-the-website-search",
    "href": "lessons/00_Intro/00_Intro.html#lets-visit-the-website-search",
    "title": "Welcome to BSTA 551!",
    "section": "Let’s visit the website: Search",
    "text": "Let’s visit the website: Search"
  },
  {
    "objectID": "lessons/00_Intro/00_Intro.html#lets-visit-the-website-homework",
    "href": "lessons/00_Intro/00_Intro.html#lets-visit-the-website-homework",
    "title": "Welcome to BSTA 551!",
    "section": "Let’s visit the website: Homework!",
    "text": "Let’s visit the website: Homework!"
  },
  {
    "objectID": "lessons/00_Intro/00_Intro.html#structure-for-this-course",
    "href": "lessons/00_Intro/00_Intro.html#structure-for-this-course",
    "title": "Welcome to BSTA 551!",
    "section": "Structure for this course",
    "text": "Structure for this course\n\nStatistics is based on mathematics, which has theoretical underpinnings.\nUnderstanding the “why” behind why we perform certain hypothesis tests and where our estimates come from.\nThink of this course as paired with the applied courses. There may be a lot of overlap, but we are going deeper with the “why” and “how”\nThis used to be two quarters of material, with more proofs and much more calculus. I tried to make this course more digestible in 10 weeks, while also including some simulation/boostrap/permutation methods. Therefore: topics will go by quickly!! We won’t have time to prove most things, you will have to take a lot of this on face value.\n\n \n\nI agree with Dr. Wakim: “It is going to feel useless at times, but I swear it is not!”"
  },
  {
    "objectID": "lessons/00_Intro/00_Intro.html#what-we-will-cover",
    "href": "lessons/00_Intro/00_Intro.html#what-we-will-cover",
    "title": "Welcome to BSTA 551!",
    "section": "What we will cover",
    "text": "What we will cover\n\nPoint Estimation\nHypothesis Testing\nConfidence Interval Estimation\nAsymptotic properties of estimators/tests\nBootstrap/resampling methods"
  },
  {
    "objectID": "lessons/00_Intro/00_Intro.html#textbook",
    "href": "lessons/00_Intro/00_Intro.html#textbook",
    "title": "Welcome to BSTA 551!",
    "section": "Textbook",
    "text": "Textbook\nModern Mathematical Statistics with Applications, 3rd ed.\n\nAuthor: JL Devore, KN Berk, MA Carlton\nTextbook available online through library\nCitation: Devore JL, Berk KN, Carlton MA. Modern Mathematical Statistics with Applications. Third edition. Springer; 2021. doi:10.1007/978-3-030-55156-8\nFocus on chapters 6-10"
  },
  {
    "objectID": "lessons/x10_Transformations/Old_notes/25_Joint_densities_key_info.html",
    "href": "lessons/x10_Transformations/Old_notes/25_Joint_densities_key_info.html",
    "title": "Key Info",
    "section": "",
    "text": "Grading HW 6 assignment today!\n\nFeel free to turn in solutions later than 11/24 since I took so long to grade\n\nI made quite a few changes to the homeworks\n\nHW 10 is gone\nHW 9 is optional\nInstead of a HW assignment due over late November break\n\nHW 8 will include material from Week 8 and 9 and be due 12/5\n\n\nI have quite a bit of grading to catch up on!\nRealizing I never gave you mid-quarter feedback forms…\n\nLet’s chat about this and our assessment breakdown in the coming weeks\n\nAnything else?"
  },
  {
    "objectID": "lessons/x10_Transformations/Old_notes/25_Joint_densities_key_info.html#announcements",
    "href": "lessons/x10_Transformations/Old_notes/25_Joint_densities_key_info.html#announcements",
    "title": "Key Info",
    "section": "",
    "text": "Grading HW 6 assignment today!\n\nFeel free to turn in solutions later than 11/24 since I took so long to grade\n\nI made quite a few changes to the homeworks\n\nHW 10 is gone\nHW 9 is optional\nInstead of a HW assignment due over late November break\n\nHW 8 will include material from Week 8 and 9 and be due 12/5\n\n\nI have quite a bit of grading to catch up on!\nRealizing I never gave you mid-quarter feedback forms…\n\nLet’s chat about this and our assessment breakdown in the coming weeks\n\nAnything else?"
  },
  {
    "objectID": "lessons/x10_Transformations/Old_notes/25_Joint_densities_key_info.html#key-dates",
    "href": "lessons/x10_Transformations/Old_notes/25_Joint_densities_key_info.html#key-dates",
    "title": "Key Info",
    "section": "Key Dates",
    "text": "Key Dates\n\nThursday: HW 7 due\nSunday: HW 6 solutions due"
  },
  {
    "objectID": "lessons/x10_Transformations/10_Transformations_key_info.html",
    "href": "lessons/x10_Transformations/10_Transformations_key_info.html",
    "title": "Key Info",
    "section": "",
    "text": "Quiz 1 grades look pretty good! Everyone got score above 11/15\n\nFeedback released at 12:10pm today\nWill go over questions next class"
  },
  {
    "objectID": "lessons/x10_Transformations/10_Transformations_key_info.html#announcements",
    "href": "lessons/x10_Transformations/10_Transformations_key_info.html#announcements",
    "title": "Key Info",
    "section": "",
    "text": "Quiz 1 grades look pretty good! Everyone got score above 11/15\n\nFeedback released at 12:10pm today\nWill go over questions next class"
  },
  {
    "objectID": "lessons/x10_Transformations/10_Transformations_key_info.html#key-dates",
    "href": "lessons/x10_Transformations/10_Transformations_key_info.html#key-dates",
    "title": "Key Info",
    "section": "Key Dates",
    "text": "Key Dates\n\nHomework 4 due 02/05 at 11pm"
  },
  {
    "objectID": "lessons/x01_Probability/01_Probability_muddy_points.html",
    "href": "lessons/x01_Probability/01_Probability_muddy_points.html",
    "title": "Muddy Points",
    "section": "",
    "text": "In class, we were wondering why/if \\(2^{|S|}\\) is the general formula for calculating the total number of possible events. We were specifically wondering if the \\(2\\) came from the fact that we had two options (heads and tails) for our outcome. Let’s work through the example of a 6-sided die to explain this further. The sample space is \\(S=\\{1, 2, 3, 4, 5, 6\\}\\). So is the total number of possible events \\(2^6\\) or \\(6^6\\) or something else? We can actually think about an event by using an indicator variable for each outcome of the sample space. An indicator variable is just a way to give us a yes/no answer to a question. So in this case, we are wondering: is this outcome a part of our event? If our event is \\(\\{1\\}\\) then for the outcome \\(1\\), the answer is “yes, the outcome is part of the event. For outcomes \\(2-6\\), the answer is”no, the outcome is not apart of the event.”\n\nFor each outcome, we have a “yes” or “no” answer. We can look at another example of an event. Let’s say our event is rolling an even number:\n\nFor \\(2\\), \\(4\\), and \\(6\\), the answer is “yes.” We can define the indicator variable for whether an outcome is in an event or not. The indicator gives a 1 or 0 for yes and no respectively.\n\nAs stated above, the \\(2\\) in \\(2^6\\) comes from the \\(2\\) options from our indicator. Each side has two options, and there are \\(6\\) sides. Thus, \\(2^6\\) possible events.\n\n\n\nI think this will become clearer when we start thinking about events in the context of probability. When we think of events outside of probability, we may think of something we actually do or something that happens, like going to a concert or coming to class or missing the streetcar. In this case, we think of the event as the single thing (out of all the options) that actually occured. For example, if I’m taking the streetcar to class, I can think of two definitive options of what might occur: I miss the streetcar or I get on the streetcar. Only one of these things can occur, which I may call an event colloquially.\nIt is important to make the distinction with events defined within probability. Events are not necessarily a single thing that occurred. Instead it can be a collection of things that may occur. In the example of the streetcar, I can define my event to include both options. Thus, my event is that I make the streetcar or I miss it. Both of these things cannot happen simultaneously, but if I want to calculate the probability that I miss or make the streetcar, then it is helpful to have the event defined.\n\n\nAn outcome is a single result. The two options in the above example, missing the streetcar or getting on the streetcar, are two potential outcomes. Events are the collection of 0, 1, or more outcomes. So the possible events are: the empty set, missing the streetcar, getting on the streetcar, or the set of missing the streetcar and getting on the streetcar."
  },
  {
    "objectID": "lessons/x01_Probability/01_Probability_muddy_points.html#fall-2024",
    "href": "lessons/x01_Probability/01_Probability_muddy_points.html#fall-2024",
    "title": "Muddy Points",
    "section": "",
    "text": "In class, we were wondering why/if \\(2^{|S|}\\) is the general formula for calculating the total number of possible events. We were specifically wondering if the \\(2\\) came from the fact that we had two options (heads and tails) for our outcome. Let’s work through the example of a 6-sided die to explain this further. The sample space is \\(S=\\{1, 2, 3, 4, 5, 6\\}\\). So is the total number of possible events \\(2^6\\) or \\(6^6\\) or something else? We can actually think about an event by using an indicator variable for each outcome of the sample space. An indicator variable is just a way to give us a yes/no answer to a question. So in this case, we are wondering: is this outcome a part of our event? If our event is \\(\\{1\\}\\) then for the outcome \\(1\\), the answer is “yes, the outcome is part of the event. For outcomes \\(2-6\\), the answer is”no, the outcome is not apart of the event.”\n\nFor each outcome, we have a “yes” or “no” answer. We can look at another example of an event. Let’s say our event is rolling an even number:\n\nFor \\(2\\), \\(4\\), and \\(6\\), the answer is “yes.” We can define the indicator variable for whether an outcome is in an event or not. The indicator gives a 1 or 0 for yes and no respectively.\n\nAs stated above, the \\(2\\) in \\(2^6\\) comes from the \\(2\\) options from our indicator. Each side has two options, and there are \\(6\\) sides. Thus, \\(2^6\\) possible events.\n\n\n\nI think this will become clearer when we start thinking about events in the context of probability. When we think of events outside of probability, we may think of something we actually do or something that happens, like going to a concert or coming to class or missing the streetcar. In this case, we think of the event as the single thing (out of all the options) that actually occured. For example, if I’m taking the streetcar to class, I can think of two definitive options of what might occur: I miss the streetcar or I get on the streetcar. Only one of these things can occur, which I may call an event colloquially.\nIt is important to make the distinction with events defined within probability. Events are not necessarily a single thing that occurred. Instead it can be a collection of things that may occur. In the example of the streetcar, I can define my event to include both options. Thus, my event is that I make the streetcar or I miss it. Both of these things cannot happen simultaneously, but if I want to calculate the probability that I miss or make the streetcar, then it is helpful to have the event defined.\n\n\nAn outcome is a single result. The two options in the above example, missing the streetcar or getting on the streetcar, are two potential outcomes. Events are the collection of 0, 1, or more outcomes. So the possible events are: the empty set, missing the streetcar, getting on the streetcar, or the set of missing the streetcar and getting on the streetcar."
  },
  {
    "objectID": "lessons/x01_Probability/01_Probability_key_info.html",
    "href": "lessons/x01_Probability/01_Probability_key_info.html",
    "title": "Key Info",
    "section": "",
    "text": "Slack is working for a few people??\n\nhttps://join.slack.com/t/bsta550/shared_invite/zt-2qtfo9s8j-j6ozEaYdT7yy1KuHoT9Mhw"
  },
  {
    "objectID": "lessons/x01_Probability/01_Probability_key_info.html#announcements",
    "href": "lessons/x01_Probability/01_Probability_key_info.html#announcements",
    "title": "Key Info",
    "section": "",
    "text": "Slack is working for a few people??\n\nhttps://join.slack.com/t/bsta550/shared_invite/zt-2qtfo9s8j-j6ozEaYdT7yy1KuHoT9Mhw"
  },
  {
    "objectID": "lessons/x13_Expected_Values/13_Expected_Values_key_info.html",
    "href": "lessons/x13_Expected_Values/13_Expected_Values_key_info.html",
    "title": "Key Info",
    "section": "",
    "text": "Quiz 2 back: everyone got at least a 12/14! Woo!\n\nMost common mistake: Question 10 - the fact that \\(f_X(x)\\) can be greater than 1 is:\n\nWrong answer: Invalid, because a pdf (like a probability) can never be greater than 1\nCorrect answer: Valid, because the area under the curve from 0 to 1 is 1, even if the height of the function is greater than 1\n\n\nFeel free to use your no-questions-asked extensions on homework\n\nAnd remember that there is no grade penalty for turning in homework late\n\nCharles is not obligated to give you feedback though\n\n\nSadly, there is no audio in the class on 11/5\n\nNot sure if human error or computer issue"
  },
  {
    "objectID": "lessons/x13_Expected_Values/13_Expected_Values_key_info.html#announcements",
    "href": "lessons/x13_Expected_Values/13_Expected_Values_key_info.html#announcements",
    "title": "Key Info",
    "section": "",
    "text": "Quiz 2 back: everyone got at least a 12/14! Woo!\n\nMost common mistake: Question 10 - the fact that \\(f_X(x)\\) can be greater than 1 is:\n\nWrong answer: Invalid, because a pdf (like a probability) can never be greater than 1\nCorrect answer: Valid, because the area under the curve from 0 to 1 is 1, even if the height of the function is greater than 1\n\n\nFeel free to use your no-questions-asked extensions on homework\n\nAnd remember that there is no grade penalty for turning in homework late\n\nCharles is not obligated to give you feedback though\n\n\nSadly, there is no audio in the class on 11/5\n\nNot sure if human error or computer issue"
  },
  {
    "objectID": "lessons/x13_Expected_Values/13_Expected_Values_key_info.html#key-dates",
    "href": "lessons/x13_Expected_Values/13_Expected_Values_key_info.html#key-dates",
    "title": "Key Info",
    "section": "Key Dates",
    "text": "Key Dates\n\nHW 6 due this Sunday"
  },
  {
    "objectID": "lessons/x13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/36_Sums_of_Independent_Normal_rv/36_Sums_of_Independent_Normal_rv_key_info.html",
    "href": "lessons/x13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/36_Sums_of_Independent_Normal_rv/36_Sums_of_Independent_Normal_rv_key_info.html",
    "title": "Key Info",
    "section": "",
    "text": "Homework 7 presentations due soon!\n\nI will check your HW 7 solutions ASAP!\n\nDepending on how far we get today and Wednesday, I might cancel the 12/9 class\nAnything else?"
  },
  {
    "objectID": "lessons/x13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/36_Sums_of_Independent_Normal_rv/36_Sums_of_Independent_Normal_rv_key_info.html#announcements",
    "href": "lessons/x13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/36_Sums_of_Independent_Normal_rv/36_Sums_of_Independent_Normal_rv_key_info.html#announcements",
    "title": "Key Info",
    "section": "",
    "text": "Homework 7 presentations due soon!\n\nI will check your HW 7 solutions ASAP!\n\nDepending on how far we get today and Wednesday, I might cancel the 12/9 class\nAnything else?"
  },
  {
    "objectID": "lessons/x13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/36_Sums_of_Independent_Normal_rv/36_Sums_of_Independent_Normal_rv_key_info.html#key-dates",
    "href": "lessons/x13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/36_Sums_of_Independent_Normal_rv/36_Sums_of_Independent_Normal_rv_key_info.html#key-dates",
    "title": "Key Info",
    "section": "Key Dates",
    "text": "Key Dates\n\nThursday: HW 8 assignment due\nSunday: HW 7 presentations due at 11pm"
  },
  {
    "objectID": "lessons/x13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/28_29_Joint_expected_value_Variance/28_29_Joint_expected_value_Variance.html",
    "href": "lessons/x13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/28_29_Joint_expected_value_Variance/28_29_Joint_expected_value_Variance.html",
    "title": "Chapter 28: Revisiting Expected Values for Joint Distributions",
    "section": "",
    "text": "Calculate the mean (expected value) of a joint distribution of continuous RV"
  },
  {
    "objectID": "lessons/x13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/28_29_Joint_expected_value_Variance/28_29_Joint_expected_value_Variance.html#learning-objectives",
    "href": "lessons/x13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/28_29_Joint_expected_value_Variance/28_29_Joint_expected_value_Variance.html#learning-objectives",
    "title": "Chapter 28: Revisiting Expected Values for Joint Distributions",
    "section": "",
    "text": "Calculate the mean (expected value) of a joint distribution of continuous RV"
  },
  {
    "objectID": "lessons/x13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/28_29_Joint_expected_value_Variance/28_29_Joint_expected_value_Variance.html#remark-on-expected-value-of-one-rv-from-joint-pdf",
    "href": "lessons/x13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/28_29_Joint_expected_value_Variance/28_29_Joint_expected_value_Variance.html#remark-on-expected-value-of-one-rv-from-joint-pdf",
    "title": "Chapter 28: Revisiting Expected Values for Joint Distributions",
    "section": "Remark on expected value of one RV from joint pdf",
    "text": "Remark on expected value of one RV from joint pdf\nIf you are given \\(f_{X,Y}(x,y)\\) and want to calculate \\(\\mathbb{E}[X]\\), you have two options:\n\nFind \\(f_X(x)\\) and use it to calculate \\(\\mathbb{E}[X]\\).\nOr, calculate \\(\\mathbb{E}[X]\\) using the joint density: \\[\\mathbb{E}[X] = \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}x f_{X,Y}(x,y)dydx.\\]"
  },
  {
    "objectID": "lessons/x13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/28_29_Joint_expected_value_Variance/28_29_Joint_expected_value_Variance.html#option-1-expected-value-from-a-joint-distribution",
    "href": "lessons/x13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/28_29_Joint_expected_value_Variance/28_29_Joint_expected_value_Variance.html#option-1-expected-value-from-a-joint-distribution",
    "title": "Chapter 28: Revisiting Expected Values for Joint Distributions",
    "section": "Option 1: Expected value from a joint distribution",
    "text": "Option 1: Expected value from a joint distribution\n\n\n\n\nExample 3\n\n\nLet \\(f_{X,Y}(x,y)= 2e^{-(x+y)}\\), for \\(0 \\leq x \\leq y\\). Find \\(\\mathbb{E}[X]\\)."
  },
  {
    "objectID": "lessons/x13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/28_29_Joint_expected_value_Variance/28_29_Joint_expected_value_Variance.html#option-2-expected-value-from-a-joint-distribution",
    "href": "lessons/x13_Expected_Values/old_notes/15_Expected_Values_of_Sums_of_rvs/28_29_Joint_expected_value_Variance/28_29_Joint_expected_value_Variance.html#option-2-expected-value-from-a-joint-distribution",
    "title": "Chapter 28: Revisiting Expected Values for Joint Distributions",
    "section": "Option 2: Expected value from a joint distribution",
    "text": "Option 2: Expected value from a joint distribution\n\n\n\n\nExample 1\n\n\nLet \\(f_{X,Y}(x,y)= 2e^{-(x+y)}\\), for \\(0 \\leq x \\leq y\\). Find \\(\\mathbb{E}[X]\\)."
  },
  {
    "objectID": "lessons/x13_Expected_Values/13_Expected_Values_muddy_points.html",
    "href": "lessons/x13_Expected_Values/13_Expected_Values_muddy_points.html",
    "title": "Muddy Points",
    "section": "",
    "text": "Here’s a pretty good video on integration by parts!\nHere’s the Calc review muddy points with a few words on integration by parts.\n\n\n\nGot a little help from Chatgpt:\nThink back to our discrete example with the die. Our expected value was the weighted average of all the possible outcomes (weighted by their probability). So our expected value for discrete RVs will always be that weighted average: \\[\\mathbb{E}[X] = \\sum_{i=1}^n x_ip_X(x_i)\\]\nFor continuous RVs, they can take infinite possible values, so we cannot sum across the pdf the same way. We still want the weighted average, so we need to find a way to “sum” the weighted outcomes for the continuous RV, which translates to an integral.\nHere’s a fairly good explanation on StackExchange..)\n\n\n\nFinal example: Let \\(f_{X,Y}(x,y)= 2e^{-(x+y)}\\), for \\(0 \\leq x \\leq y\\). Find \\(\\mathbb{E}[X]\\).\nLet’s start with a plot for the domain. We have \\(0 \\leq x \\leq y\\), so we know that \\(x\\geq0\\) and because \\(y\\geq x\\), then \\(y\\geq0\\), so we’ll have a plot of all positive values of \\(x\\) and \\(y\\):\n\n\n\n\n\n\n\n\n\nOkay, now let’s add the information that \\(y \\geq x\\). We can look at the line, \\(y=x\\), and identify the area on the side of the line that upholds \\(y \\geq x\\):\n\n\n\n\n\n\n\n\n\nThe area above the line is where \\(y\\geq x\\):\n\n\n\n\n\n\n\n\n\nSo we need to find the bounds for the orange region in terms of \\(x\\) and \\(y\\).\nWhichever random variable is on the inner integral will need to incorporate the \\(y=x\\) line. If we integrate over \\(y\\) first, we will integrate from \\(y=x\\) to \\(y=\\infty\\). Once we have incorporated the line into our first integral, then we no longer need to worry about the \\(y=x\\) line. For \\(x\\), we can integrate from \\(x=0\\) to \\(x=\\infty\\)."
  },
  {
    "objectID": "lessons/x13_Expected_Values/13_Expected_Values_muddy_points.html#fall-2025",
    "href": "lessons/x13_Expected_Values/13_Expected_Values_muddy_points.html#fall-2025",
    "title": "Muddy Points",
    "section": "",
    "text": "Here’s a pretty good video on integration by parts!\nHere’s the Calc review muddy points with a few words on integration by parts.\n\n\n\nGot a little help from Chatgpt:\nThink back to our discrete example with the die. Our expected value was the weighted average of all the possible outcomes (weighted by their probability). So our expected value for discrete RVs will always be that weighted average: \\[\\mathbb{E}[X] = \\sum_{i=1}^n x_ip_X(x_i)\\]\nFor continuous RVs, they can take infinite possible values, so we cannot sum across the pdf the same way. We still want the weighted average, so we need to find a way to “sum” the weighted outcomes for the continuous RV, which translates to an integral.\nHere’s a fairly good explanation on StackExchange..)\n\n\n\nFinal example: Let \\(f_{X,Y}(x,y)= 2e^{-(x+y)}\\), for \\(0 \\leq x \\leq y\\). Find \\(\\mathbb{E}[X]\\).\nLet’s start with a plot for the domain. We have \\(0 \\leq x \\leq y\\), so we know that \\(x\\geq0\\) and because \\(y\\geq x\\), then \\(y\\geq0\\), so we’ll have a plot of all positive values of \\(x\\) and \\(y\\):\n\n\n\n\n\n\n\n\n\nOkay, now let’s add the information that \\(y \\geq x\\). We can look at the line, \\(y=x\\), and identify the area on the side of the line that upholds \\(y \\geq x\\):\n\n\n\n\n\n\n\n\n\nThe area above the line is where \\(y\\geq x\\):\n\n\n\n\n\n\n\n\n\nSo we need to find the bounds for the orange region in terms of \\(x\\) and \\(y\\).\nWhichever random variable is on the inner integral will need to incorporate the \\(y=x\\) line. If we integrate over \\(y\\) first, we will integrate from \\(y=x\\) to \\(y=\\infty\\). Once we have incorporated the line into our first integral, then we no longer need to worry about the \\(y=x\\) line. For \\(x\\), we can integrate from \\(x=0\\) to \\(x=\\infty\\)."
  },
  {
    "objectID": "lessons/x04_Rules_of_prob/04_Conditional_Probability.html",
    "href": "lessons/x04_Rules_of_prob/04_Conditional_Probability.html",
    "title": "Chapter 4: Conditional Probability",
    "section": "",
    "text": "Use set process to calculate probability of event of interest\nCalculate the probability of an event occurring, given that another event occurred.\nDefine keys facts for conditional probabilities using notation."
  },
  {
    "objectID": "lessons/x04_Rules_of_prob/04_Conditional_Probability.html#learning-objectives",
    "href": "lessons/x04_Rules_of_prob/04_Conditional_Probability.html#learning-objectives",
    "title": "Chapter 4: Conditional Probability",
    "section": "",
    "text": "Use set process to calculate probability of event of interest\nCalculate the probability of an event occurring, given that another event occurred.\nDefine keys facts for conditional probabilities using notation."
  },
  {
    "objectID": "lessons/x04_Rules_of_prob/04_Conditional_Probability.html#where-are-we",
    "href": "lessons/x04_Rules_of_prob/04_Conditional_Probability.html#where-are-we",
    "title": "Chapter 4: Conditional Probability",
    "section": "Where are we?",
    "text": "Where are we?"
  },
  {
    "objectID": "lessons/x04_Rules_of_prob/04_Conditional_Probability.html#general-process-for-probability-word-problems",
    "href": "lessons/x04_Rules_of_prob/04_Conditional_Probability.html#general-process-for-probability-word-problems",
    "title": "Chapter 4: Conditional Probability",
    "section": "General Process for Probability Word Problems",
    "text": "General Process for Probability Word Problems\n\nClearly define your events of interest\nTranslate question to probability using defined events OR Venn Diagram\nAsk yourself:\n\nAre we sampling with or without replacement?\nDoes order matter?\n\nUse axioms, properties, partitions, facts, etc. to define the end probability calculation into smaller parts\n\nIf probabilities are given to you, Venn Diagrams may help you parse out the events and probability calculations\nIf you need to find probabilities with counting, pictures or diagrams might help here\n\nWrite out a concluding statement that gives the probability context\n(For own check) Make sure the calculated probability follows the axioms. Is is between 0 and 1?"
  },
  {
    "objectID": "lessons/x04_Rules_of_prob/04_Conditional_Probability.html#lets-revisit-our-deck-of-cards",
    "href": "lessons/x04_Rules_of_prob/04_Conditional_Probability.html#lets-revisit-our-deck-of-cards",
    "title": "Chapter 4: Conditional Probability",
    "section": "Let’s revisit our deck of cards",
    "text": "Let’s revisit our deck of cards\n\n\n\n\nExample 1\n\n\nSuppose we randomly draw 2 cards from a standard deck of cards. What is the probability that we draw a spade then a heart?\n\n\nLet\n\nLet \\(A =\\) event \\(1^{st}\\) card is spades\nLet \\(B =\\) event \\(2^{nd}\\) card is heart"
  },
  {
    "objectID": "lessons/x04_Rules_of_prob/04_Conditional_Probability.html#conditional-probability-facts-12",
    "href": "lessons/x04_Rules_of_prob/04_Conditional_Probability.html#conditional-probability-facts-12",
    "title": "Chapter 4: Conditional Probability",
    "section": "Conditional Probability facts (1/2)",
    "text": "Conditional Probability facts (1/2)\n\n\n\n\nFact 1: General Multiplication Rule\n\n\n\\[\\mathbb{P}(A\\cap B)=\\mathbb{P}(A)\\cdot\\mathbb{P}(B|A)\\]\n\n\n\n\n\nFact 2: Conditional Probability Definition\n\n\n\\[\\mathbb{P}(A|B)=\\frac{\\mathbb{P}(A\\cap B)}{\\mathbb{P}(B)}\\]"
  },
  {
    "objectID": "lessons/x04_Rules_of_prob/04_Conditional_Probability.html#conditional-probability-facts-22",
    "href": "lessons/x04_Rules_of_prob/04_Conditional_Probability.html#conditional-probability-facts-22",
    "title": "Chapter 4: Conditional Probability",
    "section": "Conditional Probability facts (2/2)",
    "text": "Conditional Probability facts (2/2)\n\n\n\n\nFact 3\n\n\nIf \\(A\\) and \\(B\\) are independent events (\\(A \\unicode{x2AEB}B\\)), then \\[\\mathbb{P}(A|B) = \\mathbb{P}(A)\\]\n\n\n\n\n\nFact 4\n\n\n\\(\\mathbb{P}(A|B)\\) is a probability, meaning that it satisfies the probability axioms. In particular, \\[\\mathbb{P}(A|B) + \\mathbb{P}(A^C|B) = 1\\]"
  },
  {
    "objectID": "lessons/x04_Rules_of_prob/04_Conditional_Probability.html#monty-hall-problem",
    "href": "lessons/x04_Rules_of_prob/04_Conditional_Probability.html#monty-hall-problem",
    "title": "Chapter 4: Conditional Probability",
    "section": "Monty Hall Problem",
    "text": "Monty Hall Problem\nSurvivor Season 42\nWith the Wiki page on it!"
  },
  {
    "objectID": "lessons/x04_Rules_of_prob/04_Conditional_Probability.html#conditional-probability-with-two-dice",
    "href": "lessons/x04_Rules_of_prob/04_Conditional_Probability.html#conditional-probability-with-two-dice",
    "title": "Chapter 4: Conditional Probability",
    "section": "Conditional probability with two dice",
    "text": "Conditional probability with two dice\n\n\n\n\nExample 2\n\n\nTwo dice (red and blue) are rolled. If the dice do not show the same face, what is the probability that one of the dice is a 1?"
  },
  {
    "objectID": "lessons/x04_Rules_of_prob/04_Rules_of_prob.html",
    "href": "lessons/x04_Rules_of_prob/04_Rules_of_prob.html",
    "title": "Lesson 4: Rules of probability",
    "section": "",
    "text": "Define independence of 2-3 events and decide whether two or more events are independent\nDefine key facts for conditional probabilities and calculate conditional probabilities.\nCalculate the probability of an event using Bayes’ Theorem, Higher Order Multiplication Rule, and the Law of Total Probabilities\n\n\n\n\n\n\n\n\n\n\n\n\nClearly define your events of interest\nTranslate question to probability using defined events OR Venn Diagram\nAsk yourself:\n\nAre we sampling with or without replacement?\nDoes order matter?\n\nUse axioms, properties, partitions, facts, etc. to define the end probability calculation into smaller parts\n\nIf probabilities are given to you, Venn Diagrams may help you parse out the events and probability calculations\nIf you need to find probabilities with counting, pictures or diagrams might help here\n\nWrite out a concluding statement that gives the probability context\n(For own check) Make sure the calculated probability follows the axioms. Is is between 0 and 1?"
  },
  {
    "objectID": "lessons/x04_Rules_of_prob/04_Rules_of_prob.html#general-process-for-probability-word-problems",
    "href": "lessons/x04_Rules_of_prob/04_Rules_of_prob.html#general-process-for-probability-word-problems",
    "title": "Lesson 4: Rules of probability",
    "section": "",
    "text": "Clearly define your events of interest\nTranslate question to probability using defined events OR Venn Diagram\nAsk yourself:\n\nAre we sampling with or without replacement?\nDoes order matter?\n\nUse axioms, properties, partitions, facts, etc. to define the end probability calculation into smaller parts\n\nIf probabilities are given to you, Venn Diagrams may help you parse out the events and probability calculations\nIf you need to find probabilities with counting, pictures or diagrams might help here\n\nWrite out a concluding statement that gives the probability context\n(For own check) Make sure the calculated probability follows the axioms. Is is between 0 and 1?"
  },
  {
    "objectID": "lessons/x04_Rules_of_prob/04_Rules_of_prob.html#independent-events",
    "href": "lessons/x04_Rules_of_prob/04_Rules_of_prob.html#independent-events",
    "title": "Lesson 4: Rules of probability",
    "section": "Independent Events",
    "text": "Independent Events\n\n\nDefinition: Independence\n\n\nEvents \\(A\\) and \\(B\\) are independent if \\[\\mathbb{P}(A \\cap B) = \\mathbb{P}(A) \\cdot  \\mathbb{P}(B).\\]\n\n\nNotation: For shorthand, we sometimes write \\(A \\mathrel{\\unicode{x2AEB}} B,\\) to denote that \\(A\\) and \\(B\\) are independent events.\n \n\nAlso note: \\[\\begin{aligned} \\mathbb{P}(A \\cap B) = \\mathbb{P}(A) \\cdot  \\mathbb{P}(B) & \\implies A \\mathrel{\\unicode{x2AEB}} B \\\\\nA \\mathrel{\\unicode{x2AEB}} B & \\implies \\mathbb{P}(A \\cap B) = \\mathbb{P}(A) \\cdot  \\mathbb{P}(B) \\end{aligned}\\]"
  },
  {
    "objectID": "lessons/x04_Rules_of_prob/04_Rules_of_prob.html#example-of-two-dice",
    "href": "lessons/x04_Rules_of_prob/04_Rules_of_prob.html#example-of-two-dice",
    "title": "Lesson 4: Rules of probability",
    "section": "Example of two dice",
    "text": "Example of two dice\n\n\nExample 1\n\n\nTwo dice (green and blue) are rolled. Let \\(A =\\) event a total of 7 appears, and \\(B =\\) event green die is a six. Are events \\(A\\) and \\(B\\) independent?"
  },
  {
    "objectID": "lessons/x04_Rules_of_prob/04_Rules_of_prob.html#example-of-two-dice-simulating-in-r-12",
    "href": "lessons/x04_Rules_of_prob/04_Rules_of_prob.html#example-of-two-dice-simulating-in-r-12",
    "title": "Lesson 4: Rules of probability",
    "section": "Example of two dice: simulating in R (1/2)",
    "text": "Example of two dice: simulating in R (1/2)\n\n\nExample 1\n\n\nTwo dice (green and blue) are rolled. Let \\(A =\\) event a total of 7 appears, and \\(B =\\) event green die is a six. Are events \\(A\\) and \\(B\\) independent?\n\n\n\nset.seed(1002)\nreps = 10000\nrolls = replicate(reps, sample(x = 1:6, size = 2, replace = TRUE))\nrolls[, 1:10]\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n[1,]    4    5    5    5    6    5    2    4    3     4\n[2,]    1    4    6    3    1    1    5    5    6     3\n\nevent_A = ( rolls[1, ] + rolls[2, ] == 7 )\nhead(event_A, 10)\n\n [1] FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE FALSE  TRUE\n\nevent_B = ( rolls[1, ] == 6 )\nhead(event_B, 10)\n\n [1] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE"
  },
  {
    "objectID": "lessons/x04_Rules_of_prob/04_Rules_of_prob.html#example-of-two-dice-simulating-in-r-12-1",
    "href": "lessons/x04_Rules_of_prob/04_Rules_of_prob.html#example-of-two-dice-simulating-in-r-12-1",
    "title": "Lesson 4: Rules of probability",
    "section": "Example of two dice: simulating in R (1/2)",
    "text": "Example of two dice: simulating in R (1/2)\n\n\nExample 1\n\n\nTwo dice (green and blue) are rolled. Let \\(A =\\) event a total of 7 appears, and \\(B =\\) event green die is a six. Are events \\(A\\) and \\(B\\) independent?\n\n\n\n( sum(event_A) / reps ) * ( sum(event_B) / reps )\n\n[1] 0.0286608\n\nevent_A_and_B = ( rolls[1, ] + rolls[2, ] == 7 ) & ( rolls[1, ] == 6 )\nhead(event_A_and_B, 10)\n\n [1] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE\n\nsum(event_A_and_B) / reps\n\n[1] 0.0284"
  },
  {
    "objectID": "lessons/x04_Rules_of_prob/04_Rules_of_prob.html#independence-of-3-events",
    "href": "lessons/x04_Rules_of_prob/04_Rules_of_prob.html#independence-of-3-events",
    "title": "Lesson 4: Rules of probability",
    "section": "Independence of 3 Events",
    "text": "Independence of 3 Events\n\n\nDefinition: Independence of 3 Events\n\n\nEvents \\(A\\), \\(B\\), and \\(C\\) are mutually independent if\n\n\n\\(\\mathbb{P}(A \\cap B) = \\mathbb{P}(A) \\cdot \\mathbb{P}(B)\\)\n\\(\\mathbb{P}(A \\cap C) = \\mathbb{P}(A) \\cdot \\mathbb{P}(C)\\)\n\\(\\mathbb{P}(B \\cap C) = \\mathbb{P}(B) \\cdot \\mathbb{P}(C)\\)\n\n\\(\\mathbb{P}(A \\cap B \\cap C) = \\mathbb{P}(A) \\cdot \\mathbb{P}(B) \\cdot \\mathbb{P}(C)\\)\n\n\n\nRemark:\nOn your homework you will show that \\((1) \\not \\Rightarrow (2)\\) and \\((2) \\not \\Rightarrow (1)\\)."
  },
  {
    "objectID": "lessons/x04_Rules_of_prob/04_Rules_of_prob.html#probability-at-least-one-smoker",
    "href": "lessons/x04_Rules_of_prob/04_Rules_of_prob.html#probability-at-least-one-smoker",
    "title": "Lesson 4: Rules of probability",
    "section": "Probability at least one smoker",
    "text": "Probability at least one smoker\n\n\n\n\nExample 2\n\n\nSuppose you take a random sample of \\(n\\) people, of which people are smokers and non-smokers independently of each other. Let\n\n\\(A_i =\\) event person \\(i\\) is a smoker, for \\(i=1, \\ldots ,n\\), and\n\\(p_i =\\) probability person \\(i\\) is a smoker, for \\(i=1, \\ldots ,n\\).\n\nFind the probability that at least one person in the random sample is a smoker."
  },
  {
    "objectID": "lessons/x04_Rules_of_prob/04_Rules_of_prob.html#building-geometric-series",
    "href": "lessons/x04_Rules_of_prob/04_Rules_of_prob.html#building-geometric-series",
    "title": "Lesson 4: Rules of probability",
    "section": "Building geometric series",
    "text": "Building geometric series\n\n\nExample 3\n\n\n\\(A, B,\\) and \\(C\\) toss a fair coin in order. The first to throw heads wins. What are their respective chances of winning?\n\n\nLet\n\n\\(A_H\\) and \\(A_T\\) be the events player A tosses heads and tails, respectively.\nSimilarly define \\(B_H\\), \\(B_T\\), \\(C_H\\), and \\(C_T\\)."
  },
  {
    "objectID": "lessons/x04_Rules_of_prob/04_Rules_of_prob.html#lets-revisit-our-deck-of-cards",
    "href": "lessons/x04_Rules_of_prob/04_Rules_of_prob.html#lets-revisit-our-deck-of-cards",
    "title": "Lesson 4: Rules of probability",
    "section": "Let’s revisit our deck of cards",
    "text": "Let’s revisit our deck of cards\n\n\n\n\nExample 1\n\n\nSuppose we randomly draw 2 cards from a standard deck of cards. What is the probability that we draw a spade then a heart?\n\n\nLet\n\nLet \\(A =\\) event \\(1^{st}\\) card is spades\nLet \\(B =\\) event \\(2^{nd}\\) card is heart"
  },
  {
    "objectID": "lessons/x04_Rules_of_prob/04_Rules_of_prob.html#conditional-probability-facts-12",
    "href": "lessons/x04_Rules_of_prob/04_Rules_of_prob.html#conditional-probability-facts-12",
    "title": "Lesson 4: Rules of probability",
    "section": "Conditional Probability facts (1/2)",
    "text": "Conditional Probability facts (1/2)\n\n\n\n\nFact 1: General Multiplication Rule\n\n\n\\[\\mathbb{P}(A\\cap B)=\\mathbb{P}(A)\\cdot\\mathbb{P}(B|A)\\]\n\n\n\n\n\nFact 2: Conditional Probability Definition\n\n\n\\[\\mathbb{P}(A|B)=\\frac{\\mathbb{P}(A\\cap B)}{\\mathbb{P}(B)}\\]"
  },
  {
    "objectID": "lessons/x04_Rules_of_prob/04_Rules_of_prob.html#conditional-probability-facts-22",
    "href": "lessons/x04_Rules_of_prob/04_Rules_of_prob.html#conditional-probability-facts-22",
    "title": "Lesson 4: Rules of probability",
    "section": "Conditional Probability facts (2/2)",
    "text": "Conditional Probability facts (2/2)\n\n\n\n\nFact 3\n\n\nIf \\(A\\) and \\(B\\) are independent events (\\(A \\unicode{x2AEB}B\\)), then \\[\\mathbb{P}(A|B) = \\mathbb{P}(A)\\]\n\n\n\n\n\nFact 4\n\n\n\\(\\mathbb{P}(A|B)\\) is a probability, meaning that it satisfies the probability axioms. In particular, \\[\\mathbb{P}(A|B) + \\mathbb{P}(A^C|B) = 1\\]"
  },
  {
    "objectID": "lessons/x04_Rules_of_prob/04_Rules_of_prob.html#conditional-probability-with-two-dice",
    "href": "lessons/x04_Rules_of_prob/04_Rules_of_prob.html#conditional-probability-with-two-dice",
    "title": "Lesson 4: Rules of probability",
    "section": "Conditional probability with two dice",
    "text": "Conditional probability with two dice\n\n\n\n\nExample 3\n\n\nTwo dice (green and blue) are rolled. If the dice do not show the same face, what is the probability that one of the dice is a 1?"
  },
  {
    "objectID": "lessons/x04_Rules_of_prob/04_Rules_of_prob.html#conditional-probability-with-two-dice-simulations",
    "href": "lessons/x04_Rules_of_prob/04_Rules_of_prob.html#conditional-probability-with-two-dice-simulations",
    "title": "Lesson 4: Rules of probability",
    "section": "Conditional probability with two dice: simulations",
    "text": "Conditional probability with two dice: simulations\n\n\nExample 3\n\n\nTwo dice (green and blue) are rolled. If the dice do not show the same face, what is the probability that one of the dice is a 1?\n\n\n\nset.seed(1002)\nrolls = replicate(reps, sample(x = 1:6, size = 2, replace = TRUE))\nrolls[, 1:10]\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n[1,]    4    5    5    5    6    5    2    4    3     4\n[2,]    1    4    6    3    1    1    5    5    6     3\n\nevent_A = ( rolls[1, ] != rolls[2, ] )\nhead(event_A, 10)\n\n [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n\nevent_B = ( rolls[1, ] == 1 | rolls[2, ] == 1 )\nhead(event_B, 10)\n\n [1]  TRUE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE\n\nsum(event_B & event_A) / sum(event_A)\n\n[1] 0.3315328"
  },
  {
    "objectID": "lessons/x04_Rules_of_prob/04_Rules_of_prob.html#monty-hall-problem",
    "href": "lessons/x04_Rules_of_prob/04_Rules_of_prob.html#monty-hall-problem",
    "title": "Lesson 4: Rules of probability",
    "section": "Monty Hall Problem",
    "text": "Monty Hall Problem\nSurvivor Season 42\nWith the Wiki page on it!"
  },
  {
    "objectID": "lessons/x04_Rules_of_prob/04_Rules_of_prob.html#bayes-rule-for-two-events",
    "href": "lessons/x04_Rules_of_prob/04_Rules_of_prob.html#bayes-rule-for-two-events",
    "title": "Lesson 4: Rules of probability",
    "section": "Bayes’ Rule for two events",
    "text": "Bayes’ Rule for two events\n\nWe can use the conditional probability (\\(\\mathbb{P}(A|B)\\)) to get information on the flipped conditional probability (\\(\\mathbb{P}(B|A)\\))\n\n\n\n\n\nTheorem: Bayes’ Rule (for two events)\n\n\nFor any two events \\(A\\) and \\(B\\) with nonzero probabilties,\n\\[\\mathbb{P}(A| B) =\n\\frac{\\mathbb{P}(A) \\cdot \\mathbb{P}(B|A)}\n{\\mathbb{P}(B)}\\]"
  },
  {
    "objectID": "lessons/x04_Rules_of_prob/04_Rules_of_prob.html#calculating-probability-with-higher-order-multiplication-rule",
    "href": "lessons/x04_Rules_of_prob/04_Rules_of_prob.html#calculating-probability-with-higher-order-multiplication-rule",
    "title": "Lesson 4: Rules of probability",
    "section": "Calculating probability with Higher Order Multiplication Rule",
    "text": "Calculating probability with Higher Order Multiplication Rule\n\n\n\n\nExample 4\n\n\nSuppose we draw 5 cards from a standard shuffled deck of 52 cards. What is the probability of a flush, that is all the cards are of the same suit (including straight flushes)?\n\n\n\n\n\nHigher Order Multiplication Rule\n\n\n\\[\\begin{aligned} \\mathbb{P}(A_1\\cap & A_2 \\cap  \\ldots \\cap A_n)= \\\\ & \\mathbb{P}(A_1)\\cdot\\mathbb{P}(A_2|A_1) \\cdot \\\\\n& \\mathbb{P}(A_3|A_1A_2)\\ldots \\cdot\\mathbb{P}(A_n|A_1A_2\\ldots A_{n-1})\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/x04_Rules_of_prob/04_Rules_of_prob.html#calculating-probability-with-law-of-total-probability",
    "href": "lessons/x04_Rules_of_prob/04_Rules_of_prob.html#calculating-probability-with-law-of-total-probability",
    "title": "Lesson 4: Rules of probability",
    "section": "Calculating probability with Law of Total Probability",
    "text": "Calculating probability with Law of Total Probability\n\n\n\n\nExample 5\n\n\nSuppose 1% of people assigned female at birth (AFAB) and 5% of people assigned male at birth (AMAB) are color-blind. Assume person born is equally likely AFAB or AMAB (not including intersex). What is the probability that a person chosen at random is color-blind?\n\n\n\n\n\nLaw of Total Probability for 2 Events\n\n\nFor events \\(A\\) and \\(B\\),\n\\[%\\left(\n\\begin{array}{ccl}\n\\mathbb{P}(B)&=&\\mathbb{P}(B \\cap A) + \\mathbb{P}(B \\cap A^C)\\\\\n           &=& \\mathbb{P}(B|A) \\cdot \\mathbb{P}(A)+ \\mathbb{P}(B | A^C)\\cdot \\mathbb{P}(A^C)\n\\end{array}\n%\\right)\\]"
  },
  {
    "objectID": "lessons/x04_Rules_of_prob/04_Rules_of_prob.html#general-law-of-total-proability",
    "href": "lessons/x04_Rules_of_prob/04_Rules_of_prob.html#general-law-of-total-proability",
    "title": "Lesson 4: Rules of probability",
    "section": "General Law of Total Proability",
    "text": "General Law of Total Proability\n\n\nLaw of Total Probability (general)\n\n\nIf \\(\\{A_i\\}_{i=1}^{n} = \\{A_1, A_2, \\ldots, A_n\\}\\) form a partition of the sample space, then for event \\(B\\),\n\\[%\\left(\n\\begin{array}{ccl}\n\\mathbb{P}(B)&=& \\sum_{i=1}^{n} \\mathbb{P}(B \\cap A_i)\\\\\n           &=& \\sum_{i=1}^{n} \\mathbb{P}(B|A_i) \\cdot \\mathbb{P}(A_i)\n\\end{array}\n%\\right)\\]"
  },
  {
    "objectID": "lessons/x04_Rules_of_prob/04_Rules_of_prob.html#calculating-probability-with-generalized-law-of-total-probability",
    "href": "lessons/x04_Rules_of_prob/04_Rules_of_prob.html#calculating-probability-with-generalized-law-of-total-probability",
    "title": "Lesson 4: Rules of probability",
    "section": "Calculating probability with generalized Law of Total Probability",
    "text": "Calculating probability with generalized Law of Total Probability"
  },
  {
    "objectID": "lessons/x04_Rules_of_prob/04_Rules_of_prob.html#calculating-probability-with-generalized-law-of-total-probability-1",
    "href": "lessons/x04_Rules_of_prob/04_Rules_of_prob.html#calculating-probability-with-generalized-law-of-total-probability-1",
    "title": "Lesson 4: Rules of probability",
    "section": "Calculating probability with generalized Law of Total Probability",
    "text": "Calculating probability with generalized Law of Total Probability\n\n\n\n\nExample 3\n\n\nIndividuals are diagnosed with a particular type of cancer that can take on three different disease forms,* \\(D_1\\), \\(D_2\\), and \\(D_3\\). It is known that amongst people diagnosed with this particular type of cancer,\n\n20% of people will eventually be diagnosed with form \\(D_1\\),\n30% with form \\(D_2\\), and\n50% with form \\(D_3\\).\n\nThe probability of requiring chemotherapy (\\(C\\)) differs among the three forms of disease:\n\n80% with \\(D_1\\),\n30% with \\(D_2\\), and\n10% with \\(D_3\\).\n\nBased solely on the preliminary test of being diagnosed with the cancer, what is the probability of requiring chemotherapy (the event C)?\n\n\n\nSkipping in class! Let me know if you would like me to post solutions to this if you work through it!"
  },
  {
    "objectID": "lessons/x04_Rules_of_prob/04_Rules_of_prob.html#lets-revisit-the-color-blind-example",
    "href": "lessons/x04_Rules_of_prob/04_Rules_of_prob.html#lets-revisit-the-color-blind-example",
    "title": "Lesson 4: Rules of probability",
    "section": "Let’s revisit the color-blind example",
    "text": "Let’s revisit the color-blind example\n\n\n\n\nExample 4\n\n\nRecall the color-blind example (Example 2), where\n\na person is AMAB with probability 0.5,\nAMAB people are color-blind with probability 0.05, and\nall people are color-blind with probability 0.03.\n\nAssuming people are AMAB or AFAB, find the probability that a color-blind person is AMAB."
  },
  {
    "objectID": "lessons/x04_Rules_of_prob/04_Rules_of_prob.html#calculate-probability-with-both-rules",
    "href": "lessons/x04_Rules_of_prob/04_Rules_of_prob.html#calculate-probability-with-both-rules",
    "title": "Lesson 4: Rules of probability",
    "section": "Calculate probability with both rules",
    "text": "Calculate probability with both rules\n\n\n\n\nExample 6\n\n\nSuppose\n\n1% of people who are AFAB aged 40-50 years have breast cancer,\nan AFAB person with breast cancer has a 90% chance of a positive test from a mammogram, and\nan AFAB person has a 10% chance of a false-positive result from a mammogram.\n\nWhat is the probability that an AFAB person has breast cancer given that they just had a positive test?"
  },
  {
    "objectID": "lessons/x04_Rules_of_prob/04_Rules_of_prob.html#bayes-rule",
    "href": "lessons/x04_Rules_of_prob/04_Rules_of_prob.html#bayes-rule",
    "title": "Lesson 4: Rules of probability",
    "section": "Bayes’ Rule",
    "text": "Bayes’ Rule\n\n\nTheorem: Bayes’ Rule\n\n\nIf \\(\\{A_i\\}_{i=1}^{n}\\) form a partition of the sample space \\(S\\), with \\(\\mathbb{P}(A_i)&gt;0\\) for \\(i=1\\ldots n\\) and \\(\\mathbb{P}(B)&gt;0\\), then\n\\[\\mathbb{P}(A_j | B) =\n\\frac{\\mathbb{P}(B|A_j) \\cdot \\mathbb{P}(A_j)}\n{\\sum_{i=1}^{n} \\mathbb{P}(B|A_i) \\cdot \\mathbb{P}(A_i)}\\]"
  },
  {
    "objectID": "lessons/x04_Rules_of_prob/03_Independent_Events.html",
    "href": "lessons/x04_Rules_of_prob/03_Independent_Events.html",
    "title": "Chapter 3: Independent Events",
    "section": "",
    "text": "Define independence of 2-3 events given probability notation\nCalculate whether two or more events are independent"
  },
  {
    "objectID": "lessons/x04_Rules_of_prob/03_Independent_Events.html#learning-objectives",
    "href": "lessons/x04_Rules_of_prob/03_Independent_Events.html#learning-objectives",
    "title": "Chapter 3: Independent Events",
    "section": "",
    "text": "Define independence of 2-3 events given probability notation\nCalculate whether two or more events are independent"
  },
  {
    "objectID": "lessons/x04_Rules_of_prob/03_Independent_Events.html#where-are-we",
    "href": "lessons/x04_Rules_of_prob/03_Independent_Events.html#where-are-we",
    "title": "Chapter 3: Independent Events",
    "section": "Where are we?",
    "text": "Where are we?"
  },
  {
    "objectID": "lessons/x04_Rules_of_prob/03_Independent_Events.html#independent-events",
    "href": "lessons/x04_Rules_of_prob/03_Independent_Events.html#independent-events",
    "title": "Chapter 3: Independent Events",
    "section": "Independent Events",
    "text": "Independent Events\n\n\nDefinition: Independence\n\n\nEvents \\(A\\) and \\(B\\) are independent if \\[\\mathbb{P}(A \\cap B) = \\mathbb{P}(A) \\cdot  \\mathbb{P}(B).\\]\n\n\nNotation: For shorthand, we sometimes write \\(A \\mathrel{\\unicode{x2AEB}} B,\\) to denote that \\(A\\) and \\(B\\) are independent events.\n \n\nAlso note: \\[\\begin{aligned} \\mathbb{P}(A \\cap B) = \\mathbb{P}(A) \\cdot  \\mathbb{P}(B) & \\implies A \\mathrel{\\unicode{x2AEB}} B \\\\\nA \\mathrel{\\unicode{x2AEB}} B & \\implies \\mathbb{P}(A \\cap B) = \\mathbb{P}(A) \\cdot  \\mathbb{P}(B) \\end{aligned}\\]"
  },
  {
    "objectID": "lessons/x04_Rules_of_prob/03_Independent_Events.html#example-of-two-dice",
    "href": "lessons/x04_Rules_of_prob/03_Independent_Events.html#example-of-two-dice",
    "title": "Chapter 3: Independent Events",
    "section": "Example of two dice",
    "text": "Example of two dice\n\n\nExample 1\n\n\nTwo dice (red and blue) are rolled. Let \\(A =\\) event a total of 7 appears, and \\(B =\\) event red die is a six. Are events \\(A\\) and \\(B\\) independent?"
  },
  {
    "objectID": "lessons/x04_Rules_of_prob/03_Independent_Events.html#independence-of-3-events",
    "href": "lessons/x04_Rules_of_prob/03_Independent_Events.html#independence-of-3-events",
    "title": "Chapter 3: Independent Events",
    "section": "Independence of 3 Events",
    "text": "Independence of 3 Events\n\n\nDefinition: Independence of 3 Events\n\n\nEvents \\(A\\), \\(B\\), and \\(C\\) are mutually independent if\n\n\n\\(\\mathbb{P}(A \\cap B) = \\mathbb{P}(A) \\cdot \\mathbb{P}(B)\\)\n\\(\\mathbb{P}(A \\cap C) = \\mathbb{P}(A) \\cdot \\mathbb{P}(C)\\)\n\\(\\mathbb{P}(B \\cap C) = \\mathbb{P}(B) \\cdot \\mathbb{P}(C)\\)\n\n\\(\\mathbb{P}(A \\cap B \\cap C) = \\mathbb{P}(A) \\cdot \\mathbb{P}(B) \\cdot \\mathbb{P}(C)\\)\n\n\n\nRemark:\nOn your homework you will show that \\((1) \\not \\Rightarrow (2)\\) and \\((2) \\not \\Rightarrow (1)\\)."
  },
  {
    "objectID": "lessons/x04_Rules_of_prob/03_Independent_Events.html#probability-at-least-one-smoker",
    "href": "lessons/x04_Rules_of_prob/03_Independent_Events.html#probability-at-least-one-smoker",
    "title": "Chapter 3: Independent Events",
    "section": "Probability at least one smoker",
    "text": "Probability at least one smoker\n\n\n\n\nExample 2\n\n\nSuppose you take a random sample of \\(n\\) people, of which people are smokers and non-smokers independently of each other. Let\n\n\\(A_i =\\) event person \\(i\\) is a smoker, for \\(i=1, \\ldots ,n\\), and\n\\(p_i =\\) probability person \\(i\\) is a smoker, for \\(i=1, \\ldots ,n\\).\n\nFind the probability that at least one person in the random sample is a smoker."
  },
  {
    "objectID": "lessons/x04_Rules_of_prob/03_Independent_Events.html#building-geometric-series",
    "href": "lessons/x04_Rules_of_prob/03_Independent_Events.html#building-geometric-series",
    "title": "Chapter 3: Independent Events",
    "section": "Building geometric series",
    "text": "Building geometric series\n\n\nExample 3\n\n\n\\(A, B,\\) and \\(C\\) toss a fair coin in order. The first to throw heads wins. What are their respective chances of winning?\n\n\nLet\n\n\\(A_H\\) and \\(A_T\\) be the events player A tosses heads and tails, respectively.\nSimilarly define \\(B_H\\), \\(B_T\\), \\(C_H\\), and \\(C_T\\)."
  },
  {
    "objectID": "lessons/x08_pdfs/24_01_Continuous_rv_key_info.html",
    "href": "lessons/x08_pdfs/24_01_Continuous_rv_key_info.html",
    "title": "Key Info",
    "section": "",
    "text": "Graded: HW 4 assignment and HW 3 solutions\nCalendly is up!!"
  },
  {
    "objectID": "lessons/x08_pdfs/24_01_Continuous_rv_key_info.html#announcements",
    "href": "lessons/x08_pdfs/24_01_Continuous_rv_key_info.html#announcements",
    "title": "Key Info",
    "section": "",
    "text": "Graded: HW 4 assignment and HW 3 solutions\nCalendly is up!!"
  },
  {
    "objectID": "lessons/x08_pdfs/24_01_Continuous_rv_key_info.html#key-dates",
    "href": "lessons/x08_pdfs/24_01_Continuous_rv_key_info.html#key-dates",
    "title": "Key Info",
    "section": "Key Dates",
    "text": "Key Dates\n\nThursday: HW 5 due\nSunday: HW 4 solutions due\nWeek of 11/11\n\nMake a meeting with me\n11/12: recordings due if you are making that\nNow on Sakai"
  },
  {
    "objectID": "lessons/x08_pdfs/08_pdfs.html",
    "href": "lessons/x08_pdfs/08_pdfs.html",
    "title": "Lesson 8: Probability density functions (PDFs)",
    "section": "",
    "text": "Distinguish between discrete and continuous random variables.\nCalculate probabilities for continuous random variables.\nUse R to simulate known continuous distributions."
  },
  {
    "objectID": "lessons/x08_pdfs/08_pdfs.html#discrete-vs.-continuous-rvs",
    "href": "lessons/x08_pdfs/08_pdfs.html#discrete-vs.-continuous-rvs",
    "title": "Lesson 8: Probability density functions (PDFs)",
    "section": "Discrete vs. Continuous RVs",
    "text": "Discrete vs. Continuous RVs\n\n\n\nFor a discrete RV, the set of possible values is either finite or can be put into a countably infinite list.\n \nContinuous RVs take on values from continuous intervals, or unions of continuous intervals\n\n\n\n\n\nFigure from Introduction to Probability TB (pg. 301)"
  },
  {
    "objectID": "lessons/x08_pdfs/08_pdfs.html#how-to-define-probabilities-for-continuous-rvs",
    "href": "lessons/x08_pdfs/08_pdfs.html#how-to-define-probabilities-for-continuous-rvs",
    "title": "Lesson 8: Probability density functions (PDFs)",
    "section": "How to define probabilities for continuous RVs?",
    "text": "How to define probabilities for continuous RVs?\n\n\nDiscrete RV \\(X\\):\n\n\n\n\n\n\n\n\n\n\npmf: \\(p_X(x) = P(X=x)\\)\n\n\nContinuous RV \\(X\\):\n\n\n\n\n\n\n\n\n\n\ndensity: \\(f_X(x)\\)\nprobability: \\(P(a \\leq X \\leq b) = \\int_a^b f_X(x)dx\\)"
  },
  {
    "objectID": "lessons/x08_pdfs/08_pdfs.html#what-is-a-probability-density-function",
    "href": "lessons/x08_pdfs/08_pdfs.html#what-is-a-probability-density-function",
    "title": "Lesson 8: Probability density functions (PDFs)",
    "section": "What is a probability density function?",
    "text": "What is a probability density function?\n\n\nProbability density function\n\n\nThe probability distribution, or probability density function (pdf), of a continuous random variable \\(X\\) is a function \\(f_X(x)\\), such that for all real values \\(a,b\\) with \\(a \\leq b\\),\n\\[\\mathbb{P}(a \\leq X \\leq b) = \\int_a^b f_X(x)dx\\]\n\n\nRemarks:\n\nNote that \\(f_X(x) \\neq \\mathbb{P}(X=x)\\)!!!\nIn order for \\(f_X(x)\\) to be a pdf, it needs to satisfy the properties\n\n\\(f_X(x) \\geq 0\\) for all \\(x\\)\n\\(\\int_{-\\infty}^{\\infty} f_X(x)dx=1\\)"
  },
  {
    "objectID": "lessons/x08_pdfs/08_pdfs.html#lets-demonstrate-the-pdf-with-an-example-15",
    "href": "lessons/x08_pdfs/08_pdfs.html#lets-demonstrate-the-pdf-with-an-example-15",
    "title": "Lesson 8: Probability density functions (PDFs)",
    "section": "Let’s demonstrate the PDF with an example (1/5)",
    "text": "Let’s demonstrate the PDF with an example (1/5)\n\n\n\n\nExample 1.1\n\n\nLet \\(f_X(x)= 2\\), for \\(a \\leq x \\leq 3\\).\n\nFind the value of \\(a\\) so that \\(f_X(x)\\) is a pdf."
  },
  {
    "objectID": "lessons/x08_pdfs/08_pdfs.html#lets-demonstrate-the-pdf-with-an-example-25",
    "href": "lessons/x08_pdfs/08_pdfs.html#lets-demonstrate-the-pdf-with-an-example-25",
    "title": "Lesson 8: Probability density functions (PDFs)",
    "section": "Let’s demonstrate the PDF with an example (2/5)",
    "text": "Let’s demonstrate the PDF with an example (2/5)\n\n\n\n\nExample 1.2\n\n\nLet \\(f_X(x)= 2\\), for \\(a \\leq x \\leq 3\\).\n\nFind \\(\\mathbb{P}(2.7 \\leq X \\leq 2.9)\\)."
  },
  {
    "objectID": "lessons/x08_pdfs/08_pdfs.html#lets-demonstrate-the-pdf-with-an-example-35",
    "href": "lessons/x08_pdfs/08_pdfs.html#lets-demonstrate-the-pdf-with-an-example-35",
    "title": "Lesson 8: Probability density functions (PDFs)",
    "section": "Let’s demonstrate the PDF with an example (3/5)",
    "text": "Let’s demonstrate the PDF with an example (3/5)\n\n\n\n\nExample 1.3\n\n\nLet \\(f_X(x)= 2\\), for \\(a \\leq x \\leq 3\\).\n\nFind \\(\\mathbb{P}(2.7 &lt; X \\leq 2.9)\\)."
  },
  {
    "objectID": "lessons/x08_pdfs/08_pdfs.html#lets-demonstrate-the-pdf-with-an-example-45",
    "href": "lessons/x08_pdfs/08_pdfs.html#lets-demonstrate-the-pdf-with-an-example-45",
    "title": "Lesson 8: Probability density functions (PDFs)",
    "section": "Let’s demonstrate the PDF with an example (4/5)",
    "text": "Let’s demonstrate the PDF with an example (4/5)\n\n\n\n\nExample 1.4\n\n\nLet \\(f_X(x)= 2\\), for \\(a \\leq x \\leq 3\\).\n\nFind \\(\\mathbb{P}(X = 2.9)\\)."
  },
  {
    "objectID": "lessons/x08_pdfs/08_pdfs.html#lets-demonstrate-the-pdf-with-an-example-55",
    "href": "lessons/x08_pdfs/08_pdfs.html#lets-demonstrate-the-pdf-with-an-example-55",
    "title": "Lesson 8: Probability density functions (PDFs)",
    "section": "Let’s demonstrate the PDF with an example (5/5)",
    "text": "Let’s demonstrate the PDF with an example (5/5)\n\n\n\n\nExample 1.5\n\n\nLet \\(f_X(x)= 2\\), for \\(a \\leq x \\leq 3\\).\n\nFind \\(\\mathbb{P}(X \\leq 2.8)\\)."
  },
  {
    "objectID": "lessons/x08_pdfs/08_pdfs.html#use-r-to-simulate-known-distributions",
    "href": "lessons/x08_pdfs/08_pdfs.html#use-r-to-simulate-known-distributions",
    "title": "Lesson 8: Probability density functions (PDFs)",
    "section": "Use R to simulate known distributions",
    "text": "Use R to simulate known distributions\n\nWe can use R to simulate continuous random variables and visualize their distributions\nFor example, we can simulate a uniform distribution between 2.5 and 3\n\n\nuniform = tibble(\n  x = runif(n=10000, min=2.5, max=3)\n)\n\nggplot(uniform, \n       aes(x = x, \n           y = after_stat(density))) +\n  geom_histogram( binwidth = 0.001) + \n  geom_abline(intercept = 2, slope = 0) + \n  labs(\n    title = \"Probability density function (pdf) of X\",\n    x = \"x\",\n    y = \"pdf\"\n  )"
  },
  {
    "objectID": "lessons/x08_pdfs/08_pdfs.html#use-r-to-simulate-any-continuous-distribution",
    "href": "lessons/x08_pdfs/08_pdfs.html#use-r-to-simulate-any-continuous-distribution",
    "title": "Lesson 8: Probability density functions (PDFs)",
    "section": "Use R to simulate any continuous distribution",
    "text": "Use R to simulate any continuous distribution\n\nWe will discuss other ways to simulate continuous distributions once we cover cumulative distribution functions (CDFs) and inverse CDFs"
  },
  {
    "objectID": "lessons/04_MVUE/04_MVUE.html",
    "href": "lessons/04_MVUE/04_MVUE.html",
    "title": "BSTA 551: Statistical Inference",
    "section": "",
    "text": "From Lesson 3:\n\n\\(\\chi^2_\\nu\\) distribution: sum of squared standard normals\n\\((n-1)S^2/\\sigma^2 \\sim \\chi^2_{n-1}\\) for normal samples\nt distribution arises when estimating \\(\\sigma\\) with \\(S\\)\nF distribution: ratio of independent chi-squared variables\n\n. . .\nToday’s Goals:\n\n\nExplore key statistics for normal random samples\nUnderstand the independence of \\(\\bar{X}\\) and \\(S^2\\)\nConstruct confidence intervals for variance\nDefine minimum variance unbiased estimators (MVUE)\nUnderstand consistency of estimators"
  },
  {
    "objectID": "lessons/04_MVUE/04_MVUE.html#review-lesson-3-key-results",
    "href": "lessons/04_MVUE/04_MVUE.html#review-lesson-3-key-results",
    "title": "BSTA 551: Statistical Inference",
    "section": "",
    "text": "From Lesson 3:\n\n\\(\\chi^2_\\nu\\) distribution: sum of squared standard normals\n\\((n-1)S^2/\\sigma^2 \\sim \\chi^2_{n-1}\\) for normal samples\nt distribution arises when estimating \\(\\sigma\\) with \\(S\\)\nF distribution: ratio of independent chi-squared variables\n\n. . .\nToday’s Goals:\n\n\nExplore key statistics for normal random samples\nUnderstand the independence of \\(\\bar{X}\\) and \\(S^2\\)\nConstruct confidence intervals for variance\nDefine minimum variance unbiased estimators (MVUE)\nUnderstand consistency of estimators"
  },
  {
    "objectID": "lessons/04_MVUE/04_MVUE.html#key-results-for-normal-samples-devore-6.4",
    "href": "lessons/04_MVUE/04_MVUE.html#key-results-for-normal-samples-devore-6.4",
    "title": "BSTA 551: Statistical Inference",
    "section": "Key Results for Normal Samples (Devore 6.4)",
    "text": "Key Results for Normal Samples (Devore 6.4)\n\n\n\n\n\n\nFundamental Results\n\n\n\nIf \\(X_1, \\ldots, X_n\\) is a random sample from \\(N(\\mu, \\sigma^2)\\):\n\n\\(\\bar{X} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\\)\n\\(\\bar{X}\\) and \\(S^2\\) are independent random variables\n\\(\\frac{(n-1)S^2}{\\sigma^2} \\sim \\chi^2_{n-1}\\)\n\\(\\frac{\\bar{X} - \\mu}{S/\\sqrt{n}} \\sim t_{n-1}\\)\n\n\n\n. . .\nThe independence of \\(\\bar{X}\\) and \\(S^2\\) is remarkable and unique to normal distributions!"
  },
  {
    "objectID": "lessons/04_MVUE/04_MVUE.html#simulation-independence-of-mean-and-variance",
    "href": "lessons/04_MVUE/04_MVUE.html#simulation-independence-of-mean-and-variance",
    "title": "BSTA 551: Statistical Inference",
    "section": "Simulation: Independence of Mean and Variance",
    "text": "Simulation: Independence of Mean and Variance\n\nn &lt;- 20\nn_sims &lt;- 2000\n\n# Generate samples and compute statistics\nsample_stats &lt;- tibble(sim = 1:n_sims) |&gt;\n  mutate(\n    data = map(sim, ~rnorm(n, mean = 50, sd = 10)),\n    x_bar = map_dbl(data, mean),\n    s_squared = map_dbl(data, var)\n  )\n\n# Check correlation\ncor_test &lt;- cor.test(sample_stats$x_bar, sample_stats$s_squared)\n\nsample_stats |&gt;\n  ggplot(aes(x = x_bar, y = s_squared)) +\n  geom_point(alpha = 0.3, color = \"steelblue\") +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  labs(\n    title = \"Sample Mean vs. Sample Variance (Normal Population)\",\n    subtitle = paste(\"Correlation:\", round(cor_test$estimate, 3), \n                     \"— These are independent!\"),\n    x = expression(bar(X)), y = expression(S^2)\n  )"
  },
  {
    "objectID": "lessons/04_MVUE/04_MVUE.html#medical-example-cholesterol-study",
    "href": "lessons/04_MVUE/04_MVUE.html#medical-example-cholesterol-study",
    "title": "BSTA 551: Statistical Inference",
    "section": "Medical Example: Cholesterol Study",
    "text": "Medical Example: Cholesterol Study\nA study measures LDL cholesterol (mg/dL) in 20 patients on a new statin medication.\n\n# LDL cholesterol levels\nldl_levels &lt;- c(95, 108, 87, 112, 99, 105, 92, 118, 103, 89,\n                110, 96, 102, 88, 115, 94, 107, 100, 91, 106)\n\nn &lt;- length(ldl_levels)\nx_bar &lt;- mean(ldl_levels)\ns &lt;- sd(ldl_levels)\n\n# Using the t distribution for inference\nt_crit &lt;- qt(0.975, df = n - 1)\nmargin_error &lt;- t_crit * s / sqrt(n)\n\ncat(\"Sample mean:\", round(x_bar, 2), \"mg/dL\\n\")\n\nSample mean: 100.85 mg/dL\n\ncat(\"Sample SD:\", round(s, 2), \"mg/dL\\n\")\n\nSample SD: 9.24 mg/dL\n\ncat(\"95% CI for μ: [\", round(x_bar - margin_error, 2), \",\", \n    round(x_bar + margin_error, 2), \"]\\n\")\n\n95% CI for μ: [ 96.53 , 105.17 ]\n\n\n. . .\nThe t-based confidence interval properly accounts for uncertainty in both \\(\\mu\\) and \\(\\sigma\\)."
  },
  {
    "objectID": "lessons/04_MVUE/04_MVUE.html#confidence-interval-for-population-variance",
    "href": "lessons/04_MVUE/04_MVUE.html#confidence-interval-for-population-variance",
    "title": "BSTA 551: Statistical Inference",
    "section": "Confidence Interval for Population Variance",
    "text": "Confidence Interval for Population Variance\n\n\n\n\n\n\nCI for \\(\\sigma^2\\)\n\n\n\nUsing \\((n-1)S^2/\\sigma^2 \\sim \\chi^2_{n-1}\\), a \\(100(1-\\alpha)\\%\\) confidence interval for \\(\\sigma^2\\) is:\n\\[\\left(\\frac{(n-1)S^2}{\\chi^2_{\\alpha/2, n-1}}, \\frac{(n-1)S^2}{\\chi^2_{1-\\alpha/2, n-1}}\\right)\\]\n\n\n. . .\n\n# 95% CI for variance in LDL example\nalpha &lt;- 0.05\nchi_upper &lt;- qchisq(1 - alpha/2, df = n - 1)\nchi_lower &lt;- qchisq(alpha/2, df = n - 1)\n\nci_var_lower &lt;- (n - 1) * s^2 / chi_upper\nci_var_upper &lt;- (n - 1) * s^2 / chi_lower\n\ncat(\"95% CI for σ²: [\", round(ci_var_lower, 1), \",\", round(ci_var_upper, 1), \"]\\n\")\n\n95% CI for σ²: [ 49.4 , 182.2 ]\n\ncat(\"95% CI for σ:  [\", round(sqrt(ci_var_lower), 2), \",\", round(sqrt(ci_var_upper), 2), \"]\")\n\n95% CI for σ:  [ 7.03 , 13.5 ]"
  },
  {
    "objectID": "lessons/04_MVUE/04_MVUE.html#your-turn-confidence-interval-for-variance",
    "href": "lessons/04_MVUE/04_MVUE.html#your-turn-confidence-interval-for-variance",
    "title": "BSTA 551: Statistical Inference",
    "section": "Your Turn: Confidence Interval for Variance",
    "text": "Your Turn: Confidence Interval for Variance\nExercise: A clinical trial measures the time (in minutes) for a drug to take effect in 12 patients. The sample variance is \\(s^2 = 16\\) minutes².\n\nConstruct a 90% confidence interval for the population variance \\(\\sigma^2\\)\nWhat is the corresponding 90% CI for the population SD \\(\\sigma\\)?\n\n. . .\nSolutions:\n\nn &lt;- 12\ns_squared &lt;- 16\nalpha &lt;- 0.10\n\nchi_lower &lt;- qchisq(alpha/2, df = n - 1)\nchi_upper &lt;- qchisq(1 - alpha/2, df = n - 1)\n\nci_var &lt;- c((n - 1) * s_squared / chi_upper, (n - 1) * s_squared / chi_lower)\nci_sd &lt;- sqrt(ci_var)\n\ncat(\"90% CI for σ²:\", round(ci_var, 2), \"\\n\")\n\n90% CI for σ²: 8.95 38.47 \n\ncat(\"90% CI for σ:\", round(ci_sd, 2))\n\n90% CI for σ: 2.99 6.2"
  },
  {
    "objectID": "lessons/04_MVUE/04_MVUE.html#minimum-variance-unbiased-estimators-mvue",
    "href": "lessons/04_MVUE/04_MVUE.html#minimum-variance-unbiased-estimators-mvue",
    "title": "BSTA 551: Statistical Inference",
    "section": "Minimum Variance Unbiased Estimators (MVUE)",
    "text": "Minimum Variance Unbiased Estimators (MVUE)\n\n\n\n\n\n\nThe MVUE Concept (Devore 7.1)\n\n\n\nGiven multiple unbiased estimators of a parameter \\(\\theta\\), the minimum variance unbiased estimator (MVUE) is the one with the smallest variance.\nIf \\(\\hat{\\theta}_1\\) and \\(\\hat{\\theta}_2\\) are both unbiased for \\(\\theta\\), prefer \\(\\hat{\\theta}_1\\) if: \\[\\text{Var}(\\hat{\\theta}_1) &lt; \\text{Var}(\\hat{\\theta}_2)\\]\n\n\n. . .\nWhy MVUE matters:\n\nUnbiased: on average, we hit the target\nMinimum variance: our estimates are tightly clustered\nBest of both worlds!"
  },
  {
    "objectID": "lessons/04_MVUE/04_MVUE.html#example-two-unbiased-estimators-of-θ-devore-example-7.10",
    "href": "lessons/04_MVUE/04_MVUE.html#example-two-unbiased-estimators-of-θ-devore-example-7.10",
    "title": "BSTA 551: Statistical Inference",
    "section": "Example: Two Unbiased Estimators of θ (Devore Example 7.10)",
    "text": "Example: Two Unbiased Estimators of θ (Devore Example 7.10)\nFor a Uniform[0, θ] distribution:\nEstimator 1: \\(\\hat{\\theta}_u = \\frac{n+1}{n} \\cdot \\max(X_1, \\ldots, X_n)\\)\n\nUnbiased: \\(E(\\hat{\\theta}_u) = \\theta\\)\nVariance: \\(\\text{Var}(\\hat{\\theta}_u) = \\frac{\\theta^2}{n(n+2)}\\)\n\nEstimator 2: \\(\\hat{\\theta}_2 = 2\\bar{X}\\)\n\nUnbiased: \\(E(\\hat{\\theta}_2) = \\theta\\)\n\nVariance: \\(\\text{Var}(\\hat{\\theta}_2) = \\frac{\\theta^2}{3n}\\)\n\n. . .\nWhich is better? Compare variances!"
  },
  {
    "objectID": "lessons/04_MVUE/04_MVUE.html#comparing-the-estimators",
    "href": "lessons/04_MVUE/04_MVUE.html#comparing-the-estimators",
    "title": "BSTA 551: Statistical Inference",
    "section": "Comparing the Estimators",
    "text": "Comparing the Estimators\n\n# Variance comparison\ncompare_vars &lt;- function(n) {\n  tibble(\n    n = n,\n    var_theta_u = 1 / (n * (n + 2)),  # Variance of θ̂_u (θ² factor omitted)\n    var_theta_2 = 1 / (3 * n),         # Variance of 2X̄\n    ratio = var_theta_2 / var_theta_u,\n    better = ifelse(var_theta_u &lt; var_theta_2, \"θ̂_u (max-based)\", \"2X̄\")\n  )\n}\n\nmap_dfr(c(5, 10, 20, 50), compare_vars) |&gt;\n  mutate(across(where(is.numeric), ~round(., 4)))\n\n# A tibble: 4 × 5\n      n var_theta_u var_theta_2 ratio better         \n  &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;          \n1     5      0.0286      0.0667  2.33 θ̂_u (max-based)\n2    10      0.0083      0.0333  4    θ̂_u (max-based)\n3    20      0.0023      0.0167  7.33 θ̂_u (max-based)\n4    50      0.0004      0.0067 17.3  θ̂_u (max-based)\n\n\n. . .\nThe max-based estimator \\(\\hat{\\theta}_u\\) is always more efficient (smaller variance)!"
  },
  {
    "objectID": "lessons/04_MVUE/04_MVUE.html#simulation-visualizing-mvue",
    "href": "lessons/04_MVUE/04_MVUE.html#simulation-visualizing-mvue",
    "title": "BSTA 551: Statistical Inference",
    "section": "Simulation: Visualizing MVUE",
    "text": "Simulation: Visualizing MVUE\n\ntheta &lt;- 100\nn &lt;- 10\nn_sims &lt;- 5000\n\nestimator_comparison &lt;- tibble(sim = 1:n_sims) |&gt;\n  mutate(\n    sample_data = map(sim, ~runif(n, 0, theta)),\n    theta_u = map_dbl(sample_data, ~(n + 1)/n * max(.x)),\n    theta_2 = map_dbl(sample_data, ~2 * mean(.x))\n  )\n\nestimator_comparison |&gt;\n  pivot_longer(cols = c(theta_u, theta_2), names_to = \"estimator\", values_to = \"estimate\") |&gt;\n  mutate(estimator = recode(estimator, \n                            theta_u = \"θ̂_u = (n+1)/n × max\",\n                            theta_2 = \"θ̂_2 = 2X̄\")) |&gt;\n  ggplot(aes(x = estimate, fill = estimator)) +\n  geom_histogram(bins = 50, alpha = 0.6, position = \"identity\", color = \"white\") +\n  geom_vline(xintercept = theta, color = \"red\", linewidth = 1.5, linetype = \"dashed\") +\n  labs(title = \"Comparing Two Unbiased Estimators of θ = 100\",\n       subtitle = \"θ̂_u has smaller variance (is the MVUE)\",\n       x = \"Estimate\", y = \"Frequency\", fill = \"Estimator\") +\n  scale_fill_brewer(palette = \"Set1\")"
  },
  {
    "objectID": "lessons/04_MVUE/04_MVUE.html#the-fundamental-mvue-result-for-normal-means",
    "href": "lessons/04_MVUE/04_MVUE.html#the-fundamental-mvue-result-for-normal-means",
    "title": "BSTA 551: Statistical Inference",
    "section": "The Fundamental MVUE Result for Normal Means",
    "text": "The Fundamental MVUE Result for Normal Means\n\n\n\n\n\n\nTheorem (Devore 7.1)\n\n\n\nLet \\(X_1, \\ldots, X_n\\) be a random sample from a normal distribution with parameters \\(\\mu\\) and \\(\\sigma\\).\nThen \\(\\hat{\\mu} = \\bar{X}\\) is the MVUE for \\(\\mu\\).\n\n\n. . .\nThis means:\n\nAmong ALL unbiased estimators of \\(\\mu\\) (not just \\(\\bar{X}\\), \\(\\tilde{X}\\), trimmed means, etc.)\nThe sample mean \\(\\bar{X}\\) has the smallest possible variance\nNo other unbiased estimator can do better!"
  },
  {
    "objectID": "lessons/04_MVUE/04_MVUE.html#your-turn-mvue-practice",
    "href": "lessons/04_MVUE/04_MVUE.html#your-turn-mvue-practice",
    "title": "BSTA 551: Statistical Inference",
    "section": "Your Turn: MVUE Practice",
    "text": "Your Turn: MVUE Practice\nExercise: A researcher measures reaction times (ms) in 16 healthy subjects:\n\nreaction_times &lt;- c(245, 267, 234, 289, 256, 278, 242, 271,\n                    263, 251, 284, 238, 275, 259, 247, 280)\n\n\nCalculate \\(\\bar{X}\\) (the MVUE for \\(\\mu\\))\nCalculate the sample median \\(\\tilde{X}\\)\nBoth are unbiased for \\(\\mu\\) if the population is symmetric. Based on our theorem, which should have smaller variance?\n\n. . .\nSolutions:\n\ncat(\"Sample mean (MVUE):\", round(mean(reaction_times), 2), \"\\n\")\n\nSample mean (MVUE): 261.19 \n\ncat(\"Sample median:\", round(median(reaction_times), 2), \"\\n\")\n\nSample median: 261 \n\ncat(\"If normal population, the mean has smaller variance than any other unbiased estimator!\")\n\nIf normal population, the mean has smaller variance than any other unbiased estimator!"
  },
  {
    "objectID": "lessons/04_MVUE/04_MVUE.html#properties-of-the-sample-variance",
    "href": "lessons/04_MVUE/04_MVUE.html#properties-of-the-sample-variance",
    "title": "BSTA 551: Statistical Inference",
    "section": "Properties of the Sample Variance",
    "text": "Properties of the Sample Variance\n\n\n\n\n\n\nKey Properties of \\(S^2\\)\n\n\n\nFor a random sample from a normal population:\n\n\\(E(S^2) = \\sigma^2\\) (unbiased for \\(\\sigma^2\\))\n\\(\\text{Var}(S^2) = \\frac{2\\sigma^4}{n-1}\\)\nAs \\(n \\to \\infty\\), \\(\\text{Var}(S^2) \\to 0\\) (consistent estimator)\n\n\n\n. . .\n\n# Demonstration of variance of S² decreasing with n\ntibble(n = c(5, 10, 25, 50, 100)) |&gt;\n  mutate(\n    var_s2 = 2 / (n - 1),  # Proportional to 2σ⁴/(n-1)\n    se_s2 = sqrt(var_s2)\n  ) |&gt;\n  mutate(across(where(is.numeric), ~round(., 4)))\n\n# A tibble: 5 × 3\n      n var_s2 se_s2\n  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1     5 0.5    0.707\n2    10 0.222  0.471\n3    25 0.0833 0.289\n4    50 0.0408 0.202\n5   100 0.0202 0.142"
  },
  {
    "objectID": "lessons/04_MVUE/04_MVUE.html#consistency-of-estimators-1",
    "href": "lessons/04_MVUE/04_MVUE.html#consistency-of-estimators-1",
    "title": "BSTA 551: Statistical Inference",
    "section": "Consistency of Estimators",
    "text": "Consistency of Estimators\n\n\n\n\n\n\nDefinition\n\n\n\nAn estimator \\(\\hat{\\theta}\\) is consistent for \\(\\theta\\) if:\n\\[\\hat{\\theta} \\xrightarrow{P} \\theta \\text{ as } n \\to \\infty\\]\n\n\n. . .\nKey consistent estimators:\n\n\\(\\bar{X}\\) for \\(\\mu\\)\n\\(S^2\\) for \\(\\sigma^2\\)\n\n\\(\\hat{p}\\) for population proportion \\(p\\)\nSample quantiles for population quantiles"
  },
  {
    "objectID": "lessons/04_MVUE/04_MVUE.html#consistency-and-mse",
    "href": "lessons/04_MVUE/04_MVUE.html#consistency-and-mse",
    "title": "BSTA 551: Statistical Inference",
    "section": "Consistency and MSE",
    "text": "Consistency and MSE\n\n\n\n\n\n\nMSE Criterion for Consistency\n\n\n\nAn estimator \\(\\hat{\\theta}\\) is consistent if:\n\\[\\text{MSE}(\\hat{\\theta}) \\to 0 \\text{ as } n \\to \\infty\\]\n\n\n. . .\nWhy does this work? Recall that \\(\\text{MSE} = \\text{Variance} + \\text{Bias}^2\\).\nFor MSE → 0, we need BOTH:\n\n\\(\\text{Var}(\\hat{\\theta}) \\to 0\\) (estimates become precise)\n\\(\\text{Bias}(\\hat{\\theta}) \\to 0\\) (estimates become accurate)\n\n. . .\nExample: For \\(\\bar{X}\\) estimating \\(\\mu\\):\n\n\\(\\text{Bias}(\\bar{X}) = 0\\) (always unbiased)\n\\(\\text{Var}(\\bar{X}) = \\sigma^2/n \\to 0\\) as \\(n \\to \\infty\\)\nTherefore \\(\\text{MSE}(\\bar{X}) = \\sigma^2/n \\to 0\\) ✓"
  },
  {
    "objectID": "lessons/04_MVUE/04_MVUE.html#simulation-consistency-in-action",
    "href": "lessons/04_MVUE/04_MVUE.html#simulation-consistency-in-action",
    "title": "BSTA 551: Statistical Inference",
    "section": "Simulation: Consistency in Action",
    "text": "Simulation: Consistency in Action\n\ntrue_mean &lt;- 50\ntrue_sd &lt;- 10\nsample_sizes &lt;- c(10, 25, 50, 100, 250, 500)\nn_sims &lt;- 1000\n\nconsistency_demo &lt;- map_dfr(sample_sizes, function(n) {\n  tibble(\n    n = n,\n    x_bar = map_dbl(1:n_sims, ~mean(rnorm(n, true_mean, true_sd)))\n  )\n})\n\nconsistency_demo |&gt;\n  mutate(n = factor(n)) |&gt;\n  ggplot(aes(x = x_bar, fill = n)) +\n  geom_histogram(bins = 40, alpha = 0.7, color = \"white\") +\n  facet_wrap(~n, scales = \"free_y\", labeller = label_both) +\n  geom_vline(xintercept = true_mean, color = \"red\", linewidth = 1, linetype = \"dashed\") +\n  labs(title = \"Consistency: Estimator Converges as Sample Size Increases\",\n       subtitle = \"Red dashed line: true mean (μ = 50)\",\n       x = expression(bar(X)), y = \"Frequency\") +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "lessons/04_MVUE/04_MVUE.html#putting-it-all-together",
    "href": "lessons/04_MVUE/04_MVUE.html#putting-it-all-together",
    "title": "BSTA 551: Statistical Inference",
    "section": "Putting It All Together",
    "text": "Putting It All Together\nEvaluating an Estimator:\n\nUnbiased? Does \\(E(\\hat{\\theta}) = \\theta\\)?\nVariance? What is \\(\\text{Var}(\\hat{\\theta})\\)?\nMVUE? Among unbiased estimators, does it have minimum variance?\nConsistent? Does \\(\\hat{\\theta} \\to \\theta\\) as \\(n \\to \\infty\\)?\nMSE? What is \\(\\text{Var}(\\hat{\\theta}) + \\text{Bias}^2\\)?\n\n. . .\n\n\n\n\n\n\nGuidelines for Choosing Estimators\n\n\n\n\nUnbiasedness is desirable but not always essential\nLower variance/SE means more precision\nMSE provides a single criterion combining both\nAn estimator with MSE → 0 as n → ∞ is consistent"
  },
  {
    "objectID": "lessons/04_MVUE/04_MVUE.html#lesson-4-summary",
    "href": "lessons/04_MVUE/04_MVUE.html#lesson-4-summary",
    "title": "BSTA 551: Statistical Inference",
    "section": "Lesson 4 Summary",
    "text": "Lesson 4 Summary\nStatistics for Normal Samples:\n\n\\(\\bar{X}\\) and \\(S^2\\) are independent (unique to normal!)\n\\((n-1)S^2/\\sigma^2 \\sim \\chi^2_{n-1}\\)\nCan construct CIs for both \\(\\mu\\) (using t) and \\(\\sigma^2\\) (using \\(\\chi^2\\))\n\nMVUE:\n\nAmong unbiased estimators, choose the one with smallest variance\n\\(\\bar{X}\\) is the MVUE for \\(\\mu\\) when sampling from normal distribution\nThe max-based estimator is MVUE for uniform upper bound\n\nConsistency:\n\nEstimator converges to parameter as \\(n \\to \\infty\\)\nEquivalent to MSE → 0 as sample size increases"
  },
  {
    "objectID": "lessons/04_MVUE/04_MVUE.html#week-2-summary",
    "href": "lessons/04_MVUE/04_MVUE.html#week-2-summary",
    "title": "BSTA 551: Statistical Inference",
    "section": "Week 2 Summary",
    "text": "Week 2 Summary\n\n\n\nProperty\nFormula\nInterpretation\n\n\n\n\nBias\n\\(E(\\hat{\\theta}) - \\theta\\)\nSystematic error\n\n\nVariance\n\\(\\text{Var}(\\hat{\\theta})\\)\nRandom variability\n\n\nMSE\n\\(\\text{Var} + \\text{Bias}^2\\)\nTotal error\n\n\nConsistency\n\\(\\text{MSE} \\to 0\\)\nConvergence with n\n\n\n\n. . .\nKey distributions this week:\n\n\\(\\chi^2_\\nu\\): sum of squared normals, used for variance inference\n\\(t_\\nu\\): used when \\(\\sigma\\) is unknown\n\\(F_{\\nu_1, \\nu_2}\\): ratio of chi-squared, used for comparing variances"
  },
  {
    "objectID": "lessons/04_MVUE/04_MVUE.html#lesson-4-practice-problems",
    "href": "lessons/04_MVUE/04_MVUE.html#lesson-4-practice-problems",
    "title": "BSTA 551: Statistical Inference",
    "section": "Lesson 4 Practice Problems",
    "text": "Lesson 4 Practice Problems\n\n\nUsing R, verify that as df increases, \\(t_{0.025, \\nu}\\) approaches \\(z_{0.025} = 1.96\\).\nSimulate 10,000 samples of size \\(n = 15\\) from Uniform[0, 100]. Compare the variances of \\(\\hat{\\theta}_u\\) and \\(2\\bar{X}\\). Which is the MVUE?\nShow that the sample proportion \\(\\hat{p} = X/n\\) from a Binomial\\((n, p)\\) distribution is consistent by showing its MSE → 0."
  },
  {
    "objectID": "lessons/04_MVUE/04_MVUE.html#next-week-preview",
    "href": "lessons/04_MVUE/04_MVUE.html#next-week-preview",
    "title": "BSTA 551: Statistical Inference",
    "section": "Next Week Preview",
    "text": "Next Week Preview\nWeek 3: Maximum Likelihood Estimation\n\nThe likelihood function and likelihood principle\nFinding MLEs analytically and numerically\n\nMethod of moments estimation\nProperties of maximum likelihood estimators\n\n. . .\nSneak peek: The MLE often is the MVUE!"
  },
  {
    "objectID": "lessons/04_MVUE/04_MVUE.html#references",
    "href": "lessons/04_MVUE/04_MVUE.html#references",
    "title": "BSTA 551: Statistical Inference",
    "section": "References",
    "text": "References\n\n\nDevore, Berk, and Carlton. Modern Mathematical Statistics with Applications (Springer). Chapters 6.3, 6.4, 7.1\nChihara and Hesterberg. Mathematical Statistics with Resampling and R (Wiley). Chapter 6, Appendix B."
  },
  {
    "objectID": "lessons/04_MVUE/04_MVUE.html#questions",
    "href": "lessons/04_MVUE/04_MVUE.html#questions",
    "title": "BSTA 551: Statistical Inference",
    "section": "Questions?",
    "text": "Questions?\nThank you!"
  },
  {
    "objectID": "lessons/x11_Joint_distributions/Old_notes/25_Joint_densities.html",
    "href": "lessons/x11_Joint_distributions/Old_notes/25_Joint_densities.html",
    "title": "Chapter 25: Joint densities",
    "section": "",
    "text": "Solve double integrals in our mini lesson!\nCalculate probabilities for a pair of continuous random variables\nCalculate a joint and marginal probability density function (pdf)\nCalculate a joint and marginal cumulative distribution function (CDF) from a pdf"
  },
  {
    "objectID": "lessons/x11_Joint_distributions/Old_notes/25_Joint_densities.html#learning-objectives",
    "href": "lessons/x11_Joint_distributions/Old_notes/25_Joint_densities.html#learning-objectives",
    "title": "Chapter 25: Joint densities",
    "section": "",
    "text": "Solve double integrals in our mini lesson!\nCalculate probabilities for a pair of continuous random variables\nCalculate a joint and marginal probability density function (pdf)\nCalculate a joint and marginal cumulative distribution function (CDF) from a pdf"
  },
  {
    "objectID": "lessons/x11_Joint_distributions/Old_notes/25_Joint_densities.html#double-integrals-mini-lesson-13",
    "href": "lessons/x11_Joint_distributions/Old_notes/25_Joint_densities.html#double-integrals-mini-lesson-13",
    "title": "Chapter 25: Joint densities",
    "section": "Double Integrals Mini Lesson (1/3)",
    "text": "Double Integrals Mini Lesson (1/3)\n\n\n\n\n\n\nDo this problem at home for extra practice. I’ll add the solution to the annotated notes!\n\n\n\n\n\n\n\nMini Lesson Example 1\n\n\nSolve the following integral: \\(\\displaystyle\\int_{2}^{3}\\displaystyle\\int_{0}^{1} xy dydx\\)"
  },
  {
    "objectID": "lessons/x11_Joint_distributions/Old_notes/25_Joint_densities.html#double-integrals-mini-lesson-23",
    "href": "lessons/x11_Joint_distributions/Old_notes/25_Joint_densities.html#double-integrals-mini-lesson-23",
    "title": "Chapter 25: Joint densities",
    "section": "Double Integrals Mini Lesson (2/3)",
    "text": "Double Integrals Mini Lesson (2/3)\n\n\n\n\n\n\nDo this problem at home for extra practice. I’ll add the solution to the annotated notes!\n\n\n\n\n\n\n\nMini Lesson Example 2\n\n\nSolve the following integral: \\(\\displaystyle\\int_{2}^{3}\\displaystyle\\int_{0}^{1} (x+y) dydx\\)"
  },
  {
    "objectID": "lessons/x11_Joint_distributions/Old_notes/25_Joint_densities.html#double-integrals-mini-lesson-33",
    "href": "lessons/x11_Joint_distributions/Old_notes/25_Joint_densities.html#double-integrals-mini-lesson-33",
    "title": "Chapter 25: Joint densities",
    "section": "Double Integrals Mini Lesson (3/3)",
    "text": "Double Integrals Mini Lesson (3/3)\n\n\n\n\n\n\nDo this problem at home for extra practice. I’ll add the solution to the annotated notes!\n\n\n\n\n\n\n\nMini Lesson Example 3\n\n\nSolve the following integral: \\(\\displaystyle\\int_{2}^{3}\\displaystyle\\int_{0}^{1} e^{x+y} dydx\\)"
  },
  {
    "objectID": "lessons/x11_Joint_distributions/Old_notes/25_Joint_densities.html#how-to-define-the-joint-pdf-for-continuous-rvs",
    "href": "lessons/x11_Joint_distributions/Old_notes/25_Joint_densities.html#how-to-define-the-joint-pdf-for-continuous-rvs",
    "title": "Chapter 25: Joint densities",
    "section": "How to define the joint pdf for continuous RVs?",
    "text": "How to define the joint pdf for continuous RVs?\n\n\nFor a single continuous RV \\(X\\) is a function \\(f_X(x)\\), such that for all real values \\(a,b\\) with \\(a \\leq b\\), \\[\\mathbb{P}(a \\leq X \\leq b) = \\int_a^b f_X(x)dx\\]\n\nFor two continuous RVs (\\(X\\) and \\(Y\\)), we can define the joint pdf, \\(f_{X,Y}(x,y)\\), such that for all real values \\(a,b, c, d\\) with \\(a \\leq b\\) and \\(c \\leq d\\), \\[\\mathbb{P}(a \\leq X \\leq b, c \\leq Y \\leq d) = \\int_a^b \\int_c^d f_{X,Y}(x,y)dydx\\]"
  },
  {
    "objectID": "lessons/x11_Joint_distributions/Old_notes/25_Joint_densities.html#important-properties-of-the-joint-pdf",
    "href": "lessons/x11_Joint_distributions/Old_notes/25_Joint_densities.html#important-properties-of-the-joint-pdf",
    "title": "Chapter 25: Joint densities",
    "section": "Important properties of the joint pdf",
    "text": "Important properties of the joint pdf\n\nNote that \\(f_{X,Y}(x,y)\\neq \\mathbb{P}(X=x, Y=y)\\)!!!\nIn order for \\(f_{X,Y}(x,y)\\) to be a pdf, it needs to satisfy the properties\n\n\\(f_{X,Y}(x,y)\\geq 0\\) for all \\(x,y\\)\n\\(\\displaystyle\\int_{-\\infty}^{\\infty}\\displaystyle\\int_{-\\infty}^{\\infty} f_{X,Y}(x,y)dxdy=1\\)"
  },
  {
    "objectID": "lessons/x11_Joint_distributions/Old_notes/25_Joint_densities.html#what-is-the-joint-cumulative-distribution-function",
    "href": "lessons/x11_Joint_distributions/Old_notes/25_Joint_densities.html#what-is-the-joint-cumulative-distribution-function",
    "title": "Chapter 25: Joint densities",
    "section": "What is the joint cumulative distribution function?",
    "text": "What is the joint cumulative distribution function?\n\n\nDefinition: Joint cumulative distribution function (Joint CDF)\n\n\nThe joint cumulative distribution function (cdf) of continuous random variables \\(X\\) and \\(Y\\), is the function \\(F_{X,Y}(x,y)\\), such that for all real values of \\(x\\) and \\(y\\), \\[F_{X,Y}(x,y)= \\mathbb{P}(X \\leq x, Y \\leq y) = \\int_{-\\infty}^x\\int_{-\\infty}^y f_{X,Y}(s,t)dtds\\]\n\n\nRemarks:\n\nThe definition above for \\(F_{X,Y}(x,y)\\) is a function of \\(x\\) and \\(y\\).\nThe joint cdf at the point \\((a,b)\\), is \\[F_{X,Y}(a,b) = \\mathbb{P}(X \\leq a, Y \\leq b) = \\int_{-\\infty}^a\\int_{-\\infty}^b f_{X,Y}(s,t)dtds\\]"
  },
  {
    "objectID": "lessons/x11_Joint_distributions/Old_notes/25_Joint_densities.html#what-are-the-marginal-pdfs",
    "href": "lessons/x11_Joint_distributions/Old_notes/25_Joint_densities.html#what-are-the-marginal-pdfs",
    "title": "Chapter 25: Joint densities",
    "section": "What are the marginal pdf’s?",
    "text": "What are the marginal pdf’s?\n\n\nDefinition: Marginal pdf’s\n\n\nSuppose \\(X\\) and \\(Y\\) are continuous r.v.’s, with joint pdf \\(f_{X,Y}(x,y)\\). Then the marginal probability density functions are \\[\\begin{aligned}\nf_X(x)&=& \\int_{-\\infty}^{\\infty} f_{X,Y}(x,y)dy\\\\\nf_Y(y)&=& \\int_{-\\infty}^{\\infty} f_{X,Y}(x,y)dx\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/x11_Joint_distributions/Old_notes/25_Joint_densities.html#common-steps-for-solving-problems",
    "href": "lessons/x11_Joint_distributions/Old_notes/25_Joint_densities.html#common-steps-for-solving-problems",
    "title": "Chapter 25: Joint densities",
    "section": "Common steps for solving problems",
    "text": "Common steps for solving problems\n\nSet up the domain of the pdf with a picture\nTranslate to needed integrands\n\nFor probability: shade in the area of interest, then translate\nFor expected value: translate domain\n\nSet up integral: \\(dxdy\\) or \\(dydx\\)?\nSolve integral!"
  },
  {
    "objectID": "lessons/x11_Joint_distributions/Old_notes/25_Joint_densities.html#example-of-joint-pdf",
    "href": "lessons/x11_Joint_distributions/Old_notes/25_Joint_densities.html#example-of-joint-pdf",
    "title": "Chapter 25: Joint densities",
    "section": "Example of joint pdf",
    "text": "Example of joint pdf\n\n\n\n\nExample 1.1\n\n\nLet \\(f_{X,Y}(x,y)= \\frac32 y^2\\), for \\(0 \\leq x \\leq 2, \\ 0 \\leq y \\leq 1\\).\n\nFind \\(\\mathbb{P}(0 \\leq X \\leq 1, 0 \\leq Y \\leq \\frac12)\\)."
  },
  {
    "objectID": "lessons/x11_Joint_distributions/Old_notes/25_Joint_densities.html#example-of-joint-pdf-1",
    "href": "lessons/x11_Joint_distributions/Old_notes/25_Joint_densities.html#example-of-joint-pdf-1",
    "title": "Chapter 25: Joint densities",
    "section": "Example of joint pdf",
    "text": "Example of joint pdf\n\n\n\n\nExample 1.2\n\n\nLet \\(f_{X,Y}(x,y)= \\frac32 y^2\\), for \\(0 \\leq x \\leq 2, \\ 0 \\leq y \\leq 1\\).\n\nFind \\(f_X(x)\\) and \\(f_Y(y)\\)."
  },
  {
    "objectID": "lessons/x11_Joint_distributions/Old_notes/25_Joint_densities.html#example-of-a-more-complicated-joint-pdf",
    "href": "lessons/x11_Joint_distributions/Old_notes/25_Joint_densities.html#example-of-a-more-complicated-joint-pdf",
    "title": "Chapter 25: Joint densities",
    "section": "Example of a more complicated joint pdf",
    "text": "Example of a more complicated joint pdf\n\n\n\n\n\n\nDo this problem at home for extra practice. I’ll add the solution to the annotated notes!\n\n\n\n\n\n\n\nExample 2.1\n\n\nLet \\(f_{X,Y}(x,y)= 2 e^{-(x+y)}\\), for \\(0 \\leq x \\leq y\\).\n\nFind \\(f_X(x)\\) and \\(f_Y(y)\\)."
  },
  {
    "objectID": "lessons/x11_Joint_distributions/Old_notes/25_Joint_densities.html#example-of-a-more-complicated-joint-pdf-1",
    "href": "lessons/x11_Joint_distributions/Old_notes/25_Joint_densities.html#example-of-a-more-complicated-joint-pdf-1",
    "title": "Chapter 25: Joint densities",
    "section": "Example of a more complicated joint pdf",
    "text": "Example of a more complicated joint pdf\n\n\n\n\n\n\nDo this problem at home for extra practice. I’ll add the solution to the annotated notes!\n\n\n\n\n\n\n\nExample 2.2\n\n\nLet \\(f_{X,Y}(x,y)= 2 e^{-(x+y)}\\), for \\(0 \\leq x \\leq y\\).\n\nFind \\(\\mathbb{P}(Y &lt; 3)\\)."
  },
  {
    "objectID": "lessons/x11_Joint_distributions/Old_notes/25_Joint_densities.html#lets-complicate-this-even-more",
    "href": "lessons/x11_Joint_distributions/Old_notes/25_Joint_densities.html#lets-complicate-this-even-more",
    "title": "Chapter 25: Joint densities",
    "section": "Let’s complicate this even more!",
    "text": "Let’s complicate this even more!\n\n\n\n\nExample 3.1\n\n\nLet \\(X\\) and \\(Y\\) have constant density on the square \\(0 \\leq X \\leq 4, 0 \\leq Y \\leq 4\\).\n\nFind \\(\\mathbb{P}(|X-Y| &lt; 2)\\)"
  },
  {
    "objectID": "lessons/x11_Joint_distributions/Old_notes/25_Joint_densities.html#finding-the-pdf-of-a-transformation",
    "href": "lessons/x11_Joint_distributions/Old_notes/25_Joint_densities.html#finding-the-pdf-of-a-transformation",
    "title": "Chapter 25: Joint densities",
    "section": "Finding the pdf of a transformation",
    "text": "Finding the pdf of a transformation\n\nLet \\(M\\) be a transformation of \\(X\\) and \\(Y\\)\nWhen we have a transformation of \\(X\\) and \\(Y\\), \\(M\\), we need to follow a specific process to find the pdf of \\(M\\)\n\nWe follow this process:\n\nStart with the joint pdf for \\(X\\) and \\(Y\\)\n\naka \\(f_{X,Y}(x, y)\\)\n\nTranslate the domain of \\(X\\) and \\(Y\\) to \\(M\\)\nFind the CDF of \\(M\\)\n\naka \\(F_M(m)\\) or \\(P(M \\leq m)\\)\n\nTake the derivative of the CDF of \\(M\\) to find the pdf of \\(M\\)\n\naka \\(f_M(m) = \\dfrac{d}{dm}F_M(m)\\)"
  },
  {
    "objectID": "lessons/x11_Joint_distributions/Old_notes/25_Joint_densities.html#lets-complicate-this-even-more-1",
    "href": "lessons/x11_Joint_distributions/Old_notes/25_Joint_densities.html#lets-complicate-this-even-more-1",
    "title": "Chapter 25: Joint densities",
    "section": "Let’s complicate this even more!",
    "text": "Let’s complicate this even more!\n\n\n\n\nExample 3.2\n\n\nLet \\(X\\) and \\(Y\\) have constant density on the square \\(0 \\leq X \\leq 4, 0 \\leq Y \\leq 4\\).\n\nLet \\(M = \\max(X,Y)\\). Find the pdf for \\(M\\), that is \\(f_M(m)\\)."
  },
  {
    "objectID": "lessons/x11_Joint_distributions/Old_notes/25_Joint_densities.html#lets-complicate-this-even-more-2",
    "href": "lessons/x11_Joint_distributions/Old_notes/25_Joint_densities.html#lets-complicate-this-even-more-2",
    "title": "Chapter 25: Joint densities",
    "section": "Let’s complicate this even more!",
    "text": "Let’s complicate this even more!\n\n\n\n\n\n\nDo this problem at home for extra practice. I’ll add the solution to the annotated notes!\n\n\n\n\n\n\n\nExample 3.3\n\n\nLet \\(X\\) and \\(Y\\) have constant density on the square \\(0 \\leq X \\leq 4, 0 \\leq Y \\leq 4\\).\n\nLet \\(Z = \\min(X,Y)\\). Find the pdf for \\(Z\\), that is \\(f_Z(z)\\)."
  },
  {
    "objectID": "lessons/x11_Joint_distributions/Old_notes/25_Joint_densities.html#lets-complicate-this-even-further",
    "href": "lessons/x11_Joint_distributions/Old_notes/25_Joint_densities.html#lets-complicate-this-even-further",
    "title": "Chapter 25: Joint densities",
    "section": "Let’s complicate this even further!",
    "text": "Let’s complicate this even further!\n\n\n\n\n\n\nDo this problem at home for extra practice. I’ll add the solution to the annotated notes!\n\n\n\n\n\n\n\nExample 4\n\n\nLet \\(X\\) and \\(Y\\) have joint density \\(f_{X,Y}(x,y)= \\frac85(x+y)\\) in the region \\(0 &lt; x &lt; 1,\\ \\frac12 &lt; y &lt;1\\). Find the pdf of the r.v. \\(Z\\), where \\(Z=XY\\)."
  },
  {
    "objectID": "lessons/x11_Joint_distributions/Old_notes/25_Joint_densities_muddy_points.html",
    "href": "lessons/x11_Joint_distributions/Old_notes/25_Joint_densities_muddy_points.html",
    "title": "Muddy Points",
    "section": "",
    "text": "Muddy points from Fall 2023:\n\n1. How do pdf, CDF, and probability interact with each other?\nLet’s say we have a pdf, \\(f_X(x) = \\dfrac{1}{9}x^2\\) for \\(0 \\leq x \\leq 3\\). This is just a function. The pdf is not used on its own to report any probability. We must integrate over the pdf to find a probability.\n\nlibrary(\"ggplot2\")\neq = function(x){(1/9)*x^2}\nggplot(data.frame(x=c(1, 50)), aes(x=x)) + \n  stat_function(fun=eq) +\n  xlab(\"x\") + ylab(\"pdf\") +\n  xlim(0,3)\n\n\n\n\n\n\n\n\nThe total area under the pdf is 1. This makes our pdf valid.\n\neq = function(x){(1/9)*x^2}\nggplot(data.frame(x=c(1, 50)), aes(x=x)) + \n  stat_function(fun=eq) +\n  xlab(\"x\") + ylab(\"pdf\") +\n  xlim(0,3) +\n  stat_function(fun=eq, \n                xlim = c(0, 3),\n                geom = \"area\", \n                aes(fill = \"red\")) +\n  theme(legend.position = \"none\") +\n  annotate(\"text\", x = 0.5, y = 0.7, label = \"AUC = 1\", color = \"black\")\n\n\n\n\n\n\n\n\nIf we only look at a proportion of the area under the pdf, then we start constructing our probabilities. For example, we can look at probability that we have a value between 0 and 1.5.\n\nggplot(data.frame(x=c(1, 50)), aes(x=x)) + \n  stat_function(fun=eq) +\n  xlab(\"x\") + ylab(\"pdf\") +\n  xlim(0,3) +\n  stat_function(fun=eq, \n                xlim = c(0, 1.5),\n                geom = \"area\", \n                aes(fill = \"blue\")) +\n  theme(legend.position = \"none\") +\n  annotate(\"text\", x = 0.5, y = 0.7, label = \"AUC = 0.125\", color = \"black\")\n\n\n\n\n\n\n\n\nInstead of calculating the EXACT probability for each value between 0 and 3, we can find the CDF of the pdf.\nThe CDF is: \\[\nF_X(x) = \\left\\{\n        \\begin{array}{ll}\n            0 & \\quad x&lt;3 \\quad \\\\\n            \\dfrac{1}{27}x^3 & \\quad 0 \\leq x \\leq 3\\quad \\\\\n            1 & \\quad x&gt;3 \\quad\n        \\end{array}\n    \\right.\n\\]\n\ncdf = function(x){(1/27)*x^3}\nggplot(data.frame(x=c(1, 50)), aes(x=x)) + \n  stat_function(fun=cdf) +\n  xlab(\"x\") + ylab(\"CDF\") +\n  xlim(0,3) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nWhen \\(x=1.5\\), we can calculate the probability using the CDF. Remember that \\(F_X(x) = P(X \\leq x)\\). So we can say \\(P(X \\leq 1.5) = F_X(1.5) = \\dfrac{1}{27}(1.5)^3\\), which equals 0.125.\n\ncdf = function(x){(1/27)*x^3}\nggplot(data.frame(x=c(1, 50)), aes(x=x)) + \n  stat_function(fun=cdf) +\n  xlab(\"x\") + ylab(\"CDF\") +\n  xlim(0,3) +\n  theme(legend.position = \"none\") +\n  geom_point(aes(x=1.5, y=.125), colour=\"blue\", size=3) +\n  annotate(\"text\", x = 0.5, y = 0.7, label = \"CDF = 0.125\", color = \"black\")\n\nWarning in geom_point(aes(x = 1.5, y = 0.125), colour = \"blue\", size = 3): All aesthetics have length 1, but the data has 2 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\nWe can also calculate the probability with an integral: \\(P(X \\leq 1.5) = \\displaystyle\\int_0^{1.5} \\dfrac{1}{9}x^2 dx\\).\nWe can also find the probability that X is between two numbers. \\(P(1\\leq X \\leq 1.5) = F_X(1.5) - F_X(1)\\) or \\(P(1\\leq X \\leq 1.5) = \\displaystyle\\int_1^{1.5} \\dfrac{1}{9}x^2 dx\\).\n\n\n2. Joint vs marginal vs conditional: How are we calculating the probability?\nIf we start at a joint probability \\(f_{X,Y}(x,y)\\)…. we can look at a few probabilities:\n\nJoint probability: \\(P(a \\leq X \\leq b, c \\leq Y \\leq d)\\)\n\\[P(a \\leq X \\leq b, c \\leq Y \\leq d) = \\displaystyle\\int_{x=a}^{x=b}\\displaystyle\\int_{y=c}^{y=d} f_{X,Y}(x,y) dydx\\]\nMarginal probability: \\(P(a \\leq X \\leq b)\\)\n\\[P(a \\leq X \\leq b) = \\displaystyle\\int_{x=a}^{x=b} f_{X}(x) dx\\]\nOR\n\\[P(a \\leq X \\leq b) = \\displaystyle\\int_{x=a}^{x=b}\\displaystyle\\int_{y=-\\inf}^{y=\\inf} f_{X,Y}(x,y) dydx\\]\nConditional probability: \\(P(a \\leq X \\leq b | Y = c)\\)\n\\[P(a \\leq X \\leq b | Y=c) = \\displaystyle\\int_{x=a}^{x=b} f_{X|Y}(x|y=c) dx\\]\nYou cannot calculate \\(P(a \\leq X \\leq b | Y = c)\\) by \\(\\dfrac{P(a \\leq X \\leq b, Y=c)}{P(Y = c)}\\) because \\(P(Y = c)\\) is 0. Instead, we need to find \\(f_{X|Y}(x|y=c)\\) by \\(\\dfrac{f_{X,Y}(x,y=c)}{f_{Y}(y=c)}\\) and THEN integrate over X.\n\n\n\n3. What are we actually finding by solving the double integral. In the first example, we found the probability was 1/16 after integrating but what does 1/16 mean in relation to the random variables X and Y?\nIt means that the volume contained by \\(0\\leq X \\leq 1\\), \\(0\\leq Y \\leq 1/2\\), and their joint pdf is 1/16 of the total volume contained by \\(0\\leq X \\leq 2\\), \\(0\\leq Y \\leq 1\\), and their joint pdf. The probability for a joint pdf is now a measure of the proportion of the volume.\nThis is not be confused with a probability from marginal pdf or pdf from one RV. The probability for marginal/single RV pdfs is the proportion of the area under the pdf for a specific range of values.\n\n\n4. Here’s a 3D plot of one of our joint pdf’s\n\\[\nf_{X,Y}(x,y) = 5e^{-x-3y} \\text{ for } 0 \\leq y \\leq x/2\n\\]\n\nlibrary(plotly)\n\nx = seq(0, 5, 0.1)\ny = seq(0, max(x)/2, 0.1/2)\nfn = expand.grid(x=x,y=y)\nfn$z = ifelse(fn$y&lt;fn$x/2, 5*exp( (-1)*fn$x - 3*fn$y), NA)\n\nz = matrix(fn$z, ncol = 51, nrow = 51, byrow = T)\n\nfig &lt;- plot_ly(x = x, y=y, z=z) %&gt;% add_surface()\n\nfig"
  },
  {
    "objectID": "lessons/x11_Joint_distributions/11_Joint_distributions_key_info.html",
    "href": "lessons/x11_Joint_distributions/11_Joint_distributions_key_info.html",
    "title": "Key Info",
    "section": "",
    "text": "Midterm feedback!\nHomework\n\nYou NEED to show your work! I am asking Charles to give half credit if you do not show sufficient work\nIf you have an html file and images of work, please insert the image into your html!\n\nHomework 2 feedback from Charles\n\nPeople forgot to use combinations for binary distribution\nSome people didn’t get all of the combinations for question 3\nSome didn’t know how to break up the venn diagram to find each prob. Or label it correctly\nSome people inserted \\(A \\cup B\\), \\(A \\cup C\\), \\(B \\cup C\\) into the equation for \\(A \\cup B \\cup C\\) …somehow it gets the same probability for A&B&C even though that the wrong way to do it"
  },
  {
    "objectID": "lessons/x11_Joint_distributions/11_Joint_distributions_key_info.html#announcements",
    "href": "lessons/x11_Joint_distributions/11_Joint_distributions_key_info.html#announcements",
    "title": "Key Info",
    "section": "",
    "text": "Midterm feedback!\nHomework\n\nYou NEED to show your work! I am asking Charles to give half credit if you do not show sufficient work\nIf you have an html file and images of work, please insert the image into your html!\n\nHomework 2 feedback from Charles\n\nPeople forgot to use combinations for binary distribution\nSome people didn’t get all of the combinations for question 3\nSome didn’t know how to break up the venn diagram to find each prob. Or label it correctly\nSome people inserted \\(A \\cup B\\), \\(A \\cup C\\), \\(B \\cup C\\) into the equation for \\(A \\cup B \\cup C\\) …somehow it gets the same probability for A&B&C even though that the wrong way to do it"
  },
  {
    "objectID": "lessons/x11_Joint_distributions/11_Joint_distributions_key_info.html#key-dates",
    "href": "lessons/x11_Joint_distributions/11_Joint_distributions_key_info.html#key-dates",
    "title": "Key Info",
    "section": "Key Dates",
    "text": "Key Dates\n\nHW 04 due this Sunday!\nMidterm feedback due Sunday as well!"
  },
  {
    "objectID": "lessons/x11_Joint_distributions/11_Joint_distributions.html",
    "href": "lessons/x11_Joint_distributions/11_Joint_distributions.html",
    "title": "Lesson 11: Joint distributions",
    "section": "",
    "text": "Define joint and marginal distributions for discrete and continuous random variables\nCalculate or find joint and marginal probabilities, pmf’s, and CDF’s for discrete random variables\nCalculate or find joint and marginal probabilities, pdf’s, and CDF’s for continuous random variables\nExtra practice on your own: solve double integrals in a mini lesson"
  },
  {
    "objectID": "lessons/x11_Joint_distributions/11_Joint_distributions.html#what-is-a-joint-distribution",
    "href": "lessons/x11_Joint_distributions/11_Joint_distributions.html#what-is-a-joint-distribution",
    "title": "Lesson 11: Joint distributions",
    "section": "What is a joint distribution?",
    "text": "What is a joint distribution?\n\n\n\n\nDefinition: joint pmf\n\n\nThe joint pmf of a pair of discrete RV’s \\(X\\) and \\(Y\\) is \\[\\begin{aligned}\np_{X,Y}(x,y) = & \\mathbb{P}(X=x \\cap Y=y) \\\\ = & \\mathbb{P}(X=x, Y=y)\n\\end{aligned}\\]\n\n\n\n\n\nDefinition: joint pdf\n\n\nThe joint pdf for two continuous RVs (\\(X\\) and \\(Y\\)) is \\(f_{X,Y}(x,y)\\), such that we have the following joint probability: \\[\\begin{aligned}\n\\mathbb{P}(a \\leq X \\leq b, & c \\leq Y \\leq d) = \\\\ & \\int_a^b \\int_c^d f_{X,Y}(x,y)dydx\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/x11_Joint_distributions/11_Joint_distributions.html#important-properties-of-joint-distributions",
    "href": "lessons/x11_Joint_distributions/11_Joint_distributions.html#important-properties-of-joint-distributions",
    "title": "Lesson 11: Joint distributions",
    "section": "Important properties of joint distributions",
    "text": "Important properties of joint distributions\n\n\n\n\nProperties of joint pmf’s\n\n\n\nA joint pmf \\(p_{X,Y}(x,y)\\) must satisfy the following properties:\n\n\\(0 \\geq p_{X,Y}(x,y) \\leq 1\\) for all \\(x, y\\)\n\n \n\n\\(\\sum \\limits_{\\{all\\ x\\}} \\sum \\limits_{\\{all\\ y\\}} p_{X,Y}(x,y)=1\\)\n\n\n\n\n\n\n\nProperties of joint pdf’s\n\n\n\nA joint pdf \\(f_{X,Y}(x,y)\\) must satisfy the following properties:\n\n\\(f_{X,Y}(x,y)\\geq 0\\) for all \\(x,y\\)\n\n \n\n\\(\\displaystyle\\int_{-\\infty}^{\\infty}\\displaystyle\\int_{-\\infty}^{\\infty} f_{X,Y}(x,y)dxdy=1\\)\n\n\n \n\nRemember that \\(f_{X,Y}(x,y)\\neq \\mathbb{P}(X=x, Y=y)\\)!!!"
  },
  {
    "objectID": "lessons/x11_Joint_distributions/11_Joint_distributions.html#marginal-distributions",
    "href": "lessons/x11_Joint_distributions/11_Joint_distributions.html#marginal-distributions",
    "title": "Lesson 11: Joint distributions",
    "section": "Marginal distributions",
    "text": "Marginal distributions\n\n\n\n\nMarginal pmf’s\n\n\nSuppose \\(X\\) and \\(Y\\) are discrete RV’s, with joint pmf \\(p_{X,Y}(x,y)\\). Then the marginal probability mass functions are\n\\[p_X(x) = \\sum \\limits_{\\{all\\ y\\}} p_{X,Y}(x,y)\\]\n\\[p_Y(y) = \\sum \\limits_{\\{all\\ x\\}} p_{X,Y}(x,y)\\]\n\n\n\n\n\nMarginal pdf’s\n\n\nSuppose \\(X\\) and \\(Y\\) are continuous RV’s, with joint pdf \\(f_{X,Y}(x,y)\\). Then the marginal probability density functions are \\[\\begin{aligned}\nf_X(x)&=& \\int_{-\\infty}^{\\infty} f_{X,Y}(x,y)dy\\\\\nf_Y(y)&=& \\int_{-\\infty}^{\\infty} f_{X,Y}(x,y)dx\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/x11_Joint_distributions/11_Joint_distributions.html#joint-cumulative-distribution-functions-cdfs",
    "href": "lessons/x11_Joint_distributions/11_Joint_distributions.html#joint-cumulative-distribution-functions-cdfs",
    "title": "Lesson 11: Joint distributions",
    "section": "Joint cumulative distribution functions (CDFs)",
    "text": "Joint cumulative distribution functions (CDFs)\n\n\n\n\nJoint CDF for discrete RVs\n\n\nThe joint CDF of a pair of discrete RV’s \\(X\\) and \\(Y\\) is \\[\\begin{aligned}\nF_{X,Y}(x,y) = &\\mathbb{P}(X \\leq x\\ and\\ Y \\leq y) \\\\ = &\\mathbb{P}(X \\leq x, Y \\leq y)\n\\end{aligned}\\]\n\n\n\n\n\nJoint CDF for continuous RVs\n\n\nThe joint CDF of continuous random variables \\(X\\) and \\(Y\\), is the function \\(F_{X,Y}(x,y)\\), such that for all real values of \\(x\\) and \\(y\\), \\[\\begin{aligned}\nF_{X,Y}(x,y)= \\mathbb{P}(X \\leq x, & Y \\leq y) = \\\\\n& \\int_{-\\infty}^x\\int_{-\\infty}^y f_{X,Y}(s,t)dtds\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/x11_Joint_distributions/11_Joint_distributions.html#joint-distribution-for-two-discrete-random-variables-15",
    "href": "lessons/x11_Joint_distributions/11_Joint_distributions.html#joint-distribution-for-two-discrete-random-variables-15",
    "title": "Lesson 11: Joint distributions",
    "section": "Joint distribution for two discrete random variables (1/5)",
    "text": "Joint distribution for two discrete random variables (1/5)\n\n\n\n\nExample 1\n\n\nLet \\(X\\) and \\(Y\\) be two random draws from a box containing balls labelled 1, 2, and 3 without replacement.\n\nFind \\(p_{X,Y}(x,y)\\)\nFind \\(\\mathbb{P}(X+Y=3)\\)\nFind \\(\\mathbb{P}(Y = 1)\\)\nFind \\(\\mathbb{P}(Y \\leq 2)\\)\nFind the joint CDF \\(F_{X,Y}(x,y)\\) for the joint pmf \\(p_{X,Y}(x,y)\\)\nFind the marginal CDFs \\(F_{X}(x)\\) and \\(F_{Y}(y)\\)"
  },
  {
    "objectID": "lessons/x11_Joint_distributions/11_Joint_distributions.html#joint-distribution-for-two-discrete-random-variables-25",
    "href": "lessons/x11_Joint_distributions/11_Joint_distributions.html#joint-distribution-for-two-discrete-random-variables-25",
    "title": "Lesson 11: Joint distributions",
    "section": "Joint distribution for two discrete random variables (2/5)",
    "text": "Joint distribution for two discrete random variables (2/5)\n\n\n\n\nExample 1\n\n\nLet \\(X\\) and \\(Y\\) be two random draws from a box containing balls labelled 1, 2, and 3 without replacement.\n\nFind \\(p_{X,Y}(x,y)\\)\nFind \\(\\mathbb{P}(X+Y=3)\\)"
  },
  {
    "objectID": "lessons/x11_Joint_distributions/11_Joint_distributions.html#joint-distribution-for-two-discrete-random-variables-35",
    "href": "lessons/x11_Joint_distributions/11_Joint_distributions.html#joint-distribution-for-two-discrete-random-variables-35",
    "title": "Lesson 11: Joint distributions",
    "section": "Joint distribution for two discrete random variables (3/5)",
    "text": "Joint distribution for two discrete random variables (3/5)\n\n\n\n\nExample 1\n\n\nLet \\(X\\) and \\(Y\\) be two random draws from a box containing balls labelled 1, 2, and 3 without replacement.\n\nFind \\(\\mathbb{P}(Y = 1)\\)\nFind \\(\\mathbb{P}(Y \\leq 2)\\)"
  },
  {
    "objectID": "lessons/x11_Joint_distributions/11_Joint_distributions.html#joint-distribution-for-two-discrete-random-variables-45",
    "href": "lessons/x11_Joint_distributions/11_Joint_distributions.html#joint-distribution-for-two-discrete-random-variables-45",
    "title": "Lesson 11: Joint distributions",
    "section": "Joint distribution for two discrete random variables (4/5)",
    "text": "Joint distribution for two discrete random variables (4/5)\n\n\n\n\nExample 1\n\n\nLet \\(X\\) and \\(Y\\) be two random draws from a box containing balls labelled 1, 2, and 3 without replacement.\n\nFind the joint CDF \\(F_{X,Y}(x,y)\\) for the joint pmf \\(p_{X,Y}(x,y)\\)"
  },
  {
    "objectID": "lessons/x11_Joint_distributions/11_Joint_distributions.html#joint-distribution-for-two-discrete-random-variables-55",
    "href": "lessons/x11_Joint_distributions/11_Joint_distributions.html#joint-distribution-for-two-discrete-random-variables-55",
    "title": "Lesson 11: Joint distributions",
    "section": "Joint distribution for two discrete random variables (5/5)",
    "text": "Joint distribution for two discrete random variables (5/5)\n\n\n\n\nExample 1\n\n\nLet \\(X\\) and \\(Y\\) be two random draws from a box containing balls labelled 1, 2, and 3 without replacement.\n\nFind the marginal CDFs \\(F_{X}(x)\\) and \\(F_{Y}(y)\\)"
  },
  {
    "objectID": "lessons/x11_Joint_distributions/11_Joint_distributions.html#quick-remarks-on-the-joint-and-marginal-cdf",
    "href": "lessons/x11_Joint_distributions/11_Joint_distributions.html#quick-remarks-on-the-joint-and-marginal-cdf",
    "title": "Lesson 11: Joint distributions",
    "section": "Quick remarks on the joint and marginal CDF",
    "text": "Quick remarks on the joint and marginal CDF\n\n\\(F_X(x)\\): right most columns of the CDF table (where the \\(Y\\) values are largest)\n\\(F_Y(y)\\): bottom row of the table (where X values are largest)\n\\(F_X(x)=\\lim\\limits_{y\\rightarrow\\infty}F_{X, Y}(x,y)\\)\n\\(F_Y(y)=\\lim\\limits_{x\\rightarrow\\infty}F_{X, Y}(x,y)\\)"
  },
  {
    "objectID": "lessons/x11_Joint_distributions/11_Joint_distributions.html#common-steps-for-joint-pdfs-and-cdfs",
    "href": "lessons/x11_Joint_distributions/11_Joint_distributions.html#common-steps-for-joint-pdfs-and-cdfs",
    "title": "Lesson 11: Joint distributions",
    "section": "Common steps for joint pdfs and CDFs",
    "text": "Common steps for joint pdfs and CDFs\n\nSet up the domain of the pdf with a picture\n\n \n\nTranslate to needed integrands\n\nFor probability: shade in the area of interest, then translate\nFor expected value: translate domain\n\n\n \n\nSet up integral: \\(dxdy\\) or \\(dydx\\)?\n\n \n\nSolve integral!"
  },
  {
    "objectID": "lessons/x11_Joint_distributions/11_Joint_distributions.html#example-2-joint-pdf-12",
    "href": "lessons/x11_Joint_distributions/11_Joint_distributions.html#example-2-joint-pdf-12",
    "title": "Lesson 11: Joint distributions",
    "section": "Example 2: Joint pdf (1/2)",
    "text": "Example 2: Joint pdf (1/2)\n\n\n\n\nExample 2.1\n\n\nLet \\(f_{X,Y}(x,y)= \\frac32 y^2\\), for \\(0 \\leq x \\leq 2, \\ 0 \\leq y \\leq 1\\).\n\nFind \\(\\mathbb{P}(0 \\leq X \\leq 1, 0 \\leq Y \\leq \\frac12)\\)\n\n\n\n\nlibrary(tidyverse)\njoint1 = tibble(\n  x = seq(0, 2, 0.01), \n  y = seq(0, 1, 0.005)\n)\n\nx_y_plot1 = ggplot(joint1, aes(x = x, y = y)) +\n  labs(x = \"x\", y = \"y\") +\n  scale_x_continuous(breaks = c(0, 1, 2)) +\n  scale_y_continuous(breaks = c(0, 0.5, 1)) +\n  theme_bw() +\n  theme(text = element_text(size=30))\nx_y_plot1"
  },
  {
    "objectID": "lessons/x11_Joint_distributions/11_Joint_distributions.html#example-2-joint-pdf-22",
    "href": "lessons/x11_Joint_distributions/11_Joint_distributions.html#example-2-joint-pdf-22",
    "title": "Lesson 11: Joint distributions",
    "section": "Example 2: Joint pdf (2/2)",
    "text": "Example 2: Joint pdf (2/2)\n\n\n\n\nExample 2.2\n\n\nLet \\(f_{X,Y}(x,y)= \\frac32 y^2\\), for \\(0 \\leq x \\leq 2, \\ 0 \\leq y \\leq 1\\).\n\nFind \\(f_X(x)\\) and \\(f_Y(y)\\).\n\n\n\n\nx_y_plot1"
  },
  {
    "objectID": "lessons/x11_Joint_distributions/11_Joint_distributions.html#example-with-more-complicated-pdf-12",
    "href": "lessons/x11_Joint_distributions/11_Joint_distributions.html#example-with-more-complicated-pdf-12",
    "title": "Lesson 11: Joint distributions",
    "section": "Example with more complicated pdf (1/2)",
    "text": "Example with more complicated pdf (1/2)\n\n\n\n\n\n\nDo this problem at home for extra practice. I’ll add the solution to the annotated notes!\n\n\n\n\n\n\n\nExample 3.1\n\n\nLet \\(f_{X,Y}(x,y)= 2 e^{-(x+y)}\\), for \\(0 \\leq x \\leq y\\).\n\nFind \\(f_X(x)\\) and \\(f_Y(y)\\)."
  },
  {
    "objectID": "lessons/x11_Joint_distributions/11_Joint_distributions.html#example-with-more-complicated-pdf-22",
    "href": "lessons/x11_Joint_distributions/11_Joint_distributions.html#example-with-more-complicated-pdf-22",
    "title": "Lesson 11: Joint distributions",
    "section": "Example with more complicated pdf (2/2)",
    "text": "Example with more complicated pdf (2/2)\n\n\n\n\n\n\nDo this problem at home for extra practice. I’ll add the solution to the annotated notes!\n\n\n\n\n\n\n\nExample 3.2\n\n\nLet \\(f_{X,Y}(x,y)= 2 e^{-(x+y)}\\), for \\(0 \\leq x \\leq y\\).\n\nFind \\(\\mathbb{P}(Y &lt; 3)\\)."
  },
  {
    "objectID": "lessons/x11_Joint_distributions/11_Joint_distributions.html#recall-finding-the-pdf-of-a-transformation",
    "href": "lessons/x11_Joint_distributions/11_Joint_distributions.html#recall-finding-the-pdf-of-a-transformation",
    "title": "Lesson 11: Joint distributions",
    "section": "Recall: Finding the pdf of a transformation",
    "text": "Recall: Finding the pdf of a transformation\n\nLet \\(M\\) be a transformation of \\(X\\) and \\(Y\\): \\(M = g(X, Y)\\)\nWhen we have a transformation of \\(X\\) and \\(Y\\), \\(M\\), we need to follow the CDF method to find the pdf of \\(M\\)\n\nWe follow CDF method:\n\nStart with the joint pdf for \\(X\\) and \\(Y\\)\n\naka \\(f_{X,Y}(x, y)\\)\n\nTranslate the domain of \\(X\\) and \\(Y\\) to \\(M\\): find possible values of \\(M\\)\nFind the CDF of \\(M\\)\n\naka \\(F_M(m) = P(M \\leq m) = P(g(X, Y) \\leq m)\\)\n\nTake the derivative of the CDF of \\(M\\) with respect to \\(m\\) to find the pdf of \\(M\\)\n\naka \\(f_M(m) = \\dfrac{d}{dm}F_M(m)\\)"
  },
  {
    "objectID": "lessons/x11_Joint_distributions/11_Joint_distributions.html#example-of-a-joint-pdf-with-a-transformation-13",
    "href": "lessons/x11_Joint_distributions/11_Joint_distributions.html#example-of-a-joint-pdf-with-a-transformation-13",
    "title": "Lesson 11: Joint distributions",
    "section": "Example of a joint pdf with a transformation (1/3)",
    "text": "Example of a joint pdf with a transformation (1/3)\n\n\n\n\nExample 4.1\n\n\nLet \\(X\\) and \\(Y\\) have constant density on the square \\(0 \\leq X \\leq 4, 0 \\leq Y \\leq 4\\).\n\nFind \\(\\mathbb{P}(|X-Y| &lt; 2)\\)\n\n\n\n\nlibrary(tidyverse)\njoint2 = tibble(\n  x = seq(0, 4, 0.5), \n  y = seq(0, 4, 0.5)\n)\n\nx_y_plot2 = ggplot(joint2, aes(x = x, y = y)) +\n  labs(x = \"x\", y = \"y\") +\n  scale_x_continuous(breaks = c(0, 2, 4)) +\n  scale_y_continuous(breaks = c(0, 2, 4)) +\n  theme_bw() +\n  theme(text = element_text(size=30))\nx_y_plot2"
  },
  {
    "objectID": "lessons/x11_Joint_distributions/11_Joint_distributions.html#example-of-a-joint-pdf-with-a-transformation-23",
    "href": "lessons/x11_Joint_distributions/11_Joint_distributions.html#example-of-a-joint-pdf-with-a-transformation-23",
    "title": "Lesson 11: Joint distributions",
    "section": "Example of a joint pdf with a transformation (2/3)",
    "text": "Example of a joint pdf with a transformation (2/3)\n\n\n\n\nExample 4.2\n\n\nLet \\(X\\) and \\(Y\\) have constant density on the square \\(0 \\leq X \\leq 4, 0 \\leq Y \\leq 4\\).\n\nLet \\(M = \\max(X,Y)\\). Find the pdf for \\(M\\), that is \\(f_M(m)\\)\n\n\n\n\nx_y_plot2"
  },
  {
    "objectID": "lessons/x11_Joint_distributions/11_Joint_distributions.html#example-of-a-joint-pdf-with-a-transformation-33",
    "href": "lessons/x11_Joint_distributions/11_Joint_distributions.html#example-of-a-joint-pdf-with-a-transformation-33",
    "title": "Lesson 11: Joint distributions",
    "section": "Example of a joint pdf with a transformation (3/3)",
    "text": "Example of a joint pdf with a transformation (3/3)\n\n\n\n\nExample 4.3\n\n\nLet \\(X\\) and \\(Y\\) have constant density on the square \\(0 \\leq X \\leq 4, 0 \\leq Y \\leq 4\\).\n\nLet \\(Z = \\min(X,Y)\\). Find the pdf for \\(Z\\), that is \\(f_Z(z)\\).\n\n\n\n\nx_y_plot2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDo this problem at home for extra practice. I’ll add the solution to the annotated notes!"
  },
  {
    "objectID": "lessons/x11_Joint_distributions/11_Joint_distributions.html#last-example-for-home-more-complicated-transformation",
    "href": "lessons/x11_Joint_distributions/11_Joint_distributions.html#last-example-for-home-more-complicated-transformation",
    "title": "Lesson 11: Joint distributions",
    "section": "Last example for home: more complicated transformation",
    "text": "Last example for home: more complicated transformation\n\n\n\n\nExample 5\n\n\nLet \\(X\\) and \\(Y\\) have joint density \\(f_{X,Y}(x,y)= \\frac85(x+y)\\) in the region \\(0 &lt; x &lt; 1,\\ \\frac12 &lt; y &lt;1\\). Find the pdf of the RV \\(Z\\), where \\(Z=XY\\).\n\n\n\njoint3 = tibble(\n  x = seq(0, 1, 0.5), \n  y = seq(0, 1, 0.5)\n)\n\nx_y_plot3 = ggplot(joint3, aes(x = x, y = y)) +\n  labs(x = \"x\", y = \"y\") +\n  scale_x_continuous(breaks = c(0, 0.5, 1)) +\n  scale_y_continuous(breaks = c(0, 0.5, 1)) +\n  theme_bw() +\n  theme(text = element_text(size=30))\nx_y_plot3"
  },
  {
    "objectID": "lessons/x11_Joint_distributions/11_Joint_distributions.html#double-integrals-mini-lesson-13",
    "href": "lessons/x11_Joint_distributions/11_Joint_distributions.html#double-integrals-mini-lesson-13",
    "title": "Lesson 11: Joint distributions",
    "section": "Double Integrals Mini Lesson (1/3)",
    "text": "Double Integrals Mini Lesson (1/3)\n\n\n\n\n\n\nDo this problem at home for extra practice. I’ll add the solution to the annotated notes!\n\n\n\n\n\n\n\nMini Lesson Example 1\n\n\nSolve the following integral: \\(\\displaystyle\\int_{2}^{3}\\displaystyle\\int_{0}^{1} xy dydx\\)"
  },
  {
    "objectID": "lessons/x11_Joint_distributions/11_Joint_distributions.html#double-integrals-mini-lesson-23",
    "href": "lessons/x11_Joint_distributions/11_Joint_distributions.html#double-integrals-mini-lesson-23",
    "title": "Lesson 11: Joint distributions",
    "section": "Double Integrals Mini Lesson (2/3)",
    "text": "Double Integrals Mini Lesson (2/3)\n\n\n\n\n\n\nDo this problem at home for extra practice. I’ll add the solution to the annotated notes!\n\n\n\n\n\n\n\nMini Lesson Example 2\n\n\nSolve the following integral: \\(\\displaystyle\\int_{2}^{3}\\displaystyle\\int_{0}^{1} (x+y) dydx\\)"
  },
  {
    "objectID": "lessons/x11_Joint_distributions/11_Joint_distributions.html#double-integrals-mini-lesson-33",
    "href": "lessons/x11_Joint_distributions/11_Joint_distributions.html#double-integrals-mini-lesson-33",
    "title": "Lesson 11: Joint distributions",
    "section": "Double Integrals Mini Lesson (3/3)",
    "text": "Double Integrals Mini Lesson (3/3)\n\n\n\n\n\n\nDo this problem at home for extra practice. I’ll add the solution to the annotated notes!\n\n\n\n\n\n\n\nMini Lesson Example 3\n\n\nSolve the following integral: \\(\\displaystyle\\int_{2}^{3}\\displaystyle\\int_{0}^{1} e^{x+y} dydx\\)"
  },
  {
    "objectID": "lessons/x11_Joint_distributions/11_Joint_distributions_review.html",
    "href": "lessons/x11_Joint_distributions/11_Joint_distributions_review.html",
    "title": "Lesson 11: Joint distributions",
    "section": "",
    "text": "Let \\(M\\) be a transformation of \\(X\\) and \\(Y\\): \\(M = g(X, Y)\\)\nWhen we have a transformation of \\(X\\) and \\(Y\\), \\(M\\), we need to follow the CDF method to find the pdf of \\(M\\)\n\nWe follow CDF method:\n\nStart with the joint pdf for \\(X\\) and \\(Y\\)\n\naka \\(f_{X,Y}(x, y)\\)\n\nTranslate the domain of \\(X\\) and \\(Y\\) to \\(M\\): find possible values of \\(M\\)\nFind the CDF of \\(M\\)\n\naka \\(F_M(m) = P(M \\leq m) = P(g(X, Y) \\leq m)\\)\n\nTake the derivative of the CDF of \\(M\\) with respect to \\(m\\) to find the pdf of \\(M\\)\n\naka \\(f_M(m) = \\dfrac{d}{dm}F_M(m)\\)"
  },
  {
    "objectID": "lessons/x11_Joint_distributions/11_Joint_distributions_review.html#recall-finding-the-pdf-of-a-transformation",
    "href": "lessons/x11_Joint_distributions/11_Joint_distributions_review.html#recall-finding-the-pdf-of-a-transformation",
    "title": "Lesson 11: Joint distributions",
    "section": "",
    "text": "Let \\(M\\) be a transformation of \\(X\\) and \\(Y\\): \\(M = g(X, Y)\\)\nWhen we have a transformation of \\(X\\) and \\(Y\\), \\(M\\), we need to follow the CDF method to find the pdf of \\(M\\)\n\nWe follow CDF method:\n\nStart with the joint pdf for \\(X\\) and \\(Y\\)\n\naka \\(f_{X,Y}(x, y)\\)\n\nTranslate the domain of \\(X\\) and \\(Y\\) to \\(M\\): find possible values of \\(M\\)\nFind the CDF of \\(M\\)\n\naka \\(F_M(m) = P(M \\leq m) = P(g(X, Y) \\leq m)\\)\n\nTake the derivative of the CDF of \\(M\\) with respect to \\(m\\) to find the pdf of \\(M\\)\n\naka \\(f_M(m) = \\dfrac{d}{dm}F_M(m)\\)"
  },
  {
    "objectID": "lessons/x11_Joint_distributions/11_Joint_distributions_review.html#example-of-a-joint-pdf-with-a-transformation-13",
    "href": "lessons/x11_Joint_distributions/11_Joint_distributions_review.html#example-of-a-joint-pdf-with-a-transformation-13",
    "title": "Lesson 11: Joint distributions",
    "section": "Example of a joint pdf with a transformation (1/3)",
    "text": "Example of a joint pdf with a transformation (1/3)\n\n\n\n\nExample 4.1\n\n\nLet \\(X\\) and \\(Y\\) have constant density on the square \\(0 \\leq X \\leq 4, 0 \\leq Y \\leq 4\\).\n\nFind \\(\\mathbb{P}(|X-Y| &lt; 2)\\)"
  },
  {
    "objectID": "lessons/x11_Joint_distributions/11_Joint_distributions_review.html#example-of-a-joint-pdf-with-a-transformation-23",
    "href": "lessons/x11_Joint_distributions/11_Joint_distributions_review.html#example-of-a-joint-pdf-with-a-transformation-23",
    "title": "Lesson 11: Joint distributions",
    "section": "Example of a joint pdf with a transformation (2/3)",
    "text": "Example of a joint pdf with a transformation (2/3)\n\n\nExample 4.2\n\n\nLet \\(X\\) and \\(Y\\) have constant density on the square \\(0 \\leq X \\leq 4, 0 \\leq Y \\leq 4\\).\n\nLet \\(M = \\max(X,Y)\\). Find the pdf for \\(M\\), that is \\(f_M(m)\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\nf_{X,Y}(x,y) = \\dfrac{1}{16} \\text{ for } 0 \\leq x \\leq 4, 0 \\leq y \\leq 4\n\\]\nPlot of \\(f_{X,Y}(x,y)\\):\n\n\n\n\n\n\nFor first problem, we want:\n\n\n\n\n\n\nNow I want to show the domain for the max, M:"
  },
  {
    "objectID": "lessons/x11_Joint_distributions/11_Joint_distributions_review.html#example-of-a-joint-pdf-with-a-transformation-33",
    "href": "lessons/x11_Joint_distributions/11_Joint_distributions_review.html#example-of-a-joint-pdf-with-a-transformation-33",
    "title": "Lesson 11: Joint distributions",
    "section": "Example of a joint pdf with a transformation (3/3)",
    "text": "Example of a joint pdf with a transformation (3/3)\n\n\n\n\nExample 4.3\n\n\nLet \\(X\\) and \\(Y\\) have constant density on the square \\(0 \\leq X \\leq 4, 0 \\leq Y \\leq 4\\).\n\nLet \\(Z = \\min(X,Y)\\). Find the pdf for \\(Z\\), that is \\(f_Z(z)\\)."
  },
  {
    "objectID": "lessons/x11_Joint_distributions/11_Joint_distributions_review.html#last-example-for-home-more-complicated-transformation",
    "href": "lessons/x11_Joint_distributions/11_Joint_distributions_review.html#last-example-for-home-more-complicated-transformation",
    "title": "Lesson 11: Joint distributions",
    "section": "Last example for home: more complicated transformation",
    "text": "Last example for home: more complicated transformation\n\n\n\n\nExample 5\n\n\nLet \\(X\\) and \\(Y\\) have joint density \\(f_{X,Y}(x,y)= \\frac85(x+y)\\) in the region \\(0 &lt; x &lt; 1,\\ \\frac12 &lt; y &lt;1\\). Find the pdf of the RV \\(Z\\), where \\(Z=XY\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can look at the joint pdf of X and Y:\n\n\n\n\n\n\nAnd let’s look at \\(P(Z \\leq 0.5)\\) aka \\(P(XY \\leq 0.5)\\) to get a better sense of the volume needed:\n\n\n\n\n\n\nNow I want to show the domain for Z:\n\n\n\n\n\n\nNow I want to show the domain for Z:"
  },
  {
    "objectID": "lessons/x03_Lang_prob/03_Lang_prob.html",
    "href": "lessons/x03_Lang_prob/03_Lang_prob.html",
    "title": "Lesson 3: Language of Probability",
    "section": "",
    "text": "Use set notation, Venn diagrams, and the concepts of unions, intersections, complements, and mutually exclusive events to represent and describe events.\nApply the axioms of probability and related properties to calculate probabilities and prove simple results.\nExplain and use De Morgan’s Laws to simplify and solve probability problems.\nConnect partitions and all rules of probability to calculate probabilities."
  },
  {
    "objectID": "lessons/x03_Lang_prob/03_Lang_prob.html#set-theory-12",
    "href": "lessons/x03_Lang_prob/03_Lang_prob.html#set-theory-12",
    "title": "Lesson 3: Language of Probability",
    "section": "Set Theory (1/2)",
    "text": "Set Theory (1/2)\n\n\n \n\n\nDefinition: Union\n\n\nThe union of events \\(A\\) and \\(B\\), denoted by \\(A \\cup B\\), contains all outcomes that are in \\(A\\) or \\(B\\) or both\n\n\n\n\nDefinition: Intersection\n\n\nThe intersection of events \\(A\\) and \\(B\\), denoted by \\(A \\cap B\\), contains all outcomes that are both in \\(A\\) and \\(B\\).\n\n\n\nVenn diagrams"
  },
  {
    "objectID": "lessons/x03_Lang_prob/03_Lang_prob.html#set-theory-22",
    "href": "lessons/x03_Lang_prob/03_Lang_prob.html#set-theory-22",
    "title": "Lesson 3: Language of Probability",
    "section": "Set Theory (2/2)",
    "text": "Set Theory (2/2)\n\n\n \n\n\nDefinition: Complement\n\n\nThe complement of event \\(A\\), denoted by \\(A^C\\) or \\(A'\\), contains all outcomes in the sample space \\(S\\) that are not in \\(A\\) .\n\n\n\n\nDefinition: Mutually Exclusive\n\n\nEvents \\(A\\) and \\(B\\) are mutually exclusive, or disjoint, if they have no outcomes in common. In this case \\(A \\cap B = \\emptyset\\), where \\(\\emptyset\\) is the empty set.\n\n\n\nVenn diagrams"
  },
  {
    "objectID": "lessons/x03_Lang_prob/03_Lang_prob.html#how-can-we-code-some-of-these-12",
    "href": "lessons/x03_Lang_prob/03_Lang_prob.html#how-can-we-code-some-of-these-12",
    "title": "Lesson 3: Language of Probability",
    "section": "How can we code some of these? (1/2)",
    "text": "How can we code some of these? (1/2)\n\n\nExample: Simulating Two Rolls of a Fair Four-Sided Die\n\n\nWe’re going to roll two four-sided dice. This time, let’s say event A is rolling matching numbers and event B is rolling at least one 2.\n\n\n\nFirst, we simulate rolling two four-sided dice 10,000 times\n\n\nset.seed(1002)\nrolls = replicate(10000, sample(x = 1:4, size = 2, replace = TRUE))\n\n\nNow, we can create logical vectors for events A and B\n\n\nevent_A = ( rolls[1, ] == rolls[2, ] )\nhead(event_A, 10)\n\n [1] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE\n\nevent_B = ( rolls[1, ] == 2 | rolls[2, ] == 2 )\nhead(event_B, 10)\n\n [1] FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE FALSE"
  },
  {
    "objectID": "lessons/x03_Lang_prob/03_Lang_prob.html#how-can-we-code-some-of-these-22",
    "href": "lessons/x03_Lang_prob/03_Lang_prob.html#how-can-we-code-some-of-these-22",
    "title": "Lesson 3: Language of Probability",
    "section": "How can we code some of these? (2/2)",
    "text": "How can we code some of these? (2/2)\n\n\n\n\nUnion\n\n\n\\(A \\cup B\\)\nA | B\n\nevent_A_or_B = event_A | event_B\nhead(event_A_or_B, 10)\n\n [1] FALSE FALSE FALSE  TRUE FALSE  TRUE  TRUE  TRUE FALSE FALSE\n\n\n\n\n\n\n\nIntersection\n\n\n\\(A \\cap B\\) A & B\n\nevent_A_and_B = event_A & event_B\nhead(event_A_and_B, 10)\n\n [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\n\n\n\n\n\n\nComplement\n\n\n\\(A^c\\) or \\(A'\\) != A\n\nevent_not_A = !event_A\nevent_not_B = event_B != TRUE\nhead(event_not_A, 10)\n\n [1]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE\n\n\n\n\n\n\n\nMutually Exclusive\n\n\n\\(A \\cap B = \\emptyset\\) A & B == NA\n\nsum(event_A_and_B == TRUE)\n\n[1] 621"
  },
  {
    "objectID": "lessons/x03_Lang_prob/03_Lang_prob.html#probability-axioms",
    "href": "lessons/x03_Lang_prob/03_Lang_prob.html#probability-axioms",
    "title": "Lesson 3: Language of Probability",
    "section": "Probability Axioms",
    "text": "Probability Axioms\n\n\n\n\nAxiom 1\n\n\nFor every event \\(A\\), \\(0\\leq\\mathbb{P}(A)\\leq 1\\). Probability is between 0 and 1.\n\n\n\n\nAxiom 2\n\n\nFor the sample space \\(S\\), \\(\\mathbb{P}(S)=1\\).\n\n\n\n\nAxiom 3\n\n\nIf \\(A_1, A_2, A_3, \\ldots\\), is a collection of disjoint events, then \\[\\mathbb{P}\\Big( \\bigcup \\limits_{i=1}^{\\infty}A_i\\Big) =  \\sum_{i=1}^{\\infty}\\mathbb{P}(A_i).\\] The probability of at least one \\(A_i\\) is the sum of the individual probabilities of each."
  },
  {
    "objectID": "lessons/x03_Lang_prob/03_Lang_prob.html#some-probability-properties",
    "href": "lessons/x03_Lang_prob/03_Lang_prob.html#some-probability-properties",
    "title": "Lesson 3: Language of Probability",
    "section": "Some probability properties",
    "text": "Some probability properties\nUsing the Axioms, we can prove all other probability properties! Events A, B, and C are not necessarily disjoint!\n\n\n\n\nProposition 1\n\n\nFor any event \\(A\\), \\(\\mathbb{P}(A)= 1 - \\mathbb{P}(A^C)\\)\n\n\n\n\nProposition 2\n\n\n\\(\\mathbb{P}(\\emptyset)=0\\)\n\n\n\n\nProposition 3\n\n\nIf \\(A \\subseteq B\\), then \\(\\mathbb{P}(A) \\leq \\mathbb{P}(B)\\)\n\n\n\n\n\nProposition 4\n\n\n\\[\\mathbb{P}(A \\cup B) = \\mathbb{P}(A) + \\mathbb{P}(B) - \\mathbb{P}(A \\cap B)\\] where \\(A\\) and \\(B\\) are not necessarily disjoint\n\n\n\n\nProposition 5\n\n\n\\(\\begin{aligned} \\mathbb{P}(A \\cup B & \\cup C) = \\mathbb{P}(A) + \\mathbb{P}(B) + \\\\ & \\mathbb{P}(C) - \\mathbb{P}(A \\cap B) - \\mathbb{P}(A \\cap C) - \\\\ & \\mathbb{P}(B \\cap C) + \\mathbb{P}(A \\cap B \\cap C) \\end{aligned}\\)"
  },
  {
    "objectID": "lessons/x03_Lang_prob/03_Lang_prob.html#proposition-1-proof",
    "href": "lessons/x03_Lang_prob/03_Lang_prob.html#proposition-1-proof",
    "title": "Lesson 3: Language of Probability",
    "section": "Proposition 1 Proof",
    "text": "Proposition 1 Proof\n\n\n\n\nProposition 1\n\n\nFor any event \\(A\\), \\(\\mathbb{P}(A)= 1 - \\mathbb{P}(A^C)\\)\n\n\n\n\n\nUse Axioms!\n\n\nA1: \\(0\\leq\\mathbb{P}(A)\\leq 1\\)\nA2: \\(\\mathbb{P}(S)=1\\)\nA3: For disjoint \\(A_i\\),\n\\(\\mathbb{P}\\Big( \\bigcup \\limits_{i=1}^{\\infty}A_i\\Big) =  \\sum_{i=1}^{\\infty}\\mathbb{P}(A_i)\\)"
  },
  {
    "objectID": "lessons/x03_Lang_prob/03_Lang_prob.html#proposition-2-proof",
    "href": "lessons/x03_Lang_prob/03_Lang_prob.html#proposition-2-proof",
    "title": "Lesson 3: Language of Probability",
    "section": "Proposition 2 Proof",
    "text": "Proposition 2 Proof\n\n\n\n\nProposition 2\n\n\n\\(\\mathbb{P}(\\emptyset)=0\\)\n\n\n\n\n\nUse Axioms!\n\n\nA1: \\(0\\leq\\mathbb{P}(A)\\leq 1\\)\nA2: \\(\\mathbb{P}(S)=1\\)\nA3: For disjoint \\(A_i\\),\n\\(\\mathbb{P}\\Big( \\bigcup \\limits_{i=1}^{\\infty}A_i\\Big) =  \\sum_{i=1}^{\\infty}\\mathbb{P}(A_i)\\)"
  },
  {
    "objectID": "lessons/x03_Lang_prob/03_Lang_prob.html#proposition-3-proof",
    "href": "lessons/x03_Lang_prob/03_Lang_prob.html#proposition-3-proof",
    "title": "Lesson 3: Language of Probability",
    "section": "Proposition 3 Proof",
    "text": "Proposition 3 Proof\n\n\n\n\nProposition 3\n\n\nIf \\(A \\subseteq B\\), then \\(\\mathbb{P}(A) \\leq \\mathbb{P}(B)\\)\n\n\n\n\n\nUse Axioms!\n\n\nA1: \\(0\\leq\\mathbb{P}(A)\\leq 1\\)\nA2: \\(\\mathbb{P}(S)=1\\)\nA3: For disjoint \\(A_i\\),\n\\(\\mathbb{P}\\Big( \\bigcup \\limits_{i=1}^{\\infty}A_i\\Big) =  \\sum_{i=1}^{\\infty}\\mathbb{P}(A_i)\\)"
  },
  {
    "objectID": "lessons/x03_Lang_prob/03_Lang_prob.html#proposition-4-visual-proof",
    "href": "lessons/x03_Lang_prob/03_Lang_prob.html#proposition-4-visual-proof",
    "title": "Lesson 3: Language of Probability",
    "section": "Proposition 4 Visual Proof",
    "text": "Proposition 4 Visual Proof\n\n\nProposition 4\n\n\n\\(\\mathbb{P}(A \\cup B) = \\mathbb{P}(A) + \\mathbb{P}(B) - \\mathbb{P}(A \\cap B)\\)"
  },
  {
    "objectID": "lessons/x03_Lang_prob/03_Lang_prob.html#proposition-5-visual-proof",
    "href": "lessons/x03_Lang_prob/03_Lang_prob.html#proposition-5-visual-proof",
    "title": "Lesson 3: Language of Probability",
    "section": "Proposition 5 Visual Proof",
    "text": "Proposition 5 Visual Proof\n\n\nProposition 5\n\n\n\\(\\mathbb{P}(A \\cup B \\cup C) = \\mathbb{P}(A) + \\mathbb{P}(B) + \\mathbb{P}(C) - \\mathbb{P}(A \\cap B) - \\mathbb{P}(A \\cap C) - \\mathbb{P}(B \\cap C) + \\mathbb{P}(A \\cap B \\cap C)\\)"
  },
  {
    "objectID": "lessons/x03_Lang_prob/03_Lang_prob.html#some-final-remarks-on-these-proposition",
    "href": "lessons/x03_Lang_prob/03_Lang_prob.html#some-final-remarks-on-these-proposition",
    "title": "Lesson 3: Language of Probability",
    "section": "Some final remarks on these proposition",
    "text": "Some final remarks on these proposition\n\nNotice how we spliced events into multiple disjoint events\n\nIt is often easier to work with disjoint events\n\n\n \n\nIf we want to calculate the probability for one event, we may need to get creative with how we manipulate other events and the sample space\n\nHelps us use any incomplete information we have"
  },
  {
    "objectID": "lessons/x03_Lang_prob/03_Lang_prob.html#de-morgans-laws",
    "href": "lessons/x03_Lang_prob/03_Lang_prob.html#de-morgans-laws",
    "title": "Lesson 3: Language of Probability",
    "section": "De Morgan’s Laws",
    "text": "De Morgan’s Laws\n\n\nTheorem: De Morgan’s 1st Law\n\n\nFor a collection of events (sets) \\(A_1, A_2, A_3, \\ldots\\)\n\\[\\bigcap\\limits_{i=1}^{n}A_i^C = \\Big(\\bigcup\\limits_{i=1}^{n}A_i\\Big)^C\\]\n\n\n“all not A = \\((\\)at least one event A\\()^C\\)” or “intersection of the complements is the complement of the union”\n\n\nTheorem: De Morgan’s 2nd Law\n\n\nFor a collection of events (sets) \\(A_1, A_2, A_3, \\ldots\\)\n\\[\\bigcup\\limits_{i=1}^{n}A_i^C = \\Big(\\bigcap\\limits_{i=1}^{n}A_i\\Big)^C\\]\n\n\n“at least one event not A = \\((\\)all A\\()^C\\)” or “union of complements is complement of the intersection”"
  },
  {
    "objectID": "lessons/x03_Lang_prob/03_Lang_prob.html#bp-example-variation-13",
    "href": "lessons/x03_Lang_prob/03_Lang_prob.html#bp-example-variation-13",
    "title": "Lesson 3: Language of Probability",
    "section": "BP example variation (1/3)",
    "text": "BP example variation (1/3)\n\nSuppose you have \\(n\\) subjects in a study.\nLet \\(H_i\\) be the event that person \\(i\\) has high BP, for \\(i=1\\ldots n\\).\n\n \nUse set theory notation to denote the following events:\n\nEvent subject \\(i\\) does not have high BP\nEvent all \\(n\\) subjects have high BP\nEvent at least one subject has high BP\nEvent all of them do not have high BP\nEvent at least one subject does not have high BP"
  },
  {
    "objectID": "lessons/x03_Lang_prob/03_Lang_prob.html#bp-example-variation-23",
    "href": "lessons/x03_Lang_prob/03_Lang_prob.html#bp-example-variation-23",
    "title": "Lesson 3: Language of Probability",
    "section": "BP example variation (2/3)",
    "text": "BP example variation (2/3)\n\nSuppose you have \\(n\\) subjects in a study.\nLet \\(H_i\\) be the event that person \\(i\\) has high BP, for \\(i=1\\ldots n\\).\n\nUse set theory notation to denote the following events:\n\nEvent subject \\(i\\) does not have high BP\n \nEvent all \\(n\\) subjects have high BP\n \n \nEvent at least one subject has high BP"
  },
  {
    "objectID": "lessons/x03_Lang_prob/03_Lang_prob.html#bp-example-variation-33",
    "href": "lessons/x03_Lang_prob/03_Lang_prob.html#bp-example-variation-33",
    "title": "Lesson 3: Language of Probability",
    "section": "BP example variation (3/3)",
    "text": "BP example variation (3/3)\n\nEvent all of them do not have high BP\n \n \n \nEvent at least one subject does not have high BP"
  },
  {
    "objectID": "lessons/x03_Lang_prob/03_Lang_prob.html#remarks-on-de-morgans-laws",
    "href": "lessons/x03_Lang_prob/03_Lang_prob.html#remarks-on-de-morgans-laws",
    "title": "Lesson 3: Language of Probability",
    "section": "Remarks on De Morgan’s Laws",
    "text": "Remarks on De Morgan’s Laws\n\nThese laws also hold for infinite collections of events.\n \nDraw Venn diagrams to convince yourself that these are true!\n \nThese laws are very useful when calculating probabilities.\n\nThis is because calculating the probability of the intersection of events is often much easier than the union of events.\nThis is not obvious right now, but we will see in the coming chapters why."
  },
  {
    "objectID": "lessons/x03_Lang_prob/03_Lang_prob.html#partitions",
    "href": "lessons/x03_Lang_prob/03_Lang_prob.html#partitions",
    "title": "Lesson 3: Language of Probability",
    "section": "Partitions",
    "text": "Partitions\n\n\n\n\nDefinition: Partition\n\n\nA set of events \\(\\{A_i\\}_{i=1}^{n}\\) create a partition of \\(A\\), if\n\nthe \\(A_i\\)’s are disjoint (mutually exclusive) and\n\\(\\bigcup \\limits_{i=1}^n A_i = A\\)\n\n\n\n\n\nExample 2\n\n\n\nIf \\(A \\subset B\\), then \\(\\{A, B \\cap A^C\\}\\) is a partition of \\(B\\).\nIf \\(S = \\bigcup \\limits_{i=1}^n A_i\\), and the \\(A_i\\)’s are disjoint, then the \\(A_i\\)’s are a partition of the sample space.\n\n\n\n\n\n\n\nCreating partitions is sometimes used to help calculate probabilities, since by Axiom 3 we can add the probabilities of disjoint events."
  },
  {
    "objectID": "lessons/x03_Lang_prob/03_Lang_prob.html#weekly-medications",
    "href": "lessons/x03_Lang_prob/03_Lang_prob.html#weekly-medications",
    "title": "Lesson 3: Language of Probability",
    "section": "Weekly medications",
    "text": "Weekly medications\n\n\n\n\nExample 3\n\n\nIf a subject has an\n\n80% chance of taking their medication this week,\n70% chance of taking their medication next week, and\n10% chance of not taking their medication either week,\n\nthen find the probability of them taking their medication exactly one of the two weeks.\n\n\n\nHint: Draw a Venn diagram labelling each of the parts to find the probability."
  },
  {
    "objectID": "lessons/x03_Lang_prob/03_Lang_prob_key_info.html",
    "href": "lessons/x03_Lang_prob/03_Lang_prob_key_info.html",
    "title": "Key Info",
    "section": "",
    "text": "Q: Can we still turn in exit tickets even if its not the same day? For example I was here the first day but forgot to do the exit ticket, if I did it today would it still count towards my grade?\n\nYes! I check them for “attendance” after 7 (sometimes more) days after the class\n\nFor muddy points, it really helps if you can asked a specific question about what you found confusing\n\nI can then address it in a more targeted way"
  },
  {
    "objectID": "lessons/x03_Lang_prob/03_Lang_prob_key_info.html#announcements",
    "href": "lessons/x03_Lang_prob/03_Lang_prob_key_info.html#announcements",
    "title": "Key Info",
    "section": "",
    "text": "Q: Can we still turn in exit tickets even if its not the same day? For example I was here the first day but forgot to do the exit ticket, if I did it today would it still count towards my grade?\n\nYes! I check them for “attendance” after 7 (sometimes more) days after the class\n\nFor muddy points, it really helps if you can asked a specific question about what you found confusing\n\nI can then address it in a more targeted way"
  },
  {
    "objectID": "lessons/x03_Lang_prob/03_Lang_prob_key_info.html#key-dates",
    "href": "lessons/x03_Lang_prob/03_Lang_prob_key_info.html#key-dates",
    "title": "Key Info",
    "section": "Key Dates",
    "text": "Key Dates\n\nHW 1 Assignment due this Thursday at 11pm"
  },
  {
    "objectID": "lessons/x05_Equally_likely_outcomes/05_Equally_likely_outcomes_key_info.html",
    "href": "lessons/x05_Equally_likely_outcomes/05_Equally_likely_outcomes_key_info.html",
    "title": "Key Info and Announcements",
    "section": "",
    "text": "Thanks for being flexible on Monday!\n\nI’m sorry if this messed up your morning flow\n\nToday: need to finish lesson 4, then start lesson 5\nFire drills this week!\nNotes on homework\n\nLook at Charles’ feedback on Sakai\nSet your seed for reproducibility\nself-contained: true\nMake sure to include the full sample space\n\nIf drawing 2, then sample space is each combo of two\n\nDon’t forget to do all the steps in the simulations!"
  },
  {
    "objectID": "lessons/x05_Equally_likely_outcomes/05_Equally_likely_outcomes_key_info.html#key-info",
    "href": "lessons/x05_Equally_likely_outcomes/05_Equally_likely_outcomes_key_info.html#key-info",
    "title": "Key Info and Announcements",
    "section": "",
    "text": "Thanks for being flexible on Monday!\n\nI’m sorry if this messed up your morning flow\n\nToday: need to finish lesson 4, then start lesson 5\nFire drills this week!\nNotes on homework\n\nLook at Charles’ feedback on Sakai\nSet your seed for reproducibility\nself-contained: true\nMake sure to include the full sample space\n\nIf drawing 2, then sample space is each combo of two\n\nDon’t forget to do all the steps in the simulations!"
  },
  {
    "objectID": "lessons/x05_Equally_likely_outcomes/05_Equally_likely_outcomes_key_info.html#announcements",
    "href": "lessons/x05_Equally_likely_outcomes/05_Equally_likely_outcomes_key_info.html#announcements",
    "title": "Key Info and Announcements",
    "section": "Announcements",
    "text": "Announcements\n\nHomework 2 due this Sunday at 11pm\nQuiz 1 opens on 10/22 at 3pm"
  },
  {
    "objectID": "lessons/x12_Independence_Conditioning/12_Independence_conditioning_key_info.html",
    "href": "lessons/x12_Independence_Conditioning/12_Independence_conditioning_key_info.html",
    "title": "Key Info",
    "section": "",
    "text": "Did everyone turn in their midquarter feedback?\nQuiz 2 opens on Wednesday\n\nLessons 7-9: pmfs, pdfs, and CDFs\n\nFinish up example from last class"
  },
  {
    "objectID": "lessons/x12_Independence_Conditioning/12_Independence_conditioning_key_info.html#announcements",
    "href": "lessons/x12_Independence_Conditioning/12_Independence_conditioning_key_info.html#announcements",
    "title": "Key Info",
    "section": "",
    "text": "Did everyone turn in their midquarter feedback?\nQuiz 2 opens on Wednesday\n\nLessons 7-9: pmfs, pdfs, and CDFs\n\nFinish up example from last class"
  },
  {
    "objectID": "lessons/x12_Independence_Conditioning/12_Independence_conditioning_key_info.html#key-dates",
    "href": "lessons/x12_Independence_Conditioning/12_Independence_conditioning_key_info.html#key-dates",
    "title": "Key Info",
    "section": "Key Dates",
    "text": "Key Dates\n\nHomework 5 due this Sunday\nQuiz 2 due this Sunday\n\nOpens Wednesday!"
  },
  {
    "objectID": "lessons/x12_Independence_Conditioning/12_Independence_conditioning.html",
    "href": "lessons/x12_Independence_Conditioning/12_Independence_conditioning.html",
    "title": "Lesson 12: Independence and Conditioning",
    "section": "",
    "text": "Identify the formula for joint distributions for independent RVs and conditional distributions (PMFs/PDFs)\nFind conditional pmf from a joint pmf and check if two RVs are independent.\nConstruct a joint distribution for two independent continuous RVs from their marginal distributions.\nCalculate conditional probabilities and distributions for continuous RVs."
  },
  {
    "objectID": "lessons/x12_Independence_Conditioning/12_Independence_conditioning.html#how-do-we-represent-conditional-pmfspdfs",
    "href": "lessons/x12_Independence_Conditioning/12_Independence_conditioning.html#how-do-we-represent-conditional-pmfspdfs",
    "title": "Lesson 12: Independence and Conditioning",
    "section": "How do we represent conditional pmfs/pdfs?",
    "text": "How do we represent conditional pmfs/pdfs?\nFor events:\n\\[P(A | B) = \\dfrac{P(A \\cap B)}{P(B)}\\]\n\n\n\n\nFor discrete RVs:\n\n\n\\[p_{X|Y}(x|y) = P(X=x|Y=y) = \\dfrac{p_{X,Y}(x,y)}{p_Y(y)}\\] \\[p_{Y|X}(y|x) = P(Y=y|X=x) = \\dfrac{p_{X,Y}(x,y)}{p_X(x)}\\]\nif denominator is greater than 0 (\\(p_Y(y) &gt; 0\\) or \\(p_X(x) &gt; 0\\))\n\n\n\n\n\n\n\nFor continuous RVs:\n\n\n\\[f_{X|Y}(x|y) = \\dfrac{f_{X,Y}(x,y)}{f_Y(y)}\\] \\[f_{Y|X}(y|x) = \\dfrac{f_{X,Y}(x,y)}{f_X(x)}\\]\nif denominator is greater than 0 (\\(f_Y(y) &gt; 0\\) or \\(f_X(x) &gt; 0\\))"
  },
  {
    "objectID": "lessons/x12_Independence_Conditioning/12_Independence_conditioning.html#how-do-we-represent-independent-rvs-in-a-joint-pmfpdf",
    "href": "lessons/x12_Independence_Conditioning/12_Independence_conditioning.html#how-do-we-represent-independent-rvs-in-a-joint-pmfpdf",
    "title": "Lesson 12: Independence and Conditioning",
    "section": "How do we represent independent RVs in a joint pmf/pdf?",
    "text": "How do we represent independent RVs in a joint pmf/pdf?\nWhat do we know about independence for events?\nFor events: If \\(A \\perp B\\)\n\\[P(A \\cap B) = P(A)P(B)\\] \\[P(A|B) = P(A)\\]\n\n\n\n\nFor discrete RVs: If \\(X \\perp Y\\)\n\n\n\\[p_{X,Y}(x,y) = p_{X}(x)p_{Y}(y)\\] \\[F_{X,Y}(x,y) = F_{X}(x)F_{Y}(y)\\] \\[p_{X|Y}(x|y) = p_{X}(x)\\] \\[p_{Y|X}(y|x) = p_{Y}(y)\\]\n\n\n\n\n\n\n\nFor continuous RVs: If \\(X \\perp Y\\)\n\n\n\\[f_{X,Y}(x,y) = f_{X}(x)f_{Y}(y)\\] \\[F_{X,Y}(x,y) = F_{X}(x)F_{Y}(y)\\] \\[f_{X|Y}(x|y) = f_{X}(x)\\] \\[f_{Y|X}(y|x) = f_{Y}(y)\\]"
  },
  {
    "objectID": "lessons/x12_Independence_Conditioning/12_Independence_conditioning.html#remember-our-probability-rules-must-hold-for-these",
    "href": "lessons/x12_Independence_Conditioning/12_Independence_conditioning.html#remember-our-probability-rules-must-hold-for-these",
    "title": "Lesson 12: Independence and Conditioning",
    "section": "Remember: our probability rules must hold for these!",
    "text": "Remember: our probability rules must hold for these!\n\n\n\n\nFor discrete RVs\n\n\nFor a valid joint pmf, we need:\n\n\\(0 \\geq p_{X,Y}(x,y) \\leq 1\\) for all \\(x, y\\)\n\\(\\sum \\limits_{\\{all\\ x\\}} \\sum \\limits_{\\{all\\ y\\}} p_{X,Y}(x,y)=1\\)\n\nFor a valid conditional pmf, we need:\n\n\\(0 \\geq p_{X|Y}(x|y) \\leq 1\\) for all \\(x, y\\)\n\\(\\sum \\limits_{\\{all\\ x\\}} p_{X|Y}(x|y)=1\\)\n\n\n\n\n\n\n\n\nFor continuous RVs\n\n\nFor a valid joint pdf, we need:\n\n\\(f_{X,Y}(x,y)\\geq 0\\) for all \\(x,y\\)\n\\(\\displaystyle\\int_{-\\infty}^{\\infty}\\displaystyle\\int_{-\\infty}^{\\infty} f_{X,Y}(x,y)dxdy=1\\)\n\nFor a valid conditional pdf, we need:\n\n\\(f_{X|Y}(x|y)\\geq 0\\) for all \\(x\\) and \\(y\\)\n\\(\\displaystyle\\int_{-\\infty}^{\\infty} f_{X|Y}(x|y)dx =1\\)"
  },
  {
    "objectID": "lessons/x12_Independence_Conditioning/12_Independence_conditioning.html#extra-notes",
    "href": "lessons/x12_Independence_Conditioning/12_Independence_conditioning.html#extra-notes",
    "title": "Lesson 12: Independence and Conditioning",
    "section": "Extra notes",
    "text": "Extra notes\n\nIf \\(X_1, X_2, …, X_n\\) are independent\n\n\\[p_{X_1, X_2, …, X_n}(x_1, x_2, …, x_n) = P(X_1=x_1, X_2=x_2, …, X_n=x_n)=\\prod\\limits_{i=1}^np_{X_i}(x_i)\\] \\[f_{X_1, X_2, …, X_n}(x_1, x_2, …, x_n) = \\prod\\limits_{i=1}^nf_{X_i}(x_i)\\] \\[F_{X_1, X_2, …, X_n}(x_1, x_2, …, x_n) = P(X_1\\leq x_1, X_2\\leq x_2, …, X_n\\leq x_n)=\\prod\\limits_{i=1}^nP(X_i \\leq x_i) = \\prod\\limits_{i=1}^nF_{X_i}(x_i)\\]\n\nDon’t forget, you can manipulate the conditional density to get the joint: \\[f_{X,Y}(x,y)= f_{X|Y}(x|y)f_Y(y)\\]"
  },
  {
    "objectID": "lessons/x12_Independence_Conditioning/12_Independence_conditioning.html#last-class-joint-distribution-for-two-discrete-random-variables-12",
    "href": "lessons/x12_Independence_Conditioning/12_Independence_conditioning.html#last-class-joint-distribution-for-two-discrete-random-variables-12",
    "title": "Lesson 12: Independence and Conditioning",
    "section": "Last class: joint distribution for two discrete random variables (1/2)",
    "text": "Last class: joint distribution for two discrete random variables (1/2)\n\n\n\n\nExample 1\n\n\nLet \\(X\\) and \\(Y\\) be two random draws from a box containing balls labelled 1, 2, and 3 without replacement.\n\nFind \\(p_{X|Y}(x|y)\\)."
  },
  {
    "objectID": "lessons/x12_Independence_Conditioning/12_Independence_conditioning.html#last-class-joint-distribution-for-two-discrete-random-variables-22",
    "href": "lessons/x12_Independence_Conditioning/12_Independence_conditioning.html#last-class-joint-distribution-for-two-discrete-random-variables-22",
    "title": "Lesson 12: Independence and Conditioning",
    "section": "Last class: joint distribution for two discrete random variables (2/2)",
    "text": "Last class: joint distribution for two discrete random variables (2/2)\n\n\n\n\nExample 1\n\n\nLet \\(X\\) and \\(Y\\) be two random draws from a box containing balls labelled 1, 2, and 3 without replacement.\n\nAre \\(X\\) and \\(Y\\) independent? Why or why not?\n\n\n\n\n\nRemark:\n\nTo show that \\(X\\) and \\(Y\\) are not independent, we just need to find one counter example\nHowever, to show that they are independent, we need to verify this for all possible pairs of \\(x\\) and \\(y\\)"
  },
  {
    "objectID": "lessons/x12_Independence_Conditioning/12_Independence_conditioning.html#constructing-a-joint-pdf-from-two-independent-continuous-rvs",
    "href": "lessons/x12_Independence_Conditioning/12_Independence_conditioning.html#constructing-a-joint-pdf-from-two-independent-continuous-rvs",
    "title": "Lesson 12: Independence and Conditioning",
    "section": "Constructing a joint pdf from two independent, continuous RVs",
    "text": "Constructing a joint pdf from two independent, continuous RVs\n\n\n\n\nExample 1.1\n\n\nLet \\(X\\) and \\(Y\\) be independent r.v.’s with \\(f_X(x)= \\frac12\\), for \\(0 \\leq x \\leq 2\\) and \\(f_Y(y)= 3y^2\\), for \\(0 \\leq y \\leq 1\\).\n\nFind \\(f_{X,Y}(x,y)\\)"
  },
  {
    "objectID": "lessons/x12_Independence_Conditioning/12_Independence_conditioning.html#probability-from-joint-pdf-from-two-independent-continuous-rvs",
    "href": "lessons/x12_Independence_Conditioning/12_Independence_conditioning.html#probability-from-joint-pdf-from-two-independent-continuous-rvs",
    "title": "Lesson 12: Independence and Conditioning",
    "section": "Probability from joint pdf from two independent, continuous RVs",
    "text": "Probability from joint pdf from two independent, continuous RVs\n\n\n\n\nExample 1.2\n\n\nLet \\(X\\) and \\(Y\\) be independent r.v.’s with \\(f_X(x)= \\frac12\\), for \\(0 \\leq x \\leq 2\\) and \\(f_Y(y)= 3y^2\\), for \\(0 \\leq y \\leq 1\\).\n\nFind \\(\\mathbb{P}(0 \\leq X \\leq 1, 0 \\leq Y \\leq \\frac12)\\)"
  },
  {
    "objectID": "lessons/x12_Independence_Conditioning/12_Independence_conditioning.html#showing-independence-from-joint-pdf",
    "href": "lessons/x12_Independence_Conditioning/12_Independence_conditioning.html#showing-independence-from-joint-pdf",
    "title": "Lesson 12: Independence and Conditioning",
    "section": "Showing independence from joint pdf",
    "text": "Showing independence from joint pdf\n\n\n\n\nExample 2.1\n\n\nLet \\(f_{X,Y}(x,y)= 18 x^2 y^5\\), for \\(0 \\leq x \\leq 1, \\ 0 \\leq y \\leq 1\\).\n\nAre \\(X\\) and \\(Y\\) independent?"
  },
  {
    "objectID": "lessons/x12_Independence_Conditioning/12_Independence_conditioning.html#finding-cdf-from-two-independent-rvs",
    "href": "lessons/x12_Independence_Conditioning/12_Independence_conditioning.html#finding-cdf-from-two-independent-rvs",
    "title": "Lesson 12: Independence and Conditioning",
    "section": "Finding CDF from two independent RVs",
    "text": "Finding CDF from two independent RVs\n\n\n\n\nExample 2.2\n\n\nLet \\(f_{X,Y}(x,y)= 18 x^2 y^5\\), for \\(0 \\leq x \\leq 1, \\ 0 \\leq y \\leq 1\\).\n\nFind \\(F_{X,Y}(x,y)\\)."
  },
  {
    "objectID": "lessons/x12_Independence_Conditioning/12_Independence_conditioning.html#showing-independence-from-joint-pdf-1",
    "href": "lessons/x12_Independence_Conditioning/12_Independence_conditioning.html#showing-independence-from-joint-pdf-1",
    "title": "Lesson 12: Independence and Conditioning",
    "section": "Showing independence from joint pdf",
    "text": "Showing independence from joint pdf\n\n\n\n\n\n\nDo this problem at home for extra practice. The solution is available in Meike’s video!\n\n\n\n\n\n\n\nExample 3\n\n\nLet \\(f_{X,Y}(x,y)= 2 e^{-(x+y)}\\), for \\(0 \\leq x \\leq y\\). Are \\(X\\) and \\(Y\\) independent?"
  },
  {
    "objectID": "lessons/x12_Independence_Conditioning/12_Independence_conditioning.html#final-statement-on-independence",
    "href": "lessons/x12_Independence_Conditioning/12_Independence_conditioning.html#final-statement-on-independence",
    "title": "Lesson 12: Independence and Conditioning",
    "section": "Final statement on independence",
    "text": "Final statement on independence\n\nIf \\(f_{X,Y}(x,y)= g(x)h(y)\\), where \\(g(x)\\) and \\(h(y)\\) are pdf’s, then \\(X\\) and \\(Y\\) are independent.\n\nThe domain of the joint pdf needs to be independent as well!!\n\n\n       \n\nIf \\(F_{X,Y}(x,y)= G(x)H(y)\\), where \\(G(x)\\) and \\(H(y)\\) are cdf’s, then \\(X\\) and \\(Y\\) are independent.\n\nThe domain of the joint CDF needs to be independent as well!!\n\n\n       \n\nMake sure that:\n\n\\(X\\) domain does NOT depend on \\(Y\\)\n\\(Y\\) domain does NOT depend on \\(X\\)"
  },
  {
    "objectID": "lessons/x12_Independence_Conditioning/12_Independence_conditioning.html#example-starting-from-a-joint-pdf-first-try",
    "href": "lessons/x12_Independence_Conditioning/12_Independence_conditioning.html#example-starting-from-a-joint-pdf-first-try",
    "title": "Lesson 12: Independence and Conditioning",
    "section": "Example starting from a joint pdf: first try!",
    "text": "Example starting from a joint pdf: first try!\n\n\n\n\nExample 1.1\n\n\nLet \\(f_{X,Y}(x,y)= 5 e^{-x-3y}\\), for \\(0 &lt; y &lt; \\frac{x}{2}\\).\n\nFind \\(\\mathbb{P}(2&lt;X&lt;10|Y=4)\\)"
  },
  {
    "objectID": "lessons/x12_Independence_Conditioning/12_Independence_conditioning.html#example-starting-from-a-joint-pdf-second-try-12",
    "href": "lessons/x12_Independence_Conditioning/12_Independence_conditioning.html#example-starting-from-a-joint-pdf-second-try-12",
    "title": "Lesson 12: Independence and Conditioning",
    "section": "Example starting from a joint pdf: second try! (1/2)",
    "text": "Example starting from a joint pdf: second try! (1/2)\n\n\n\n\nExample 1.1\n\n\nLet \\(f_{X,Y}(x,y)= 5 e^{-x-3y}\\), for \\(0 &lt; y &lt; \\frac{x}{2}\\).\n\nFind \\(\\mathbb{P}(2&lt;X&lt;10|Y=4)\\)"
  },
  {
    "objectID": "lessons/x12_Independence_Conditioning/12_Independence_conditioning.html#example-starting-from-a-joint-pdf-second-try-22",
    "href": "lessons/x12_Independence_Conditioning/12_Independence_conditioning.html#example-starting-from-a-joint-pdf-second-try-22",
    "title": "Lesson 12: Independence and Conditioning",
    "section": "Example starting from a joint pdf: second try! (2/2)",
    "text": "Example starting from a joint pdf: second try! (2/2)"
  },
  {
    "objectID": "lessons/x12_Independence_Conditioning/12_Independence_conditioning.html#example-starting-from-a-joint-pdf",
    "href": "lessons/x12_Independence_Conditioning/12_Independence_conditioning.html#example-starting-from-a-joint-pdf",
    "title": "Lesson 12: Independence and Conditioning",
    "section": "Example starting from a joint pdf",
    "text": "Example starting from a joint pdf\n\n\n\n\n\n\nDo this problem at home for extra practice. The solution is available in Meike’s video!\n\n\n\n\n\n\n\nExample 1.2\n\n\nLet \\(f_{X,Y}(x,y)= 5 e^{-x-3y}\\), for \\(0 &lt; y &lt; \\frac{x}{2}\\).\n\nFind \\(\\mathbb{P}(X&gt;20 |Y=5)\\)"
  },
  {
    "objectID": "lessons/x12_Independence_Conditioning/12_Independence_conditioning.html#finding-probability-with-conditional-domain-and-pdf",
    "href": "lessons/x12_Independence_Conditioning/12_Independence_conditioning.html#finding-probability-with-conditional-domain-and-pdf",
    "title": "Lesson 12: Independence and Conditioning",
    "section": "Finding probability with conditional domain and pdf",
    "text": "Finding probability with conditional domain and pdf\n\n\n\n\n\n\nDo this problem at home for extra practice. The solution is available in Meike’s video!\n\n\n\n\n\n\n\nExample 2\n\n\nRandomly choose a point \\(X\\) from the interval \\([0,1]\\), and given \\(X=x\\), randomly choose a point \\(Y\\) from \\([0,x]\\). Find \\(\\mathbb{P}(0 &lt; Y &lt; \\frac14)\\)."
  },
  {
    "objectID": "lessons/x09_CDFs/09_CDFs_key_info.html",
    "href": "lessons/x09_CDFs/09_CDFs_key_info.html",
    "title": "Key Info and Announcements",
    "section": "",
    "text": "Quiz 1 questions?\nCharles is still grading HW 2\n\nPosted solutions so you have as a reference during quiz\n\nIf links do not work on this site, there are two places where the files live:\n\nGithub page: https://github.com/nwakim/BSTA_550_25F\n\nLink also on this homepage\n\nOHSU OneDrive\n\nYou should have access to this folder\n\n\nIf you want to attend class virtually when we’re in-person, then you can use: https://pdx.zoom.us/j/82027616951\n\nHowever, I will not support virtual attendance when we’re in-person\nI will not be monitoring the chat or helping with technical issues"
  },
  {
    "objectID": "lessons/x09_CDFs/09_CDFs_key_info.html#key-info",
    "href": "lessons/x09_CDFs/09_CDFs_key_info.html#key-info",
    "title": "Key Info and Announcements",
    "section": "",
    "text": "Quiz 1 questions?\nCharles is still grading HW 2\n\nPosted solutions so you have as a reference during quiz\n\nIf links do not work on this site, there are two places where the files live:\n\nGithub page: https://github.com/nwakim/BSTA_550_25F\n\nLink also on this homepage\n\nOHSU OneDrive\n\nYou should have access to this folder\n\n\nIf you want to attend class virtually when we’re in-person, then you can use: https://pdx.zoom.us/j/82027616951\n\nHowever, I will not support virtual attendance when we’re in-person\nI will not be monitoring the chat or helping with technical issues"
  },
  {
    "objectID": "lessons/x09_CDFs/09_CDFs_key_info.html#announcements",
    "href": "lessons/x09_CDFs/09_CDFs_key_info.html#announcements",
    "title": "Key Info and Announcements",
    "section": "Announcements",
    "text": "Announcements\n\nHomework 3 due 02/01 at 11pm\nQuiz 1 opens on 01/28 at 3pm\n\nCloses 02/01 at 11pm"
  },
  {
    "objectID": "syllabus.html#key-course-info",
    "href": "syllabus.html#key-course-info",
    "title": "BSTA 551 Syllabus",
    "section": "Key Course Info",
    "text": "Key Course Info\n\nIf an assignment on Sakai is closed or you are submitting late work, please email me AND the TA your work\nThe in-person class instruction will end on Wednesday, March 11, 2026. All coursework is expected to be completed by Thursday of finals week, March 19, 2026 at 11pm."
  },
  {
    "objectID": "syllabus.html#description",
    "href": "syllabus.html#description",
    "title": "BSTA 551 Syllabus",
    "section": "Description",
    "text": "Description\nWelcome to BSTA 551! In this course we will study the theoretical foundaton of statistical inference which includes estimation and hypothesis testing. This is the official course description:\nThis course introduces theoretical foundation in Biostatistics. Topics will include distributions of random variables (location-scale families and exponential families), data reduction (sufficiency and completeness of statistics), methods of estimation (method of moment estimators and maximum likelihood estimators), convergence, finite and large sample properties of estimators, interval estimation, hypothesis testing, asymptotic tests (likelihood-ratio tests, score tests, and Wald tests), and statistical simulations to evaluate statistical methods.\n\nCourse Learning Objectives\nAt the end of this course, students should be able to…\n\nExplain the major concepts and theorems in statistical inference.\nConnect theoretical concepts to statistical analyses.\nConduct simulations to study and evaluate statistical methods."
  },
  {
    "objectID": "syllabus.html#instructors",
    "href": "syllabus.html#instructors",
    "title": "BSTA 551 Syllabus",
    "section": "Instructors",
    "text": "Instructors\nHere is the instructor page."
  },
  {
    "objectID": "syllabus.html#meeting-times",
    "href": "syllabus.html#meeting-times",
    "title": "BSTA 551 Syllabus",
    "section": "Meeting Times",
    "text": "Meeting Times\nMondays          10:00 AM – 12:00 PM PST (see Sakai for room)\nWednesdays    10:00 AM – 12:00 PM PST\n\nKnown Exceptions\n\nMonday, January 19: No class (holiday)\nWednesday, February 10: No class\nMonday, February 16: No class (holiday)\nMarch 16 and 18: Finals week, no in person teaching, possibly virtual office hours as needed"
  },
  {
    "objectID": "syllabus.html#materials",
    "href": "syllabus.html#materials",
    "title": "BSTA 551 Syllabus",
    "section": "Materials",
    "text": "Materials\n\nTextbook\n\nModern Mathematical Statistics with Applications, 3rd ed.\n\nAuthor: JL Devore, KN Berk, MA Carlton\nTextbook available online through library\nCitation: Devore JL, Berk KN, Carlton MA. Modern Mathematical Statistics with Applications. Third edition. Springer; 2021. doi:10.1007/978-3-030-55156-8\nFocus on chapters 6-10\n\n\n\nSupplemental Readings (Optional)\n\nStatistical Inference, Casella and Berger, 2nd ed. (This was the previous textbook BSTA 551-552 Math Stat., very theoretical) ebook OHSU library link\nAn Introduction to R (free pdf available)\nStatistical Inference via Data Science, A ModernDive into R and the Tidyverse, Ismay, Kim, Valdivia, 2nd ed. free ebook online\n\n\n\n\nOnline Resources\n\nSakai\nWhile most course materials will be delivered online through this website, assignments will be turned in through Sakai, OHSU’s course management system. I will include a link on this website to the Sakai assignment page. \n\n\nPennState STAT 415 Website\nDr. Wakim linked to Penn State’s probability course materials for 550. They also have the subsequent inference course on the website as well. They have all their course notes posted on this page if you wish to have an alternative reference.\n\n\nUniversity of South Carolina STAT 713 Website\nThis is a more thoeretical version (similar to previous math/stats courses taught at OHSU) of statistical inference taught by Professor Tebbs at U of SC. Their notes which are linked on the course website are very well written. If you want more theoretical context for what we are learning in this course this is a good source. Note, it uses Casella & Berger’s textbook Statistical Inference as a reference.\n\n\nR: Statistical Computing Software\nR/Rstudio will be used to complete some homework assignments. Please see Dr. Wakim’s BSTA 550 syllabus section on R if you need any references or refreshers on how to use."
  },
  {
    "objectID": "syllabus.html#assessment",
    "href": "syllabus.html#assessment",
    "title": "BSTA 551 Syllabus",
    "section": "Assessment",
    "text": "Assessment\n\nBreakdown\n50% Homework (Weekly)\n20% Midterm\n20% Final\n10% Attendence (Two exit tickets a week)\n\nGrading & Requirements\nLetter grades will be assigned roughly according to the following scheme: A (&gt;=93%), A- (90-92%), B+ (88-89%), B(83-87%), B- (82-80%), C+(78-79%), C(73-77%), C- (70-72%), D (60 – 69%), F(&lt;60%).\n\n\nHomework grading\n[Identical to Dr. Nicky Wakim’s BSTA 550 approach to homework!]\nNo student has the same amount of time available to dedicate to homework. This class may not be a priority to you, you may be taking several other courses, or you may need to dedicate time to other activities. Homework assignments are formative assessments, meaning its purpose is to help you learn and practice. To reduce the pressure on you to have perfect homework (the first time around), I have a very simple grading policy: Your homework will be given a check mark if you turn in 75% of the question parts completed (whether the 75% is correct or wrong). I highly encourage you to stay up-to-date with the homeworks and put in as much effort as you can. This will be the most helpful work in this class!\nIf you turn in the homework on time, I will give you feedback (on one or more complete problems). There is no penalty for turning in the homework late, but you will not get feedback on your work. Please make sure to check the solutions or go to office hours to assess your work.\n\n\nViewing Grades in Sakai\nPoints you receive for graded activities will be posted to the Sakai Gradebook. Click on the Gradebook link on the left navigation to view your points."
  },
  {
    "objectID": "syllabus.html#course-instructor-evaluations",
    "href": "syllabus.html#course-instructor-evaluations",
    "title": "BSTA 551 Syllabus",
    "section": "Course & Instructor Evaluations",
    "text": "Course & Instructor Evaluations\n\nOngoing Course Feedback\nSubmit feedback throughout the course here.\n\n\nFinal Course Feedback\nAt the conclusion of the course, you will be asked to complete a formal online review of the course and the instructor. Your feedback on this University evaluation is critical to improving future student learning in this course as well as providing metrics relevant to the instructor’s career advancement (or lack of). Since our class is on the smaller side, everyone’s participation is needed for feedback to be released."
  },
  {
    "objectID": "syllabus.html#schedule",
    "href": "syllabus.html#schedule",
    "title": "BSTA 551 Syllabus",
    "section": "Schedule",
    "text": "Schedule\nPlease refer to the Schedule page. I will make changes to this schedule if we need more or less time on a concept. You do not need to read the corresponding chapters in the textbook for each class.\n\nAssignments\nWritten problem sets, approximately weekly, usually assigned Wednesday and due the following Thursday.\nHomework problems will be posted on the course website. Solutions should be posted as a pdf in the corresponding Sakai submission box."
  },
  {
    "objectID": "syllabus.html#course-policies-and-resources",
    "href": "syllabus.html#course-policies-and-resources",
    "title": "BSTA 551 Syllabus",
    "section": "Course Policies and Resources",
    "text": "Course Policies and Resources\n\nLate Work Policy\nI encourage you to make your best effort to submit all assignments on time, but I understand circumstances arise that are beyond our control. Please see this Swansea University’s page on extenuating circumstances for some examples. Not all circumstances are covered here, so please reach out if you have questions. \n\nThe class will end on Wednesday, March 18, 2026. All coursework is expected to be completed by March 19, 2026 at 11pm. If you have extenuating circumstances, and need additional time to complete class assignments, please contact me. Together, we will come up with a plan for completion and to sort out registrar logistics. If you need an Incomplete, generally this requires request two weeks before the end of class.\nIf you have extenuating circumstances that may jeopardize your ability to do work for several weeks, please contact me. We will come up with a plan to keep you on track in the course and prevent any delay in your education.\nFor homework, you will have TWO no-questions-asked, 3-day extensions: one for the first assignment part and one for either the solutions or presentation. Please use this wisely! You just need to send me a quick email saying “I am using my no-questions-asked extension for Homework __ assignment/solutions/presentation.”\nFor homework, I ask you to email me directly about any late submissions. You can explain your circumstances and may ask us for an extension. I am very likely to grant an extension, but I want to emphasize how important it will be to stay on track with your homework! Your group is depending on you, and delaying homework may only add stress on the next homework!\nIf you have a emergency involving your self, family, pet, friend, classmate, or anything/one deemed important to you, please do not worry about immediately contacting me. We can work something out after your emergency. If I contact you during an emergency, it is only because I am worried, and you do NOT need to respond until you are able. \n\n\n\nRegrade Policy\nIf you think a question was incorrectly graded, first compare your answer to the answer key. If you believe a re-grade would be appropriate, write an email to me containing the question and a short explanation as to why the question(s) was/were incorrectly graded. Deadline: One week after assignments were returned to class (late requests will not be considered).\n\n\nAttendance Policy\nYou are expected to attend class, participate in-class polls, and complete the exit ticket. For students who miss class or need a review, I will make video and audio recordings of lectures available. There are no guarantees against technical or other challenges for the recording availability or quality.\nYou will need to attend all classes. There are 17 classes total, so you are welcome to watch the recordings or come in-person. While I want attendance to be a flexible thing, I need to set certain requirements around in-person attendance to align with the school’s policy. Attendance is measured through exit tickets that will be due 7 days after each class.\nThis is meant to keep you on track within the course and prevent a pile up of material. Make sure to complete the exit ticket at the end of class to demonstrate attendance.\n\n\nPlagiarism and Attribution\nPlease note that this section is directly sourced from Dr. Nicky Wakim’s Course Policies for BSTA 550 which has itself has been motivated by Dr. Steven Bedrick’s Course Policies and Grading site for BMI 525. (Note that this is a good example of informal attribution of someone else’s work.)\nIn this class, it is easy to use ChatGPT or other AI tools to solve your homework for you. Many problems follow a basic structure that is especially easy for ChatGPT to solve. In this class, you may use ChatGPT to help with your homework. You may even ask for direct answers. However, there are a few things I do not want you to do:\n\nDo not copy ChatGPT’s answer directly into your homework. Your homework is graded for full credit if you turn it in, in any state, so turning in ChatGPT’s answers is unacceptable. I rather see half-written answers that show what you’re thinking than see a correct answer from ChatGPT.\nDo not stop once ChatGPT answered a question. If it gives an explanation, interact with it! Make sure you understand the thought process of ChatGPT. Try writing out the process to help cement it in your head. Check the answer with what we learn in class.\nDo not use ChatGPT on our exams! Hence, you need to really understand how to solve these problems even if you use ChatGPT on the homework.\n\nAt the end of the day, ChatGPT is a resource that will be available to you in a job and outside of school. Thus, we should use it as a tool in school as well! Let me know if ChatGPT helped you understand something! I would love to incorporate it into future classes!\n\n\n\n\n\n\nImportant\n\n\n\nYou can think of this class as assembling a toolbox. When a handyperson starts working for the first time, they need to buy their tools. For their first few jobs, they might need help finding their tools or remembering which tool is best used for what action. Eventually, they get to know their tools well, and using them appropriately becomes second nature.\nFor now, ChatGPT can help us find and use our tools, but we need to work towards using them as second nature!"
  },
  {
    "objectID": "syllabus.html#course-expectations",
    "href": "syllabus.html#course-expectations",
    "title": "BSTA 551 Syllabus",
    "section": "Course Expectations",
    "text": "Course Expectations\n\nInstructor Expectations\nCommitment to your learning and your success\nI believe that everyone has the ability to be successful in this course and I have put a lot of effort into designing the course in a way that maximizes your learning to ensure your success. Please talk to me before or after class or stop by my office if there is anything you want to discuss or about which you are unclear. I want to be supportive of your learning and growth.\nInclusive & supportive learning community\nI believe that learning happens best when we all learn together, as a community. This means creating a space characterized by generous listening, civility, humility, patience, and hospitality. I will attempt to promote a safe climate where we examine content from multiple perspectives. I will strive to create and maintain a classroom atmosphere in which you feel free to both listen to others and express your views and ask questions to increase your learning.\nOpenness to feedback\nI appreciate straightforward feedback from you regarding how well the class is meeting your needs. Let me know if material is not clear or when its relevance to the student learning outcomes for the course is not apparent. In particular, let me know if you identify bias or stereotyping in my teaching materials as I will seek to continuously improve. Please also let me know if there’s an aspect of the class you find particularly interesting, helpful, or enjoyable!\nResponsiveness\nI will monitor email as well as the discussion board daily and try respond to all messages within 24 hours Monday-Friday.\nClear guidelines and prompt feedback on assignments\nI will provide clear instructions for all assignments, and a grading rubric when applicable. The TAs and I will provide detailed feedback on your submissions and will update grades promptly in Sakai.\n\n\nStudent Expectations and Resources\nAttend class\nYou are expected to attend at least 10 scheduled class meetings in-person. Attendance is taken through exit tickets. If you have issues accessing the poll on a specific day, please let me know. \nParticipate\nI encourage you to participate actively in class. I will expect all students, and all instructors, to be respectful of each other’s contributions, whether I agree with them or not. Professional interactions are expected.\nBuild rapport\nIf you find that you have any trouble keeping up with assignments or other aspects of the course, make sure you let me know as early as possible. As you will find, building rapport and effective relationships are key to becoming an effective professional. Make sure that you are proactive in informing me when difficulties arise during the quarter so that I can help you find a solution in regards to coursework.\nComplete assignments\nAll assignments for this course will be submitted electronically through Sakai unless otherwise instructed. I encourage you to make your best effort to submit all assignments on time, but I understand that sometimes circumstances arise that are beyond our control. If you need an extension, please contact me in congruence with the Late Policy.\nSeek help if you need it\nI believe it is important to support the physical and emotional well‐being of my students. If you are experiencing physical and/or mental health issues, I encourage you to use the resources on campus such as those listed below. If you have a health issue that is affecting your performance or participation in the course, and/or if you need help connecting with these resources, please contact me.\n\nStudent Health and Wellness Center (SHW), Website, 503-494-8665 (OHSU Students only)\nStudent Health and Counseling (SHAC), Website, 503-725-2800\n\nInform your instructor of any accommodations needed\nYou should speak with or email me before or during the first week of classes regarding any special needs. Students seeking academic accommodations should register with the appropriate service under the School policies below.\nSome religious holidays may occur on regularly scheduled class days. Because available class hours are so limited in number, we will have to hold class on all such days. Class video recordings will be available and you are encouraged to engage with the material outside of the regular class time. Please email me about your absence. I will excuse the absece from your grade. You are also encouraged to come to office hours with questions from the session.\nCommit to integrity\nAs a student in this course (and at PSU or OHSU) you are expected to maintain high degrees of professionalism, commitment to active learning and participation in this class and also integrity in your behavior in and out of the classroom.\nCheating and other forms of academic misconduct will not be tolerated in this course and will be dealt with firmly. Student academic misconduct refers to behavior that includes plagiarism, cheating on assignments, fabrication of data, falsification of records or official documents, intentional misuse of equipment or materials (including library materials), or aiding and abetting the perpetration of such acts. Preparation of exams, assigned on an individual basis, must represent each student’s own individual effort. When used, resource materials should be cited in conventional reference format."
  },
  {
    "objectID": "syllabus.html#course-communications",
    "href": "syllabus.html#course-communications",
    "title": "BSTA 551 Syllabus",
    "section": "Course Communications",
    "text": "Course Communications\nSakai announcements\nFor important/urgent matters, I will communicate with you using announcements via Sakai that will be delivered to your OHSU Email account as well as displayed in the Sakai course site Announcements section.\nE-mail\nE-mail should be used only for messages that are private in nature. Please send private messages to my OHSU email address (minnier@ohsu.edu)."
  },
  {
    "objectID": "syllabus.html#further-student-resources",
    "href": "syllabus.html#further-student-resources",
    "title": "BSTA 551 Syllabus",
    "section": "Further Student Resources",
    "text": "Further Student Resources\n\nAcademic Success Center\nOHSU houses an Academic Success Center for all students. Their mission is to create a center for learning support where ALL learners can discover the resources and community that they need for finding academic success at OHSU. They provide many services to students, including: learning skills support, writing support, English for speakers of other languages (ESOL) support, and individual and group content tutoring. Check out the SharePoint site for the Academic Success Center.\n\n\nStudent Wellness\nI am committed to supporting the physical and emotional well-being of my students. Both PSU and OHSU have designated centers for student health. For OHSU, students can visit the Behavioral Health site, where you can find more information including the number to make an appointment. All student visits are free. OHSU students also have access to PSU’s Counseling Services through the school’s Student Health & Counseling. Information on additional student resources for OHSU students are available on the OHSU Health and Wellness Resource page. \n\n\nSupport for Food Insecurity\nStudents across the country experience food insecurity at high rates. OHSU and PSU both provide a list of resources to help combat food insecurity. Of note, the Committee to Improve Student Food Security (CISFS) at PSU provides a Free Food Market on the second Monday of each month. OHSU also provides SNAP Enrollment Assistance. The Supplemental Nutrition Assistance Program (SNAP) allocates money towards food for individuals below a certain income level. If you make less than $2,430 monthly, you may wish to enroll.\n\n\nSupport for Students with Children\nStudents who have children can use the PSU resource: Resource Center for Students with Children. Resources are mostly focused on students with younger children. There are several great resources available, including: family-friendly study spaces, new baby starter packs, free kids clothing, and further information on financial resources for childcare."
  },
  {
    "objectID": "lessons/01_Intro_Inference/01_Intro_Inference.html#simulation-building-a-sampling-distribution-output",
    "href": "lessons/01_Intro_Inference/01_Intro_Inference.html#simulation-building-a-sampling-distribution-output",
    "title": "BSTA 551: Statistical Inference",
    "section": "Simulation: Building a Sampling Distribution",
    "text": "Simulation: Building a Sampling Distribution"
  },
  {
    "objectID": "lessons/02_Point_Estimation/02_Point_Estimation.html#simulation-visualizing-the-biased-estimator-output",
    "href": "lessons/02_Point_Estimation/02_Point_Estimation.html#simulation-visualizing-the-biased-estimator-output",
    "title": "BSTA 551: Statistical Inference",
    "section": "Simulation: Visualizing the Biased Estimator",
    "text": "Simulation: Visualizing the Biased Estimator\n\n# A tibble: 1 × 4\n  theoretical_E empirical_mean theoretical_bias empirical_bias\n          &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;          &lt;dbl&gt;\n1            90           90.3              -10          -9.70"
  },
  {
    "objectID": "lessons/00_Intro/00_Intro_key_info.html",
    "href": "lessons/00_Intro/00_Intro_key_info.html",
    "title": "Key Info and Announcements",
    "section": "",
    "text": "Welcome! I am heavily borrowing from Dr. Nicky Wakim’s 550 website and format, because it’s awesome."
  },
  {
    "objectID": "lessons/00_Intro/00_Intro_key_info.html#announcements",
    "href": "lessons/00_Intro/00_Intro_key_info.html#announcements",
    "title": "Key Info and Announcements",
    "section": "",
    "text": "Welcome! I am heavily borrowing from Dr. Nicky Wakim’s 550 website and format, because it’s awesome."
  },
  {
    "objectID": "lessons/00_Intro/00_Intro_key_info.html#key-dates",
    "href": "lessons/00_Intro/00_Intro_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key dates",
    "text": "Key dates\n\nHomework 0 is due THIS THURSDAY at 11pm!"
  },
  {
    "objectID": "lessons/00_Intro/00_Intro_key_info.html#last-class",
    "href": "lessons/00_Intro/00_Intro_key_info.html#last-class",
    "title": "Key Info and Announcements",
    "section": "Last class",
    "text": "Last class\n\nHolidays"
  }
]
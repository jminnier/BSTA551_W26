{
  "hash": "850008b4add1e6b0324184132e27be43",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"BSTA 551: Statistical Inference\"\nsubtitle: \"Lesson 2: Point Estimation; Bias, Variance, and MSE\"\nauthor: \"Jessica Minnier\"\ntitle-slide-attributes:\n    data-background-color: \"#006a4e\"\ndate: \"2026-01-07\"\nformat: \n  revealjs:\n    theme: \"../simple_NW.scss\"\n    chalkboard: true\n    scrollable: true\n    slide-number: true\n    show-slide-number: all\n    width: 1955\n    height: 1100\n    footer: Lesson 2\n    html-math-method: mathjax\n    highlight-style: atom-one\nexecute:\n  echo: true\n  warning: false\n  message: false\n---\n\n\n\n# Lesson 2: Point Estimation\n\n## Review: Where We Left Off\n\n**Key concepts from Lesson 1:**\n\n-   Population vs. sample; parameters vs. statistics\n-   Sampling distributions\n-   $E(\\bar{X}) = \\mu$ and $\\text{Var}(\\bar{X}) = \\sigma^2/n$\n-   Numerical optimization with `optimize()`\n\n. . .\n\n**Today's Goals:**\n\n-   Define point estimators formally\n-   Understand bias and how to calculate it\n-   Learn about standard error and precision\n-   Introduce Mean Squared Error (MSE)\n-   Explore the bias-variance tradeoff\n\n# Point Estimation: Core Concepts {background-color=\"#27ae60\"}\n\n## What is a Point Estimator? (Devore 7.1)\n\n::: callout-important\n## Definitions\n\n-   A **parameter** is a fixed (but unknown) characteristic of a population (e.g., $\\mu$, $\\sigma$, $p$)\n-   An **estimator** is a rule/formula for calculating an estimate from sample data\n-   An **estimate** is the actual number you calculate from a specific sample\n:::\n\n. . .\n\n**Example:**\n\n| Concept   | Symbol                          | Example                   |\n|-----------|---------------------------------|---------------------------|\n| Parameter | $\\mu$                           | True mean BP reduction    |\n| Estimator | $\\bar{X} = \\frac{1}{n}\\sum X_i$ | The formula \"sample mean\" |\n| Estimate  | $\\bar{x} = 11.2$                | The number from our data  |\n\n## Key Distinction: Estimator vs. Estimate\n\n**Estimator:** A random variable (before data is collected)\n\n-   $\\bar{X}$ is a function of random variables $X_1, \\ldots, X_n$\n-   Has a sampling distribution\n-   Can calculate $E(\\bar{X})$, $\\text{Var}(\\bar{X})$\n\n. . .\n\n**Estimate:** A specific number (after data is collected)\n\n-   $\\bar{x} = 11.2$ is a specific value\n-   Just one realization from the sampling distribution\n\n. . .\n\n::: callout-tip\nThink of it like this: the estimator is the recipe; the estimate is what you get when you cook it with specific ingredients.\n:::\n\n## The Sampling Distribution (Revisited)\n\nDifferent samples give different estimates. The **sampling distribution** describes this variability.\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n| Sample| Sample Mean (x̄)|\n|------:|---------------:|\n|      1|            9.90|\n|      2|           10.31|\n|      3|           10.03|\n|      4|           10.85|\n|      5|            9.17|\n|      6|            9.31|\n\n\n:::\n:::\n\n\n. . .\n\nEach sample gives a **different estimate**, but they cluster around the true value $\\mu = 10$.\n\n## What Makes a Good Estimator?\n\nWe want estimators that are:\n\n1.  **Accurate** (unbiased): On average, hits the true value\n2.  **Precise** (low variance): Estimates are clustered together\n3.  **Efficient**: Best combination of accuracy and precision\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02_Point_Estimation_files/figure-revealjs/unnamed-chunk-2-1.png){width=960}\n:::\n:::\n\n\n# Bias: Measuring Accuracy {background-color=\"#8e44ad\"}\n\n## Bias: Formal Definition\n\n::: callout-important\n## Definition\n\nThe **bias** of an estimator $\\hat{\\theta}$ is: $$\\text{Bias}(\\hat{\\theta}) = E(\\hat{\\theta}) - \\theta$$\n\nAn estimator is **unbiased** if $\\text{Bias}(\\hat{\\theta}) = 0$, i.e., $E(\\hat{\\theta}) = \\theta$.\n:::\n\n. . .\n\n**Interpretation:**\n\n-   Bias measures systematic error\n-   Positive bias = tends to overestimate\n-   Negative bias = tends to underestimate\n-   Zero bias = correct on average\n\n## Worked Example: Proving Sample Mean is Unbiased\n\n**Claim:** The sample mean $\\bar{X}$ is an unbiased estimator of $\\mu$.\n\n**Proof:** We need to show $E(\\bar{X}) = \\mu$.\n\n. . .\n\n$$\\text{Bias}(\\bar{X}) = E(\\bar{X}) - \\mu$$\n\n. . .\n\nWe already showed that $E(\\bar{X}) = \\mu$, so:\n\n$$\\text{Bias}(\\bar{X}) = \\mu - \\mu = 0 \\checkmark$$\n\n. . .\n\n**Conclusion:** The sample mean is unbiased for estimating the population mean.\n\n## Worked Example: Sample Proportion\n\n**Setup:** In a vaccine trial, $X$ patients out of $n$ develop immunity. The estimator is $\\hat{p} = X/n$.\n\n**Claim:** $\\hat{p}$ is unbiased for the true immunity rate $p$.\n\n. . .\n\n**Proof:** Since $X \\sim \\text{Binomial}(n, p)$, we know $E(X) = np$.\n\n$$E(\\hat{p}) = E\\left(\\frac{X}{n}\\right) = \\frac{1}{n} E(X) = \\frac{1}{n} \\cdot np = p$$\n\n. . .\n\n$$\\text{Bias}(\\hat{p}) = E(\\hat{p}) - p = p - p = 0 \\checkmark$$\n\n## Concrete Calculation: Bias of Sample Proportion\n\n**Data:** In a study of 80 patients, 52 showed improvement.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 80\nx <- 52\np_hat <- x / n\n\ncat(\"Sample proportion:\", p_hat, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSample proportion: 0.65 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"If true p = 0.65, what is the bias of this single estimate?\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nIf true p = 0.65, what is the bias of this single estimate?\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Observed - True =\", p_hat - 0.65)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nObserved - True = 0\n```\n\n\n:::\n:::\n\n\n. . .\n\n**Important distinction:**\n\n-   A single estimate can be above or below the true value\n-   **Bias** refers to the average behavior across many samples\n-   An unbiased estimator still gives wrong answers for individual samples!\n\n## Simulation: Verifying Unbiasedness\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Verify that sample proportion is unbiased\ntrue_p <- 0.65\nn_patients <- 80\nn_simulations <- 10000\n\nproportion_simulation <- tibble(sim = 1:n_simulations) |> \n  mutate(\n    successes = rbinom(n_simulations, size = n_patients, prob = true_p),\n    p_hat = successes / n_patients\n  )\n\nproportion_simulation |> \n  summarize(\n    true_p = true_p,\n    mean_of_estimates = mean(p_hat),\n    empirical_bias = mean(p_hat) - true_p\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  true_p mean_of_estimates empirical_bias\n   <dbl>             <dbl>          <dbl>\n1   0.65             0.650      -0.000183\n```\n\n\n:::\n:::\n\n\nThe bias is essentially zero (just simulation noise)!\n\n## A Biased Estimator: The Maximum\n\n**Problem:** Estimate the upper bound $\\theta$ of a Uniform\\[0, $\\theta$\\] distribution.\n\n**Natural idea:** Use the largest observation: $\\hat{\\theta} = \\max(X_1, \\ldots, X_n)$\n\n. . .\n\n**Think about it:** Can this estimator ever *overestimate* $\\theta$?\n\n. . .\n\n**No!** The sample maximum is always ≤ the population maximum.\n\nThis means the estimator is **biased low**.\n\n## Calculating the Bias Mathematically\n\nFor $X_1, \\ldots, X_n \\sim \\text{Uniform}[0, \\theta]$, it can be shown that:\n\n$$E(\\max(X_1, \\ldots, X_n)) = \\frac{n}{n+1} \\theta$$\n\n. . .\n\n**Bias calculation:**\n\n$$\\text{Bias} = E(\\hat{\\theta}) - \\theta = \\frac{n}{n+1}\\theta - \\theta = -\\frac{\\theta}{n+1}$$\n\n. . .\n\n**Example:** If $\\theta = 10$ and $n = 5$:\n\n$$\\text{Bias} = -\\frac{10}{6} = -1.67$$\n\nThe estimator underestimates by about 1.67 on average.\n\n## Your Turn: Calculate Bias\n\n**Exercise:** A lab instrument has a maximum detection limit $\\theta$. We take $n = 9$ measurements from Uniform\\[0, $\\theta$\\] and use the maximum as our estimate.\n\n1.  If $\\theta = 100$, what is $E(\\hat{\\theta})$?\n2.  What is the bias?\n3.  By what percentage does this estimator underestimate on average?\n\n. . .\n\n**Solution:**\n\n1.  $E(\\hat{\\theta}) = \\frac{9}{10} \\times 100 = 90$\n2.  $\\text{Bias} = 90 - 100 = -10$\n3.  Underestimates by $\\frac{10}{100} = 10\\%$\n\n## Simulation: Visualizing the Biased Estimator\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\ntrue_theta <- 100\nn <- 9\nn_sims <- 5000\n\nmax_simulation <- tibble(sim = 1:n_sims) |> \n  mutate(\n    max_estimate = map_dbl(sim, \\(s) max(runif(n, 0, true_theta)))\n  )\n\n# Calculate empirical bias\nmax_simulation |> \n  summarize(\n    theoretical_E = n / (n + 1) * true_theta,\n    empirical_mean = mean(max_estimate),\n    theoretical_bias = -true_theta / (n + 1),\n    empirical_bias = mean(max_estimate) - true_theta\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 4\n  theoretical_E empirical_mean theoretical_bias empirical_bias\n          <dbl>          <dbl>            <dbl>          <dbl>\n1            90           90.3              -10          -9.70\n```\n\n\n:::\n:::\n\n\n## Visualizing the Biased Estimator\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02_Point_Estimation_files/figure-revealjs/unnamed-chunk-6-1.png){width=960}\n:::\n:::\n\n\n## Correcting the Bias\n\n**Idea:** Multiply by a correction factor to \"un-bias\" the estimator.\n\nSince $E(\\max) = \\frac{n}{n+1}\\theta$, we can define:\n\n$$\\hat{\\theta}_{\\text{unbiased}} = \\frac{n+1}{n} \\cdot \\max(X_1, \\ldots, X_n)$$\n\n. . .\n\n**Check:**\n\n$$E(\\hat{\\theta}_{\\text{unbiased}}) = \\frac{n+1}{n} \\cdot E(\\max) = \\frac{n+1}{n} \\cdot \\frac{n}{n+1}\\theta = \\theta \\checkmark$$\n\n## Your Turn: Apply the Correction\n\n**Exercise:** Using the lab instrument example with $n = 9$ and $\\theta = 100$:\n\n1.  If you observe $\\max = 92$, what is the biased estimate?\n2.  What is the unbiased estimate?\n\n. . .\n\n**Solution:**\n\n1.  Biased estimate: $\\hat{\\theta}_b = 92$\n2.  Unbiased estimate: $\\hat{\\theta}_u = \\frac{10}{9} \\times 92 = 102.2$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nobserved_max <- 92\nn <- 9\n\nbiased_est <- observed_max\nunbiased_est <- (n + 1) / n * observed_max\n\ncat(\"Biased estimate:\", biased_est, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBiased estimate: 92 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Unbiased estimate:\", round(unbiased_est, 1))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nUnbiased estimate: 102.2\n```\n\n\n:::\n:::\n\n\n# Standard Error: Measuring Precision {background-color=\"#2c3e50\"}\n\n## Standard Error: Definition\n\n::: callout-important\n## Definition\n\nThe **standard error** of an estimator is its standard deviation: $$SE(\\hat{\\theta}) = \\sqrt{\\text{Var}(\\hat{\\theta})}$$\n:::\n\n. . .\n\n**Key Standard Errors:**\n\n| Estimator                   | Standard Error            |\n|-----------------------------|---------------------------|\n| Sample mean $\\bar{X}$       | $\\frac{\\sigma}{\\sqrt{n}}$ |\n| Sample proportion $\\hat{p}$ | $\\sqrt{\\frac{p(1-p)}{n}}$ |\n\n## Worked Example: Standard Error of Sample Mean\n\n**Problem:** In a blood pressure study, the population SD is $\\sigma = 15$ mmHg. Calculate the standard error of $\\bar{X}$ for sample sizes $n = 25$ and $n = 100$.\n\n. . .\n\n**Solution:**\n\nFor $n = 25$: $$SE(\\bar{X}) = \\frac{15}{\\sqrt{25}} = \\frac{15}{5} = 3 \\text{ mmHg}$$\n\nFor $n = 100$: $$SE(\\bar{X}) = \\frac{15}{\\sqrt{100}} = \\frac{15}{10} = 1.5 \\text{ mmHg}$$\n\n. . .\n\n**Interpretation:** With 100 patients, our estimate is twice as precise as with 25 patients.\n\n## Your Turn: Calculate Standard Error\n\n**Exercise:** A survey measures patient satisfaction on a 0-100 scale. The population standard deviation is $\\sigma = 20$.\n\n1.  What is the SE of $\\bar{X}$ for $n = 16$ patients?\n2.  What sample size is needed to achieve $SE = 2$?\n\n. . .\n\n**Solutions:**\n\n1.  $SE = \\frac{20}{\\sqrt{16}} = \\frac{20}{4} = 5$\n\n2.  We need $\\frac{20}{\\sqrt{n}} = 2$, so $\\sqrt{n} = 10$, thus $n = 100$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsigma <- 20\n# Problem 1\nse_n16 <- sigma / sqrt(16)\n# Problem 2\nn_needed <- (sigma / 2)^2\n\ncat(\"SE for n=16:\", se_n16, \"\\nSample size for SE=2:\", n_needed)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSE for n=16: 5 \nSample size for SE=2: 100\n```\n\n\n:::\n:::\n\n\n## The Problem with Unknown Parameters\n\n**Issue:** Standard errors often involve unknown parameters!\n\n-   SE of $\\bar{X}$ requires knowing $\\sigma$\n-   SE of $\\hat{p}$ requires knowing $p$\n\n. . .\n\n**Solution:** **Estimated standard error** — substitute estimates for unknown parameters\n\n$$\\widehat{SE}(\\bar{X}) = \\frac{s}{\\sqrt{n}}$$\n\n$$\\widehat{SE}(\\hat{p}) = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}$$\n\n## Example: Estimated Standard Error\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Blood pressure data from 25 patients\nbp_reductions <- c(12, 8, 15, 10, 7, 14, 11, 9, 13, 16,\n                   8, 12, 10, 14, 11, 9, 15, 13, 7, 12,\n                   10, 8, 14, 11, 13)\n\nn <- length(bp_reductions)\nx_bar <- mean(bp_reductions)\ns <- sd(bp_reductions)\n\n# Estimated standard error\nse_estimated <- s / sqrt(n)\n\ntibble(\n  Statistic = c(\"Sample Mean\", \"Sample SD\", \"Sample Size\", \"Estimated SE\"),\n  Value = c(x_bar, round(s, 2), n, round(se_estimated, 2))\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 2\n  Statistic    Value\n  <chr>        <dbl>\n1 Sample Mean  11.3 \n2 Sample SD     2.64\n3 Sample Size  25   \n4 Estimated SE  0.53\n```\n\n\n:::\n:::\n\n\n# Mean Squared Error {background-color=\"#e74c3c\"}\n\n## Mean Squared Error: Combining Bias and Variance\n\nWhat if we have to choose between a biased estimator with low variance and an unbiased estimator with high variance?\n\n. . .\n\n::: callout-important\n## Definition: Mean Squared Error\n\n$$\\text{MSE}(\\hat{\\theta}) = E[(\\hat{\\theta} - \\theta)^2]$$\n\n**Key Formula:** $$\\text{MSE} = \\text{Variance} + \\text{Bias}^2$$\n:::\n\n. . .\n\n**MSE captures total error** — both systematic (bias) and random (variance).\n\n## Proving the MSE Formula\n\n$$\\text{MSE}(\\hat{\\theta}) = E[(\\hat{\\theta} - \\theta)^2]$$\n\n. . .\n\nAdd and subtract $E(\\hat{\\theta})$:\n\n$$= E[(\\hat{\\theta} - E(\\hat{\\theta}) + E(\\hat{\\theta}) - \\theta)^2]$$\n\n. . .\n\nExpand the square:\n\n$$= E[(\\hat{\\theta} - E(\\hat{\\theta}))^2] + 2E[(\\hat{\\theta} - E(\\hat{\\theta}))](E(\\hat{\\theta}) - \\theta) + (E(\\hat{\\theta}) - \\theta)^2$$\n\n. . .\n\nThe middle term equals zero because $E[\\hat{\\theta} - E(\\hat{\\theta})] = 0$:\n\n$$= \\text{Var}(\\hat{\\theta}) + [\\text{Bias}(\\hat{\\theta})]^2$$\n\n## Worked Example: Computing MSE\n\n**Setup:** Estimating $\\theta$ from Uniform\\[0, $\\theta$\\] with $n = 9$ and $\\theta = 100$.\n\n**Biased estimator:** $\\hat{\\theta}_b = \\max(X_i)$\n\nFrom theory:\n\n-   $E(\\hat{\\theta}_b) = \\frac{9}{10}(100) = 90$\n-   $\\text{Var}(\\hat{\\theta}_b) = \\frac{n \\theta^2}{(n+1)^2(n+2)} = \\frac{9 \\times 100^2}{100 \\times 11} = 81.82$\n\n. . .\n\n$$\\text{Bias} = 90 - 100 = -10$$ $$\\text{MSE} = 81.82 + (-10)^2 = 81.82 + 100 = 181.82$$\n\n## Your Turn: Calculate MSE\n\n**Exercise:** For the **unbiased** estimator $\\hat{\\theta}_u = \\frac{n+1}{n}\\max(X_i)$ with $n = 9$ and $\\theta = 100$:\n\n1.  What is the bias?\n2.  If $\\text{Var}(\\hat{\\theta}_u) = 101.01$, what is the MSE?\n3.  Which estimator has lower MSE: biased or unbiased?\n\n. . .\n\n**Solutions:**\n\n1.  Bias = 0 (it's unbiased!)\n2.  $\\text{MSE} = 101.01 + 0^2 = 101.01$\n3.  Unbiased has lower MSE (101.01 \\< 181.82)\n\nBut this isn't always the case!\n\n# The Bias-Variance Tradeoff {background-color=\"#8e44ad\"}\n\n## The Bias-Variance Tradeoff\n\nSometimes a **biased** estimator has **lower MSE** than an unbiased one!\n\n. . .\n\n**Example:** Estimating a proportion $p$ with $n = 20$ observations\n\n**Estimator 1:** Standard: $\\hat{p}_1 = \\frac{X}{n}$ (unbiased)\n\n**Estimator 2:** \"Add-two\": $\\hat{p}_2 = \\frac{X + 2}{n + 4}$ (biased toward 0.5)\n\n## Comparing Proportion Estimators: Theory\n\nFor $\\hat{p}_1 = X/n$ (standard):\n\n-   Bias = 0\n-   Variance = $\\frac{p(1-p)}{n}$\n-   MSE = $\\frac{p(1-p)}{n}$\n\n. . .\n\nFor $\\hat{p}_2 = \\frac{X+2}{n+4}$ (add-two):\n\n-   Bias = $\\frac{2 - 4p}{n+4}$\n-   Variance = $\\frac{np(1-p)}{(n+4)^2}$\n-   MSE = Variance + Bias²\n\n## Your Turn: Calculate Bias of Add-Two Estimator\n\n**Exercise:** For $n = 20$ and $p = 0.3$:\n\n1.  Calculate the bias of $\\hat{p}_2 = \\frac{X+2}{n+4}$\n\n**Hint:** $E(X) = np$ for binomial, so $E(\\hat{p}_2) = \\frac{np + 2}{n + 4}$\n\n. . .\n\n**Solution:**\n\n$$E(\\hat{p}_2) = \\frac{20(0.3) + 2}{24} = \\frac{8}{24} = 0.333$$\n\n$$\\text{Bias} = 0.333 - 0.3 = 0.033$$\n\nThe add-two estimator is biased **toward 0.5** (and 0.333 is closer to 0.5 than 0.3 is).\n\n## Simulation: Comparing the Estimators\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"|1-3|5-13|15-22\"}\ntrue_p <- 0.3\nn <- 20\nn_sims <- 10000\n\n# Simulate both estimators\ncomparison_sim <- tibble(sim = 1:n_sims) |> \n  mutate(\n    x = rbinom(n_sims, size = n, prob = true_p),\n    p_hat_standard = x / n,\n    p_hat_addtwo = (x + 2) / (n + 4)\n  )\n\n# Compare MSE\ncomparison_sim |> \n  summarize(\n    `Standard Bias` = mean(p_hat_standard) - true_p,\n    `Add-Two Bias` = mean(p_hat_addtwo) - true_p,\n    `Standard Variance` = var(p_hat_standard),\n    `Add-Two Variance` = var(p_hat_addtwo),\n    `Standard MSE` = mean((p_hat_standard - true_p)^2),\n    `Add-Two MSE` = mean((p_hat_addtwo - true_p)^2)\n  ) |> \n  pivot_longer(everything(), names_to = \"Metric\", values_to = \"Value\") |> \n  mutate(Value = round(Value, 5))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 2\n  Metric              Value\n  <chr>               <dbl>\n1 Standard Bias     0.00049\n2 Add-Two Bias      0.0337 \n3 Standard Variance 0.0104 \n4 Add-Two Variance  0.00722\n5 Standard MSE      0.0104 \n6 Add-Two MSE       0.00835\n```\n\n\n:::\n:::\n\n\n## Visualizing the Tradeoff\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02_Point_Estimation_files/figure-revealjs/unnamed-chunk-11-1.png){width=960}\n:::\n:::\n\n\n## MSE Comparison Across Different True Values\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02_Point_Estimation_files/figure-revealjs/unnamed-chunk-12-1.png){width=960}\n:::\n:::\n\n\n**Key Insight:** The \"best\" estimator depends on the true parameter value!\n\n## Medical Application: Disease Prevalence\n\n**Scenario:** Estimating prevalence of a rare disease ($p \\approx 0.05$) vs. a common condition ($p \\approx 0.5$).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compare MSE at different prevalence levels\nn <- 50\n\nmse_at_p <- function(p, n) {\n  mse_std <- p * (1-p) / n\n  bias_add2 <- (2 - 4*p) / (n + 4)\n  var_add2 <- n * p * (1-p) / (n + 4)^2\n  mse_add2 <- var_add2 + bias_add2^2\n  \n  tibble(p = p, MSE_Standard = mse_std, MSE_AddTwo = mse_add2,\n         Better = ifelse(mse_std < mse_add2, \"Standard\", \"Add-Two\"))\n}\n\nbind_rows(\n  mse_at_p(0.05, n),\n  mse_at_p(0.50, n)\n) |> \n  mutate(across(where(is.numeric), \\(x) round(x, 5)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n      p MSE_Standard MSE_AddTwo Better  \n  <dbl>        <dbl>      <dbl> <chr>   \n1  0.05      0.00095    0.00193 Standard\n2  0.5       0.005      0.00429 Add-Two \n```\n\n\n:::\n:::\n\n\n## Sample Variance: Why n-1?\n\nTwo formulas for sample variance:\n\n$$S^2 = \\frac{\\sum(X_i - \\bar{X})^2}{n-1} \\quad \\text{vs.} \\quad \\tilde{S}^2 = \\frac{\\sum(X_i - \\bar{X})^2}{n}$$\n\n. . .\n\n**Question:** Why do we divide by $n-1$ instead of $n$?\n\n**Answer:** Dividing by $n$ gives a biased estimator!\n\n## Simulation: Comparing Variance Estimators\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrue_variance <- 100  # σ² = 100\nn <- 10\nn_sims <- 10000\n\nvariance_sim <- tibble(sim = 1:n_sims) |> \n  mutate(\n    sample_data = map(sim, \\(s) rnorm(n, 0, sqrt(true_variance))),\n    s2_n_minus_1 = map_dbl(sample_data, var),\n    s2_n = map_dbl(sample_data, \\(x) sum((x - mean(x))^2) / n)\n  )\n\nvariance_sim |> \n  summarize(\n    `True σ²` = true_variance,\n    `E[S² with n-1]` = mean(s2_n_minus_1),\n    `E[S² with n]` = mean(s2_n),\n    `Bias (n-1)` = mean(s2_n_minus_1) - true_variance,\n    `Bias (n)` = mean(s2_n) - true_variance\n  ) |> \n  mutate(across(where(is.numeric), \\(x) round(x, 2)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 5\n  `True σ²` `E[S² with n-1]` `E[S² with n]` `Bias (n-1)` `Bias (n)`\n      <dbl>            <dbl>          <dbl>        <dbl>      <dbl>\n1       100             100.           90.1         0.15      -9.86\n```\n\n\n:::\n:::\n\n\n## Your Turn: Comprehensive Example\n\n**Exercise:** A clinical trial measures cholesterol reduction. Based on $n = 36$ patients:\n\n-   Sample mean: $\\bar{x} = 25$ mg/dL\n-   Sample SD: $s = 12$ mg/dL\n\nCalculate:\n\n1.  The estimated standard error of $\\bar{X}$\n2.  If the true mean reduction is $\\mu = 24$, and we repeated this trial many times, what would be the expected MSE of $\\bar{X}$?\n\n. . .\n\n**Solutions:**\n\n1.  $\\widehat{SE} = \\frac{12}{\\sqrt{36}} = 2$ mg/dL\n\n2.  $\\bar{X}$ is unbiased, so $\\text{MSE} = \\text{Var}(\\bar{X}) = \\frac{\\sigma^2}{n} \\approx \\frac{144}{36} = 4$\n\n# Summary and Looking Ahead {background-color=\"#3498db\"}\n\n## Putting It All Together: Estimator Summary\n\n| Property  | Formula                                 | Interpretation     |\n|-----------|-----------------------------------------|--------------------|\n| Bias      | $E(\\hat{\\theta}) - \\theta$              | Systematic error   |\n| Variance  | $E[(\\hat{\\theta} - E(\\hat{\\theta}))^2]$ | Random variability |\n| Std Error | $\\sqrt{\\text{Var}(\\hat{\\theta})}$       | Typical deviation  |\n| MSE       | $\\text{Var} + \\text{Bias}^2$            | Total error        |\n\n. . .\n\n::: callout-tip\n## Guidelines for Choosing Estimators\n\n1.  Unbiasedness is desirable but not always essential\n2.  Lower variance/SE means more precision\n3.  MSE provides a single criterion combining both\n4.  Sometimes biased estimators have lower MSE!\n:::\n\n## Lesson 2 Summary\n\n**Key Concepts:**\n\n1.  **Point Estimators:** Rules for calculating estimates from data\n    -   Estimator = random variable; estimate = specific value\n\n2.  **Bias:** $\\text{Bias}(\\hat{\\theta}) = E(\\hat{\\theta}) - \\theta$\n    -   Unbiased if $E(\\hat{\\theta}) = \\theta$\n    -   Can sometimes correct biased estimators\n\n3.  **Standard Error:** $SE(\\hat{\\theta}) = \\sqrt{\\text{Var}(\\hat{\\theta})}$\n    -   Measures precision of the estimator\n\n4.  **Mean Squared Error:** $\\text{MSE} = \\text{Var} + \\text{Bias}^2$\n    -   Captures total estimation error\n    -   Bias-variance tradeoff: sometimes biased is better!\n\n## Lesson 2 Practice Problems\n\n1.  For a Uniform\\[0, $\\theta$\\] distribution with $n = 20$ observations and $\\theta = 50$, calculate:\n    -   The expected value of the maximum\n    -   The bias of using the maximum as an estimator\n    -   The corrected unbiased estimator\n\n2.  Using simulation, compare the MSE of the standard proportion estimator vs. the add-two estimator for $n = 10$ and $p = 0.1, 0.3, 0.5$.\n\n3.  For a sample of size $n$ from Exponential($\\lambda$), the MLE is $\\hat{\\lambda} = 1/\\bar{X}$. It can be shown that $E(\\hat{\\lambda}) = \\frac{n}{n-1}\\lambda$. Calculate the bias and propose an unbiased estimator.\n\n## Next Week Preview\n\n**Week 2: Minimum Variance Unbiased Estimators**\n\n-   Among all unbiased estimators, which has smallest variance?\n-   The Cramér-Rao lower bound\n-   Efficiency of estimators\n-   Introduction to Maximum Likelihood Estimation\n\n## References\n\n::: nonincremental\n-   Devore, Berk, and Carlton. *Modern Mathematical Statistics with Applications* (Springer). Chapter 7.1\n-   Chihara and Hesterberg. *Mathematical Statistics with Resampling and R* (Wiley). Chapter 6.\n:::\n\n## Questions? {.center background-color=\"#3498db\"}\n\nThank you!\n",
    "supporting": [
      "02_Point_Estimation_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
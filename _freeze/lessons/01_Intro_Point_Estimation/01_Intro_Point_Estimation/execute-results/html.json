{
  "hash": "0f12811ebb6e21456cf54fea34826381",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"BSTA 551: Statistical Inference\"\nsubtitle: \"Lesson 1: Introduction to Point Estimation\"\nauthor: \"Jessica Minnier\"\ntitle-slide-attributes:\n    data-background-color: \"#006a4e\"\ndate: \"2026-01-05\"\nformat: \n  revealjs:\n    theme: \"../simple_NW.scss\"\n    chalkboard: true\n    scrollable: true\n    slide-number: true\n    show-slide-number: all\n    width: 1955\n    height: 1100\n    footer: Lesson 1 Slides\n    html-math-method: mathjax\n    highlight-style: atom-one\nexecute:\n  echo: true\n  warning: false\n  message: false\n---\n\n\n\n# Lesson 1: Mathematical Foundations & Introduction to Estimation\n\n## Welcome to Statistical Inference! {.center}\n\n**Course Focus:** How do we learn about populations from samples?\n\n::: incremental\n-   **Point Estimation** (Weeks 1-4): What's our best guess for a parameter?\n-   **Confidence Intervals** (Weeks 5-6): What's a plausible range?\n-   **Hypothesis Testing** (Weeks 7-9): Can we make decisions from data?\n-   **Two-Sample Methods** (Week 10): Comparing groups\n:::\n\n## Today's Goals\n\n::: incremental\n1.  Explain what an estimator is and how it differs from an estimate\n2.  Use R to simulate sampling distributions\n3.  Understand how optimization finds \"best\" values numerically\n4.  Define bias and calculate it for simple estimators\n:::\n\n## Motivating Example: Clinical Trial\n\nA pharmaceutical company is testing a new blood pressure medication.\n\n**The Question:** What is the true average reduction in systolic blood pressure?\n\n. . .\n\n**What we have:** Data from 25 patients in a trial\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Actual blood pressure reductions (mmHg) from 25 patients\nbp_reductions <- c(12, 8, 15, 10, 7, 14, 11, 9, 13, 16,\n                   8, 12, 10, 14, 11, 9, 15, 13, 7, 12,\n                   10, 8, 14, 11, 13)\n\n# What's our estimate of the true mean reduction?\nmean(bp_reductions)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 11.28\n```\n\n\n:::\n:::\n\n\n. . .\n\nBut how **reliable** is this estimate? Would we get the same answer with different patients?\n\n## The Big Picture: Population vs Sample\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](01_Intro_Point_Estimation_files/figure-revealjs/unnamed-chunk-2-1.png){width=960}\n:::\n:::\n\n\n::: lob\n**Key insight:** We use sample data to make inferences about population parameters.\n:::\n\n\n## Review: Expected Value\n\nThe **expected value** $E(X)$ is the long-run average of a random variable.\n\n::: callout-note\n## Key Properties We'll Use Today\n\n1.  $E(c) = c$ for any constant $c$\n2.  $E(cX) = c \\cdot E(X)$\n3.  $E(X + Y) = E(X) + E(Y)$\n4.  $E\\left(\\sum_{i=1}^n X_i\\right) = \\sum_{i=1}^n E(X_i)$\n:::\n\n## Worked Example: Expected Value of Sample Mean\n\n**Problem:** If $X_1, X_2, \\ldots, X_n$ are *iid* observations from a population with mean $\\mu$, what is $E(\\bar{X})$?\n\n. . .\n\n**Solution:** Let's work through this step by step.\n\n$$E(\\bar{X}) = E\\left(\\frac{1}{n}\\sum_{i=1}^n X_i\\right)$$\n\n. . .\n\n$$= \\frac{1}{n} E\\left(\\sum_{i=1}^n X_i\\right) \\quad \\text{(Property 2: constants come out)}$$\n\n. . .\n\n$$= \\frac{1}{n} \\sum_{i=1}^n E(X_i) \\quad \\text{(Property 4: sum of expectations)}$$\n\n. . .\n\n$$= \\frac{1}{n} \\cdot n\\mu = \\mu \\quad \\text{(Each } E(X_i) = \\mu \\text{)}$$\n\n## Your Turn: Calculate Expected Value\n\n**Exercise:** A hospital measures the recovery time (in days) for patients after surgery. Let $X_1, X_2, X_3$ be recovery times for 3 patients. The population mean recovery time is $\\mu = 5$ days.\n\n**Questions:**\n\n1.  What is $E(X_1)$?\n2.  What is $E(X_1 + X_2 + X_3)$?\n3.  What is $E(\\bar{X})$ where $\\bar{X} = \\frac{X_1 + X_2 + X_3}{3}$?\n\n. . .\n\n**Answers:**\n\n1.  $E(X_1) = \\mu = 5$ days\n2.  $E(X_1 + X_2 + X_3) = 5 + 5 + 5 = 15$ days\n3.  $E(\\bar{X}) = \\frac{15}{3} = 5$ days\n\n## Review: Variance\n\n**Variance** measures the spread of a distribution: $\\text{Var}(X) = E[(X - \\mu)^2]$\n\n::: callout-note\n## Key Properties We'll Use Today\n\n1.  $\\text{Var}(c) = 0$ for any constant\n2.  $\\text{Var}(cX) = c^2 \\cdot \\text{Var}(X)$\n3.  If $X$ and $Y$ are **independent**: $\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y)$\n:::\n\n## Worked Example: Variance of Sample Mean\n\n**Problem:** If $X_1, \\ldots, X_n$ are independent observations with variance $\\sigma^2$, what is $\\text{Var}(\\bar{X})$?\n\n. . .\n\n$$\\text{Var}(\\bar{X}) = \\text{Var}\\left(\\frac{1}{n}\\sum_{i=1}^n X_i\\right)$$\n\n. . .\n\n$$= \\frac{1}{n^2} \\text{Var}\\left(\\sum_{i=1}^n X_i\\right) \\quad \\text{(Property 2)}$$\n\n. . .\n\n$$= \\frac{1}{n^2} \\sum_{i=1}^n \\text{Var}(X_i) \\quad \\text{(Property 3: independence)}$$\n\n. . .\n\n$$= \\frac{1}{n^2} \\cdot n\\sigma^2 = \\frac{\\sigma^2}{n}$$\n\n::: callout-important\n## Key Result\n\nThe variance of $\\bar{X}$ **decreases** as sample size $n$ increases!\n:::\n\n## Simulation: Seeing Variance Decrease\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"|1-3|5-12|14-15\"}\n# Population parameters\ntrue_mean <- 120  # True mean systolic BP\ntrue_sd <- 15     # Population standard deviation\n\n# Simulate sample means for different sample sizes\nsimulation_data <- tibble(n = c(5, 25, 100)) |> \n  cross_join(tibble(sim = 1:2000)) |> \n  mutate(\n    sample_mean = map2_dbl(n, sim, \\(size, s) {\n      mean(rnorm(size, true_mean, true_sd))\n    })\n  )\n\n# Calculate observed standard deviation for each sample size\nsimulation_data |> \n  group_by(n) |> \n  summarize(\n    observed_sd = sd(sample_mean),\n    theoretical_sd = true_sd / sqrt(first(n))\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 3\n      n observed_sd theoretical_sd\n  <dbl>       <dbl>          <dbl>\n1     5        6.91           6.71\n2    25        3.01           3   \n3   100        1.50           1.5 \n```\n\n\n:::\n:::\n\n\n## Visualizing the Effect of Sample Size\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](01_Intro_Point_Estimation_files/figure-revealjs/unnamed-chunk-4-1.png){width=960}\n:::\n:::\n\n\n# Understanding Optimization {background-color=\"#2c3e50\"}\n\n## Why Optimization Matters in Statistics\n\nMany statistical methods require finding the \"best\" value of a parameter.\n\n**Examples:**\n\n-   **Maximum Likelihood:** Find the parameter value that makes the observed data most probable\n-   **Least Squares:** Find the parameter value that minimizes prediction errors\n-   **Minimum Variance:** Find the estimator with the smallest spread\n\n. . .\n\n**The Problem:** How do we find maximums and minimums without calculus?\n\n## Optimization: The Graphical Intuition\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](01_Intro_Point_Estimation_files/figure-revealjs/unnamed-chunk-5-1.png){width=960}\n:::\n:::\n\n\n**The maximum occurs where the function reaches its peak.**\n\n## Concrete Example: Finding the Best Estimate\n\n**Scenario:** In a clinical trial, 7 out of 10 patients respond to treatment. What's the best estimate of the true response rate $p$?\n\n. . .\n\n**Approach:** Find the value of $p$ that makes observing \"7 out of 10\" most likely.\n\nThe probability of observing exactly 7 successes in 10 trials is: $$P(X = 7) = \\binom{10}{7} p^7 (1-p)^3$$\n\n. . .\n\n**Question:** For what value of $p$ is this probability largest?\n\n## Grid Search: A Simple Numerical Approach\n\n**Idea:** Try many values and see which gives the largest result.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Try different values of p\ngrid_search <- tibble(p = seq(0.01, 0.99, by = 0.01)) |> \n  mutate(\n    likelihood = dbinom(7, size = 10, prob = p)\n  )\n\n# Find the maximum\ngrid_search |> \n  slice_max(likelihood, n = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 2\n      p likelihood\n  <dbl>      <dbl>\n1   0.7      0.267\n```\n\n\n:::\n:::\n\n\n. . .\n\nThe maximum likelihood estimate is $\\hat{p} = 0.70 = \\frac{7}{10}$. This makes intuitive sense!\n\n## Using R's Optimizer\n\nR has built-in functions to find maximums and minimums more precisely:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define the likelihood function\nlikelihood_function <- function(p) {\n  dbinom(7, size = 10, prob = p)\n}\n\n# Use optimize() to find the maximum\n# Note: optimize finds MINIMUM by default, so we negate for maximum\nresult <- optimize(\n  f = function(p) -likelihood_function(p),  # Negative to find max\n  interval = c(0, 1)                         # Search between 0 and 1\n)\n\n# The maximum occurs at:\ncat(\"Maximum likelihood estimate: p =\", result$minimum)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMaximum likelihood estimate: p = 0.6999843\n```\n\n\n:::\n:::\n\n\n## How Numerical Optimization Works\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](01_Intro_Point_Estimation_files/figure-revealjs/unnamed-chunk-8-1.png){width=960}\n:::\n:::\n\n\n**Key idea:** The algorithm evaluates the function at different points and iteratively narrows in on the maximum.\n\n## Your Turn: Numerical Optimization\n\n**Exercise:** A diagnostic test correctly identifies a disease in 18 out of 25 patients who have it. Find the maximum likelihood estimate for the test's sensitivity $p$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fill in the blanks:\nlikelihood_fn <- function(p) {\n  dbinom(___, size = ___, prob = p)  # What goes here?\n}\n\nresult <- optimize(\n  f = function(p) -likelihood_fn(p),\n  interval = c(0, 1)\n)\n\nresult$minimum  # This is the MLE\n```\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Solution:\nlikelihood_fn <- function(p) {\n  dbinom(18, size = 25, prob = p)\n}\n\nresult <- optimize(f = function(p) -likelihood_fn(p), interval = c(0, 1))\ncat(\"MLE of sensitivity:\", result$minimum)  # Should be 18/25 = 0.72\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMLE of sensitivity: 0.7200103\n```\n\n\n:::\n:::\n\n\n## When Optimization Gets Harder\n\nSometimes we need to optimize over multiple parameters or complex functions:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Example: Finding mean and SD that best fit data\npatient_data <- c(120, 135, 128, 142, 131, 125, 138, 129, 133, 127)\n\n# Negative log-likelihood for normal distribution\nneg_log_lik <- function(params) {\n  mu <- params[1]\n  sigma <- params[2]\n  if (sigma <= 0) return(Inf)  # sigma must be positive\n  -sum(dnorm(patient_data, mean = mu, sd = sigma, log = TRUE))\n}\n\n# Use optim() for multiple parameters\nresult <- optim(par = c(130, 10), fn = neg_log_lik)\ncat(\"MLE for mean:\", round(result$par[1], 2), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMLE for mean: 130.8 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"MLE for SD:\", round(result$par[2], 2), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMLE for SD: 6.13 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Compare to sample mean:\", round(mean(patient_data), 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCompare to sample mean: 130.8\n```\n\n\n:::\n:::\n\n\n## --- Break (15 minutes) --- {.center background-color=\"#95a5a6\"}\n\n# Point Estimation: Core Concepts {background-color=\"#27ae60\"}\n\n## What is a Point Estimator?\n\n::: callout-important\n## Definitions\n\n-   A **parameter** is a fixed (but unknown) characteristic of a population (e.g., $\\mu$, $\\sigma$, $p$)\n-   An **estimator** is a rule/formula for calculating an estimate from sample data\n-   An **estimate** is the actual number you calculate from a specific sample\n:::\n\n. . .\n\n**Example:**\n\n| Concept   | Symbol                          | Example                   |\n|-----------|---------------------------------|---------------------------|\n| Parameter | $\\mu$                           | True mean BP reduction    |\n| Estimator | $\\bar{X} = \\frac{1}{n}\\sum X_i$ | The formula \"sample mean\" |\n| Estimate  | $\\bar{x} = 11.2$                | The number from our data  |\n\n## The Sampling Distribution\n\nDifferent samples give different estimates. The **sampling distribution** describes this variability.\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n| Sample| Sample Mean (x̄)|\n|------:|---------------:|\n|      1|            9.90|\n|      2|           10.31|\n|      3|           10.03|\n|      4|           10.85|\n|      5|            9.17|\n|      6|            9.31|\n\n\n:::\n:::\n\n\n. . .\n\nEach sample gives a **different estimate**, but they cluster around the true value $\\mu = 10$.\n\n## Simulation: Building a Sampling Distribution\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\n# Parameters\ntrue_effect <- 10  # True mean BP reduction (mmHg)\ntrue_sd <- 4       # Standard deviation\nn_patients <- 25   # Patients per trial\nn_trials <- 5000   # Number of simulated trials\n\n# Simulate many clinical trials\nsampling_distribution <- tibble(trial = 1:n_trials) |> \n  mutate(\n    sample_mean = map_dbl(trial, \\(t) {\n      patients <- rnorm(n_patients, true_effect, true_sd)\n      mean(patients)\n    })\n  )\n\n# Visualize\nsampling_distribution |> \n  ggplot(aes(x = sample_mean)) +\n  geom_histogram(bins = 50, fill = \"steelblue\", alpha = 0.7, color = \"white\") +\n  geom_vline(xintercept = true_effect, color = \"red\", linewidth = 1.5) +\n  labs(title = \"Sampling Distribution of the Sample Mean\",\n       subtitle = str_glue(\"True μ = {true_effect}, n = {n_patients}, {n_trials} simulated trials\"),\n       x = \"Sample Mean (estimated BP reduction)\", y = \"Frequency\")\n```\n\n::: {.cell-output-display}\n![](01_Intro_Point_Estimation_files/figure-revealjs/unnamed-chunk-13-1.png){width=960}\n:::\n:::\n\n\n## What Makes a Good Estimator?\n\nWe want estimators that are:\n\n1.  **Accurate** (unbiased): On average, hits the true value\n2.  **Precise** (low variance): Estimates are clustered together\n3.  **Efficient**: Best combination of accuracy and precision\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](01_Intro_Point_Estimation_files/figure-revealjs/unnamed-chunk-14-1.png){width=960}\n:::\n:::\n\n\n## Bias: Formal Definition\n\n::: callout-important\n## Definition\n\nThe **bias** of an estimator $\\hat{\\theta}$ is: $$\\text{Bias}(\\hat{\\theta}) = E(\\hat{\\theta}) - \\theta$$\n\nAn estimator is **unbiased** if $\\text{Bias}(\\hat{\\theta}) = 0$, i.e., $E(\\hat{\\theta}) = \\theta$.\n:::\n\n. . .\n\n**Interpretation:**\n\n-   Bias measures systematic error\n-   Positive bias = tends to overestimate\n-   Negative bias = tends to underestimate\n-   Zero bias = correct on average\n\n## Worked Example: Proving Sample Mean is Unbiased\n\n**Claim:** The sample mean $\\bar{X}$ is an unbiased estimator of $\\mu$.\n\n**Proof:** We need to show $E(\\bar{X}) = \\mu$.\n\n. . .\n\n$$\\text{Bias}(\\bar{X}) = E(\\bar{X}) - \\mu$$\n\n. . .\n\nWe already showed that $E(\\bar{X}) = \\mu$, so:\n\n$$\\text{Bias}(\\bar{X}) = \\mu - \\mu = 0 \\checkmark$$\n\n. . .\n\n**Conclusion:** The sample mean is unbiased for estimating the population mean.\n\n## Worked Example: Sample Proportion\n\n**Setup:** In a vaccine trial, $X$ patients out of $n$ develop immunity. The estimator is $\\hat{p} = X/n$.\n\n**Claim:** $\\hat{p}$ is unbiased for the true immunity rate $p$.\n\n. . .\n\n**Proof:** Since $X \\sim \\text{Binomial}(n, p)$, we know $E(X) = np$.\n\n$$E(\\hat{p}) = E\\left(\\frac{X}{n}\\right) = \\frac{1}{n} E(X) = \\frac{1}{n} \\cdot np = p$$\n\n. . .\n\n$$\\text{Bias}(\\hat{p}) = E(\\hat{p}) - p = p - p = 0 \\checkmark$$\n\n## Concrete Calculation: Bias of Sample Proportion\n\n**Data:** In a study of 80 patients, 52 showed improvement.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 80\nx <- 52\np_hat <- x / n\n\ncat(\"Sample proportion:\", p_hat, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSample proportion: 0.65 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"If true p = 0.65, what is the bias of this single estimate?\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nIf true p = 0.65, what is the bias of this single estimate?\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Observed - True =\", p_hat - 0.65)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nObserved - True = 0\n```\n\n\n:::\n:::\n\n\n. . .\n\n**Important distinction:**\n\n-   A single estimate can be above or below the true value\n-   **Bias** refers to the average behavior across many samples\n-   An unbiased estimator still gives wrong answers for individual samples!\n\n## Simulation: Verifying Unbiasedness\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Verify that sample proportion is unbiased\ntrue_p <- 0.65\nn_patients <- 80\nn_simulations <- 10000\n\nproportion_simulation <- tibble(sim = 1:n_simulations) |> \n  mutate(\n    successes = rbinom(n_simulations, size = n_patients, prob = true_p),\n    p_hat = successes / n_patients\n  )\n\nproportion_simulation |> \n  summarize(\n    true_p = true_p,\n    mean_of_estimates = mean(p_hat),\n    empirical_bias = mean(p_hat) - true_p\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  true_p mean_of_estimates empirical_bias\n   <dbl>             <dbl>          <dbl>\n1   0.65             0.650      -0.000183\n```\n\n\n:::\n:::\n\n\nThe bias is essentially zero (just simulation noise)!\n\n## A Biased Estimator: The Maximum\n\n**Problem:** Estimate the upper bound $\\theta$ of a Uniform\\[0, $\\theta$\\] distribution.\n\n**Natural idea:** Use the largest observation: $\\hat{\\theta} = \\max(X_1, \\ldots, X_n)$\n\n. . .\n\n**Think about it:** Can this estimator ever *overestimate* $\\theta$?\n\n. . .\n\n**No!** The sample maximum is always ≤ the population maximum.\n\nThis means the estimator is **biased low**.\n\n## Calculating the Bias Mathematically\n\nFor $X_1, \\ldots, X_n \\sim \\text{Uniform}[0, \\theta]$, it can be shown that:\n\n$$E(\\max(X_1, \\ldots, X_n)) = \\frac{n}{n+1} \\theta$$\n\n. . .\n\n**Bias calculation:**\n\n$$\\text{Bias} = E(\\hat{\\theta}) - \\theta = \\frac{n}{n+1}\\theta - \\theta = -\\frac{\\theta}{n+1}$$\n\n. . .\n\n**Example:** If $\\theta = 10$ and $n = 5$:\n\n$$\\text{Bias} = -\\frac{10}{6} = -1.67$$\n\nThe estimator underestimates by about 1.67 on average.\n\n## Your Turn: Calculate Bias\n\n**Exercise:** A lab instrument has a maximum detection limit $\\theta$. We take $n = 9$ measurements from Uniform\\[0, $\\theta$\\] and use the maximum as our estimate.\n\n1.  If $\\theta = 100$, what is $E(\\hat{\\theta})$?\n2.  What is the bias?\n3.  By what percentage does this estimator underestimate on average?\n\n. . .\n\n**Solution:**\n\n1.  $E(\\hat{\\theta}) = \\frac{9}{10} \\times 100 = 90$\n2.  $\\text{Bias} = 90 - 100 = -10$\n3.  Underestimates by $\\frac{10}{100} = 10\\%$\n\n## Simulation: Visualizing the Biased Estimator\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\ntrue_theta <- 100\nn <- 9\nn_sims <- 5000\n\nmax_simulation <- tibble(sim = 1:n_sims) |> \n  mutate(\n    max_estimate = map_dbl(sim, \\(s) max(runif(n, 0, true_theta)))\n  )\n\n# Calculate empirical bias\nmax_simulation |> \n  summarize(\n    theoretical_E = n / (n + 1) * true_theta,\n    empirical_mean = mean(max_estimate),\n    theoretical_bias = -true_theta / (n + 1),\n    empirical_bias = mean(max_estimate) - true_theta\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 4\n  theoretical_E empirical_mean theoretical_bias empirical_bias\n          <dbl>          <dbl>            <dbl>          <dbl>\n1            90           90.3              -10          -9.70\n```\n\n\n:::\n:::\n\n\n## Visualizing the Biased Estimator\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](01_Intro_Point_Estimation_files/figure-revealjs/unnamed-chunk-18-1.png){width=960}\n:::\n:::\n\n\n## Correcting the Bias\n\n**Idea:** Multiply by a correction factor to \"un-bias\" the estimator.\n\nSince $E(\\max) = \\frac{n}{n+1}\\theta$, we can define:\n\n$$\\hat{\\theta}_{\\text{unbiased}} = \\frac{n+1}{n} \\cdot \\max(X_1, \\ldots, X_n)$$\n\n. . .\n\n**Check:**\n\n$$E(\\hat{\\theta}_{\\text{unbiased}}) = \\frac{n+1}{n} \\cdot E(\\max) = \\frac{n+1}{n} \\cdot \\frac{n}{n+1}\\theta = \\theta \\checkmark$$\n\n## Your Turn: Apply the Correction\n\n**Exercise:** Using the lab instrument example with $n = 9$ and $\\theta = 100$:\n\n1.  If you observe $\\max = 92$, what is the biased estimate?\n2.  What is the unbiased estimate?\n\n. . .\n\n**Solution:**\n\n1.  Biased estimate: $\\hat{\\theta}_b = 92$\n2.  Unbiased estimate: $\\hat{\\theta}_u = \\frac{10}{9} \\times 92 = 102.2$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nobserved_max <- 92\nn <- 9\n\nbiased_est <- observed_max\nunbiased_est <- (n + 1) / n * observed_max\n\ncat(\"Biased estimate:\", biased_est, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBiased estimate: 92 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Unbiased estimate:\", round(unbiased_est, 1))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nUnbiased estimate: 102.2\n```\n\n\n:::\n:::\n\n\n## Lesson 1 Summary\n\n**Key Concepts:**\n\n1.  **Expected Value:** $E(\\bar{X}) = \\mu$ (sample mean is centered at population mean)\n\n2.  **Variance:** $\\text{Var}(\\bar{X}) = \\sigma^2/n$ (precision improves with larger $n$)\n\n3.  **Optimization:** Finding maximum/minimum values numerically\n\n    -   Grid search: try many values\n    -   `optimize()`: efficient numerical search\n\n4.  **Bias:** $\\text{Bias}(\\hat{\\theta}) = E(\\hat{\\theta}) - \\theta$\n\n    -   Unbiased if $E(\\hat{\\theta}) = \\theta$\n    -   Can sometimes correct biased estimators\n\n## Lesson 1 Practice Problems\n\n1.  Calculate $E(\\bar{X})$ and $\\text{Var}(\\bar{X})$ for a sample of size $n = 16$ from a population with $\\mu = 50$ and $\\sigma = 12$.\n\n2.  Use `optimize()` to find the MLE for $p$ when you observe 23 successes in 40 trials.\n\n3.  For a Uniform\\[0, $\\theta$\\] distribution with $n = 20$ observations and $\\theta = 50$, calculate:\n\n    -   The expected value of the maximum\n    -   The bias of using the maximum as an estimator\n    -   The corrected unbiased estimator\n\n------------------------------------------------------------------------\n\n# Day 2: Evaluating Estimators {background-color=\"#8e44ad\"}\n\n## Review: Where We Left Off\n\n**Key concepts from Lesson 1:**\n\n-   Estimator vs. estimate\n-   Sampling distribution\n-   Bias: $\\text{Bias}(\\hat{\\theta}) = E(\\hat{\\theta}) - \\theta$\n-   Unbiased estimators: $E(\\hat{\\theta}) = \\theta$\n\n. . .\n\n**Today's Goals:**\n\n-   Standard error and precision\n-   Mean squared error (MSE)\n-   The bias-variance tradeoff\n-   Comparing estimators with simulations\n\n## Standard Error: Measuring Precision\n\n::: callout-important\n## Definition\n\nThe **standard error** of an estimator is its standard deviation: $$SE(\\hat{\\theta}) = \\sqrt{\\text{Var}(\\hat{\\theta})}$$\n:::\n\n. . .\n\n**Key Standard Errors:**\n\n| Estimator                   | Standard Error            |\n|-----------------------------|---------------------------|\n| Sample mean $\\bar{X}$       | $\\frac{\\sigma}{\\sqrt{n}}$ |\n| Sample proportion $\\hat{p}$ | $\\sqrt{\\frac{p(1-p)}{n}}$ |\n\n## Worked Example: Standard Error of Sample Mean\n\n**Problem:** In a blood pressure study, the population SD is $\\sigma = 15$ mmHg. Calculate the standard error of $\\bar{X}$ for sample sizes $n = 25$ and $n = 100$.\n\n. . .\n\n**Solution:**\n\nFor $n = 25$: $$SE(\\bar{X}) = \\frac{15}{\\sqrt{25}} = \\frac{15}{5} = 3 \\text{ mmHg}$$\n\nFor $n = 100$: $$SE(\\bar{X}) = \\frac{15}{\\sqrt{100}} = \\frac{15}{10} = 1.5 \\text{ mmHg}$$\n\n. . .\n\n**Interpretation:** With 100 patients, our estimate is twice as precise as with 25 patients.\n\n## Your Turn: Calculate Standard Error\n\n**Exercise:** A survey measures patient satisfaction on a 0-100 scale. The population standard deviation is $\\sigma = 20$.\n\n1.  What is the SE of $\\bar{X}$ for $n = 16$ patients?\n2.  What sample size is needed to achieve $SE = 2$?\n\n. . .\n\n**Solutions:**\n\n1.  $SE = \\frac{20}{\\sqrt{16}} = \\frac{20}{4} = 5$\n\n2.  We need $\\frac{20}{\\sqrt{n}} = 2$, so $\\sqrt{n} = 10$, thus $n = 100$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsigma <- 20\n# Problem 1\nse_n16 <- sigma / sqrt(16)\n# Problem 2\nn_needed <- (sigma / 2)^2\n\ncat(\"SE for n=16:\", se_n16, \"\\nSample size for SE=2:\", n_needed)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSE for n=16: 5 \nSample size for SE=2: 100\n```\n\n\n:::\n:::\n\n\n## The Problem with Unknown Parameters\n\n**Issue:** Standard errors often involve unknown parameters!\n\n-   SE of $\\bar{X}$ requires knowing $\\sigma$\n-   SE of $\\hat{p}$ requires knowing $p$\n\n. . .\n\n**Solution:** **Estimated standard error** — substitute estimates for unknown parameters\n\n$$\\widehat{SE}(\\bar{X}) = \\frac{s}{\\sqrt{n}}$$\n\n$$\\widehat{SE}(\\hat{p}) = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}$$\n\n## Example: Estimated Standard Error\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Blood pressure data from 25 patients\nbp_reductions <- c(12, 8, 15, 10, 7, 14, 11, 9, 13, 16,\n                   8, 12, 10, 14, 11, 9, 15, 13, 7, 12,\n                   10, 8, 14, 11, 13)\n\nn <- length(bp_reductions)\nx_bar <- mean(bp_reductions)\ns <- sd(bp_reductions)\n\n# Estimated standard error\nse_estimated <- s / sqrt(n)\n\ntibble(\n  Statistic = c(\"Sample Mean\", \"Sample SD\", \"Sample Size\", \"Estimated SE\"),\n  Value = c(x_bar, round(s, 2), n, round(se_estimated, 2))\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 2\n  Statistic    Value\n  <chr>        <dbl>\n1 Sample Mean  11.3 \n2 Sample SD     2.64\n3 Sample Size  25   \n4 Estimated SE  0.53\n```\n\n\n:::\n:::\n\n\n## Mean Squared Error: Combining Bias and Variance\n\nWhat if we have to choose between a biased estimator with low variance and an unbiased estimator with high variance?\n\n. . .\n\n::: callout-important\n## Definition: Mean Squared Error\n\n$$\\text{MSE}(\\hat{\\theta}) = E[(\\hat{\\theta} - \\theta)^2]$$\n\n**Key Formula:** $$\\text{MSE} = \\text{Variance} + \\text{Bias}^2$$\n:::\n\n. . .\n\n**MSE captures total error** — both systematic (bias) and random (variance).\n\n## Proving the MSE Formula\n\n$$\\text{MSE}(\\hat{\\theta}) = E[(\\hat{\\theta} - \\theta)^2]$$\n\n. . .\n\nAdd and subtract $E(\\hat{\\theta})$:\n\n$$= E[(\\hat{\\theta} - E(\\hat{\\theta}) + E(\\hat{\\theta}) - \\theta)^2]$$\n\n. . .\n\nExpand the square:\n\n$$= E[(\\hat{\\theta} - E(\\hat{\\theta}))^2] + 2E[(\\hat{\\theta} - E(\\hat{\\theta}))](E(\\hat{\\theta}) - \\theta) + (E(\\hat{\\theta}) - \\theta)^2$$\n\n. . .\n\nThe middle term equals zero because $E[\\hat{\\theta} - E(\\hat{\\theta})] = 0$:\n\n$$= \\text{Var}(\\hat{\\theta}) + [\\text{Bias}(\\hat{\\theta})]^2$$\n\n## Worked Example: Computing MSE\n\n**Setup:** Estimating $\\theta$ from Uniform\\[0, $\\theta$\\] with $n = 9$ and $\\theta = 100$.\n\n**Biased estimator:** $\\hat{\\theta}_b = \\max(X_i)$\n\nFrom theory:\n\n-   $E(\\hat{\\theta}_b) = \\frac{9}{10}(100) = 90$\n-   $\\text{Var}(\\hat{\\theta}_b) = \\frac{n \\theta^2}{(n+1)^2(n+2)} = \\frac{9 \\times 100^2}{100 \\times 11} = 81.82$\n\n. . .\n\n$$\\text{Bias} = 90 - 100 = -10$$ $$\\text{MSE} = 81.82 + (-10)^2 = 81.82 + 100 = 181.82$$\n\n## Your Turn: Calculate MSE\n\n**Exercise:** For the **unbiased** estimator $\\hat{\\theta}_u = \\frac{n+1}{n}\\max(X_i)$ with $n = 9$ and $\\theta = 100$:\n\n1.  What is the bias?\n2.  If $\\text{Var}(\\hat{\\theta}_u) = 101.01$, what is the MSE?\n3.  Which estimator has lower MSE: biased or unbiased?\n\n. . .\n\n**Solutions:**\n\n1.  Bias = 0 (it's unbiased!)\n2.  $\\text{MSE} = 101.01 + 0^2 = 101.01$\n3.  Unbiased has lower MSE (101.01 \\< 181.82)\n\nBut this isn't always the case!\n\n## --- Break (15 minutes) --- {.center background-color=\"#95a5a6\"}\n\n## The Bias-Variance Tradeoff\n\nSometimes a **biased** estimator has **lower MSE** than an unbiased one!\n\n. . .\n\n**Example:** Estimating a proportion $p$ with $n = 20$ observations\n\n**Estimator 1:** Standard: $\\hat{p}_1 = \\frac{X}{n}$ (unbiased)\n\n**Estimator 2:** \"Add-two\": $\\hat{p}_2 = \\frac{X + 2}{n + 4}$ (biased toward 0.5)\n\n## Comparing Proportion Estimators: Theory\n\nFor $\\hat{p}_1 = X/n$ (standard):\n\n-   Bias = 0\n-   Variance = $\\frac{p(1-p)}{n}$\n-   MSE = $\\frac{p(1-p)}{n}$\n\n. . .\n\nFor $\\hat{p}_2 = \\frac{X+2}{n+4}$ (add-two):\n\n-   Bias = $\\frac{2 - 4p}{n+4}$\n-   Variance = $\\frac{np(1-p)}{(n+4)^2}$\n-   MSE = Variance + Bias²\n\n## Your Turn: Calculate Bias of Add-Two Estimator\n\n**Exercise:** For $n = 20$ and $p = 0.3$:\n\n1.  Calculate the bias of $\\hat{p}_2 = \\frac{X+2}{n+4}$\n\n**Hint:** $E(X) = np$ for binomial, so $E(\\hat{p}_2) = \\frac{np + 2}{n + 4}$\n\n. . .\n\n**Solution:**\n\n$$E(\\hat{p}_2) = \\frac{20(0.3) + 2}{24} = \\frac{8}{24} = 0.333$$\n\n$$\\text{Bias} = 0.333 - 0.3 = 0.033$$\n\nThe add-two estimator is biased **toward 0.5** (and 0.333 is closer to 0.5 than 0.3 is).\n\n## Simulation: Comparing the Estimators\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"|1-3|5-13|15-22\"}\ntrue_p <- 0.3\nn <- 20\nn_sims <- 10000\n\n# Simulate both estimators\ncomparison_sim <- tibble(sim = 1:n_sims) |> \n  mutate(\n    x = rbinom(n_sims, size = n, prob = true_p),\n    p_hat_standard = x / n,\n    p_hat_addtwo = (x + 2) / (n + 4)\n  )\n\n# Compare MSE\ncomparison_sim |> \n  summarize(\n    `Standard Bias` = mean(p_hat_standard) - true_p,\n    `Add-Two Bias` = mean(p_hat_addtwo) - true_p,\n    `Standard Variance` = var(p_hat_standard),\n    `Add-Two Variance` = var(p_hat_addtwo),\n    `Standard MSE` = mean((p_hat_standard - true_p)^2),\n    `Add-Two MSE` = mean((p_hat_addtwo - true_p)^2)\n  ) |> \n  pivot_longer(everything(), names_to = \"Metric\", values_to = \"Value\") |> \n  mutate(Value = round(Value, 5))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 2\n  Metric              Value\n  <chr>               <dbl>\n1 Standard Bias     0.00049\n2 Add-Two Bias      0.0337 \n3 Standard Variance 0.0104 \n4 Add-Two Variance  0.00722\n5 Standard MSE      0.0104 \n6 Add-Two MSE       0.00835\n```\n\n\n:::\n:::\n\n\n## Visualizing the Tradeoff\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](01_Intro_Point_Estimation_files/figure-revealjs/unnamed-chunk-23-1.png){width=960}\n:::\n:::\n\n\n## MSE Comparison Across Different True Values\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](01_Intro_Point_Estimation_files/figure-revealjs/unnamed-chunk-24-1.png){width=960}\n:::\n:::\n\n\n**Key Insight:** The \"best\" estimator depends on the true parameter value!\n\n## Medical Application: Disease Prevalence\n\n**Scenario:** Estimating prevalence of a rare disease ($p \\approx 0.05$) vs. a common condition ($p \\approx 0.5$).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compare MSE at different prevalence levels\nn <- 50\n\nmse_at_p <- function(p, n) {\n  mse_std <- p * (1-p) / n\n  bias_add2 <- (2 - 4*p) / (n + 4)\n  var_add2 <- n * p * (1-p) / (n + 4)^2\n  mse_add2 <- var_add2 + bias_add2^2\n  \n  tibble(p = p, MSE_Standard = mse_std, MSE_AddTwo = mse_add2,\n         Better = ifelse(mse_std < mse_add2, \"Standard\", \"Add-Two\"))\n}\n\nbind_rows(\n  mse_at_p(0.05, n),\n  mse_at_p(0.50, n)\n) |> \n  mutate(across(where(is.numeric), \\(x) round(x, 5)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n      p MSE_Standard MSE_AddTwo Better  \n  <dbl>        <dbl>      <dbl> <chr>   \n1  0.05      0.00095    0.00193 Standard\n2  0.5       0.005      0.00429 Add-Two \n```\n\n\n:::\n:::\n\n\n## Sample Variance: Why n-1?\n\nTwo formulas for sample variance:\n\n$$S^2 = \\frac{\\sum(X_i - \\bar{X})^2}{n-1} \\quad \\text{vs.} \\quad \\tilde{S}^2 = \\frac{\\sum(X_i - \\bar{X})^2}{n}$$\n\n. . .\n\n**Question:** Why do we divide by $n-1$ instead of $n$?\n\n**Answer:** Dividing by $n$ gives a biased estimator!\n\n## Simulation: Comparing Variance Estimators\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrue_variance <- 100  # σ² = 100\nn <- 10\nn_sims <- 10000\n\nvariance_sim <- tibble(sim = 1:n_sims) |> \n  mutate(\n    sample_data = map(sim, \\(s) rnorm(n, 0, sqrt(true_variance))),\n    s2_n_minus_1 = map_dbl(sample_data, var),\n    s2_n = map_dbl(sample_data, \\(x) sum((x - mean(x))^2) / n)\n  )\n\nvariance_sim |> \n  summarize(\n    `True σ²` = true_variance,\n    `E[S² with n-1]` = mean(s2_n_minus_1),\n    `E[S² with n]` = mean(s2_n),\n    `Bias (n-1)` = mean(s2_n_minus_1) - true_variance,\n    `Bias (n)` = mean(s2_n) - true_variance\n  ) |> \n  mutate(across(where(is.numeric), \\(x) round(x, 2)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 5\n  `True σ²` `E[S² with n-1]` `E[S² with n]` `Bias (n-1)` `Bias (n)`\n      <dbl>            <dbl>          <dbl>        <dbl>      <dbl>\n1       100             100.           90.1         0.15      -9.86\n```\n\n\n:::\n:::\n\n\n## Your Turn: Comprehensive Example\n\n**Exercise:** A clinical trial measures cholesterol reduction. Based on $n = 36$ patients:\n\n-   Sample mean: $\\bar{x} = 25$ mg/dL\n-   Sample SD: $s = 12$ mg/dL\n\nCalculate:\n\n1.  The estimated standard error of $\\bar{X}$\n2.  If the true mean reduction is $\\mu = 24$, and we repeated this trial many times, what would be the expected MSE of $\\bar{X}$?\n\n. . .\n\n**Solutions:**\n\n1.  $\\widehat{SE} = \\frac{12}{\\sqrt{36}} = 2$ mg/dL\n\n2.  $\\bar{X}$ is unbiased, so $\\text{MSE} = \\text{Var}(\\bar{X}) = \\frac{\\sigma^2}{n} \\approx \\frac{144}{36} = 4$\n\n## Putting It All Together: Estimator Summary\n\n| Property  | Formula                                 | Interpretation     |\n|-----------|-----------------------------------------|--------------------|\n| Bias      | $E(\\hat{\\theta}) - \\theta$              | Systematic error   |\n| Variance  | $E[(\\hat{\\theta} - E(\\hat{\\theta}))^2]$ | Random variability |\n| Std Error | $\\sqrt{\\text{Var}(\\hat{\\theta})}$       | Typical deviation  |\n| MSE       | $\\text{Var} + \\text{Bias}^2$            | Total error        |\n\n. . .\n\n::: callout-tip\n## Guidelines for Choosing Estimators\n\n1.  Unbiasedness is desirable but not always essential\n2.  Lower variance/SE means more precision\n3.  MSE provides a single criterion combining both\n4.  Sometimes biased estimators have lower MSE!\n:::\n\n## Week 1 Summary\n\n**Day 1:**\n\n-   Population parameters vs. sample estimates\n-   Expected value and variance of $\\bar{X}$\n-   Numerical optimization (grid search, `optimize()`)\n-   Bias: definition, calculation, and correction\n\n**Day 2:**\n\n-   Standard error and estimated standard error\n-   Mean Squared Error = Variance + Bias²\n-   Bias-variance tradeoff\n-   No single \"best\" estimator for all situations\n\n## Homework Problems\n\n1.  **Bias Calculation:** For a sample of size $n$ from Exponential($\\lambda$), the MLE is $\\hat{\\lambda} = 1/\\bar{X}$. It can be shown that $E(\\hat{\\lambda}) = \\frac{n}{n-1}\\lambda$. Calculate the bias and propose an unbiased estimator.\n\n2.  **MSE Comparison:** Using simulation, compare the MSE of the standard proportion estimator vs. the add-two estimator for $n = 10$ and $p = 0.1, 0.3, 0.5$.\n\n3.  **Numerical Optimization:** Use `optimize()` to find the MLE when you observe $x_1 = 2.1, x_2 = 3.5, x_3 = 1.8, x_4 = 2.9$ from an Exponential($\\lambda$) distribution. The likelihood is $L(\\lambda) = \\lambda^4 e^{-\\lambda \\sum x_i}$.\n\n## Next Week Preview\n\n**Week 2: Minimum Variance Unbiased Estimators**\n\n-   Among all unbiased estimators, which has smallest variance?\n-   The Cramér-Rao lower bound\n-   Efficiency of estimators\n-   Introduction to Maximum Likelihood Estimation\n\n## References\n\n::: nonincremental\n-   Devore, Berk, and Carlton. *Modern Mathematical Statistics with Applications* (Springer). Chapter 7.1\n-   Chihara and Hesterberg. *Mathematical Statistics with Resampling and R* (Wiley). Chapter 6.\n:::\n\n## Questions? {.center background-color=\"#3498db\"}\n\nThank you!\n\nOffice hours: \\[Time/Location\\]\n\nCourse website: \\[URL\\]\n",
    "supporting": [
      "01_Intro_Point_Estimation_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
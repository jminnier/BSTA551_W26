{
  "hash": "17c92bf68254446d8ee6be2f2aaacc4d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"BSTA 551: Statistical Inference\"\nsubtitle: \"Lesson 4: Normal Sample Statistics and MVUE\"\nauthor: \"Jessica Minnier\"\ntitle-slide-attributes:\n    data-background-color: \"#006a4e\"\ndate: \"2026-01-14\"\nformat: \n  revealjs:\n    theme: \"../simple_NW.scss\"\n    chalkboard: true\n    scrollable: true\n    slide-number: true\n    show-slide-number: all\n    width: 1955\n    height: 1100\n    footer: Lesson 4\n    html-math-method: mathjax\n    highlight-style: atom-one\nexecute:\n  echo: true\n  warning: false\n  message: false\n---\n\n\n\n# Lesson 4: Normal Sample Statistics and MVUE\n\n## Review: Lesson 3 Key Results\n\n**From Lesson 3:**\n\n-   $\\chi^2_\\nu$ distribution: sum of squared standard normals\n-   $(n-1)S^2/\\sigma^2 \\sim \\chi^2_{n-1}$ for normal samples\n-   t distribution arises when estimating $\\sigma$ with $S$\n-   F distribution: ratio of independent chi-squared variables\n\n. . .\n\n**Today's Goals:**\n\n::: incremental\n1.  Explore key statistics for normal random samples\n2.  Understand the independence of $\\bar{X}$ and $S^2$\n3.  Construct confidence intervals for variance\n4.  Define minimum variance unbiased estimators (MVUE)\n5.  Understand consistency of estimators\n:::\n\n# Key Results for Normal Samples {background-color=\"#2c3e50\"}\n\n## Key Results for Normal Samples (Devore 6.4)\n\n::: callout-important\n## Fundamental Results\n\nIf $X_1, \\ldots, X_n$ is a random sample from $N(\\mu, \\sigma^2)$:\n\n1.  $\\bar{X} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right)$\n\n2.  $\\bar{X}$ and $S^2$ are **independent** random variables\n\n3.  $\\frac{(n-1)S^2}{\\sigma^2} \\sim \\chi^2_{n-1}$\n\n4.  $\\frac{\\bar{X} - \\mu}{S/\\sqrt{n}} \\sim t_{n-1}$\n:::\n\n. . .\n\nThe independence of $\\bar{X}$ and $S^2$ is remarkable and unique to normal distributions!\n\n## Simulation: Independence of Mean and Variance\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 20\nn_sims <- 2000\n\n# Generate samples and compute statistics\nsample_stats <- tibble(sim = 1:n_sims) |>\n  mutate(\n    data = map(sim, ~rnorm(n, mean = 50, sd = 10)),\n    x_bar = map_dbl(data, mean),\n    s_squared = map_dbl(data, var)\n  )\n\n# Check correlation\ncor_test <- cor.test(sample_stats$x_bar, sample_stats$s_squared)\n\nsample_stats |>\n  ggplot(aes(x = x_bar, y = s_squared)) +\n  geom_point(alpha = 0.3, color = \"steelblue\") +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  labs(\n    title = \"Sample Mean vs. Sample Variance (Normal Population)\",\n    subtitle = paste(\"Correlation:\", round(cor_test$estimate, 3), \n                     \"— These are independent!\"),\n    x = expression(bar(X)), y = expression(S^2)\n  )\n```\n\n::: {.cell-output-display}\n![](04_MVUE_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n## Medical Example: Cholesterol Study\n\nA study measures LDL cholesterol (mg/dL) in 20 patients on a new statin medication.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# LDL cholesterol levels\nldl_levels <- c(95, 108, 87, 112, 99, 105, 92, 118, 103, 89,\n                110, 96, 102, 88, 115, 94, 107, 100, 91, 106)\n\nn <- length(ldl_levels)\nx_bar <- mean(ldl_levels)\ns <- sd(ldl_levels)\n\n# Using the t distribution for inference\nt_crit <- qt(0.975, df = n - 1)\nmargin_error <- t_crit * s / sqrt(n)\n\ncat(\"Sample mean:\", round(x_bar, 2), \"mg/dL\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSample mean: 100.85 mg/dL\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Sample SD:\", round(s, 2), \"mg/dL\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSample SD: 9.24 mg/dL\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"95% CI for μ: [\", round(x_bar - margin_error, 2), \",\", \n    round(x_bar + margin_error, 2), \"]\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n95% CI for μ: [ 96.53 , 105.17 ]\n```\n\n\n:::\n:::\n\n\n. . .\n\nThe t-based confidence interval properly accounts for uncertainty in both $\\mu$ and $\\sigma$.\n\n## Confidence Interval for Population Variance\n\n::: callout-note\n## CI for $\\sigma^2$\n\nUsing $(n-1)S^2/\\sigma^2 \\sim \\chi^2_{n-1}$, a $100(1-\\alpha)\\%$ confidence interval for $\\sigma^2$ is:\n\n$$\\left(\\frac{(n-1)S^2}{\\chi^2_{\\alpha/2, n-1}}, \\frac{(n-1)S^2}{\\chi^2_{1-\\alpha/2, n-1}}\\right)$$\n:::\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 95% CI for variance in LDL example\nalpha <- 0.05\nchi_upper <- qchisq(1 - alpha/2, df = n - 1)\nchi_lower <- qchisq(alpha/2, df = n - 1)\n\nci_var_lower <- (n - 1) * s^2 / chi_upper\nci_var_upper <- (n - 1) * s^2 / chi_lower\n\ncat(\"95% CI for σ²: [\", round(ci_var_lower, 1), \",\", round(ci_var_upper, 1), \"]\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n95% CI for σ²: [ 49.4 , 182.2 ]\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"95% CI for σ:  [\", round(sqrt(ci_var_lower), 2), \",\", round(sqrt(ci_var_upper), 2), \"]\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n95% CI for σ:  [ 7.03 , 13.5 ]\n```\n\n\n:::\n:::\n\n\n## Your Turn: Confidence Interval for Variance\n\n**Exercise:** A clinical trial measures the time (in minutes) for a drug to take effect in 12 patients. The sample variance is $s^2 = 16$ minutes².\n\n1.  Construct a 90% confidence interval for the population variance $\\sigma^2$\n2.  What is the corresponding 90% CI for the population SD $\\sigma$?\n\n. . .\n\n**Solutions:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 12\ns_squared <- 16\nalpha <- 0.10\n\nchi_lower <- qchisq(alpha/2, df = n - 1)\nchi_upper <- qchisq(1 - alpha/2, df = n - 1)\n\nci_var <- c((n - 1) * s_squared / chi_upper, (n - 1) * s_squared / chi_lower)\nci_sd <- sqrt(ci_var)\n\ncat(\"90% CI for σ²:\", round(ci_var, 2), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n90% CI for σ²: 8.95 38.47 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"90% CI for σ:\", round(ci_sd, 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n90% CI for σ: 2.99 6.2\n```\n\n\n:::\n:::\n\n\n# Minimum Variance Unbiased Estimators {background-color=\"#27ae60\"}\n\n## Minimum Variance Unbiased Estimators (MVUE)\n\n::: callout-important\n## The MVUE Concept (Devore 7.1)\n\nGiven multiple unbiased estimators of a parameter $\\theta$, the **minimum variance unbiased estimator (MVUE)** is the one with the smallest variance.\n\nIf $\\hat{\\theta}_1$ and $\\hat{\\theta}_2$ are both unbiased for $\\theta$, prefer $\\hat{\\theta}_1$ if:\n$$\\text{Var}(\\hat{\\theta}_1) < \\text{Var}(\\hat{\\theta}_2)$$\n:::\n\n. . .\n\n**Why MVUE matters:**\n\n-   Unbiased: on average, we hit the target\n-   Minimum variance: our estimates are tightly clustered\n-   Best of both worlds!\n\n## Example: Two Unbiased Estimators of θ (Devore Example 7.10)\n\nFor a Uniform[0, θ] distribution:\n\n**Estimator 1:** $\\hat{\\theta}_u = \\frac{n+1}{n} \\cdot \\max(X_1, \\ldots, X_n)$\n\n-   Unbiased: $E(\\hat{\\theta}_u) = \\theta$\n-   Variance: $\\text{Var}(\\hat{\\theta}_u) = \\frac{\\theta^2}{n(n+2)}$\n\n**Estimator 2:** $\\hat{\\theta}_2 = 2\\bar{X}$\n\n-   Unbiased: $E(\\hat{\\theta}_2) = \\theta$  \n-   Variance: $\\text{Var}(\\hat{\\theta}_2) = \\frac{\\theta^2}{3n}$\n\n. . .\n\nWhich is better? Compare variances!\n\n## Comparing the Estimators\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Variance comparison\ncompare_vars <- function(n) {\n  tibble(\n    n = n,\n    var_theta_u = 1 / (n * (n + 2)),  # Variance of θ̂_u (θ² factor omitted)\n    var_theta_2 = 1 / (3 * n),         # Variance of 2X̄\n    ratio = var_theta_2 / var_theta_u,\n    better = ifelse(var_theta_u < var_theta_2, \"θ̂_u (max-based)\", \"2X̄\")\n  )\n}\n\nmap_dfr(c(5, 10, 20, 50), compare_vars) |>\n  mutate(across(where(is.numeric), ~round(., 4)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 5\n      n var_theta_u var_theta_2 ratio better         \n  <dbl>       <dbl>       <dbl> <dbl> <chr>          \n1     5      0.0286      0.0667  2.33 θ̂_u (max-based)\n2    10      0.0083      0.0333  4    θ̂_u (max-based)\n3    20      0.0023      0.0167  7.33 θ̂_u (max-based)\n4    50      0.0004      0.0067 17.3  θ̂_u (max-based)\n```\n\n\n:::\n:::\n\n\n. . .\n\nThe max-based estimator $\\hat{\\theta}_u$ is always more efficient (smaller variance)!\n\n## Simulation: Visualizing MVUE\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntheta <- 100\nn <- 10\nn_sims <- 5000\n\nestimator_comparison <- tibble(sim = 1:n_sims) |>\n  mutate(\n    sample_data = map(sim, ~runif(n, 0, theta)),\n    theta_u = map_dbl(sample_data, ~(n + 1)/n * max(.x)),\n    theta_2 = map_dbl(sample_data, ~2 * mean(.x))\n  )\n\nestimator_comparison |>\n  pivot_longer(cols = c(theta_u, theta_2), names_to = \"estimator\", values_to = \"estimate\") |>\n  mutate(estimator = recode(estimator, \n                            theta_u = \"θ̂_u = (n+1)/n × max\",\n                            theta_2 = \"θ̂_2 = 2X̄\")) |>\n  ggplot(aes(x = estimate, fill = estimator)) +\n  geom_histogram(bins = 50, alpha = 0.6, position = \"identity\", color = \"white\") +\n  geom_vline(xintercept = theta, color = \"red\", linewidth = 1.5, linetype = \"dashed\") +\n  labs(title = \"Comparing Two Unbiased Estimators of θ = 100\",\n       subtitle = \"θ̂_u has smaller variance (is the MVUE)\",\n       x = \"Estimate\", y = \"Frequency\", fill = \"Estimator\") +\n  scale_fill_brewer(palette = \"Set1\")\n```\n\n::: {.cell-output-display}\n![](04_MVUE_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n## The Fundamental MVUE Result for Normal Means\n\n::: callout-important\n## Theorem (Devore 7.1)\n\nLet $X_1, \\ldots, X_n$ be a random sample from a normal distribution with parameters $\\mu$ and $\\sigma$.\n\nThen $\\hat{\\mu} = \\bar{X}$ is the **MVUE** for $\\mu$.\n:::\n\n. . .\n\nThis means:\n\n-   Among ALL unbiased estimators of $\\mu$ (not just $\\bar{X}$, $\\tilde{X}$, trimmed means, etc.)\n-   The sample mean $\\bar{X}$ has the smallest possible variance\n-   No other unbiased estimator can do better!\n\n## Your Turn: MVUE Practice\n\n**Exercise:** A researcher measures reaction times (ms) in 16 healthy subjects:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreaction_times <- c(245, 267, 234, 289, 256, 278, 242, 271,\n                    263, 251, 284, 238, 275, 259, 247, 280)\n```\n:::\n\n\n1.  Calculate $\\bar{X}$ (the MVUE for $\\mu$)\n2.  Calculate the sample median $\\tilde{X}$\n3.  Both are unbiased for $\\mu$ if the population is symmetric. Based on our theorem, which should have smaller variance?\n\n. . .\n\n**Solutions:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncat(\"Sample mean (MVUE):\", round(mean(reaction_times), 2), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSample mean (MVUE): 261.19 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Sample median:\", round(median(reaction_times), 2), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSample median: 261 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"If normal population, the mean has smaller variance than any other unbiased estimator!\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nIf normal population, the mean has smaller variance than any other unbiased estimator!\n```\n\n\n:::\n:::\n\n\n## Properties of the Sample Variance\n\n::: callout-note\n## Key Properties of $S^2$\n\nFor a random sample from a normal population:\n\n1.  $E(S^2) = \\sigma^2$ (unbiased for $\\sigma^2$)\n\n2.  $\\text{Var}(S^2) = \\frac{2\\sigma^4}{n-1}$\n\n3.  As $n \\to \\infty$, $\\text{Var}(S^2) \\to 0$ (consistent estimator)\n:::\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Demonstration of variance of S² decreasing with n\ntibble(n = c(5, 10, 25, 50, 100)) |>\n  mutate(\n    var_s2 = 2 / (n - 1),  # Proportional to 2σ⁴/(n-1)\n    se_s2 = sqrt(var_s2)\n  ) |>\n  mutate(across(where(is.numeric), ~round(., 4)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 3\n      n var_s2 se_s2\n  <dbl>  <dbl> <dbl>\n1     5 0.5    0.707\n2    10 0.222  0.471\n3    25 0.0833 0.289\n4    50 0.0408 0.202\n5   100 0.0202 0.142\n```\n\n\n:::\n:::\n\n\n# Consistency of Estimators {background-color=\"#8e44ad\"}\n\n## Consistency of Estimators\n\n::: callout-important\n## Definition\n\nAn estimator $\\hat{\\theta}$ is **consistent** for $\\theta$ if:\n\n$$\\hat{\\theta} \\xrightarrow{P} \\theta \\text{ as } n \\to \\infty$$\n:::\n\n. . .\n\n**Key consistent estimators:**\n\n-   $\\bar{X}$ for $\\mu$\n-   $S^2$ for $\\sigma^2$  \n-   $\\hat{p}$ for population proportion $p$\n-   Sample quantiles for population quantiles\n\n## Consistency and MSE\n\n::: callout-important\n## MSE Criterion for Consistency\n\nAn estimator $\\hat{\\theta}$ is **consistent** if:\n\n$$\\text{MSE}(\\hat{\\theta}) \\to 0 \\text{ as } n \\to \\infty$$\n:::\n\n. . .\n\n**Why does this work?** Recall that $\\text{MSE} = \\text{Variance} + \\text{Bias}^2$.\n\nFor MSE → 0, we need BOTH:\n\n-   $\\text{Var}(\\hat{\\theta}) \\to 0$ (estimates become precise)\n-   $\\text{Bias}(\\hat{\\theta}) \\to 0$ (estimates become accurate)\n\n. . .\n\n**Example:** For $\\bar{X}$ estimating $\\mu$:\n\n-   $\\text{Bias}(\\bar{X}) = 0$ (always unbiased)\n-   $\\text{Var}(\\bar{X}) = \\sigma^2/n \\to 0$ as $n \\to \\infty$\n-   Therefore $\\text{MSE}(\\bar{X}) = \\sigma^2/n \\to 0$ ✓\n\n## Simulation: Consistency in Action\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrue_mean <- 50\ntrue_sd <- 10\nsample_sizes <- c(10, 25, 50, 100, 250, 500)\nn_sims <- 1000\n\nconsistency_demo <- map_dfr(sample_sizes, function(n) {\n  tibble(\n    n = n,\n    x_bar = map_dbl(1:n_sims, ~mean(rnorm(n, true_mean, true_sd)))\n  )\n})\n\nconsistency_demo |>\n  mutate(n = factor(n)) |>\n  ggplot(aes(x = x_bar, fill = n)) +\n  geom_histogram(bins = 40, alpha = 0.7, color = \"white\") +\n  facet_wrap(~n, scales = \"free_y\", labeller = label_both) +\n  geom_vline(xintercept = true_mean, color = \"red\", linewidth = 1, linetype = \"dashed\") +\n  labs(title = \"Consistency: Estimator Converges as Sample Size Increases\",\n       subtitle = \"Red dashed line: true mean (μ = 50)\",\n       x = expression(bar(X)), y = \"Frequency\") +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](04_MVUE_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n## Putting It All Together\n\n**Evaluating an Estimator:**\n\n1.  **Unbiased?** Does $E(\\hat{\\theta}) = \\theta$?\n2.  **Variance?** What is $\\text{Var}(\\hat{\\theta})$?\n3.  **MVUE?** Among unbiased estimators, does it have minimum variance?\n4.  **Consistent?** Does $\\hat{\\theta} \\to \\theta$ as $n \\to \\infty$?\n5.  **MSE?** What is $\\text{Var}(\\hat{\\theta}) + \\text{Bias}^2$?\n\n. . .\n\n::: callout-tip\n## Guidelines for Choosing Estimators\n\n1.  Unbiasedness is desirable but not always essential\n2.  Lower variance/SE means more precision\n3.  MSE provides a single criterion combining both\n4.  An estimator with MSE → 0 as n → ∞ is consistent\n:::\n\n# Summary and Looking Ahead {background-color=\"#3498db\"}\n\n## Lesson 4 Summary\n\n**Statistics for Normal Samples:**\n\n-   $\\bar{X}$ and $S^2$ are independent (unique to normal!)\n-   $(n-1)S^2/\\sigma^2 \\sim \\chi^2_{n-1}$\n-   Can construct CIs for both $\\mu$ (using t) and $\\sigma^2$ (using $\\chi^2$)\n\n**MVUE:**\n\n-   Among unbiased estimators, choose the one with smallest variance\n-   $\\bar{X}$ is the MVUE for $\\mu$ when sampling from normal distribution\n-   The max-based estimator is MVUE for uniform upper bound\n\n**Consistency:**\n\n-   Estimator converges to parameter as $n \\to \\infty$\n-   Equivalent to MSE → 0 as sample size increases\n\n## Week 2 Summary\n\n| Property | Formula | Interpretation |\n|----------|---------|----------------|\n| Bias | $E(\\hat{\\theta}) - \\theta$ | Systematic error |\n| Variance | $\\text{Var}(\\hat{\\theta})$ | Random variability |\n| MSE | $\\text{Var} + \\text{Bias}^2$ | Total error |\n| Consistency | $\\text{MSE} \\to 0$ | Convergence with n |\n\n. . .\n\n**Key distributions this week:**\n\n-   $\\chi^2_\\nu$: sum of squared normals, used for variance inference\n-   $t_\\nu$: used when $\\sigma$ is unknown\n-   $F_{\\nu_1, \\nu_2}$: ratio of chi-squared, used for comparing variances\n\n## Lesson 4 Practice Problems\n\n<!-- 1.  A sample of $n = 25$ from a normal distribution has $s^2 = 16$. Construct a 95% CI for $\\sigma^2$. -->\n\n1.  Using R, verify that as df increases, $t_{0.025, \\nu}$ approaches $z_{0.025} = 1.96$.\n\n2.  Simulate 10,000 samples of size $n = 15$ from Uniform[0, 100]. Compare the variances of $\\hat{\\theta}_u$ and $2\\bar{X}$. Which is the MVUE?\n\n3.  Show that the sample proportion $\\hat{p} = X/n$ from a Binomial$(n, p)$ distribution is consistent by showing its MSE → 0.\n\n## Next Week Preview\n\n**Week 3: Maximum Likelihood Estimation**\n\n-   The likelihood function and likelihood principle\n-   Finding MLEs analytically and numerically  \n-   Method of moments estimation\n-   Properties of maximum likelihood estimators\n\n. . .\n\n**Sneak peek:** The MLE often *is* the MVUE!\n\n## References\n\n::: nonincremental\n-   Devore, Berk, and Carlton. *Modern Mathematical Statistics with Applications* (Springer). Chapters 6.3, 6.4, 7.1\n-   Chihara and Hesterberg. *Mathematical Statistics with Resampling and R* (Wiley). Chapter 6, Appendix B.\n:::\n\n## Questions? {.center background-color=\"#3498db\"}\n\nThank you!\n",
    "supporting": [
      "04_MVUE_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
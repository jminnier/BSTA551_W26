{
  "hash": "001429ad4ee21bc76ca5b435264a5136",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lesson 15: Some Important Discrete RVs\"\nauthor: \"Meike Niederhausen and Nicky Wakim\"\ntitle-slide-attributes:\n    data-background-color: \"#006a4e\"\ndate: \"2025-11-17\"\nformat: \n  revealjs:\n    theme: \"../simple_NW.scss\"\n    slide-number: true\n    show-slide-number: all\n    width: 1955\n    height: 1100\n    footer: Lesson 15 Slides\n    html-math-method: mathjax\neditor: \n  markdown: \n    wrap: 72\n---\n\n\n\n# Learning Objectives\n\n1.  Distinguish between Bernoulli, Binomial, Geometric, Hypergeometric,\n    Discrete Uniform, Negative Binomial, and Poisson distributions when\n    reading a world problem.\n2.  Identify the variable and the parameters in a world problem, and\n    state what the variable and parameters mean.\n3.  Use the formulas for the pmf/CDF, expected value, and variance to\n    answer questions and find probabilities.\n\n## Where are we?\n\n![](../img_slides/course_map.png){fig-align=\"center\"}\n\n# Bernoulli RVs\n\n## Properties of Bernoulli RVs\n\n-   **Scenario:** One trial, with outcome success or failure\n-   Shorthand: $X \\sim \\text{Bernoulli}(p)$\n\n$$\nX = \\left\\{\n        \\begin{array}{ll}\n            1 & \\quad \\mathrm{with\\ probability}\\ p \\quad \\\\\n            0 & \\quad \\mathrm{with\\ probability}\\ 1-p \\quad\n        \\end{array}\n    \\right.\n$$\n\n$$\np_X(x) = P(X=x) = p^x(1-p)^{1-x} \\text{ for } x=0,1\n$$\n\n$$\\text{E}(X) = p$$\n\n$$\\text{Var}(X) = pq = p(1-p)$$\n\n## Bernoulli Example 1\n\n::::::: columns\n:::::: {.column width=\"35%\"}\n::::: example\n::: ex-title\nExample 1\n:::\n\n::: ex-cont\n-   We roll a fair 6-sided die.\n\n-   We get \\$1 if we roll a 5, and nothing otherwise.\n\n-   Let $X$ be how much money we get.\n\n-   Find the mean and variance of $X$.\n:::\n:::::\n::::::\n:::::::\n\n# Binomial RVs {#chapter-15-binomial-RVs .unnumbered}\n\n## Properties of Binomial RVs\n\n-   **Scenario:** There are $n$ independent trials, each resulting in a\n    success or failure, with constant probability, $p$, in each trial.\n    We are counting the number of successes (or failures).\n\n-   Shorthand: $X \\sim \\text{Binomial}(n, p)$\n\n$$\nX = \\text{Number of successes of } n \\text{ independent trials}\n$$\n\n$$\np_X(x) = P(X=x) = {n \\choose x}p^x(1-p)^{n-x} \\text{ for } x=0,1,2,  ..., n\n$$\n\n$$\\text{E}(X) = np$$ $$\\text{Var}(X) = npq = np(1-p)$$\n\n## Our beloved fair-sided die\n\n::::::: columns\n:::::: {.column width=\"35%\"}\n::::: example\n::: ex-title\nExample 2\n:::\n\n::: ex-cont\n-   Suppose we roll a fair 6-sided die 50 times.\n\n-   We get \\$1 every time we roll a 5, and nothing otherwise.\n\n-   Let $X$ be how much money we get on the 50 rolls.\n\n-   Find the mean and variance of $X$.\n:::\n:::::\n::::::\n:::::::\n\n# Geometric RVs {#chapter-16-geometric-RVs .unnumbered}\n\n## Geometric RVs\n\n-   **Scenario:** There are repeated independent trials, each resulting\n    in a success or failure, with constant probability of success for\n    each trial. We are counting the number of trials until the first\n    success.\n\n-   Shorthand: $X \\sim \\text{Geo}(p)$ or $X \\sim \\text{Geometric}(p)$ or\n    $X \\sim \\text{G}(p)$\n\n+---------------------------------+---------------------------------+\n| $X =$ Number of trials needed   | $X =$ Number of failures before |\n| for first success (count $x$    | first success (count $x$ *does  |\n| *includes* the success)         | not include* the success)       |\n+:===============================:+:===============================:+\n| \\$p \\_                          | \\$ p \\_X (x)= P(X=x) =          |\n|                                 | (1-p)\\^{x}p\\$                   |\n| X( x ) = P(X=x) =               |                                 |\n| (1-p)\\^{x-1}p\\$                 | for $x=0, 1,2,...$              |\n|                                 |                                 |\n| for $x=1,2, 3,...$              | \\$\\$F_X ( x )                   |\n|                                 |                                 |\n| \\$\\$F\\_ X ( x                   | = P(X\\leq x) =                  |\n|                                 | 1-(1-p)\\^{x+1}\\$\\$              |\n| ) = P(X\\leq x) = 1-(1-p)\\^x\\$\\$ |                                 |\n|                                 | for $x=0, 1,2,...$              |\n| for $x=1,2, 3,...$              |                                 |\n+---------------------------------+---------------------------------+\n| $E(X)=\\dfrac{1}{p}$             | $E(X)=\\dfrac{1-p}{p}$           |\n|                                 |                                 |\n| $Var(X)= \\dfrac{1-p}{p^2}$      | $Var(X) = \\dfrac{1-p}{p^2}$     |\n+---------------------------------+---------------------------------+\n\n## Bullseye (1/4)\n\n::::::: columns\n:::::: {.column width=\"100%\"}\n::::: example\n::: ex-title\nExample 3\n:::\n\n::: ex-cont\nWe throw darts at a dartboard until we hit the bullseye. Assume throws\nare independent and the probability of hitting the bullseye is 0.01 for\neach throw.\n\n1.  What is the pmf for the number of throws needed to hit the bullseye?\n\n2.  What are the mean and variance for the number of throws needed to\n    hit the bullseye?\n\n3.  Find the probability that our first bullseye:\n\n    -   is on one of the first fifty tries\n\n    -   is after the $50^{th}$ try, given that it did not happen on the\n        first 20 tries\n:::\n:::::\n::::::\n:::::::\n\n## Bullseye (2/4)\n\n::::::: columns\n:::::: {.column width=\"35%\"}\n::::: example\n::: ex-title\nExample 3\n:::\n\n::: ex-cont\nWe throw darts at a dartboard until we hit the bullseye. Assume throws\nare independent and the probability of hitting the bullseye is 0.01 for\neach throw.\n\n1.  What is the pmf for the number of throws needed to hit the bullseye?\n:::\n:::::\n::::::\n:::::::\n\n## Bullseye (3/4)\n\n::::::: columns\n:::::: {.column width=\"35%\"}\n::::: example\n::: ex-title\nExample 3\n:::\n\n::: ex-cont\nWe throw darts at a dartboard until we hit the bullseye. Assume throws\nare independent and the probability of hitting the bullseye is 0.01 for\neach throw.\n\n2.  What are the mean and variance for the number of throws needed to\n    hit the bullseye?\n:::\n:::::\n::::::\n:::::::\n\n## Bullseye (4/4)\n\n::::::: columns\n:::::: {.column width=\"35%\"}\n::::: example\n::: ex-title\nExample 3\n:::\n\n::: ex-cont\nWe throw darts at a dartboard until we hit the bullseye. Assume throws\nare independent and the probability of hitting the bullseye is 0.01 for\neach throw.\n\n3.  Find the probability that our first bullseye:\n\n    -   is on one of the first fifty tries\n\n    -   is after the $50^{th}$ try, given that it did not happen on the\n        first 20 tries\n:::\n:::::\n::::::\n:::::::\n\n## Memoryless property for Geometric RVs\n\nIf we know $X$ is greater than some number (aka given $X >j$), then the\nprobability of $X > k+j$ is just the probability that $X>k$.\n\nÂ \n\n$P(X > k+j |X > j) = P(X > k)$\n\n$$ P(X > k+j |X > j) = \\dfrac{P(X>k+j \\text{ and } X>j)}{P(X>j)} = \\dfrac{P(X>k+j)}{P(X>j)} = \\dfrac{(1-p)^{k+j}}{(1-p)^{j}} = (1-p)^{k} $$\n\n# Negative Binomial RVs\n\n## Properties of Negative Binomial RVs\n\n-   **Scenario:** There are repeated independent trials, each resulting\n    in a success or failure, with constant probability of success for\n    each trial. We are counting the number of trials until the $r^{th}$\n    success.\n-   Shorthand: $X \\sim \\text{NegBin}(p, r)$ or $X \\sim \\text{NB}(p, r)$\n-   Negative binomial is sum of $r$ geometric distributions\n\n$$\nX = \\text{Number of independent trials until } r^{th} \\text{ success}\n$$\n\n$$ p_X(x) = P(X=x) = {x-1 \\choose r-1}(1-p)^{x-r}p^r \\text{ for } x = r, r+1, r+2, ...$$\n\n$$ E(X) = \\dfrac{r}{p}$$\n\n$$Var(X) = \\dfrac{rq}{p^2} = \\dfrac{r(1-p)}{p^2}$$\n\n## Hitting more than 1 bullseye\n\n::::::: columns\n:::::: {.column width=\"35%\"}\n::::: example\n::: ex-title\nExample 1\n:::\n\n::: ex-cont\nConsider again the bullseye example, where we throw darts at a dartboard\nuntil we hit the bullseye. Assume throws are independent and the\nprobability of hitting the bullseye is 0.01 for each throw.\n\n1.  What is the expected value and variance of the number of throws\n    needed to hit 5 bullseyes?\n:::\n:::::\n::::::\n:::::::\n\n## Hitting more than 1 bullseye\n\n::::::: columns\n:::::: {.column width=\"35%\"}\n::::: example\n::: ex-title\nExample 1\n:::\n\n::: ex-cont\nConsider again the bullseye example, where we throw darts at a dartboard\nuntil we hit the bullseye. Assume throws are independent and the\nprobability of hitting the bullseye is 0.01 for each throw.\n\n2.  What is the probability that the $5^{th}$ bullseye is on the\n    $20^{th}$ throw?\n:::\n:::::\n::::::\n:::::::\n\n# 5 minute break\n\n# Poisson RVs\n\n## Properties of Poisson RVs\n\n-   **Scenario:** We are counting the number of successes in a fixed\n    time period (or fixed space), which has a constant rate ($\\lambda$)\n    of successes\n-   Shorthand: $X \\sim \\text{Poisson}(\\lambda)$ or\n    $X \\sim \\text{Pois}(\\lambda)$\n\n$$\nX = \\text{Number of successes in a given period}\n$$\n\n$$ p_X(x) = P(X=x) = \\dfrac{e^{-\\lambda}\\lambda^x}{x!} \\text{ for } x = 0, 1, 2,3, ...$$\n\n$$ \\text{E}(X) = \\lambda$$\n\n$$\\text{Var}(X) = \\lambda$$\n\n## Distinguishing between Binomial and Poisson RVs\n\n-   Recall that if $X\\sim \\text{Binomial}(n,p)$, then\n\n    -   $X$ models the number of successes ...\n\n    -   in $n$ independent (Bernoulli) trials ...\n\n    -   that each have the same probability of success $p$.\n\n-   Poisson RV's are similar,\n\n    -   except that instead of having $n$ discrete independent trials,\n\n    -   there is a **fixed time period** (or space) during which the\n        successes happen\n\n## Examples of Poisson RVs\n\n-   Number of visitors to an emergency room in an hour during a weekend\n    night\n\n-   Number of study participants enrolled in a study per week\n\n-   Number of pedestrians walking through a square mile\n\n-   Any more?\n\n## Emergency Room Visitors\n\n::::::: columns\n:::::: {.column width=\"35%\"}\n::::: example\n::: ex-title\nExample 1\n:::\n\n::: ex-cont\nSuppose an emergency room has an average of 50 visitors per day. Find\nthe following probabilities.\n\n1.  Probability of 30 visitors in a day.\n\n2.  Probability of 8 visitors in an hour.\n\n3.  Probability of at least 8 visitors in an hour.\n:::\n:::::\n::::::\n:::::::\n\n## Combining independent Poisson distributions\n\n::::: theorem\n::: thm-title\nTheorem 1\n:::\n\n::: thm-cont\nIf $X\\sim Pois(\\lambda_1)$ and $Y\\sim Pois(\\lambda_2)$ are independent\nof each other, then $Z=X+Y\\sim Pois(\\lambda_1 + \\lambda_2)$.\n:::\n:::::\n\n## Two emergency rooms\n\n::::::: columns\n:::::: {.column width=\"35%\"}\n::::: example\n::: ex-title\nExample 2\n:::\n\n::: ex-cont\nSuppose emergency room 1 has an average of 50 visitors per day, and\nemergency room 2 has an average of 70 visitors per day, independently of\neach other. What is the probability distribution to model of the total\nnumber of visitors to both?\n:::\n:::::\n::::::\n:::::::\n\n## Poisson Approximation of the Binomial\n\nBoth Poisson and Binomial RV's are counting the number of successes\n\n-   If for a Binomial RV\n\n    -   the number of trials $n$ is very large, and\n\n    -   the probability of success $p$ is close to 0 or 1,\n\n-   Then the Poisson distribution can be used to approximate Binomial\n    probabilities\n\n    -   and we use $\\lambda = np$\n\n-   **Rule of thumb:** We can use the Poisson approximation when\n    $\\dfrac{1}{10} \\leq np(1-p) \\leq 10$\n\n## Medical lab errors\n\n:::::::: columns\n:::::: {.column width=\"35%\"}\n::::: example\n::: ex-title\nExample 3\n:::\n\n::: ex-cont\nSuppose that in the long run, errors in a medical testing lab are made\n0.1% of the time. Find the probability that fewer than 4 mistakes are\nmade in the next 2,000 tests.\n\n1.  Find the probability using the Binomial distribution.\n\n2.  Approximate the probability in part (1) using the Poisson\n    distribution.\n:::\n:::::\n::::::\n\n::: {.column width=\"60%\"}\nTo do for extra practice - will also see a similar problem in BSTA 511\n:::\n::::::::\n\n# Hypergeometric RVs {#chapter-19-hypergeometric-RVs .unnumbered}\n\n## Hypergeometric RVs\n\n-   **Scenario:** There are a fixed number of successes and failures\n    (which are known in advance), from which we make $n$ draws without\n    replacement. We are counting the number of successes from the $n$\n    trials.\n    -   There is a finite population of $N$ items\n    -   Each item in the population is either a success or a failure,\n        and there are $M$ successes total.\n    -   We randomly select (sample) $n$ items from the population\n        without replacement\n-   Shorthand: $X \\sim \\text{Hypergeo}(M, N, n)$\n\n::::: columns\n::: column\n$$\nX = \\text{Number of successes in } n \\text{ draws}\n$$\n\n$$\np_X(x) = P(X=x) = \\dfrac{{M \\choose x}{N-M \\choose n-x}}{{N \\choose n}} \n$$\n$$\\text{ for } x \\text{ integer-valued } \\\\ \\max(0, n-(N-M)) \\leq x \\leq \\min(n, M)$$\n:::\n\n::: column\n$$\\text{E}(X) =\\dfrac{nM}{N}$$\n\n$$\\text{Var}(X) = n \\dfrac{M}{N} \\bigg(1- \\dfrac{M}{N} \\bigg)\\bigg(\\dfrac{N-n}{N-1} \\bigg)$$\n:::\n:::::\n\n## Wolf population\n\n::::::: columns\n:::::: {.column width=\"35%\"}\n::::: example\n::: ex-title\nExample 4\n:::\n\n::: ex-cont\nA wildlife biologist is using mark-recapture to research a wolf\npopulation. Suppose a specific study region is known to have 24 wolves,\nof which 11 have already been tagged. If 5 wolves are randomly captured,\nwhat is the probability that 3 of them have already been tagged?\n:::\n:::::\n::::::\n:::::::\n\n## Binomial approximation of the hypergeometric RV\n\nSuppose a hypergeometric RV $X$ has the following properties:\n\n-   the population size $N$ is really big,\n\n-   the number of successes $M$ in the population is relatively large,\n\n    -   $\\frac{M}{N}$ shouldn't be close to 0 or 1\n\n-   and the number of items $n$ selected is small\n\n-   **Rule of thumb:** $\\dfrac{n}{N}<0.05$ or $N>20n$\n\nThen, in this case, making $n$ draws from the population doesn't change\nthe probability of success much, and the **hypergeometric RV. can be\napproximated by a binomial RV**\n\n## Wolf population revisited\n\n::::::: columns\n:::::: {.column width=\"28%\"}\n::::: example\n::: ex-title\nExample 5\n:::\n\n::: ex-cont\nSuppose a specific study region is known to have 2400 wolves, of which\n1100 have already been tagged.\n\n1.  If 50 wolves are randomly captured, what is the probability that 20\n    of them have already been tagged?\n\n2.  Approximate the probability in part (1) using the binomial\n    distribution.\n:::\n:::::\n::::::\n:::::::\n\n# Discrete Uniform RVs {#chapter-20-discrete-uniform-RVs .unnumbered}\n\n## Discrete Uniform RVs\n\n-   **Scenario:** There are $N$ possible outcomes, which are all equally\n    likely.\n-   Shorthand: $X \\sim \\text{Uniform}(N)$\n\n$$\nX = \\text{Outcome of interest, with } x=1, 2, ..., N\n$$\n\n$$\np_X(x) = P(X=x) = \\dfrac{1}{N} \\text{ for } x=1, 2, 3, ..., N\n$$\n\n$$\\text{E}(X) =\\dfrac{N+1}{2}$$\n\n$$\\text{Var}(X) = \\dfrac{N^2 -1}{12}$$\n\n## What discrete uniform RVs have we seen already?\n\n::::::: columns\n:::::: {.column width=\"35%\"}\n::::: example\n::: ex-title\nExample 6\n:::\n\n::: ex-cont\nExamples of discrete uniform RVs\n:::\n:::::\n::::::\n:::::::\n\n## All the R code for these!\n\n[Check out this page with all the different functions for distributions\nin R](https://www.stat.umn.edu/geyer/old/5101/rlook.html)\n\nExample of R commands for hypergeometric distribution with their\n[input]{style=\"color:#BF396F\"} and [output]{style=\"color:#367B79\"}:\n\n+----------------------------------+----------------------------------+\n| R code                           | What does it return?             |\n+==================================+==================================+\n| `rhyper()`                       | returns [sample of random        |\n|                                  | v                                |\n|                                  | ariables]{style=\"color:#367B79\"} |\n|                                  | with [specified                  |\n|                                  | dist                             |\n|                                  | ribution]{style=\"color:#BF396F\"} |\n+----------------------------------+----------------------------------+\n| `dhyper()`                       | returns [value of probability    |\n|                                  | density]{style=\"color:#367B79\"}  |\n|                                  | at [certain point of the         |\n|                                  | dist                             |\n|                                  | ribution]{style=\"color:#BF396F\"} |\n+----------------------------------+----------------------------------+\n| `phyper()`                       | returns [cumulative              |\n|                                  | pro                              |\n|                                  | bability]{style=\"color:#367B79\"} |\n|                                  | of getting [certain point (or    |\n|                                  | less) of the normal              |\n|                                  | dist                             |\n|                                  | ribution]{style=\"color:#BF396F\"} |\n+----------------------------------+----------------------------------+\n| `qhyper()`                       | returns [inverse                 |\n|                                  | CDF]{style=\"color:#367B79\"}      |\n|                                  | corresponding to [desired        |\n|                                  | quantile]{style=\"color:#BF396F\"} |\n+----------------------------------+----------------------------------+\n",
    "supporting": [
      "15_Discrete_RVs_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
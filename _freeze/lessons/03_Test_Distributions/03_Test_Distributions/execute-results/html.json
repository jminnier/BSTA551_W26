{
  "hash": "e067b44893396b1d11f701c5f9aaa72e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"BSTA 551: Statistical Inference\"\nsubtitle: \"Lesson 3: The Chi-Squared, t, and F Distributions\"\nauthor: \"Jessica Minnier\"\ntitle-slide-attributes:\n    data-background-color: \"#006a4e\"\ndate: \"2026-01-12\"\nformat: \n  revealjs:\n    theme: \"../simple_NW.scss\"\n    chalkboard: true\n    scrollable: true\n    slide-number: true\n    show-slide-number: all\n    width: 1955\n    height: 1100\n    footer: Lesson 3\n    html-math-method: mathjax\n    highlight-style: atom-one\nexecute:\n  echo: true\n  warning: false\n  message: false\n---\n\n\n\n# Lesson 3: The Chi-Squared, t, and F Distributions\n\n## Review: Where We Left Off\n\n**Key concepts from Lessons 1-2:**\n\n-   Estimator vs. estimate; parameters vs. statistics\n-   Bias: $\\text{Bias}(\\hat{\\theta}) = E(\\hat{\\theta}) - \\theta$\n-   Variance and Standard Error: $SE(\\hat{\\theta}) = \\sqrt{\\text{Var}(\\hat{\\theta})}$\n-   Mean Squared Error: $\\text{MSE} = \\text{Var} + \\text{Bias}^2$\n\n. . .\n\n**Today's Goals:**\n\n::: incremental\n1.  Understand the chi-squared distribution and its connection to normal samples\n2.  Learn the t distribution and when it arises\n3.  Understand the F distribution as a ratio of chi-squared variables\n4.  Use R to work with these distributions\n:::\n\n## Motivating Example: Variability in Drug Response\n\nA pharmaceutical company is studying individual variation in drug metabolism.\n\n**Question:** How do we characterize the *variability* in patient responses, not just the average?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Liver enzyme levels (U/L) from 15 patients\nenzyme_levels <- c(42, 38, 51, 45, 40, 55, 48, 37, 44, 52, \n                   46, 43, 49, 41, 47)\n\ntibble(\n  Statistic = c(\"Sample Mean\", \"Sample SD\", \"Sample Variance\"),\n  Value = c(mean(enzyme_levels), sd(enzyme_levels), var(enzyme_levels))\n) |> mutate(Value = round(Value, 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 2\n  Statistic       Value\n  <chr>           <dbl>\n1 Sample Mean     45.2 \n2 Sample SD        5.23\n3 Sample Variance 27.3 \n```\n\n\n:::\n:::\n\n\n. . .\n\n**Key insight:** To make inferences about variability, we need to understand the *sampling distribution of the variance*.\n\n# The Chi-Squared Distribution {background-color=\"#2c3e50\"}\n\n## The Chi-Squared Distribution: Definition (Devore 6.3)\n\n::: callout-important\n## Definition\n\nFor a positive integer $\\nu$, let $Z_1, \\ldots, Z_\\nu$ be **independent** standard normal random variables. The **chi-squared distribution with $\\nu$ degrees of freedom** is the distribution of:\n\n$$\\chi^2_\\nu = Z_1^2 + Z_2^2 + \\cdots + Z_\\nu^2$$\n:::\n\n. . .\n\n**Key Properties:**\n\n| Property | Value |\n|----------|-------|\n| Mean | $E(\\chi^2_\\nu) = \\nu$ |\n| Variance | $\\text{Var}(\\chi^2_\\nu) = 2\\nu$ |\n| Shape | Right-skewed, becomes more symmetric as $\\nu$ increases |\n\n## Visualizing the Chi-Squared Distribution\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(x = seq(0.01, 30, length.out = 500)) |>\n  crossing(df = c(1, 2, 5, 10)) |>\n  mutate(\n    density = dchisq(x, df),\n    df = factor(df, labels = paste(c(1, 2, 5, 10), \"df\"))\n  ) |>\n  ggplot(aes(x = x, y = density, color = df)) +\n  geom_line(linewidth = 1.2) +\n  labs(\n    title = \"Chi-Squared Distribution for Various Degrees of Freedom\",\n    x = \"x\", y = \"Density\", color = \"Degrees of\\nFreedom\"\n  ) +\n  scale_color_brewer(palette = \"Set1\") +\n  theme(legend.position = c(0.8, 0.7))\n```\n\n::: {.cell-output-display}\n![](03_Test_Distributions_files/figure-revealjs/unnamed-chunk-2-1.png){width=960}\n:::\n:::\n\n\n## Chi-Squared and the Gamma Distribution\n\n::: callout-note\n## Connection to Gamma\n\nThe chi-squared distribution with $\\nu$ df is equivalent to a **Gamma distribution** with:\n\n-   Shape parameter $\\alpha = \\nu/2$\n-   Scale parameter $\\beta = 2$\n\nThis means $\\chi^2_\\nu = \\text{Gamma}(\\nu/2, 2)$.\n:::\n\n. . .\n\n**Why this matters:**\n\n-   We can use gamma distribution properties\n-   Chi-squared variables are **additive**: if $X_1 \\sim \\chi^2_{\\nu_1}$ and $X_2 \\sim \\chi^2_{\\nu_2}$ are independent, then $X_1 + X_2 \\sim \\chi^2_{\\nu_1 + \\nu_2}$\n\n## Medical Example: LED Lamp Lifecycle (Devore Example 6.12)\n\nFrom Devore, Berk & Carlton: The lifecycle (in thousands of hours) of certain LED medical lamps follows a $\\chi^2_8$ distribution.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Parameters for χ²₈ distribution\ndf_lamps <- 8\nmean_life <- df_lamps  # E(X) = ν\nsd_life <- sqrt(2 * df_lamps)  # SD(X) = √(2ν)\n\ncat(\"Mean lifecycle:\", mean_life, \"thousand hours\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMean lifecycle: 8 thousand hours\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"SD of lifecycle:\", round(sd_life, 2), \"thousand hours\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSD of lifecycle: 4 thousand hours\n```\n\n\n:::\n\n```{.r .cell-code}\n# Probability lamp lasts between 6 and 10 thousand hours\nprob_range <- pchisq(10, df_lamps) - pchisq(6, df_lamps)\ncat(\"P(6 ≤ X ≤ 10):\", round(prob_range, 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nP(6 ≤ X ≤ 10): 0.382\n```\n\n\n:::\n:::\n\n\n## Your Turn: Chi-Squared Calculations\n\n**Exercise:** For the LED lamp example ($\\chi^2_8$ distribution):\n\n1.  What is the probability a lamp lasts more than 15,000 hours?\n2.  Find the 90th percentile of the lifecycle distribution.\n3.  What is the \"middle 95%\" range of lifecycles?\n\n. . .\n\n**Solutions:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 1. P(X > 15)\nprob_over_15 <- 1 - pchisq(15, 8)\ncat(\"P(X > 15):\", round(prob_over_15, 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nP(X > 15): 0.0591 \n```\n\n\n:::\n\n```{.r .cell-code}\n# 2. 90th percentile\np90 <- qchisq(0.90, 8)\ncat(\"90th percentile:\", round(p90, 2), \"thousand hours\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n90th percentile: 13.36 thousand hours\n```\n\n\n:::\n\n```{.r .cell-code}\n# 3. Middle 95%\nlower <- qchisq(0.025, 8)\nupper <- qchisq(0.975, 8)\ncat(\"Middle 95%: [\", round(lower, 2), \",\", round(upper, 2), \"]\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMiddle 95%: [ 2.18 , 17.53 ]\n```\n\n\n:::\n:::\n\n\n## The Critical Connection: Sample Variance (Devore 6.4)\n\n::: callout-important\n## Fundamental Theorem\n\nIf $X_1, X_2, \\ldots, X_n$ is a random sample from a **normal distribution** $N(\\mu, \\sigma^2)$, then:\n\n$$\\frac{(n-1)S^2}{\\sigma^2} \\sim \\chi^2_{n-1}$$\n\nwhere $S^2 = \\frac{1}{n-1}\\sum_{i=1}^n (X_i - \\bar{X})^2$ is the sample variance.\n:::\n\n. . .\n\n**This is crucial because:**\n\n-   It tells us exactly how sample variances behave\n-   The $(n-1)$ degrees of freedom arise because we \"use up\" one df estimating $\\mu$ with $\\bar{X}$\n-   This forms the basis for inference about population variances\n\n## Simulation: Verifying the Chi-Squared Result\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 10\nsigma_sq <- 100  # True variance\nn_sims <- 10000\n\n# Simulate the transformation (n-1)S²/σ²\nchi_sq_sim <- tibble(sim = 1:n_sims) |>\n  mutate(\n    sample_data = map(sim, ~rnorm(n, mean = 50, sd = sqrt(sigma_sq))),\n    s_squared = map_dbl(sample_data, var),\n    chi_sq_stat = (n - 1) * s_squared / sigma_sq\n  )\n\n# Compare to theoretical χ²(n-1) distribution\nchi_sq_sim |>\n  ggplot(aes(x = chi_sq_stat)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 50, \n                 fill = \"steelblue\", alpha = 0.7, color = \"white\") +\n  stat_function(fun = dchisq, args = list(df = n - 1), \n                color = \"red\", linewidth = 1.5) +\n  labs(title = \"Simulated (n-1)S²/σ² vs. Theoretical χ²(n-1)\",\n       subtitle = paste(\"n =\", n, \", σ² =\", sigma_sq, \",\", n_sims, \"simulations\"),\n       x = \"Value\", y = \"Density\") +\n  annotate(\"text\", x = 20, y = 0.08, label = \"Red: χ²(9) density\", \n           color = \"red\", size = 5)\n```\n\n::: {.cell-output-display}\n![](03_Test_Distributions_files/figure-revealjs/unnamed-chunk-5-1.png){width=960}\n:::\n:::\n\n\n# The t Distribution {background-color=\"#27ae60\"}\n\n## The t Distribution: Definition (Devore 6.3)\n\n::: callout-important\n## Definition\n\nLet $Z$ be a standard normal random variable and $Y$ be a $\\chi^2_\\nu$ random variable, **independent** of $Z$. Then the **t distribution with $\\nu$ degrees of freedom** is the distribution of:\n\n$$T = \\frac{Z}{\\sqrt{Y/\\nu}}$$\n:::\n\n. . .\n\n**Key Properties:**\n\n| Property | Value |\n|----------|-------|\n| Mean | $E(T) = 0$ (for $\\nu > 1$) |\n| Variance | $\\text{Var}(T) = \\frac{\\nu}{\\nu - 2}$ (for $\\nu > 2$) |\n| Shape | Symmetric, bell-shaped, heavier tails than normal |\n\n## Comparing t and Normal Distributions\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(x = seq(-4, 4, length.out = 500)) |>\n  crossing(df = c(1, 2, 5, 30)) |>\n  mutate(\n    t_density = dt(x, df),\n    df = factor(df, labels = paste(c(1, 2, 5, 30), \"df\"))\n  ) |>\n  ggplot(aes(x = x, y = t_density, color = df)) +\n  geom_line(linewidth = 1.2) +\n  stat_function(fun = dnorm, color = \"black\", linewidth = 1.5, linetype = \"dashed\") +\n  labs(\n    title = \"t Distribution Compared to Standard Normal\",\n    subtitle = \"Dashed black line: Standard Normal (Z)\",\n    x = \"x\", y = \"Density\", color = \"t Distribution\"\n  ) +\n  scale_color_brewer(palette = \"Set1\") +\n  annotate(\"text\", x = 2.5, y = 0.35, label = \"As df → ∞, t → Z\", size = 5)\n```\n\n::: {.cell-output-display}\n![](03_Test_Distributions_files/figure-revealjs/unnamed-chunk-6-1.png){width=960}\n:::\n:::\n\n\n## Gosset's Theorem: The Practical t Distribution (Devore 6.4)\n\n::: callout-important\n## Gosset's Theorem (1908)\n\nIf $X_1, X_2, \\ldots, X_n$ is a random sample from a **normal distribution** $N(\\mu, \\sigma)$, then:\n\n$$T = \\frac{\\bar{X} - \\mu}{S/\\sqrt{n}} \\sim t_{n-1}$$\n\nThis is the **one-sample t statistic**.\n:::\n\n. . .\n\n**Why this matters:**\n\n-   In practice, we rarely know $\\sigma$\n-   Replacing $\\sigma$ with $S$ introduces additional uncertainty\n-   The t distribution accounts for this extra variability\n\n## Comparing Z and T Statistics\n\nTwo related quantities for inference about $\\mu$:\n\n$$Z = \\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}} \\quad \\text{vs.} \\quad T = \\frac{\\bar{X} - \\mu}{S/\\sqrt{n}}$$\n\n. . .\n\n| When to use | Distribution | Requires knowing |\n|-------------|--------------|------------------|\n| Z statistic | Standard Normal | Population $\\sigma$ |\n| T statistic | $t_{n-1}$ | Only sample data |\n\n. . .\n\n**Key insight:** The t distribution has heavier tails because $S$ varies from sample to sample.\n\n## Medical Example: Blood Pressure Reduction\n\nA clinical trial measures systolic blood pressure reduction (mmHg) in 12 patients taking a new medication.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Blood pressure reductions from 12 patients\nbp_reduction <- c(8.2, 12.5, 9.1, 7.8, 11.3, 10.4, \n                  9.7, 13.1, 8.9, 10.8, 12.0, 9.5)\n\nn <- length(bp_reduction)\nx_bar <- mean(bp_reduction)\ns <- sd(bp_reduction)\n\n# T statistic (testing if true mean reduction is μ₀ = 8)\nmu_0 <- 8\nt_stat <- (x_bar - mu_0) / (s / sqrt(n))\n\ncat(\"Sample mean:\", round(x_bar, 2), \"mmHg\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSample mean: 10.28 mmHg\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Sample SD:\", round(s, 2), \"mmHg\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSample SD: 1.7 mmHg\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"T statistic (vs μ₀ = 8):\", round(t_stat, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nT statistic (vs μ₀ = 8): 4.629 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"P-value:\", round(2 * pt(-abs(t_stat), df = n - 1), 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nP-value: 7e-04\n```\n\n\n:::\n:::\n\n\n## Your Turn: t Distribution Application\n\n**Exercise:** A hospital measures recovery times (days) for 8 post-surgery patients:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrecovery_times <- c(4.2, 5.8, 3.9, 5.1, 4.7, 6.2, 4.4, 5.5)\n```\n:::\n\n\n1.  Calculate the sample mean and sample standard deviation\n2.  Compute the t statistic for testing whether the true mean is 5 days\n3.  Find the critical value $t_{0.025, 7}$ (for a two-sided 95% test)\n\n. . .\n\n**Solutions:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- length(recovery_times)\nx_bar <- mean(recovery_times)\ns <- sd(recovery_times)\nt_stat <- (x_bar - 5) / (s / sqrt(n))\nt_crit <- qt(0.975, df = n - 1)\n\ncat(\"Mean:\", round(x_bar, 3), \"SD:\", round(s, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMean: 4.975 SD: 0.814 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"T statistic:\", round(t_stat, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nT statistic: -0.087 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Critical value t₀.₀₂₅,₇:\", round(t_crit, 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCritical value t₀.₀₂₅,₇: 2.365\n```\n\n\n:::\n:::\n\n\n# The F Distribution {background-color=\"#8e44ad\"}\n\n## The F Distribution: Definition (Devore 6.3)\n\n::: callout-important\n## Definition\n\nLet $Y_1 \\sim \\chi^2_{\\nu_1}$ and $Y_2 \\sim \\chi^2_{\\nu_2}$ be **independent** chi-squared random variables. The **F distribution** with $\\nu_1$ numerator df and $\\nu_2$ denominator df is:\n\n$$F = \\frac{Y_1/\\nu_1}{Y_2/\\nu_2} \\sim F_{\\nu_1, \\nu_2}$$\n:::\n\n. . .\n\n**Key Properties:**\n\n-   $E(F) = \\frac{\\nu_2}{\\nu_2 - 2}$ (for $\\nu_2 > 2$)\n-   Right-skewed, positive values only\n-   Used for comparing variances and in ANOVA\n\n## Visualizing the F Distribution\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexpand_grid(\n  df1 = c(1, 5, 10),\n  df2 = c(5, 20)\n) |>\n  mutate(df_label = paste0(\"(\", df1, \", \", df2, \")\")) |>\n  crossing(x = seq(0.01, 5, length.out = 300)) |>\n  mutate(density = df(x, df1, df2)) |>\n  ggplot(aes(x = x, y = density, color = df_label)) +\n  geom_line(linewidth = 1.2) +\n  labs(\n    title = \"F Distribution for Various Degrees of Freedom\",\n    subtitle = \"Label format: (numerator df, denominator df)\",\n    x = \"x\", y = \"Density\", color = \"Degrees of\\nFreedom\"\n  ) +\n  scale_color_brewer(palette = \"Set2\") +\n  coord_cartesian(ylim = c(0, 1))\n```\n\n::: {.cell-output-display}\n![](03_Test_Distributions_files/figure-revealjs/unnamed-chunk-10-1.png){width=960}\n:::\n:::\n\n\n## The F Statistic for Comparing Variances\n\n::: callout-note\n## Comparing Two Population Variances\n\nIf we have independent samples from two normal populations:\n\n-   Sample 1: $n_1$ observations, sample variance $S_1^2$\n-   Sample 2: $n_2$ observations, sample variance $S_2^2$\n\nThen under $H_0: \\sigma_1^2 = \\sigma_2^2$:\n\n$$F = \\frac{S_1^2}{S_2^2} \\sim F_{n_1-1, n_2-1}$$\n:::\n\n. . .\n\nThis will be essential for ANOVA and comparing treatment groups.\n\n## Medical Example: Treatment Variability Comparison\n\nTwo pain medications are being compared. Is the *variability* in pain relief different?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Pain relief scores (higher = more relief)\ndrug_A <- c(6.2, 7.1, 5.8, 6.9, 7.3, 6.5, 7.0, 6.4, 6.8, 7.2)\ndrug_B <- c(5.5, 8.2, 4.9, 7.8, 6.1, 8.5, 5.2, 7.4, 6.3, 8.0)\n\n# Compare variances\nvar_A <- var(drug_A)\nvar_B <- var(drug_B)\nf_stat <- var_B / var_A  # Put larger variance in numerator\n\ncat(\"Var(Drug A):\", round(var_A, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nVar(Drug A): 0.233 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Var(Drug B):\", round(var_B, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nVar(Drug B): 1.805 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"F statistic:\", round(f_stat, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nF statistic: 7.752 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"P-value (two-sided):\", round(2 * pf(f_stat, 9, 9, lower.tail = FALSE), 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nP-value (two-sided): 0.0054\n```\n\n\n:::\n:::\n\n\n. . .\n\nDrug B shows significantly more variable responses!\n\n## Relationships Between Distributions\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](03_Test_Distributions_files/figure-revealjs/unnamed-chunk-12-1.png){width=1152}\n:::\n:::\n\n\n## R Functions for These Distributions\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"|1-4|6-9|11-14\"}\n# Chi-squared distribution\npchisq(10, df = 5)       # P(χ²₅ ≤ 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9247648\n```\n\n\n:::\n\n```{.r .cell-code  code-line-numbers=\"|1-4|6-9|11-14\"}\nqchisq(0.95, df = 5)     # 95th percentile\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 11.0705\n```\n\n\n:::\n\n```{.r .cell-code  code-line-numbers=\"|1-4|6-9|11-14\"}\nrchisq(5, df = 5)        # 5 random values\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.913437 9.932238 2.024977 3.125219 2.908850\n```\n\n\n:::\n\n```{.r .cell-code  code-line-numbers=\"|1-4|6-9|11-14\"}\n# t distribution  \npt(2, df = 10)           # P(t₁₀ ≤ 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.963306\n```\n\n\n:::\n\n```{.r .cell-code  code-line-numbers=\"|1-4|6-9|11-14\"}\nqt(0.975, df = 10)       # 97.5th percentile (for 95% CI)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.228139\n```\n\n\n:::\n\n```{.r .cell-code  code-line-numbers=\"|1-4|6-9|11-14\"}\nrt(5, df = 10)           # 5 random values\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -0.9367386  0.8773822  1.2727263 -0.4356913  1.3334783\n```\n\n\n:::\n\n```{.r .cell-code  code-line-numbers=\"|1-4|6-9|11-14\"}\n# F distribution\npf(3, df1 = 5, df2 = 20) # P(F₅,₂₀ ≤ 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9647987\n```\n\n\n:::\n\n```{.r .cell-code  code-line-numbers=\"|1-4|6-9|11-14\"}\nqf(0.95, df1 = 5, df2 = 20)  # 95th percentile\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.71089\n```\n\n\n:::\n\n```{.r .cell-code  code-line-numbers=\"|1-4|6-9|11-14\"}\nrf(5, df1 = 5, df2 = 20)     # 5 random values\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9798762 0.9294727 0.5288312 0.2722935 1.1200395\n```\n\n\n:::\n:::\n\n\n# Summary and Looking Ahead {background-color=\"#3498db\"}\n\n## Lesson 3 Summary\n\n**The Chi-Squared Distribution:**\n\n-   Sum of squared standard normals: $\\chi^2_\\nu = \\sum Z_i^2$\n-   Mean = $\\nu$, Variance = $2\\nu$\n-   $(n-1)S^2/\\sigma^2 \\sim \\chi^2_{n-1}$ for normal samples\n\n**The t Distribution:**\n\n-   Ratio: $T = Z/\\sqrt{\\chi^2_\\nu/\\nu}$\n-   Arises when we estimate $\\sigma$ with $S$\n-   $(\\bar{X} - \\mu)/(S/\\sqrt{n}) \\sim t_{n-1}$\n\n**The F Distribution:**\n\n-   Ratio of two scaled chi-squared: $F = (Y_1/\\nu_1)/(Y_2/\\nu_2)$\n-   Used for comparing variances\n-   Key relationship: $t_\\nu^2 = F_{1,\\nu}$\n\n## Lesson 3 Practice Problems\n\n1.  A sample of 16 observations from a normal distribution has sample variance $s^2 = 25$. If $\\sigma^2 = 20$, what is $P\\left(\\frac{(n-1)S^2}{\\sigma^2} \\leq 18.75\\right)$?\n\n2.  Find the critical values $t_{0.05, 15}$ and $t_{0.025, 15}$.\n\n3.  Two independent samples from normal populations have $s_1^2 = 45$ (n=10) and $s_2^2 = 20$ (n=8). Calculate the F statistic for testing equal variances.\n\n4.  Verify that $t_{0.05,10}^2 = F_{0.10, 1, 10}$ using R.\n\n## Next Lesson Preview\n\n**Lesson 4: Normal Sample Statistics and MVUE**\n\n-   Key results for statistics from normal samples\n-   Independence of $\\bar{X}$ and $S^2$\n-   Confidence intervals for variance\n-   Minimum variance unbiased estimators (MVUE)\n-   Consistency of estimators\n\n## References\n\n::: nonincremental\n-   Devore, Berk, and Carlton. *Modern Mathematical Statistics with Applications* (Springer). Chapters 6.3, 6.4\n-   Chihara and Hesterberg. *Mathematical Statistics with Resampling and R* (Wiley). Appendix B.\n:::\n\n## Questions? {.center background-color=\"#3498db\"}\n\nThank you!\n",
    "supporting": [
      "03_Test_Distributions_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
{
  "hash": "184d44a6d0c3718350406ec29b4ba322",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"BSTA 551: Statistical Inference\"\nsubtitle: \"Lesson 1: Introduction to Statistical Inference; Statistics\"\nauthor: \"Jessica Minnier\"\ntitle-slide-attributes:\n    data-background-color: \"#006a4e\"\ndate: \"2026-01-05\"\nformat: \n  revealjs:\n    theme: \"../simple_NW.scss\"\n    chalkboard: true\n    scrollable: true\n    slide-number: true\n    show-slide-number: all\n    width: 1955\n    height: 1100\n    footer: Lesson 1\n    html-math-method: mathjax\n    highlight-style: atom-one\nexecute:\n  echo: true\n  warning: false\n  message: false\n---\n\n\n\n# Lesson 1: Introduction to Statistical Inference\n\n## Welcome to Statistical Inference! {.center}\n\n**Course Focus:** How do we learn about populations from samples?\n\n::: incremental\n-   **Point Estimation** (Weeks 1-4): What's our best guess for a parameter?\n-   **Confidence Intervals** (Weeks 5-6): What's a plausible range?\n-   **Hypothesis Testing** (Weeks 7-9): Can we make decisions from data?\n-   **Two-Sample Methods** (Week 10): Comparing groups\n:::\n\n## Today's Goals\n\n::: incremental\n1.  Understand the fundamental problem of statistical inference\n2.  Distinguish between populations and samples, parameters and statistics\n3.  Use R to simulate sampling distributions\n4.  Review key properties of expected value and variance\n:::\n\n## Motivating Example: Clinical Trial\n\nA pharmaceutical company is testing a new blood pressure medication.\n\n**The Question:** What is the true average reduction in systolic blood pressure?\n\n. . .\n\n**What we have:** Data from 25 patients in a trial\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Actual blood pressure reductions (mmHg) from 25 patients\nbp_reductions <- c(12, 8, 15, 10, 7, 14, 11, 9, 13, 16,\n                   8, 12, 10, 14, 11, 9, 15, 13, 7, 12,\n                   10, 8, 14, 11, 13)\n\n# What's our estimate of the true mean reduction?\nmean(bp_reductions)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 11.28\n```\n\n\n:::\n:::\n\n\n. . .\n\nThis \"estimate\" is the observed value of the (random variable) sample mean statistic: $\\bar{X} = \\sum_{i=1}^n X_i$\n\n. . .\n\nBut how **reliable** is this estimate? Would we get the same answer with different patients?\n\n# Statistical Inference: The Big Picture {background-color=\"#2c3e50\"}\n\n## The Big Picture: Population vs Sample\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](01_Intro_Inference_files/figure-revealjs/unnamed-chunk-2-1.png){width=960}\n:::\n:::\n\n\n::: lob\n**Key insight:** We use sample data to make inferences about population parameters.\n:::\n\n## Key Terminology (Devore 6.1)\n\n::: callout-important\n## Definitions\n\n-   **Population:** The entire collection of individuals or measurements of interest\n-   **Sample:** A subset of the population that we actually observe\n-   **Parameter:** A numerical characteristic of the population (e.g., $\\mu$, $\\sigma$, $p$)\n-   **Statistic:** A numerical characteristic computed from sample data (e.g., $\\bar{X}$, $S$, $\\hat{p}$)\n:::\n\n. . .\n\n**The Central Challenge:** Parameters are fixed but unknown; statistics are known but random.\n\n## Parameters vs. Statistics\n\n| Concept   | Population (Parameter) | Sample (Statistic)        |\n|-----------|------------------------|---------------------------|\n| Mean      | $\\mu$                  | $\\bar{X} = \\frac{1}{n}\\sum X_i$ |\n| Variance  | $\\sigma^2$             | $S^2 = \\frac{1}{n-1}\\sum(X_i - \\bar{X})^2$ |\n| Proportion| $p$                    | $\\hat{p} = X/n$           |\n| Maximum   | $\\theta$ (upper bound) | $\\max(X_1, \\ldots, X_n)$  |\n\n. . .\n\n**Key Point:** Parameters use Greek letters; statistics use Roman letters or \"hats.\"\n\n## The Sampling Process (Devore 6.2)\n\n**Random Sampling Assumptions:**\n\n1. Each observation $X_i$ is a random variable\n2. The $X_i$ are **independent** of each other\n3. Each $X_i$ has the **same distribution** (identically distributed)\n\n. . .\n\nTogether: $X_1, X_2, \\ldots, X_n$ are **i.i.d.** (independent and identically distributed)\n\n. . .\n\n::: callout-note\n## Why This Matters\nThe i.i.d. assumption allows us to derive the properties of statistics mathematically.\n:::\n\n# Review: Expected Value and Variance {background-color=\"#27ae60\"}\n\n## Review: Expected Value\n\nThe **expected value** $E(X)$ is the long-run average of a random variable.\n\n::: callout-note\n## Key Properties We'll Use Today\n\n1.  $E(c) = c$ for any constant $c$\n2.  $E(cX) = c \\cdot E(X)$\n3.  $E(X + Y) = E(X) + E(Y)$\n4.  $E\\left(\\sum_{i=1}^n X_i\\right) = \\sum_{i=1}^n E(X_i)$\n:::\n\n## Worked Example: Expected Value of Sample Mean\n\n**Problem:** If $X_1, X_2, \\ldots, X_n$ are *iid* observations from a population with mean $\\mu$, what is $E(\\bar{X})$?\n\n. . .\n\n**Solution:** Let's work through this step by step.\n\n$$E(\\bar{X}) = E\\left(\\frac{1}{n}\\sum_{i=1}^n X_i\\right)$$\n\n. . .\n\n$$= \\frac{1}{n} E\\left(\\sum_{i=1}^n X_i\\right) \\quad \\text{(Property 2: constants come out)}$$\n\n. . .\n\n$$= \\frac{1}{n} \\sum_{i=1}^n E(X_i) \\quad \\text{(Property 4: sum of expectations)}$$\n\n. . .\n\n$$= \\frac{1}{n} \\cdot n\\mu = \\mu \\quad \\text{(Each } E(X_i) = \\mu \\text{)}$$\n\n## Your Turn: Calculate Expected Value\n\n**Exercise:** A hospital measures the recovery time (in days) for patients after surgery. Let $X_1, X_2, X_3$ be recovery times for 3 patients. The population mean recovery time is $\\mu = 5$ days.\n\n**Questions:**\n\n1.  What is $E(X_1)$?\n2.  What is $E(X_1 + X_2 + X_3)$?\n3.  What is $E(\\bar{X})$ where $\\bar{X} = \\frac{X_1 + X_2 + X_3}{3}$?\n\n. . .\n\n**Answers:**\n\n1.  $E(X_1) = \\mu = 5$ days\n2.  $E(X_1 + X_2 + X_3) = 5 + 5 + 5 = 15$ days\n3.  $E(\\bar{X}) = \\frac{15}{3} = 5$ days\n\n## Review: Variance\n\n**Variance** measures the spread of a distribution: $\\text{Var}(X) = E[(X - \\mu)^2]$\n\n::: callout-note\n## Key Properties We'll Use Today\n\n1.  $\\text{Var}(c) = 0$ for any constant\n2.  $\\text{Var}(cX) = c^2 \\cdot \\text{Var}(X)$\n3.  If $X$ and $Y$ are **independent**: $\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y)$\n:::\n\n## Worked Example: Variance of Sample Mean\n\n**Problem:** If $X_1, \\ldots, X_n$ are independent observations with variance $\\sigma^2$, what is $\\text{Var}(\\bar{X})$?\n\n. . .\n\n$$\\text{Var}(\\bar{X}) = \\text{Var}\\left(\\frac{1}{n}\\sum_{i=1}^n X_i\\right)$$\n\n. . .\n\n$$= \\frac{1}{n^2} \\text{Var}\\left(\\sum_{i=1}^n X_i\\right) \\quad \\text{(Property 2)}$$\n\n. . .\n\n$$= \\frac{1}{n^2} \\sum_{i=1}^n \\text{Var}(X_i) \\quad \\text{(Property 3: independence)}$$\n\n. . .\n\n$$= \\frac{1}{n^2} \\cdot n\\sigma^2 = \\frac{\\sigma^2}{n}$$\n\n::: callout-important\n## Key Result\n\nThe variance of $\\bar{X}$ **decreases** as sample size $n$ increases!\n:::\n\n# The Sampling Distribution {background-color=\"#8e44ad\"}\n\n## What is a Sampling Distribution?\n\nDifferent samples give different values of a statistic. The **sampling distribution** describes this variability.\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n| Sample| Sample Mean (x̄)|\n|------:|---------------:|\n|      1|            9.90|\n|      2|           10.31|\n|      3|           10.03|\n|      4|           10.85|\n|      5|            9.17|\n|      6|            9.31|\n\n\n:::\n:::\n\n\n. . .\n\nEach sample gives a **different estimate**, but they cluster around the true value $\\mu = 10$.\n\n## Simulation: Building a Sampling Distribution\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\n# Parameters\ntrue_effect <- 10  # True mean BP reduction (mmHg)\ntrue_sd <- 4       # Standard deviation\nn_patients <- 25   # Patients per trial\nn_trials <- 5000   # Number of simulated trials\n\n# Simulate many clinical trials\nsampling_distribution <- tibble(trial = 1:n_trials) |> \n  mutate(\n    sample_mean = map_dbl(trial, \\(t) {\n      patients <- rnorm(n_patients, true_effect, true_sd)\n      mean(patients)\n    })\n  )\n\n# Visualize\nsampling_distribution |> \n  ggplot(aes(x = sample_mean)) +\n  geom_histogram(bins = 50, fill = \"steelblue\", alpha = 0.7, color = \"white\") +\n  geom_vline(xintercept = true_effect, color = \"red\", linewidth = 1.5) +\n  labs(title = \"Sampling Distribution of the Sample Mean\",\n       subtitle = str_glue(\"True μ = {true_effect}, n = {n_patients}, {n_trials} simulated trials\"),\n       x = \"Sample Mean (estimated BP reduction)\", y = \"Frequency\")\n```\n\n::: {.cell-output-display}\n![](01_Intro_Inference_files/figure-revealjs/unnamed-chunk-4-1.png){width=960}\n:::\n:::\n\n\n## Simulation: Seeing Variance Decrease\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"|1-3|5-12|14-15\"}\n# Population parameters\ntrue_mean <- 120  # True mean systolic BP\ntrue_sd <- 15     # Population standard deviation\n\n# Simulate sample means for different sample sizes\nsimulation_data <- tibble(n = c(5, 25, 100)) |> \n  cross_join(tibble(sim = 1:2000)) |> \n  mutate(\n    sample_mean = map2_dbl(n, sim, \\(size, s) {\n      mean(rnorm(size, true_mean, true_sd))\n    })\n  )\n\n# Calculate observed standard deviation for each sample size\nsimulation_data |> \n  group_by(n) |> \n  summarize(\n    observed_sd = sd(sample_mean),\n    theoretical_sd = true_sd / sqrt(first(n))\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 3\n      n observed_sd theoretical_sd\n  <dbl>       <dbl>          <dbl>\n1     5        6.64           6.71\n2    25        3.00           3   \n3   100        1.50           1.5 \n```\n\n\n:::\n:::\n\n\n## Visualizing the Effect of Sample Size\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](01_Intro_Inference_files/figure-revealjs/unnamed-chunk-6-1.png){width=960}\n:::\n:::\n\n\n# Understanding Optimization {background-color=\"#2c3e50\"}\n\n## Why Optimization Matters in Statistics\n\nMany statistical methods require finding the \"best\" value of a parameter.\n\n**Examples:**\n\n-   **Maximum Likelihood:** Find the parameter value that makes the observed data most probable\n-   **Least Squares:** Find the parameter value that minimizes prediction errors\n-   **Minimum Variance:** Find the estimator with the smallest spread\n\n. . .\n\n**The Problem:** How do we find maximums and minimums numerically?\n\n## Optimization: The Graphical Intuition\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](01_Intro_Inference_files/figure-revealjs/unnamed-chunk-7-1.png){width=960}\n:::\n:::\n\n\n**The maximum occurs where the function reaches its peak.**\n\n## Concrete Example: Finding the Best Estimate\n\n**Scenario:** In a clinical trial, 7 out of 10 patients respond to treatment. What's the best estimate of the true response rate $p$?\n\n. . .\n\n**Approach:** Find the value of $p$ that makes observing \"7 out of 10\" most likely.\n\nThe probability of observing exactly 7 successes in 10 trials is: $$P(X = 7) = \\binom{10}{7} p^7 (1-p)^3$$\n\n. . .\n\n**Question:** For what value of $p$ is this probability largest?\n\n## Grid Search: A Simple Numerical Approach\n\n**Idea:** Try many values and see which gives the largest result.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Try different values of p\ngrid_search <- tibble(p = seq(0.01, 0.99, by = 0.01)) |> \n  mutate(\n    likelihood = dbinom(7, size = 10, prob = p)\n  )\n\n# Find the maximum\ngrid_search |> \n  slice_max(likelihood, n = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 2\n      p likelihood\n  <dbl>      <dbl>\n1   0.7      0.267\n```\n\n\n:::\n:::\n\n\n. . .\n\nThe maximum likelihood estimate is $\\hat{p} = 0.70 = \\frac{7}{10}$. This makes intuitive sense!\n\n## Using R's Optimizer\n\nR has built-in functions to find maximums and minimums more precisely:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define the likelihood function\nlikelihood_function <- function(p) {\n  dbinom(7, size = 10, prob = p)\n}\n\n# Use optimize() to find the maximum\n# Note: optimize finds MINIMUM by default, so we negate for maximum\nresult <- optimize(\n  f = function(p) -likelihood_function(p),  # Negative to find max\n  interval = c(0, 1)                         # Search between 0 and 1\n)\n\n# The maximum occurs at:\ncat(\"Maximum likelihood estimate: p =\", result$minimum)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMaximum likelihood estimate: p = 0.6999843\n```\n\n\n:::\n:::\n\n\n## How Numerical Optimization Works\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](01_Intro_Inference_files/figure-revealjs/unnamed-chunk-10-1.png){width=960}\n:::\n:::\n\n\n**Key idea:** The algorithm evaluates the function at different points and iteratively narrows in on the maximum.\n\n## Your Turn: Numerical Optimization\n\n**Exercise:** A diagnostic test correctly identifies a disease in 18 out of 25 patients who have it. Find the maximum likelihood estimate for the test's sensitivity $p$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fill in the blanks:\nlikelihood_fn <- function(p) {\n  dbinom(___, size = ___, prob = p)  # What goes here?\n}\n\nresult <- optimize(\n  f = function(p) -likelihood_fn(p),\n  interval = c(0, 1)\n)\n\nresult$minimum  # This is the MLE\n```\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Solution:\nlikelihood_fn <- function(p) {\n  dbinom(18, size = 25, prob = p)\n}\n\nresult <- optimize(f = function(p) -likelihood_fn(p), interval = c(0, 1))\ncat(\"MLE of sensitivity:\", result$minimum)  # Should be 18/25 = 0.72\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMLE of sensitivity: 0.7200103\n```\n\n\n:::\n:::\n\n\n## When Optimization Gets Harder\n\nSometimes we need to optimize over multiple parameters or complex functions:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Example: Finding mean and SD that best fit data\npatient_data <- c(120, 135, 128, 142, 131, 125, 138, 129, 133, 127)\n\n# Negative log-likelihood for normal distribution\nneg_log_lik <- function(params) {\n  mu <- params[1]\n  sigma <- params[2]\n  if (sigma <= 0) return(Inf)  # sigma must be positive\n  -sum(dnorm(patient_data, mean = mu, sd = sigma, log = TRUE))\n}\n\n# Use optim() for multiple parameters\nresult <- optim(par = c(130, 10), fn = neg_log_lik)\ncat(\"MLE for mean:\", round(result$par[1], 2), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMLE for mean: 130.8 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"MLE for SD:\", round(result$par[2], 2), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMLE for SD: 6.13 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Compare to sample mean:\", round(mean(patient_data), 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCompare to sample mean: 130.8\n```\n\n\n:::\n:::\n\n\n# Summary and Looking Ahead {background-color=\"#3498db\"}\n\n## Lesson 1 Summary\n\n**Key Concepts:**\n\n1.  **Statistical Inference:** Using sample data to learn about population parameters\n\n2.  **Parameters vs. Statistics:**\n    -   Parameters ($\\mu$, $\\sigma$, $p$): Fixed but unknown population values\n    -   Statistics ($\\bar{X}$, $S$, $\\hat{p}$): Calculated from sample data\n\n3.  **Sampling Distribution:** The distribution of a statistic across many samples\n    -   $E(\\bar{X}) = \\mu$ (centered at population mean)\n    -   $\\text{Var}(\\bar{X}) = \\sigma^2/n$ (precision improves with larger $n$)\n\n4.  **Numerical Optimization:** Finding maximum/minimum values\n    -   Grid search: try many values\n    -   `optimize()`: efficient numerical search\n\n## Lesson 1 Practice Problems\n\n1.  Calculate $E(\\bar{X})$ and $\\text{Var}(\\bar{X})$ for a sample of size $n = 16$ from a population with $\\mu = 50$ and $\\sigma = 12$.\n\n2.  Use `optimize()` to find the MLE for $p$ when you observe 23 successes in 40 trials.\n\n3.  A quality control engineer samples 5 items from a production line. If the population mean weight is 100g with SD = 5g, what is the expected value and variance of the sample mean?\n\n4.  Simulate the sampling distribution of the sample median for $n = 30$ observations from a Normal(100, 15) distribution. Compare to the sampling distribution of the sample mean.\n\n## Next Lesson Preview\n\n**Lesson 2: Point Estimation; Bias, Variance, and MSE**\n\n-   What is a point estimator?\n-   Bias: systematic error in estimation\n-   Standard error: precision of estimators\n-   Mean Squared Error: combining bias and variance\n-   The bias-variance tradeoff\n\n## References\n\n::: nonincremental\n-   Devore, Berk, and Carlton. *Modern Mathematical Statistics with Applications* (Springer). Chapters 6.1, 6.2\n-   Chihara and Hesterberg. *Mathematical Statistics with Resampling and R* (Wiley). Chapter 6.\n:::\n\n## Questions? {.center background-color=\"#3498db\"}\n\nThank you!\n\n\n",
    "supporting": [
      "01_Intro_Inference_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}